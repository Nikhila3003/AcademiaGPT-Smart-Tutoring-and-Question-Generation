{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb00f712-8f00-40d3-9dce-d9ccee67c7cc",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad2423f-a3c0-4cb1-a28e-8224b2b52ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import dotenv\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.document import Document\n",
    "from helper_functions import embed_documents\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b480d72-fd02-4e6c-920b-6bac2f1af17d",
   "metadata": {},
   "source": [
    "## Connect Database to Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f52ce58-747d-4f81-abd6-e89673a022c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vecs\n",
    "\n",
    "DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "\n",
    "# create vector store client\n",
    "vx = vecs.create_client(DB_CONNECTION)\n",
    "\n",
    "# create a collection of vectors with 3 dimensions\n",
    "docs = vx.get_or_create_collection(name=\"documents\", dimension=1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a5657-a7e2-4fd8-99fb-6d6cf27c43b8",
   "metadata": {},
   "source": [
    "# Define Chunk SRT Files Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4279e4c-79a3-4d0d-8b37-04b19dc8fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rex = re.compile(r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2}),\\d{3} --> (\\d{2}:\\d{2}:\\d{2}),\\d{3}\")\n",
    "\n",
    "def chunk_srt_files(full_text, chunk_length):\n",
    "\n",
    "    # split on the regex.\n",
    "    splits = rex.split(full_text)[1:]\n",
    "\n",
    "    # combine parts into a list of 3-tuples (start, end, txt)\n",
    "    parts = []\n",
    "    for i in range(0, len(splits), 3):\n",
    "        start_time = splits[i]\n",
    "        end_time = splits[i+1]\n",
    "        content = splits[i+2].strip()\n",
    "        parts.append((start_time, end_time, content))\n",
    "        \n",
    "\n",
    "    # combine multiple parts to get desired chunk length\n",
    "    # will be a list of 3-tuples (start, end, txt)\n",
    "    chunks = []\n",
    "    ix = 0\n",
    "    current_chunk_text = \"\"\n",
    "    for i, part in enumerate(parts):\n",
    "        current_chunk_text = current_chunk_text + \" \" + part[2]\n",
    "        if len(current_chunk_text) > chunk_length or i == len(parts) - 1:\n",
    "            # if we have a long enough chunk OR we are on the last piece of content...\n",
    "            current_chunk = (\n",
    "                parts[ix][0],  # starting timestamp\n",
    "                part[1],\n",
    "                current_chunk_text.strip()\n",
    "            )\n",
    "            chunks.append(current_chunk)\n",
    "            ix = i  # we repeat this chunk one more time for overlap\n",
    "            current_chunk_text =  part[2]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c84c62-ae00-4218-9c75-bad7eb25fe78",
   "metadata": {},
   "source": [
    "# Define Process Video Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "505d6602-e5ca-4be5-a44b-7b2dd1472ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(file_path):\n",
    "    \"\"\"Read the content of a .txt file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c9cbe1-99f6-4075-aeeb-6547e1f387d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: use the srt files directly from whisper, not ones we have modified\n",
    "# you can find these in this folder: jupyteach-ai/videos/transcripts_tiny\n",
    "\n",
    "def process_video(path_to_transcript):\n",
    "    # step 1: read in the srt_content from the file at `path_to_transcript`\n",
    "    srt_content = read_txt_file(path_to_transcript)\n",
    "\n",
    "    # step 2: call `chunk_srt_files` (see above) on the srt_content\n",
    "    chunks = chunk_srt_files(srt_content, 1000)\n",
    "    \n",
    "    # step 3: convert the (start, end, txt) tuples you get back from `chunk_srt_files`\n",
    "    #         into langchain.schema.document.Document with metadata set properly\n",
    "    docs = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            \"source\": path_to_transcript, \n",
    "            \"chunk_number\": i, \n",
    "            \"timestamps\": f\"{chunk[0]} --> {chunk[1]}\"\n",
    "        }\n",
    "        doc = Document(page_content = chunk[2], metadata=metadata)\n",
    "        docs.append(doc)\n",
    "        \n",
    "    # step 4: call `embed_documents` to create/store emebddings for the video\n",
    "    return embed_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035380b-243e-4867-a48e-64146fb37017",
   "metadata": {},
   "source": [
    "## Call Function on SRT Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfebbe3-88a4-4ce3-93a1-8c68f7947844",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e92e774-cb3b-42de-a894-505ef100955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_intro = process_video(\"videos/transcripts_tiny/2.1.1 pandas intro.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e5535d-12dd-4f3b-a26f-5322a65afa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_core = process_video(\"videos/transcripts_tiny/2.1.2 pandas core functionality.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8ec24-ee9a-4994-8cee-5fde2ba3a4bf",
   "metadata": {},
   "source": [
    "#### 2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "460756a7-0831-400a-a896-4d36fe50ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chains_1 = process_video(\"videos/transcripts_tiny/2.10.1 Markov Chains 1 (1 of 4).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d53d032-9366-4098-8571-12a71b371c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chains_2 = process_video(\"videos/transcripts_tiny/2.10.2 Markov Chains 2 (2 of 4).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f3e4e40-4546-4904-9889-6d65af26920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chains_3 = process_video(\"videos/transcripts_tiny/2.10.3 Markov Chains 3 (3 of 4).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be82e901-5212-419c-978b-99b295c66d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chains_4 = process_video(\"videos/transcripts_tiny/2.10.4 Markov Chains 4 (4 of 4).srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3dd531-7a4b-4bcf-b482-c85f47cb1e9e",
   "metadata": {},
   "source": [
    "#### 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b48f176f-f4aa-4b1c-8320-f37745e88828",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_model = process_video(\"videos/transcripts_tiny/2.11.1 Lake Model.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "451a2669-0081-4e4d-9464-c7168c7049c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_model_data_1 = process_video(\"videos/transcripts_tiny/2.11.2 Lake Model Meets Data (1 of 2).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4911c43c-243a-4775-8440-78f9b7225d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_model_data_2 = process_video(\"videos/transcripts_tiny/2.11.3 Lake Model Meets Data (2 of 2).srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaa397-a9f3-48e8-8df8-c110ed541ee6",
   "metadata": {},
   "source": [
    "#### 2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cbb7ee4-c7d6-4f2b-a57f-4821af0e3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scraping = process_video(\"videos/transcripts_tiny/2.12.1 Web Scraping Intro.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df939dfd-cb5c-4fb6-b08f-360105424e13",
   "metadata": {},
   "source": [
    "#### 2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c38cf3f-ddf1-4242-a665-bca9ba6813ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_results = process_video(\"videos/transcripts_tiny/2.13.1 Sharing Results.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe03ea2-b292-442c-b80f-1bed309a2fb6",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bb88575-7694-4aa9-a0b1-1c734e5b0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index = process_video(\"videos/transcripts_tiny/2.2.1 the pandas index.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5923f000-f43b-4dc1-806a-7957667128fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_storage = process_video(\"videos/transcripts_tiny/2.2.2 data storage formats with pandas.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e9369-1af1-4452-84d8-92e4a817b8b7",
   "metadata": {},
   "source": [
    "#### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68791b72-2e87-4d13-991f-fa17ab83db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning = process_video(\"videos/transcripts_tiny/2.3.1 cleaning data with pandas.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82f6411c-f702-42c8-b940-1fc9768c4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaping_data = process_video(\"videos/transcripts_tiny/2.3.2 Reshaping data with pandas.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "160dac1e-60ed-47ef-bb71-0e089343ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merging_datasets = process_video(\"videos/transcripts_tiny/2.3.3 Merging datasets with Pandas.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec2750-23e4-4812-8fef-bbd3d0905e69",
   "metadata": {},
   "source": [
    "#### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f68ba03d-b725-4b41-a87c-03decfb64676",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_core_review = process_video(\"videos/transcripts_tiny/2.4.1 Pandas Core Review (1 of 2).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c4db3d3-6785-49fd-8658-bb9a99b669dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_review = process_video(\"videos/transcripts_tiny/2.4.2 Pandas Review UN Population data (2 of 2).srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f502b3-408e-4305-8d66-71390687cd5d",
   "metadata": {},
   "source": [
    "#### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87225aa3-eba1-4772-a92f-0d5094fcc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_operations = process_video(\"videos/transcripts_tiny/2.5.1 Groupby Operations.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bf3c5-08f8-482d-906f-82d33adb3c7d",
   "metadata": {},
   "source": [
    "#### 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d8d0c82-748b-4454-acbf-da19d2ee2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessing_data = process_video(\"videos/transcripts_tiny/2.6.1 Accessing Data via APIs (1 of 4).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e36224fc-e357-44c1-aaea-5643e98a72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labor_markets = process_video(\"videos/transcripts_tiny/2.6.2 Data on Labor Markets (2 of 4).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63799a81-9411-4da0-b32f-e93b84c3166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bls_api = process_video(\"videos/transcripts_tiny/2.6.3 Using the BLS Api (3 of 4).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ff3a383-121d-416f-8fd9-6085133c84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploring_labor_market = process_video(\"videos/transcripts_tiny/2.6.4 Exploring Labor Market Data from BLS (4 of 4).srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0f597-747f-4d7b-8b2a-923153b160f0",
   "metadata": {},
   "source": [
    "#### 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "287656ee-e981-42b8-ad30-07ffc5d79e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_intro = process_video(\"videos/transcripts_tiny/2.7.1 Plotting Introduction.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1089d099-dbc0-4536-bdd1-18f0907fdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_rules = process_video(\"videos/transcripts_tiny/2.7.2 Visualization Rules.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02daf1aa-7ab0-45b6-ac46-cecf3883d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_plotting = process_video(\"videos/transcripts_tiny/2.7.3 Web plotting introduction.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ad5312d-4581-40c3-872f-89b6bf314e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "altair = process_video(\"videos/transcripts_tiny/2.7.4 Altair.srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0bd29-e4c3-4001-8d58-3286859bcb93",
   "metadata": {},
   "source": [
    "#### 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35f851a5-46fc-4e3a-b20b-184224e043b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_sql_1 = process_video(\"videos/transcripts_tiny/2.8.1 Intro to SQL (part 1 of 2).srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2322be1d-5be5-47fd-ac3a-fee8846058e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_sql_2 = process_video(\"videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25500ffe-b65a-4e50-a738-c7dee5dedada",
   "metadata": {},
   "source": [
    "#### 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0edf20d1-8d61-49e1-997f-e9b367f457f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_data = process_video(\"videos/transcripts_tiny/2.9.1 Temporal Data In pandas.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa239b17-9351-4a95-9039-0c420f697cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
