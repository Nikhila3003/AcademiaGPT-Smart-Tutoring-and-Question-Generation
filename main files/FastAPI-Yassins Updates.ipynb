{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad53ae3-2c7b-48e5-999a-46252225500a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 155\u001b[0m\n\u001b[1;32m    152\u001b[0m COLLECTION_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# create vector store client\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m vx \u001b[38;5;241m=\u001b[39m \u001b[43mvecs\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_client(DB_CONNECTION)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# create a collection of vectors with 3 dimensions\u001b[39;00m\n\u001b[1;32m    158\u001b[0m docs \u001b[38;5;241m=\u001b[39m vx\u001b[38;5;241m.\u001b[39mget_or_create_collection(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1536\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vecs' is not defined"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "import nbformat\n",
    "import os\n",
    "import re\n",
    "import moviepy.editor as mp\n",
    "import whisper\n",
    "from whisper.utils import get_writer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Function to convert notebooks to markdown\n",
    "def convert_notebooks_to_markdown(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        raise HTTPException(status_code=404, detail=f\"Directory '{directory_path}' not found.\")\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".ipynb\"):\n",
    "            notebook_path = os.path.join(directory_path, filename)\n",
    "            with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n",
    "                notebook_content = nbformat.read(notebook_file, as_version=4)\n",
    "            \n",
    "            def cells_to_markdown(cells):\n",
    "                markdown_output = \"\"\n",
    "                for cell in cells:\n",
    "                    if cell.cell_type == 'markdown':\n",
    "                        markdown_output += cell.source + '\\n\\n'\n",
    "                    elif cell.cell_type == 'code':\n",
    "                        markdown_output += f'```python\\n{cell.source}\\n```\\n\\n'\n",
    "                return markdown_output\n",
    "            \n",
    "            markdown_content = cells_to_markdown(notebook_content.cells)\n",
    "            \n",
    "            file_name = os.path.splitext(filename)[0]\n",
    "            md_file_name = f\"{file_name}.md\"\n",
    "            \n",
    "            with open(md_file_name, 'w', encoding='utf-8') as md_file:\n",
    "                md_file.write(markdown_content)\n",
    "            \n",
    "            print(f\"Conversion complete for {filename}. Generated {md_file_name}.\")\n",
    "    \n",
    "    print(\"Conversion process completed for all notebooks.\")\n",
    "\n",
    "# Function to transcribe videos and store in SRT files\n",
    "def transcribe_videos(transcripts_tiny):\n",
    "    if not os.path.exists(transcripts_tiny):\n",
    "        raise HTTPException(status_code=404, detail=f\"Directory '{transcripts_tiny}' not found.\")\n",
    "    \n",
    "    model = whisper.load_model(\"tiny\")\n",
    "    output_directory = os.path.join(transcripts_tiny, 'transcripts_tiny')\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "    \n",
    "    for filename in os.listdir(transcripts_tiny):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            video_path = os.path.join(transcripts_tiny, filename)\n",
    "            output_audio_path = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}.mp3\")\n",
    "            \n",
    "            clip = mp.VideoFileClip(video_path)\n",
    "            audio_file = clip.audio\n",
    "            audio_file.write_audiofile(output_audio_path)\n",
    "            \n",
    "            result = model.transcribe(output_audio_path)\n",
    "            \n",
    "            srt_output_path = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}.srt\")\n",
    "            options = {\n",
    "                'max_line_width': None,\n",
    "                'max_line_count': None,\n",
    "                'highlight_words': False\n",
    "            }\n",
    "            srt_writer = get_writer(\"srt\", output_directory)\n",
    "            srt_writer(result, output_audio_path, options)\n",
    "            \n",
    "            print(f\"Transcription complete for {filename}.\")\n",
    "    \n",
    "    print(\"Transcription process completed for all videos.\")\n",
    "\n",
    "# Function to split markdown files\n",
    "def split_md_files(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        raise HTTPException(status_code=404, detail=f\"Directory '{directory_path}' not found.\")\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".md\"):\n",
    "            input_file_path = os.path.join(directory_path, filename)\n",
    "            with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                markdown_content = file.read()\n",
    "            \n",
    "            ss = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "            split_content = ss.split_text(markdown_content)\n",
    "            \n",
    "            output_directory = os.path.join(directory_path, 'split_md_files')\n",
    "            if not os.path.exists(output_directory):\n",
    "                os.mkdir(output_directory)\n",
    "            \n",
    "            for i, chunk in enumerate(split_content):\n",
    "                output_file_path = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}_part_{i+1}.md\")\n",
    "                with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                    output_file.write(chunk)\n",
    "            \n",
    "            print(f\"Splitting complete for {filename}.\")\n",
    "    \n",
    "    print(\"Splitting process completed for all Markdown files.\")\n",
    "\n",
    "# Function to chunk SRT files\n",
    "def chunk_srt_files(full_text, chunk_length):\n",
    "    rex = re.compile(r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2}),\\d{3} --> (\\d{2}:\\d{2}:\\d{2}),\\d{3}\")\n",
    "    splits = rex.split(full_text)[1:]\n",
    "    \n",
    "    parts = []\n",
    "    for i in range(0, len(splits), 3):\n",
    "        start_time = splits[i]\n",
    "        end_time = splits[i+1]\n",
    "        content = splits[i+2].strip()\n",
    "        parts.append((start_time, end_time, content))\n",
    "\n",
    "    chunks = []\n",
    "    ix = 0\n",
    "    current_chunk_text = \"\"\n",
    "    for i, part in enumerate(parts):\n",
    "        current_chunk_text = current_chunk_text + \" \" + part[2]\n",
    "        if len(current_chunk_text) > chunk_length or i == len(parts) - 1:\n",
    "            current_chunk = (\n",
    "                parts[ix][0],\n",
    "                part[1],\n",
    "                current_chunk_text.strip()\n",
    "            )\n",
    "            chunks.append(current_chunk)\n",
    "            ix = i\n",
    "            current_chunk_text =  part[2]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_all_srt_files(input_folder_srt, output_folder_chunk, chunk_length):\n",
    "    if not os.path.exists(output_folder_chunk):\n",
    "        os.makedirs(output_folder_chunk)\n",
    "\n",
    "    srt_files = [f for f in os.listdir(input_folder_srt) if f.endswith('.srt')]\n",
    "\n",
    "    for srt_file in srt_files:\n",
    "        with open(os.path.join(input_folder_srt, srt_file)) as f:\n",
    "            txt = f.read()\n",
    "\n",
    "        chunks = chunk_srt_files(txt, chunk_length)\n",
    "\n",
    "        output_file = os.path.join(output_folder_chunk, srt_file)\n",
    "        with open(output_file, 'w') as f:\n",
    "            for chunk in chunks:\n",
    "                f.write(f\"{chunk[0]} --> {chunk[1]}\\n{chunk[2]}\\n\\n\")\n",
    "\n",
    "DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "\n",
    "# create vector store client\n",
    "vx = vecs.create_client(DB_CONNECTION)\n",
    "\n",
    "# create a collection of vectors with 3 dimensions\n",
    "docs = vx.get_or_create_collection(name=\"documents\", dimension=1536)\n",
    "\n",
    "import re\n",
    "rex = re.compile(r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2}),\\d{3} --> (\\d{2}:\\d{2}:\\d{2}),\\d{3}\")\n",
    "\n",
    "def process_notebook(path):\n",
    "    # step 1: parse notebook to convert .ipynb to .md\n",
    "    # TODO: need to implement this in a function\n",
    "    \n",
    "    # step 2: split markdown into chunks\n",
    "    docs = create_chunks_for_notebook(path)\n",
    "\n",
    "    # step 3: create and store embeddings for chunks\n",
    "    return embed_documents(docs)\n",
    "\n",
    "def process_videos_in_directory(srt_input_folder):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(srt_input_folder):\n",
    "        print(f\"Directory '{srt_input_folder}' not found.\")\n",
    "        return\n",
    "\n",
    "    # List all SRT files in the directory\n",
    "    srt_files = [f for f in os.listdir(srt_input_folder) if f.endswith('.srt')]\n",
    "\n",
    "    for srt_file in srt_files:\n",
    "        path_to_transcript = os.path.join(srt_input_folder, srt_file)\n",
    "        srt_content = read_txt_file(path_to_transcript)\n",
    "        chunks = chunk_srt_files(srt_content, 1000)\n",
    "        \n",
    "        docs = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadata = {\n",
    "                \"source\": path_to_transcript, \n",
    "                \"chunk_number\": i, \n",
    "                \"timestamps\": f\"{chunk[0]} --> {chunk[1]}\"\n",
    "            }\n",
    "            doc = Document(page_content=chunk[2], metadata=metadata)\n",
    "            docs.append(doc)\n",
    "            \n",
    "        embed_documents(docs)\n",
    "\n",
    "def create_chunks_for_notebook(path):\n",
    "    loader = TextLoader(path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # add a chunk number and document type\n",
    "    for i, d in enumerate(docs):\n",
    "        old = d.metadata\n",
    "        d.metadata = {**old, \"chunk_number\": i, \"type\": \"notebook\"}\n",
    "        \n",
    "    return docs\n",
    "\n",
    "\n",
    "def create_chunks_for_video(path):\n",
    "    loader = TextLoader(path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Add a chunk number and document type.\n",
    "    for i, d in enumerate(docs):\n",
    "        old = d.metadata\n",
    "        d.metadata = {**old, \"chunk_number\": i, \"type\": \"video\"}\n",
    "\n",
    "    return docs\n",
    "\n",
    "def initialize_environment():\n",
    "    load_dotenv()\n",
    "\n",
    "def get_vectorstore():\n",
    "    COLLECTION_NAME = \"documents\"\n",
    "    DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    db = PGVector(embedding_function=embeddings,\n",
    "                  collection_name=COLLECTION_NAME,\n",
    "                  connection_string=DB_CONNECTION)\n",
    "    return db\n",
    "\n",
    "def create_chain(system_message_text, retriever):\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"search_course_content\",\n",
    "        \"Searches and returns documents regarding the contents of the course and notes from the instructor.\",\n",
    "    )\n",
    "    tools = [tool]\n",
    "\n",
    "    system_message = SystemMessage(content=system_message_text)\n",
    "\n",
    "    return create_conversational_retrieval_agent(\n",
    "        llm=llm,\n",
    "        tools=tools,\n",
    "        verbose=False,\n",
    "        system_message=system_message\n",
    "    )\n",
    "\n",
    "def report_on_message(msg):\n",
    "    print(\"any intermediate_steps?: \", len(msg[\"intermediate_steps\"]) > 0)\n",
    "    print(\"output:\\n\", msg[\"output\"])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "def chat_and_report(chat_conv, query):\n",
    "    msg = chat_conv({\"input\": query})\n",
    "    report_on_message(msg)\n",
    "    return msg\n",
    "\n",
    "def evaluate_prompt(prompt, queries=None, **kw):\n",
    "    if queries is None:\n",
    "        queries = []\n",
    "\n",
    "    chat_conv = create_chain(prompt, **kw)\n",
    "    out = []\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"********** Query {i+1}\\n\")\n",
    "        print(f\"input: {q}\")\n",
    "        out.append(chat_and_report(chat_conv, q))\n",
    "    return out\n",
    "\n",
    "\n",
    "# Define FastAPI endpoint\n",
    "@app.get(\"/process_data\")\n",
    "async def process_data():\n",
    "    # Replace these paths with your own\n",
    "    notebook_dir = 'notebooks/All_notebooks'\n",
    "    video_transcripts_dir = 'transcripts_tiny'\n",
    "    markdown_dir = '/path/to/your/directory'\n",
    "    srt_input_folder = \"/videos/transcripts_tiny\"\n",
    "    srt_output_folder = \"/videos/transcripts_chunked\"\n",
    "    chunk_length = 1000\n",
    "    parsed_notebooks_folder = \"./Parsed Notebooks/\"\n",
    "\n",
    "    # Listing all markdown files in the folder\n",
    "    markdown_files = [f for f in os.listdir(parsed_notebooks_folder) if f.endswith(\".md\")]\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    # Processing each markdown file in the folder\n",
    "    for markdown_file in markdown_files:\n",
    "        file_path = os.path.join(parsed_notebooks_folder, markdown_file)\n",
    "        docs = create_chunks_for_notebook(file_path)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    parsed_video_files_folder = \"./Embedded Videos/\"\n",
    "\n",
    "    # Listing all Python files in the folder\n",
    "    python_files = [f for f in os.listdir(parsed_video_files_folder) if f.endswith(\".ipynb\")]\n",
    "\n",
    "    all_docs = []\n",
    "\n",
    "    # Processing each Python file in the folder\n",
    "    for python_file in python_files:\n",
    "        file_path = os.path.join(parsed_video_files_folder, python_file)\n",
    "        docs = create_chunks_for_video(file_path)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    convert_notebooks_to_markdown(notebook_dir)\n",
    "    transcribe_videos(video_transcripts_dir)\n",
    "    split_md_files(markdown_dir)\n",
    "    process_all_srt_files(srt_input_folder, srt_output_folder, chunk_length)\n",
    "    docs = create_chunks_for_notebook(path)\n",
    "    return embed_documents(docs)\n",
    "    \n",
    "    return {\"status\": 200, \"response\": \"All data processing complete.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f949feb9-13bd-4c89-8744-b7980cf21a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1994596273.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    uvicorn FastAPI:app --reload\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9902a-deab-4f28-9664-efc9fa7506f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
