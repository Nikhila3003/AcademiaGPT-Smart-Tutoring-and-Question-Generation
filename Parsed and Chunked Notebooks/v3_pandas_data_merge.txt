[[{'cell_type': 'markdown', 'source': '# Merge\n\n**Prerequisites**\n\n- [Reshape](./v2_pandas_data_reshape.ipynb)  \n\n\n**Outcomes**\n\n- Know the different pandas routines for combining datasets  \n- Know when to use `pd.concat` vs `pd.merge` vs `pd.join`  \n- Be able to apply the three main combining routines  \n\n\n**Data**\n\n- WDI data on GDP components, population, and square miles of countries  \n- Book ratings: 6,000,000 ratings for the 10,000 most rated books on\n  [Goodreads](https://www.goodreads.com/)', 'metadata': {'slideshow': {'slide_type': 'slide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'import pandas as pd\n\n%matplotlib inline\n\nfrom IPython.display import display', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '## Outline\n\n- [Merge](#Merge)  \n  - [Combining Datasets](#Combining-Datasets)  \n  - [`pd.concat`](#`pd.concat`)  \n  - [`pd.merge`](#`pd.merge`)  \n  - [Arguments to `merge`](#Arguments-to-`merge`)  \n  - [`df.join`](#`df.join`)  \n  - [Case Study](#Case-Study)  \n  - [Visualizing Merge Operations](#Visualizing-Merge-Operations)  ', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'markdown', 'source': '## Combining Datasets\n\nOften, we will want perform joint analysis on data from different sources.\n\nFor example, when analyzing the regional sales for a company, we might\nwant to include industry aggregates or demographic information for each\nregion.\n\nOr perhaps we are working with product-level data, have a list of\nproduct groups in a separate dataset, and want to compute aggregate\nstatistics for each group.', 'metadata': {'slideshow': {'slide_type': 'slide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# from WDI. Units trillions of 2010 USD\nurl = "https://datascience.quantecon.org/assets/data/wdi_data.csv"\nwdi = pd.read_csv(url).set_index(["country", "year"])\nwdi.info()\n\nwdi2017 = wdi.xs(2017, level="year")\nwdi2017', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'wdi2016_17 = wdi.loc[pd.IndexSlice[:, [2016, 2017]],: ]\nwdi2016_17', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# Data from https://www.nationmaster.com/country-info/stats/Geography/Land-area/Square-miles\n# units -- millions of square miles\nsq_miles = pd.Series({\n   "United States": 3.8,\n   "Canada": 3.8,\n   "Germany": 0.137,\n   "United Kingdom": 0.0936,\n   "Russia": 6.6,\n}, name="sq_miles").to_frame()\nsq_miles.index.name = "country"\nsq_miles', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# from WDI. Units millions of people\npop_url = "https://datascience.quantecon.org/assets/data/wdi_population.csv"\npop = pd.read_csv(pop_url).set_index(["country", "year"])\npop.info()\npop.head(10)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'markdown', 'source': 'Suppose that we were asked to compute a number of statistics with the data above:\n\n- As a measure of land usage or productivity, what is Consumption per square mile?  \n- What is GDP per capita (per person) for each country in each year? How about\n  Consumption per person?  \n- What is the population density of each country? How much does it change over time?  \n\n\nNotice that to answer any of the questions from above, we will have to use data\nfrom more than one of our DataFrames.\n\nIn this lecture, we will learn many techniques for combining datasets that\noriginate from different sources, careful to ensure that data is properly\naligned.\n\nIn pandas three main methods can combine datasets:\n\n1. `pd.concat([dfs...])`  \n1. `pd.merge(df1, df2)`  \n1. `df1.join(df2)`  \n\n\nWe’ll look at each one.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'markdown', 'source': '## `pd.concat`\n\nThe `pd.concat` function is used to stack two or more DataFrames\ntogether.\n\nAn example of when you might want to do this is if you have monthly data\nin separate files on your computer and would like to have 1 year of data\nin a single DataFrame.\n\nThe first argument to `pd.concat` is a list of DataFrames to be\nstitched together.\n\nThe other commonly used argument is named `axis`.\n\nAs we have seen before, many pandas functions have an `axis` argument\nthat specifies whether a particular operation should happen down rows\n(`axis=0`) or along columns (`axis=1`).\n\nIn the context of `pd.concat`, setting `axis=0` (the default case)\nwill stack DataFrames on top of one another while `axis=1` stacks them\nside by side.\n\nWe’ll look at each case separately.', 'metadata': {'slideshow': {'slide_type': 'slide'}}}], [{'cell_type': 'markdown', 'source': '### `axis=0`\n\nWhen we call `pd.concat` and set `axis=0`, the list of DataFrames\npassed in the first argument will be stacked on top of one another.\n\nLet’s try it out here.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# equivalent to pd.concat([wdi2017, sq_miles]) -- axis=0 is default\npd.concat([wdi2017, sq_miles], axis=0)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': "Notice a few things:\n\n- \n  <dl style='margin: 20px 0;'>\n  <dt>The number of rows in the output is the total number</dt>\n  <dd>\n  of rows in all inputs. The labels are all from the original\n  DataFrames.  \n  </dd>\n  \n  </dl>\n  \n- The column labels are all the distinct column labels from all the inputs.  \n- For columns that appeared only in one input, the value for all row labels\n  originating from a different input is equal to `NaN` (marked as missing).  ", 'metadata': {'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'markdown', 'source': '### `axis=1`\n\nIn this example, concatenating by stacking\nside-by-side makes more sense.\n\nWe accomplish this by passing `axis=1` to `pd.concat`:', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.concat([wdi2017, sq_miles], axis=1)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': 'Notice here that\n\n- The index entries are all unique index entries that appeared in any DataFrame.  \n- The column labels are all column labels from the inputs.  \n- As `wdi2017` didn’t have a `Russia` row, the value for all of its columns\n  is `NaN`.  \n\n\nNow we can answer one of our questions from above: What is\nConsumption per square mile?', 'metadata': {'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'temp = pd.concat([wdi2017, sq_miles], axis=1)\ntemp["Consumption"] / temp["sq_miles"]', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'markdown', 'source': '## `pd.merge`\n\n`pd.merge` operates on two DataFrames at a time and is primarily used\nto bring columns from one DataFrame into another, *aligning* data based\non one or more “key” columns.\n\nThis is a somewhat difficult concept to grasp by reading, so let’s look at some\nexamples.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2017, sq_miles, on="country")', 'outputs': [], 'metadata': {'hide-output': False, 'scrolled': True, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'The output here looks very similar to what we saw with `concat` and\n`axis=1`, except that the row for `Russia` does not appear.\n\nWe will talk more about why this happened soon.\n\nFor now, let’s look at a slightly more intriguing example:', 'metadata': {'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2016_17, sq_miles, on="country")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'markdown', 'source': 'Here’s how we think about what happened:\n\n- The data in `wdi2016_17` is copied over exactly as is.  \n- Because `country` was on the index for both DataFrames, it is on the\n  index of the output.  \n- We lost the year on the index – we’ll work on getting it back below.  \n- The additional column in `sq_miles` was added to column labels for the\n  output.  \n- The data from the `sq_miles` column was added to the output by looking up\n  rows where the `country` in the two DataFrames lined up.\n  -  Note that all the countries appeared twice, and the data in `sq_miles` was repeated. This is because `wdi2016_17` had two rows for each country.\n  -  Also note that because `Russia` did not appear in `wdi2016_17`, the value `sq_miles.loc["Russia"]` (i.e. `6.6`) is not used the output.  \n\n\nHow do we get the year back?\n\nWe must first call `reset_index` on `wdi2016_17` so\nthat in the first step when all columns are copied over, `year` is included.', 'metadata': {'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2016_17.reset_index(), sq_miles, on="country")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'markdown', 'source': '### Multiple Columns\n\nSometimes, we need to merge multiple columns.\n\nFor example our `pop` and `wdi2016_17` DataFrames both have observations\norganized by country and year.\n\nTo properly merge these datasets, we would need to align the data by\nboth country and year.\n\nWe pass a list to the `on` argument to accomplish this:', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2016_17.reset_index(), pop, on=["country", "year"])', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'markdown', 'source': 'Now, we can answer more of our questions from above: What is GDP per capita (per\nperson) for each country in each year? How about Consumption per person?', 'metadata': {'slideshow': {'slide_type': '-'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'wdi_pop = pd.merge(wdi2016_17, pop, on=["country", "year"])\nwdi_pop["GDP"] / wdi_pop["Population"]', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'wdi_pop["Consumption"] / wdi_pop["Population"]', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '**Exercise 1**\n\nUse your new `merge` skills to answer the final question from above: What\nis the population density of each country? How much does it change over\ntime?\n', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '\n', 'outputs': [], 'metadata': {'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': '## Arguments to `merge`\n\nThe `pd.merge` function can take many optional arguments.\n\nWe’ll talk about a few of the most commonly-used ones here and refer you\nto the\n[documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html#pandas.merge)\nfor more details.\n\nWe’ll follow the pandas convention and refer to the first argument to\n`pd.merge` as `left` and call the second `right`.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'markdown', 'source': '### `on`\n\nWe have already seen this one used before, but we want to point out that on\nis optional.\n\nIf nothing is given for this argument, pandas will use **all** columns\nin `left` and `right` with the same name.\n\nIn our example, `country` is the only column that appears in both\nDataFrames, so it is used for `on` if we don’t pass anything.\n\nThe following two are equivalent.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2017, sq_miles, on="country")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# if we move index back to columns, the `on` is un-necessary\npd.merge(wdi2017.reset_index(), sq_miles.reset_index())', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '### `left_on`, `right_on`\n\nAbove, we used the `on` argument to identify a column in both `left`\nand `right` that was used to align data.\n\nSometimes, both DataFrames don’t have the same name for this column.\n\nIn that case, we use the `left_on` and `right_on` arguments, passing\nthe proper column name(s) to align the data.\n\nWe’ll show you an example below, but it is somewhat silly as our\nDataFrames do both have the `country` column.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2017, sq_miles, left_on="country", right_on="country")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '### `left_index`, `right_index`\n\nSometimes, as in our example, the key used to align data is actually in the\nindex instead of one of the columns.\n\nIn this case, we can use the `left_index` or `right_index` arguments.\n\nWe should only set these values to a boolean (`True` or `False`).\n\nLet’s practice with this.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(wdi2017, sq_miles, left_on="country", right_index=True)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '### `how`\n\nThe `how` is perhaps the most powerful, but most conceptually\ndifficult of the arguments we will cover.\n\nThis argument controls which values from the key column(s) appear in the\noutput.\n\nThe 4 possible options for this argument are summarized in\nthe image below.\n\n<img src="https://datascience.quantecon.org/_images/merge_venns.png" alt="merge\\_venns.png" style="">\n\n  \nIn words, we have:\n\n- `left`: Default and what we described above. It uses\n  the keys from the `left` DataFrame.  \n- `right`: Output will contain all keys from `right`.  \n- `inner`: The output will only contain keys that appear in *both*\n  `left` and `right`.  \n- `outer`: The output will contain any key found in either `left`\n  or `right`.  \n\n\nIn addition to the above, we will use the following two DataFrames to\nillustrate the `how` option.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'wdi2017_no_US = wdi2017.drop("United States")\nwdi2017_no_US', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'sq_miles_no_germany = sq_miles.drop("Germany")\nsq_miles_no_germany', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Now, let’s see all the possible `how` options.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# default\npd.merge(wdi2017_no_US, sq_miles, on="country", how="left")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# notice ``Russia`` is included\npd.merge(wdi2017, sq_miles, on="country", how="right")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# notice no United States or Russia\npd.merge(wdi2017, sq_miles, on="country", how="inner")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': '# includes all 5, even though they don\'t all appear in either DataFrame\npd.merge(wdi2017_no_US, sq_miles_no_germany, on="country", how="outer")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'markdown', 'source': '### `df.merge(df2)`\n\nNote that the DataFrame type has a `merge` *method*.\n\nIt is the same as the function we have been working with, but passes the\nDataFrame before the period as `left`.\n\nThus `df.merge(other)` is equivalent to `pd.merge(df, other)`.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'wdi2017.merge(sq_miles, on="country", how="right")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '## `df.join`\n\nThe `join` method for a DataFrame is very similar to the `merge`\nmethod described above, but only allows you to use the index of the\n`right` DataFrame as the join key.\n\nThus, `left.join(right, on="country")` is equivalent to calling\n`pd.merge(left, right, left_on="country", right_index=True)`.\n\nThe implementation of the `join` method calls `merge` internally,\nbut sets the `left_on` and `right_index` arguments for you.\n\nYou can do anything with `df.join` that you can do with\n`df.merge`, but df.join` is more convenient to use if the keys of `right`\nare in the index.', 'metadata': {'slideshow': {'slide_type': 'slide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'wdi2017.join(sq_miles, on="country")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'wdi2017.merge(sq_miles, left_on="country", right_index=True)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '## Case Study\n\nLet’s put these tools to practice by loading some real datasets and\nseeing how these functions can be applied.\n\nWe’ll analyze ratings of books from the website [Goodreads](https://www.goodreads.com/).\n\nWe accessed the data [here](https://github.com/zygmuntz/goodbooks-10k).\n\nLet’s load it up.', 'metadata': {'slideshow': {'slide_type': 'slide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'url = "https://datascience.quantecon.org/assets/data/goodreads_ratings.csv.zip"\nratings = pd.read_csv(url)\ndisplay(ratings.head())\nratings.info()', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'We can already do some interesting things with just the ratings data.\n\nLet’s see how many ratings of each number are in our dataset.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'ratings["rating"].value_counts().sort_index().plot(kind="bar");', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': 'Let’s also see how many users have rated `N` books, for all `N`\npossible.\n\nTo do this, we will use `value_counts` twice (can you think of why?).\n\nWe will see a more flexible way of performing similar grouped operations in\na future lecture.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'ratings.head()', 'outputs': [], 'metadata': {}}, {'cell_type': 'code', 'execution_count': None, 'source': 'ratings["user_id"].value_counts()', 'outputs': [], 'metadata': {}}, {'cell_type': 'code', 'execution_count': None, 'source': 'users_by_n = (\n    ratings["user_id"]\n    .value_counts()  # Series. Index: user_id, value: n ratings by user\n    .value_counts()  # Series. Index: n_ratings by user, value: N_users with this many ratings\n    .sort_index()    # Sort our Series by the index (number of ratings)\n    .reset_index()   # Dataframe with columns `index` (from above) and `user_id`\n    .rename(columns={"index": "N_ratings", "user_id": "N_users"})\n)\nusers_by_n.head(10)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Let’s look at some statistics on that dataset.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'users_by_n.describe()', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'We can see the same data visually in a box plot.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'users_by_n.plot(kind="box", subplots=True)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Let’s practice applying the want operator…\n\n**Want**: Determine whether a relationship between the number of\nratings a user has written and the distribution of the ratings exists. (Maybe we\nare an author hoping to inflate our ratings and wonder if we should\ntarget “more experienced” Goodreads users, or focus on newcomers.)\n\nLet’s start from the result and work our way backwards:\n\n1. We can answer our question if we have two similar DataFrames:  \n  - All ratings by the `N` (e.g. 25) users with the most ratings  \n  - All ratings by the `N` users with the least number of\n    ratings  \n1. To get that, we will need to extract rows of `ratings` with\n  `user_id` associated with the `N` most and least prolific raters  \n1. For that, we need the most and least active `user_id`s  \n1. To get that info, we need a count of how many ratings each user left.  \n  - We can get that with `df["user_id"].value_counts()`, so let’s\n    start there.  ', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# step 4\nn_ratings = ratings["user_id"].value_counts()\nn_ratings.head()', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': '# step 3\nN = 25\nmost_prolific_users = n_ratings.nlargest(N).index.tolist()\nleast_prolific_users = n_ratings.nsmallest(N).index.tolist()\n\nprint(most_prolific_users)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# step 2\nactive_ratings = ratings.loc[ratings["user_id"].isin(most_prolific_users), :]\ninactive_ratings = ratings.loc[ratings["user_id"].isin(least_prolific_users), :]\n\nactive_ratings.head()', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# step 1 -- get the answer!\nactive_ratings["rating"].value_counts().sort_index().plot(\n    kind="bar", title="Distribution of ratings by most active users"\n)\nprint(active_ratings["rating"].mean())\nprint(active_ratings["rating"].median())\nprint(active_ratings["rating"].std())', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'inactive_ratings["rating"].value_counts().sort_index().plot(\n    kind="bar", title="Distribution of ratings by least active users"\n)\nprint(inactive_ratings["rating"].mean())\nprint(inactive_ratings["rating"].median())\nprint(inactive_ratings["rating"].std())', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}, {'cell_type': 'markdown', 'source': 'Nice! From the picture above, the new users look much more\nlikely to leave 5 star ratings than more experienced users.', 'metadata': {'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': '### Book Data\n\nWe know what you are probably thinking: “Isn’t this a lecture on merging?\nWhy are we only using one dataset?”\n\nWe hear you.\n\nLet’s also load a dataset containing information on the actual books.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'url = "https://datascience.quantecon.org/assets/data/goodreads_books.csv"\nbooks = pd.read_csv(url)\n\n# we only need a few of the columns\nbooks = books[["book_id", "authors", "title"]]\nprint("shape: ", books.shape)\nprint("dtypes:\\n", books.dtypes, sep="")\nbooks.head()', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'We could do similar interesting things with just the books dataset,\nbut we will skip it for now and merge them together.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'rated_books = pd.merge(ratings, books, on="book_id", how="left")\n\nrated_books.shape\n', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Now, let’s see which books have been most often rated.', 'metadata': {'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'code', 'execution_count': None, 'source': 'most_rated_books_id = rated_books["book_id"].value_counts().nlargest(10).index\nmost_rated_books = rated_books.loc[rated_books["book_id"].isin(most_rated_books_id), :]\nlist(most_rated_books["title"].unique())', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Let’s use our `pivot_table` knowledge to compute the average rating\nfor each of these books.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'most_rated_books.pivot_table(values="rating", index="title").sort_values("rating")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Let’s compute the average rating for each book in our sample.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'average_ratings = (\n    rated_books\n    .pivot_table(values="rating", index="title")\n    .sort_values(by="rating", ascending=False)\n)\naverage_ratings.head(10)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': 'What does the overall distribution of average ratings look like?', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '# plot a kernel density estimate of average ratings\naverage_ratings.plot.density(xlim=(1, 5))\n\n# or a histogram\naverage_ratings.plot.hist(bins=30, xlim=(1, 5))', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'It looks like most books have an average rating of just below 4.', 'metadata': {'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '## Visualizing Merge Operations\n\nAs we did in the [reshape lecture](reshape.ipynb), we will visualize the\nvarious merge operations using artificial DataFrames.\n\nFirst, we create some dummy DataFrames.', 'metadata': {'slideshow': {'slide_type': 'slide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'dfL = pd.DataFrame(\n    {"Key": ["A", "B", "A", "C"], "C1":[1, 2, 3, 4], "C2": [10, 20, 30, 40]},\n    index=["L1", "L2", "L3", "L4"]\n)[["Key", "C1", "C2"]]\n\nprint("This is dfL: ")\ndisplay(dfL)\n\ndfR = pd.DataFrame(\n    {"Key": ["A", "B", "C", "D"], "C3": [100, 200, 300, 400]},\n    index=["R1", "R2", "R3", "R4"]\n)[["Key", "C3"]]\n\nprint("This is dfR:")\ndisplay(dfR)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': '### `pd.concat`\n\nRecall that calling `pd.concat(..., axis=0)` will stack DataFrames on top of\none another:', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.concat([dfL, dfR], axis=0)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'Here’s how we might visualize that.\n\n<img src="https://datascience.quantecon.org/_images/concat_axis0.gif" alt="concat\\_axis0.gif" style="">\n\n  ', 'metadata': {'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': 'We can also set `axis=1` to stack side by side.', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.concat([dfL, dfR], axis=1)', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}], [{'cell_type': 'markdown', 'source': 'Here’s how we might visualize that.\n\n<img src="https://datascience.quantecon.org/_images/concat_axis1.gif" alt="concat\\_axis1.gif" style="">', 'metadata': {'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '### `pd.merge`\n\nThe animation below shows a visualization of what happens when we call', 'metadata': {'slideshow': {'slide_type': 'subslide'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(dfL, dfR, on="Key")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': '-'}}}, {'cell_type': 'markdown', 'source': '<img src="https://datascience.quantecon.org/_images/left_merge.gif" alt="left\\_merge.gif" style="">\n\n  \nNow, let’s focus on what happens when we set `how="right"`.\n\nPay special attention to what happens when filling the output value for\nthe key `A`.', 'metadata': {'slideshow': {'slide_type': '-'}}}, {'cell_type': 'code', 'execution_count': None, 'source': 'pd.merge(dfL, dfR, on="Key", how="right")', 'outputs': [], 'metadata': {'hide-output': False, 'slideshow': {'slide_type': 'fragment'}}}], [{'cell_type': 'markdown', 'source': '<img src="https://datascience.quantecon.org/_images/right_merge.gif" alt="right\\_merge.gif" style="">', 'metadata': {'slideshow': {'slide_type': '-'}}}, {'cell_type': 'code', 'execution_count': None, 'source': '', 'outputs': [], 'metadata': {}}]]