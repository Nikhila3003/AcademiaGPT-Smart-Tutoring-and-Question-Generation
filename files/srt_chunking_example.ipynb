{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc59238-7ff6-46b0-bd85-e4ef8d677985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rex = re.compile(r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2}),\\d{3} --> (\\d{2}:\\d{2}:\\d{2}),\\d{3}\")\n",
    "\n",
    "def chunk_srt_files(full_text, chunk_length):\n",
    "\n",
    "    # split on the regex.\n",
    "    splits = rex.split(full_text)[1:]\n",
    "\n",
    "    # combine parts into a list of 3-tuples (start, end, txt)\n",
    "    parts = []\n",
    "    for i in range(0, len(splits), 3):\n",
    "        start_time = splits[i]\n",
    "        end_time = splits[i+1]\n",
    "        content = splits[i+2].strip()\n",
    "        parts.append((start_time, end_time, content))\n",
    "        \n",
    "\n",
    "    # combine multiple parts to get desired chunk length\n",
    "    # will be a list of 3-tuples (start, end, txt)\n",
    "    chunks = []\n",
    "    ix = 0\n",
    "    current_chunk_text = \"\"\n",
    "    for i, part in enumerate(parts):\n",
    "        current_chunk_text = current_chunk_text + \" \" + part[2]\n",
    "        if len(current_chunk_text) > chunk_length or i == len(parts) - 1:\n",
    "            # if we have a long enough chunk OR we are on the last piece of content...\n",
    "            current_chunk = (\n",
    "                parts[ix][0],  # starting timestamp\n",
    "                part[1],\n",
    "                current_chunk_text.strip()\n",
    "            )\n",
    "            chunks.append(current_chunk)\n",
    "            ix = i  # we repeat this chunk one more time for overlap\n",
    "            current_chunk_text =  part[2]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f5cd245-0152-408e-97cb-3207323858e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_srt(chunks, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for idx, (start_time, end_time, content) in enumerate(chunks):\n",
    "            f.write(f\"{idx+1}\\n\")\n",
    "            f.write(f\"{start_time},000 --> {end_time},000\\n\")\n",
    "            f.write(f\"{content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69c22a1-d666-479f-9922-ba6e31544694",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"videos/transcripts_tiny/2.1.1 pandas intro.srt\") as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7832db1a-9fd9-464d-af80-3dc5bfac3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_srt_files(txt, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdcbd58-8eea-4a49-93ee-9eb021e37923",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_srt(chunks, \"videos/transcripts_chunked/2.1.1 pandas intro_chunked.srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092f78ab-0c2f-4a8f-a7a3-0f9b371f5212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyteach-msda/jupyteach-ai/videos'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e488766-ad59-4d4f-91d8-5409f7baaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/jupyteach-msda/jupyteach-ai/videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f3776b-b694-40da-b123-dfbc1cc9648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "rex = re.compile(r\"\\d+\\n(\\d{2}:\\d{2}:\\d{2}),\\d{3} --> (\\d{2}:\\d{2}:\\d{2}),\\d{3}\")\n",
    "\n",
    "#def chunk_srt_files(full_text, chunk_length):\n",
    "    # ... (unchanged function from your original code)\n",
    "\n",
    "def process_all_srt_files(input_folder_srt, output_folder_chunk, chunk_length):\n",
    "    # Ensure the output folder exists, or create it if not\n",
    "    if not os.path.exists(output_folder_chunk):\n",
    "        os.makedirs(output_folder_chunk)\n",
    "\n",
    "    # List all SRT files in the input folder\n",
    "    srt_files = [f for f in os.listdir(input_folder_srt) if f.endswith('.srt')]\n",
    "\n",
    "    for srt_file in srt_files:\n",
    "        # Read the content of the SRT file\n",
    "        with open(os.path.join(input_folder_srt, srt_file)) as f:\n",
    "            txt = f.read()\n",
    "\n",
    "        # Call the chunk_srt_files function to process the SRT content\n",
    "        chunks = chunk_srt_files(txt, chunk_length)\n",
    "\n",
    "        # Save the chunked content to the output folder\n",
    "        output_file = os.path.join(output_folder_chunk, srt_file)\n",
    "        with open(output_file, 'w') as f:\n",
    "            for chunk in chunks:\n",
    "                f.write(f\"{chunk[0]} --> {chunk[1]}\\n{chunk[2]}\\n\\n\")\n",
    "\n",
    "# Set the paths for the input and output folders and specify the chunk length\n",
    "input_folder_srt = output_folder_srt\n",
    "output_folder_chunk = \"transcripts_tiny_output\"\n",
    "chunk_length = 1000\n",
    "\n",
    "# Call the function to process all SRT files in the input folder\n",
    "process_all_srt_files(input_folder_srt, output_folder_chunk, chunk_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
