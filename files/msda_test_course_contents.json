[
  {
    "id": 5,
    "position": 0,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 6,
    "description": "Pandas Introduction",
    "title": "Lecture 01",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 81,
    "position": 9,
    "content_id": 170,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 170,
    "type": "video",
    "description": "Learn data aggregation and transformation techniques",
    "title": "Pandas: Basic Functionality",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=VPw5QG_8WwI\", \"youtubeVideoId\": \"VPw5QG_8WwI\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 5,
    "position": 0,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 6,
    "description": "Pandas Introduction",
    "title": "Lecture 01",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 82,
    "position": 10,
    "content_id": 186,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 186,
    "type": "notebook",
    "description": "Learn data aggregation and transformation techniques",
    "title": "v02_pandas_basics.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Basic Functionality\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [pandas Intro](./v01_pandas_intro.ipynb)\\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Be familiar with `datetime`  \\n\", \"- Use built-in aggregation functions and be able to create your own and\\n\", \"  apply them using `agg`  \\n\", \"- Use built-in Series transformation functions and be able to create your\\n\", \"  own and apply them using `apply`  \\n\", \"- Use built-in scalar transformation functions and be able to create your\\n\", \"  own and apply them using `applymap`  \\n\", \"- Be able to select subsets of the DataFrame using boolean selection  \\n\", \"- Know what the “want operator” is and how to apply it  \\n\", \"\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- US state unemployment data from Bureau of Labor Statistics  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Basic Functionality](#Basic-Functionality)  \\n\", \"  - [State Unemployment Data](#State-Unemployment-Data)  \\n\", \"  - [Dates in pandas](#Dates-in-pandas)  \\n\", \"  - [DataFrame Aggregations](#DataFrame-Aggregations)  \\n\", \"  - [Transforms](#Transforms)  \\n\", \"  - [Boolean Selection](#Boolean-Selection)  \\n\", \"  - [Exercises](#Exercises)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## State Unemployment Data\\n\", \"\\n\", \"In this lecture, we will use unemployment data by state at a monthly\\n\", \"frequency.\\n\", \"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pandas as pd\\n\", \"\\n\", \"%matplotlib inline\\n\", \"# activate plot theme\\n\", \"import qeds\\n\", \"qeds.themes.mpl_style();\\n\", \"\\n\", \"pd.__version__\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"First, we will download the data directly from a url and read it into a pandas DataFrame.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Load up the data -- this will take a couple seconds\\n\", \"url = \\\"https://datascience.quantecon.org/assets/data/state_unemployment.csv\\\"\\n\", \"unemp_raw = pd.read_csv(url, parse_dates=[\\\"Date\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The pandas `read_csv` will determine most datatypes of the underlying columns.  The\\n\", \"exception here is that we need to give pandas a hint so it can load up the `Date` column as a Python datetime type: the `parse_dates=[\\\"Date\\\"]`.\\n\", \"\\n\", \"We can see the basic structure of the downloaded data by getting the first 5 rows, which directly matches\\n\", \"the underlying CSV file.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_raw.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Note that a row has a date, state, labor force size, and unemployment rate.\\n\", \"\\n\", \"For our analysis, we want to look at the unemployment rate across different states over time, which\\n\", \"requires a transformation of the data similar to an Excel pivot-table.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Don't worry about the details here quite yet\\n\", \"unemp_all = (\\n\", \"    unemp_raw\\n\", \"    .reset_index()\\n\", \"    .pivot_table(index=\\\"Date\\\", columns=\\\"state\\\", values=\\\"UnemploymentRate\\\")\\n\", \")\\n\", \"unemp_all.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Finally, we can filter it to look at a subset of the columns (i.e. “state” in this case).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"states = [\\n\", \"    \\\"Arizona\\\", \\\"California\\\", \\\"Florida\\\", \\\"Illinois\\\",\\n\", \"    \\\"Michigan\\\", \\\"New York\\\", \\\"Texas\\\"\\n\", \"]\\n\", \"unemp = unemp_all[states]\\n\", \"unemp.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"When plotting, a DataFrame knows the column and index names.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.plot(figsize=(8, 6))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Dates in pandas\\n\", \"\\n\", \"You might have noticed that our index now has a nice format for the\\n\", \"dates (`YYYY-MM-DD`) rather than just a year.\\n\", \"\\n\", \"This is because the `dtype` of the index is a variant of `datetime`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.index\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can index into a DataFrame with a `DatetimeIndex` using string\\n\", \"representations of dates.\\n\", \"\\n\", \"For example\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Data corresponding to a single date\\n\", \"unemp.loc[\\\"2000-01-01\\\", :]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Data for all days between New Years Day and June first in the year 2000\\n\", \"unemp.loc[\\\"01/01/2000\\\":\\\"06/01/2000\\\", :]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We will learn more about what pandas can do with dates and times in an\\n\", \"upcoming lecture on time series data.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## DataFrame Aggregations\\n\", \"\\n\", \"Let’s talk about *aggregations*.\\n\", \"\\n\", \"Loosely speaking, an aggregation is an operation that combines multiple\\n\", \"values into a single value.\\n\", \"\\n\", \"For example, computing the mean of three numbers (for example\\n\", \"`[0, 1, 2]`) returns a single number (1).\\n\", \"\\n\", \"We will use aggregations extensively as we analyze and manipulate our data.\\n\", \"\\n\", \"Thankfully, pandas makes this easy!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Built-in Aggregations\\n\", \"\\n\", \"pandas already has some of the most frequently used aggregations.\\n\", \"\\n\", \"For example:\\n\", \"\\n\", \"- Mean  (`mean`)  \\n\", \"- Variance (`var`)  \\n\", \"- Standard deviation (`std`)  \\n\", \"- Minimum (`min`)  \\n\", \"- Median (`median`)  \\n\", \"- Maximum (`max`)  \\n\", \"- etc…  \\n\", \"\\n\", \"\\n\", \">**Note**\\n\", \">\\n\", \">When looking for common operations, using “tab completion” goes a long way.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.mean()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"As seen above, the aggregation’s default is to aggregate each column.\\n\", \"\\n\", \"However, by using the `axis` keyword argument, you can do aggregations by\\n\", \"row as well.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.var(axis=1).head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Writing Your Own Aggregation\\n\", \"\\n\", \"The built-in aggregations will get us pretty far in our analysis, but\\n\", \"sometimes we need more flexibility.\\n\", \"\\n\", \"We can have pandas perform custom aggregations by following these two\\n\", \"steps:\\n\", \"\\n\", \"1. Write a Python function that takes a `Series` as an input and\\n\", \"  outputs a single value.  \\n\", \"1. Call the `agg` method with our new function as an argument.  \\n\", \"\\n\", \"\\n\", \"For example, below, we will classify states as “low unemployment” or\\n\", \"“high unemployment” based on whether their mean unemployment level is\\n\", \"above or below 6.5.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#\\n\", \"# Step 1: We write the (aggregation) function that we'd like to use\\n\", \"#\\n\", \"def high_or_low(s):\\n\", \"    \\\"\\\"\\\"\\n\", \"    This function takes a pandas Series object and returns high\\n\", \"    if the mean is above 6.5 and low if the mean is below 6.5\\n\", \"    \\\"\\\"\\\"\\n\", \"    if s.mean() < 6.5:\\n\", \"        out = \\\"Low\\\"\\n\", \"    else:\\n\", \"        out = \\\"High\\\"\\n\", \"\\n\", \"    return out\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#\\n\", \"# Step 2: Apply it via the agg method.\\n\", \"#\\n\", \"unemp.agg(high_or_low)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# How does this differ from unemp.agg(high_or_low)?\\n\", \"unemp.agg(high_or_low, axis=1).head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that `agg` can also accept multiple functions at once.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.agg([min, max, high_or_low])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise 2**\\n\", \"\\n\", \"Do the following exercises in separate code cells below:\\n\", \"\\n\", \"- At each date, what is the minimum unemployment rate across all states\\n\", \"  in our sample?  \\n\", \"- What was the median unemployment rate in each state?  \\n\", \"- What was the maximum unemployment rate across the states in our\\n\", \"  sample? What state did it happen in? In what month/year was this\\n\", \"  achieved?  \\n\", \"  - Hint 1: What Python type (not `dtype`) is returned by the\\n\", \"    aggregation?  \\n\", \"  - Hint 2: Read documentation for the method `idxmax`  \\n\", \"- Classify each state as high or low volatility based on whether the\\n\", \"  variance of their unemployment is above or below 4.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.min(axis=1)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp.median()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"print(unemp.max().max())\\n\", \"print(unemp.max().idxmax())\\n\", \"print(unemp.max(axis=1).idxmax())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"def ur_volatility_classify(s):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Classifies the volatility of a series. A series is deemed\\n\", \"    high volatility if the variance is higher than 4\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    s : Series\\n\", \"        The series that we are classifying\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    _ : str\\n\", \"        Either \\\"high\\\" or \\\"low\\\"\\n\", \"    \\\"\\\"\\\"\\n\", \"    var = s.var()\\n\", \"    \\n\", \"    return \\\"high\\\" if var > 4 else \\\"low\\\"\\n\", \"\\n\", \"unemp.apply(ur_volatility_classify)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Transforms\\n\", \"\\n\", \"Many analytical operations do not necessarily involve an aggregation.\\n\", \"\\n\", \"The output of a function applied to a Series might need to be a new\\n\", \"Series.\\n\", \"\\n\", \"Some examples:\\n\", \"\\n\", \"- Compute the percentage change in unemployment from month to month.  \\n\", \"- Calculate the cumulative sum of elements in each column.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Built-in Transforms\\n\", \"\\n\", \"pandas comes with many transform functions including:\\n\", \"\\n\", \"- Cumulative sum/max/min/product (`cum(sum|min|max|prod)`)  \\n\", \"- Difference  (`diff`)  \\n\", \"- Elementwise addition/subtraction/multiplication/division (`+`, `-`, `*`, `/`)  \\n\", \"- Percent change (`pct_change`)  \\n\", \"- Number of occurrences of each distinct value (`value_counts`)  \\n\", \"- Absolute value (`abs`)  \\n\", \"\\n\", \"\\n\", \"Again, tab completion is helpful when trying to find these functions.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp.pct_change().head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp.diff().head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Transforms can be split into to several main categories:\\n\", \"\\n\", \"1. *Series transforms*: functions that take in one Series and produce another Series. The index of the input and output does not need to be the same.  \\n\", \"1. *Scalar transforms*: functions that take a single value and produce a single value. An example is the `abs` method, or adding a constant to each value of a Series.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Custom Series Transforms\\n\", \"\\n\", \"pandas also simplifies applying custom Series transforms to a Series or the\\n\", \"columns of a DataFrame. The steps are:\\n\", \"\\n\", \"1. Write a Python function that takes a Series and outputs a new Series.  \\n\", \"1. Pass our new function as an argument to the `apply` method (alternatively, the `transform` method).  \\n\", \"\\n\", \"\\n\", \"As an example, we will standardize our unemployment data to have mean 0\\n\", \"and standard deviation 1.\\n\", \"\\n\", \"After doing this, we can use an aggregation to determine at which date the\\n\", \"unemployment rate is most different from “normal times” for each state.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#\\n\", \"# Step 1: We write the Series transform function that we'd like to use\\n\", \"#\\n\", \"def standardize_data(x):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Changes the data in a Series to become mean 0 with standard deviation 1\\n\", \"    \\\"\\\"\\\"\\n\", \"    mu = x.mean()\\n\", \"    std = x.std()\\n\", \"\\n\", \"    return (x - mu)/std\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#\\n\", \"# Step 2: Apply our function via the apply method.\\n\", \"#\\n\", \"std_unemp = unemp.apply(standardize_data)\\n\", \"std_unemp.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Takes the absolute value of all elements of a function\\n\", \"abs_std_unemp = std_unemp.abs()\\n\", \"\\n\", \"abs_std_unemp.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# find the date when unemployment was \\\"most different from normal\\\" for each State\\n\", \"def idxmax(x):\\n\", \"    # idxmax of Series will return index of maximal value\\n\", \"    return x.idxmax()\\n\", \"\\n\", \"abs_std_unemp.agg(idxmax)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Custom Scalar Transforms\\n\", \"\\n\", \"As you may have predicted, we can also apply custom scalar transforms to our\\n\", \"pandas data.\\n\", \"\\n\", \"To do this, we use the following pattern:\\n\", \"\\n\", \"1. Define a Python function that takes in a scalar and produces a scalar.  \\n\", \"1. Pass this function as an argument to the `applymap` Series or DataFrame method.  \\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Example**\\n\", \"\\n\", \"Imagine that we want to determine whether unemployment was high (> 6.5),\\n\", \"medium (4.5 < x <= 6.5), or low (<= 4.5) for each state and each month.\\n\", \"\\n\", \"1. Write a Python function that takes a single number as an input and\\n\", \"  outputs a single string noting if that number is high, medium, or low.  \\n\", \"2. Pass your function to `applymap` (quiz: why `applymap` and not\\n\", \"  `agg` or `apply`?) and save the result in a new DataFrame called\\n\", \"  `unemp_bins`.  \\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def unemployment_classifier(ur):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Classifies the unemployment rate as high, medium, or low\\n\", \"    based on the value\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    ur : scalar(float)\\n\", \"        The unemployment rate\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    out : str\\n\", \"        The classification \\\"high\\\", \\\"medium\\\", or \\\"low\\\"\\n\", \"    \\\"\\\"\\\"\\n\", \"    if ur > 6.5:\\n\", \"        return \\\"high\\\"\\n\", \"    elif ur > 4.5:\\n\", \"        return \\\"medium\\\"\\n\", \"    else:\\n\", \"        return \\\"low\\\"\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_bins = unemp.applymap(unemployment_classifier)\\n\", \"unemp_bins.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"3. (Challenging) This exercise has multiple parts:  \\n\", \"  1. Use another transform on `unemp_bins` to count how many\\n\", \"    times each state had each of the three classifications.  \\n\", \"    - Hint 1: Will this value counting function be a Series or scalar\\n\", \"      transform?  \\n\", \"    - Hint 2: Try googling \\\"pandas count unique value\\\" or something\\n\", \"      similar to find the right transform.  \\n\", \"  2. Construct a horizontal bar chart of the number of occurrences of\\n\", \"    each level with one bar per state and classification (21 total\\n\", \"    bars).  \\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_class_count = unemp_bins.apply(\\n\", \"    pd.Series.value_counts\\n\", \")\\n\", \"\\n\", \"unemp_class_count.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_class_count.T.loc[:, [\\\"low\\\", \\\"medium\\\", \\\"high\\\"]].plot(kind=\\\"bar\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"4. (Challenging) Repeat the previous step, but count how many states had\\n\", \"  each classification in each month. Which month had the most states\\n\", \"  with high unemployment? What about medium and low?  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_monthly_count = unemp_bins.apply(\\n\", \"    pd.Series.value_counts, axis=1\\n\", \").fillna(0.0)\\n\", \"\\n\", \"unemp_monthly_count.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_monthly_count[\\\"high\\\"].idxmax()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_monthly_count[\\\"medium\\\"].idxmax()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_monthly_count[\\\"low\\\"].idxmax()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Boolean Selection\\n\", \"\\n\", \"We have seen how we can select subsets of data by referring to the index\\n\", \"or column names.\\n\", \"\\n\", \"However, we often want to select based on conditions met by\\n\", \"the data itself.\\n\", \"\\n\", \"Some examples are:\\n\", \"\\n\", \"- Restrict analysis to all individuals older than 18.  \\n\", \"- Look at data that corresponds to particular time periods.  \\n\", \"- Analyze only data that corresponds to a recession.  \\n\", \"- Obtain data for a specific product or customer ID.  \\n\", \"\\n\", \"\\n\", \"We will be able to do this by using a Series or list of boolean values\\n\", \"to index into a Series or DataFrame.\\n\", \"\\n\", \"Let’s look at some examples.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_small = unemp.head()  # Create smaller data so we can see what's happening\\n\", \"unemp_small\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# list of booleans selects rows\\n\", \"unemp_small.loc[[True, True, True, False, False], :]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"pd.DataFrame({'Arizona': [4.1, 4.1, 4.0, 4.0, 4.0],\\n\", \" 'California': [5.0, 5.0, 5.0, 5.1, 5.1],\\n\", \" 'Florida': [3.7, 3.7, 3.7, 3.7, 3.7],\\n\", \" 'Illinois': [4.2, 4.2, 4.3, 4.3, 4.3],\\n\", \" 'Michigan': [3.3, 3.2, 3.2, 3.3, 3.5],\\n\", \" 'New York': [4.7, 4.7, 4.6, 4.6, 4.6],\\n\", \" 'Texas': [4.6, 4.6, 4.5, 4.4, 4.3]})\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df = unemp_small\\n\", \"df.loc[(df[\\\"California\\\"] < 5.1) & (df[\\\"Texas\\\"].isin([4.6, 4.3]))]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_small.to_dict(orient=\\\"list\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# second argument selects columns, the  ``:``  means \\\"all\\\".\\n\", \"# here we use it to select all columns\\n\", \"unemp_small.loc[[True, False, True, False, True], :]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# can use booleans to select both rows and columns\\n\", \"unemp_small.loc[[True, True, True, False, False], [True, False, False, False, False, True, True]]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Creating Boolean DataFrames/Series\\n\", \"\\n\", \"We can use [conditional statements](../python_fundamentals/control_flow.ipynb) to\\n\", \"construct Series of booleans from our data.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_small[\\\"Texas\\\"] < 4.5\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Once we have our Series of bools, we can use it to extract subsets of\\n\", \"rows from our DataFrame.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_small.loc[unemp_small[\\\"Texas\\\"] < 4.5, :]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_small[\\\"New York\\\"] > unemp_small[\\\"Texas\\\"]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"big_NY = unemp_small[\\\"New York\\\"] > unemp_small[\\\"Texas\\\"]\\n\", \"unemp_small.loc[big_NY]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Multiple Conditions\\n\", \"\\n\", \"In the boolean section of the [basics lecture](../python_fundamentals/basics.ipynb), we saw\\n\", \"that we can use the words `and` and `or` to combine multiple booleans into\\n\", \"a single bool.\\n\", \"\\n\", \"Recall:\\n\", \"\\n\", \"- `True and False -> False`  \\n\", \"- `True and True -> True`  \\n\", \"- `False and False -> False`  \\n\", \"- `True or False -> True`  \\n\", \"- `True or True -> True`  \\n\", \"- `False or False -> False`  \\n\", \"\\n\", \"\\n\", \"We can do something similar in pandas, but instead of\\n\", \"`bool1 and bool2` we write:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"```python\\n\", \"(bool_series1) & (bool_series2)\\n\", \"```\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"Likewise, instead of `bool1 or bool2` we write:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"```python\\n\", \"(bool_series1) | (bool_series2)\\n\", \"```\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"small_NYTX = (unemp_small[\\\"Texas\\\"] < 4.7) & (unemp_small[\\\"New York\\\"] < 4.7)\\n\", \"small_NYTX\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_small[small_NYTX]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### `isin`\\n\", \"\\n\", \"Sometimes, we will want to check whether a data point takes on one of a\\n\", \"several fixed values.\\n\", \"\\n\", \"We could do this by writing `(df[\\\"x\\\"] == val_1) | (df[\\\"x\\\"] == val_2)`\\n\", \"(like we did above), but there is a better way: the `.isin` method\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"unemp_small[\\\"Michigan\\\"].isin([3.3, 3.2])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# now select full rows where this Series is True\\n\", \"unemp_small.loc[unemp_small[\\\"Michigan\\\"].isin([3.3, 3.2])]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### `.any` and `.all`\\n\", \"\\n\", \"Recall from the boolean section of the [basics lecture](../python_fundamentals/basics.ipynb)\\n\", \"that the Python functions `any` and `all` are aggregation functions that\\n\", \"take a collection of booleans and return a single boolean.\\n\", \"\\n\", \"`any` returns True whenever at least one of the inputs are True while\\n\", \"`all` is True only when all the inputs are `True`.\\n\", \"\\n\", \"Series and DataFrames with `dtype` bool have `.any` and `.all`\\n\", \"methods that apply this logic to pandas objects.\\n\", \"\\n\", \"Let’s use these methods to count how many months all the states in our\\n\", \"sample had high unemployment.\\n\", \"\\n\", \"As we work through this example, consider the [“want\\n\", \"operator”](http://albertjmenkveld.com/2014/07/07/endogeneous-price-dispersion/), a helpful\\n\", \"concept we learned from [Tom\\n\", \"Sargent](http://www.tomsargent.com) for clearly stating the goal of our analysis and\\n\", \"determining the steps necessary to reach the goal.\\n\", \"\\n\", \"We always begin by writing `Want:` followed by what we want to\\n\", \"accomplish.\\n\", \"\\n\", \"In this case, we would write:\\n\", \"\\n\", \"> Want: Count the number of months in which all states in our sample\\n\", \"had unemployment above 6.5%\\n\", \"\\n\", \"\\n\", \"After identifying the **want**, we work *backwards* to identify the\\n\", \"steps necessary to accomplish our goal.\\n\", \"\\n\", \"So, starting from the result, we have:\\n\", \"\\n\", \"1. Sum the number of `True` values in a Series indicating dates for\\n\", \"  which all states had high unemployment.  \\n\", \"1. Build the Series used in the last step by using the `.all` method\\n\", \"  on a DataFrame containing booleans indicating whether each state had\\n\", \"  high unemployment at each date.  \\n\", \"1. Build the DataFrame used in the previous step using a `>`\\n\", \"  comparison.  \\n\", \"\\n\", \"\\n\", \"Now that we have a clear plan, let’s follow through and *apply* the want\\n\", \"operator:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Step 3: construct the DataFrame of bools\\n\", \"high = unemp > 6.5\\n\", \"high.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Step 2: use the .all method on axis=1 to get the dates where all states have a True\\n\", \"all_high = high.all(axis=1)\\n\", \"all_high.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Step 1: Call .sum to add up the number of True values in `all_high`\\n\", \"#         (note that True == 1 and False == 0 in Python, so .sum will count Trues)\\n\", \"msg = \\\"Out of {} months, {} had high unemployment across all states\\\"\\n\", \"print(msg.format(len(all_high), all_high.sum()))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"date\": 1595352471.665684, \"title\": \"Basic Functionality\", \"filename\": \"basics.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"download_nb\": false, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"pandas/basics\"}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v02_pandas_basics.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.784Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 5,
    "position": 0,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 6,
    "description": "Pandas Introduction",
    "title": "Lecture 01",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 80,
    "position": 9,
    "content_id": 185,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 185,
    "type": "notebook",
    "description": "Learn the core datatypes of pandas: DataFrame and Series",
    "title": "v01_pandas_intro.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Introduction\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Python Fundamentals](https://datascience.quantecon.org/python_fundamentals/index.html) (Summer pre course)\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand the core pandas objects (`Series` and `DataFrame`)  \\n\", \"- Index into particular elements of a Series and DataFrame  \\n\", \"- Understand what `.dtype`/`.dtypes` do  \\n\", \"- Make basic visualizations  \\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- US regional unemployment data from Bureau of Labor Statistics  \\n\", \"\\n\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"1+1\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Introduction](#Introduction)  \\n\", \"  - [pandas](#pandas)  \\n\", \"  - [Series](#Series)  \\n\", \"  - [DataFrame](#DataFrame)  \\n\", \"  - [Data Types](#Data-Types)  \\n\", \"  - [Changing DataFrames](#Changing-DataFrames)  \\n\", \"  - [Exercises](#Exercises)  \"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"# Uncomment following line to install on colab\\n\", \"#! pip install qeds\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## pandas\\n\", \"\\n\", \"This lecture begins the material on `pandas`.\\n\", \"\\n\", \"To start, we will import the pandas package and give it the alias\\n\", \"`pd`, which is conventional practice.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"import pandas as pd\\n\", \"\\n\", \"# Don't worry about this line for now!\\n\", \"%matplotlib inline\\n\", \"# activate plot theme\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Sometimes, knowing which pandas version we are\\n\", \"using is helpful.\\n\", \"\\n\", \"We can check this by running the code below.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"pd.__version__\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Series\\n\", \"\\n\", \"The first main pandas type we will introduce is called Series.\\n\", \"\\n\", \"A Series is a single column of data, with row labels for each\\n\", \"observation.\\n\", \"\\n\", \"pandas refers to the row labels as the *index* of the Series.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/assets/_static/intro_files/PandasSeries.png\\\" alt=\\\"PandasSeries.png\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \\n\", \"Below, we create a Series which contains the US unemployment rate every\\n\", \"other year starting in 1995.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"values = [5.6, 5.3, 4.3, 4.2, 5.8, 5.3, 4.6, 7.8, 9.1, 8., 5.7]\\n\", \"years = list(range(1995, 2017, 2))\\n\", \"\\n\", \"unemp = pd.Series(data=values, index=years, name=\\\"Unemployment\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can look at the index and values in our Series.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp.index\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp.values\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### What Can We Do with a Series object?\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"#### `.head` and `.tail`\\n\", \"\\n\", \"Often, our data will have many rows, and we won’t want to display it all\\n\", \"at once.\\n\", \"\\n\", \"The methods `.head` and `.tail` show rows at the beginning and end\\n\", \"of our Series, respectively.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp.tail()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Basic Plotting\\n\", \"\\n\", \"We can also plot data using the `.plot` method.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp.plot()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\">**Note**\\n\", \">\\n\", \">This is why we needed the `%matplotlib inline` — it tells the notebook\\n\", \"to display figures inside the notebook itself. Also, pandas has much greater visualization functionality than this, but we will study that later on.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"#### Unique Values\\n\", \"\\n\", \"Though it doesn’t make sense in this data set, we may want to find the\\n\", \"unique values in a Series – which can be done with the `.unique` method.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp.unique()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Indexing\\n\", \"\\n\", \"Sometimes, we will want to select particular elements from a Series.\\n\", \"\\n\", \"We can do this using `.loc[index_items]`; where `index_items` is\\n\", \"an item from the index, or a list of items in the index.\\n\", \"\\n\", \"We will see this more in-depth in a coming lecture, but for now, we\\n\", \"demonstrate how to select one or multiple elements of the Series.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp.loc[1995]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp.loc[[1995, 2005, 2015]]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-0'></a>\\n\", \"> See exercise 1 in the [*exercise list*](#exerciselist-0)\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"## DataFrame\\n\", \"\\n\", \"A DataFrame is how pandas stores one or more columns of data.\\n\", \"\\n\", \"We can think a DataFrames a multiple Series stacked side by side as\\n\", \"columns.\\n\", \"\\n\", \"This is similar to a sheet in an Excel workbook or a table in a SQL\\n\", \"database.\\n\", \"\\n\", \"In addition to row labels (an index), DataFrames also have column labels.\\n\", \"\\n\", \"We refer to these column labels as the columns or column names.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/assets/_static/intro_files/PandasDataFrame.png\\\" alt=\\\"PandasDataFrame.png\\\" style=\\\"\\\">\\n\", \"\\n\", \"Below, we create a DataFrame that contains the unemployment rate every\\n\", \"other year by region of the US starting in 1995.\\n\", \"\\n\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"data = {\\n\", \"    \\\"NorthEast\\\": [5.9,  5.6,  4.4,  3.8,  5.8,  4.9,  4.3,  7.1,  8.3,  7.9,  5.7],\\n\", \"    \\\"MidWest\\\": [4.5,  4.3,  3.6,  4. ,  5.7,  5.7,  4.9,  8.1,  8.7,  7.4,  5.1],\\n\", \"    \\\"South\\\": [5.3,  5.2,  4.2,  4. ,  5.7,  5.2,  4.3,  7.6,  9.1,  7.4,  5.5],\\n\", \"    \\\"West\\\": [6.6, 6., 5.2, 4.6, 6.5, 5.5, 4.5, 8.6, 10.7, 8.5, 6.1],\\n\", \"    \\\"National\\\": [5.6, 5.3, 4.3, 4.2, 5.8, 5.3, 4.6, 7.8, 9.1, 8., 5.7]\\n\", \"}\\n\", \"\\n\", \"unemp_region = pd.DataFrame(data, index=years)\\n\", \"unemp_region\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can retrieve the index and the DataFrame values as we\\n\", \"did with a Series.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp_region.index\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.values\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### What Can We Do with a DataFrame?\\n\", \"\\n\", \"Pretty much everything we can do with a Series.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"#### `.head` and `.tail`\\n\", \"\\n\", \"As with Series, we can use `.head` and `.tail` to show only the\\n\", \"first or last `n` rows.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp_region.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.tail(3)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Plotting\\n\", \"\\n\", \"We can generate plots with the `.plot` method.\\n\", \"\\n\", \"Notice we now have a separate line for each column of data.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp_region.plot()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Indexing\\n\", \"\\n\", \"We can also do indexing using `.loc`.\\n\", \"\\n\", \"This is slightly more advanced than before because we can choose\\n\", \"subsets of both row and columns.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp_region.loc[1995, \\\"NorthEast\\\"]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.loc[[1995, 2005], \\\"South\\\"]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.loc[1995, [\\\"NorthEast\\\", \\\"National\\\"]]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.loc[:, \\\"NorthEast\\\"]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# `[string]` with no `.loc` extracts a whole column\\n\", \"unemp_region[\\\"MidWest\\\"]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Computations with Columns\\n\", \"\\n\", \"pandas can do various computations and mathematical operations on\\n\", \"columns.\\n\", \"\\n\", \"Let’s take a look at a few of them.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"# Divide by 100 to move from percent units to a rate\\n\", \"unemp_region[\\\"West\\\"] / 100\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Find maximum\\n\", \"unemp_region[\\\"West\\\"].max()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Find the difference between two columns\\n\", \"# Notice that pandas applies `-` to _all rows_ at once\\n\", \"# We'll see more of this throughout these materials\\n\", \"unemp_region[\\\"West\\\"] - unemp_region[\\\"MidWest\\\"]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Find correlation between two columns\\n\", \"unemp_region.West.corr(unemp_region[\\\"MidWest\\\"])\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# find correlation between all column pairs\\n\", \"unemp_region.corr()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-1'></a>\\n\", \"> See exercise 2 in the [*exercise list*](#exerciselist-0)\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"## Data Types\\n\", \"\\n\", \"We asked you to run the commands `unemp.dtype` and\\n\", \"`unemp_region.dtypes` and think about the outputs.\\n\", \"\\n\", \"You might have guessed that they return the type of the values inside\\n\", \"each column.\\n\", \"\\n\", \"Occasionally, you might need to investigate what types you have in your\\n\", \"DataFrame when an operation isn’t behaving as expected.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp.dtype\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.dtypes\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"DataFrames will only distinguish between a few types.\\n\", \"\\n\", \"- Booleans (`bool`)  \\n\", \"- Floating point numbers (`float64`)  \\n\", \"- Integers (`int64`)  \\n\", \"- Dates (`datetime`) — we will learn this soon  \\n\", \"- Categorical data (`categorical`)  \\n\", \"- Everything else, including strings (`object`)  \\n\", \"\\n\", \"\\n\", \"In the future, we will often refer to the type of data stored in a\\n\", \"column as its `dtype`.\\n\", \"\\n\", \"Let’s look at an example for when having an incorrect `dtype` can\\n\", \"cause problems.\\n\", \"\\n\", \"Suppose that when we imported the data the `South` column was\\n\", \"interpreted as a string.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"str_unemp = unemp_region.copy()\\n\", \"str_unemp[\\\"South\\\"] = str_unemp[\\\"South\\\"].astype(str)\\n\", \"str_unemp.dtypes\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Everything *looks* ok…\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"str_unemp.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"But if we try to do something like compute the sum of all the columns,\\n\", \"we get unexpected results…\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"str_unemp.sum()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This happened because `.sum` effectively calls `+` on all rows in\\n\", \"each column.\\n\", \"\\n\", \"Recall that when we apply `+` to two strings, the result is the two\\n\", \"strings concatenated.\\n\", \"\\n\", \"So, in this case, we saw that the entries in all rows of the South\\n\", \"column were stitched together into one long string.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"## Changing DataFrames\\n\", \"\\n\", \"We can change the data inside of a DataFrame in various ways:\\n\", \"\\n\", \"- Adding new columns  \\n\", \"- Changing index labels or column names  \\n\", \"- Altering existing data (e.g. doing some arithmetic or making a column\\n\", \"  of strings lowercase)  \\n\", \"\\n\", \"\\n\", \"Some of these “mutations” will be topics of future lectures, so we will\\n\", \"only briefly discuss a few of the things we can do below.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"### Creating New Columns\\n\", \"\\n\", \"We can create new data by assigning values to a column similar to how\\n\", \"we assign values to a variable.\\n\", \"\\n\", \"In pandas, we create a new column of a DataFrame by writing:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"```python\\n\", \"df[\\\"New Column Name\\\"] = new_values\\n\", \"```\\n\"], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"Below, we create an unweighted mean of the unemployment rate across the\\n\", \"four regions of the US — notice that this differs from the national\\n\", \"unemployment rate.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp_region[\\\"UnweightedMean\\\"] = (unemp_region[\\\"NorthEast\\\"] +\\n\", \"                                  unemp_region[\\\"MidWest\\\"] +\\n\", \"                                  unemp_region[\\\"South\\\"] +\\n\", \"                                  unemp_region[\\\"West\\\"])/4\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Changing Values\\n\", \"\\n\", \"Changing the values inside of a DataFrame should be done sparingly.\\n\", \"\\n\", \"However, it can be done by assigning a value to a location in the\\n\", \"DataFrame.\\n\", \"\\n\", \"`df.loc[index, column] = value`\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"unemp_region.loc[1995, \\\"UnweightedMean\\\"] = 0.0\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Renaming Columns\\n\", \"\\n\", \"We can also rename the columns of a DataFrame, which is helpful because the names that sometimes come with datasets are\\n\", \"unbearable…\\n\", \"\\n\", \"For example, the original name for the North East unemployment rate\\n\", \"given by the Bureau of Labor Statistics was `LASRD910000000000003`…\\n\", \"\\n\", \"They have their reasons for using these names, but it can make our job\\n\", \"difficult since we often need to type it repeatedly.\\n\", \"\\n\", \"We can rename columns by passing a dictionary to the `rename` method.\\n\", \"\\n\", \"This dictionary contains the old names as the keys and new names as the\\n\", \"values.\\n\", \"\\n\", \"See the example below.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [\"names = {\\\"NorthEast\\\": \\\"NE\\\",\\n\", \"         \\\"MidWest\\\": \\\"MW\\\",\\n\", \"         \\\"South\\\": \\\"S\\\",\\n\", \"         \\\"West\\\": \\\"W\\\"}\\n\", \"unemp_region.rename(columns=names)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"unemp_region.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We renamed our columns… Why does the DataFrame still show the old\\n\", \"column names?\\n\", \"\\n\", \"Many pandas operations create a copy of your data by\\n\", \"default to protect your data and prevent you from overwriting\\n\", \"information you meant to keep.\\n\", \"\\n\", \"We can make these operations permanent by either:\\n\", \"\\n\", \"1. Assigning the output back to the variable name\\n\", \"  `df = df.rename(columns=rename_dict)`  \\n\", \"1. Looking into whether the method has an `inplace` option. For\\n\", \"  example, `df.rename(columns=rename_dict, inplace=True)`  \\n\", \"\\n\", \"\\n\", \"Setting `inplace=True` will sometimes make your code faster\\n\", \"(e.g. if you have a very large DataFrame and you don’t want to copy all\\n\", \"the data), but that doesn’t always happen.\\n\", \"\\n\", \"We recommend using the first option until you get comfortable with\\n\", \"pandas because operations that don’t alter your data are (usually)\\n\", \"safer.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"names = {\\\"NorthEast\\\": \\\"NE\\\",\\n\", \"         \\\"MidWest\\\": \\\"MW\\\",\\n\", \"         \\\"South\\\": \\\"S\\\",\\n\", \"         \\\"West\\\": \\\"W\\\"}\\n\", \"\\n\", \"unemp_shortname = unemp_region.rename(columns=names)\\n\", \"unemp_shortname.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Exercises\\n\", \"\\n\", \"\\n\", \"<a id='exerciselist-0'></a>\\n\", \"**Exercise 1**\\n\", \"\\n\", \"For each of the following exercises, we recommend reading the documentation\\n\", \"for help.\\n\", \"\\n\", \"- Display only the first 2 elements of the Series using the `.head` method.  \\n\", \"- Using the `plot` method, make a bar plot.  \\n\", \"- Use `.loc` to select the lowest/highest unemployment rate shown in the Series.  \\n\", \"- Run the code `unemp.dtype` below. What does it give you? Where do you think it comes from?  \\n\", \"\\n\", \"\\n\", \"([*back to text*](#exercise-0))\\n\", \"\\n\", \"**Exercise 2**\\n\", \"\\n\", \"For each of the following, we recommend reading the documentation for help.\\n\", \"\\n\", \"- Use introspection (or google-fu) to find a way to obtain a list with\\n\", \"  all of the column names in `unemp_region`.  \\n\", \"- Using the `plot` method, make a bar plot. What does it look like\\n\", \"  now?  \\n\", \"- Use `.loc` to select the the unemployment data for the\\n\", \"  `NorthEast` and `West` for the years 1995, 2005, 2011, and 2015.  \\n\", \"- Run the code `unemp_region.dtypes` below. What does it give you?\\n\", \"  How does this compare with `unemp.dtype`?  \\n\", \"\\n\", \"\\n\", \"([*back to text*](#exercise-1))\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"date\": 1595352472.360632, \"title\": \"Introduction\", \"filename\": \"intro.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"download_nb\": false, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"pandas/intro\"}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v01_pandas_intro.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.784Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 5,
    "position": 0,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 6,
    "description": "Pandas Introduction",
    "title": "Lecture 01",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 79,
    "position": 9,
    "content_id": 157,
    "lecture_id": 6,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 157,
    "type": "video",
    "description": "Learn the core datatypes of pandas: DataFrame and Series",
    "title": "Introduction to Pandas",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=rRR3oYHG_3U\", \"youtubeVideoId\": \"rRR3oYHG_3U\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 7,
    "position": 1,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 8,
    "description": "Organizing Data in Pandas",
    "title": "Lecture 02",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 86,
    "position": 8,
    "content_id": 147,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 147,
    "type": "notebook",
    "description": "Learn about common data storage formats and how to interact with them in pandas",
    "title": "02_storage_formats.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Storage Formats\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Intro to DataFrames and Series](../p01_pandas_intro/v01_pandas_intro.ipynb)\\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand that data can be saved in various formats  \\n\", \"- Know where to get help on file input and output  \\n\", \"- Know when to use csv, xlsx, feather, and sql formats  \\n\", \"\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- Results for all NFL games between September 1920 to February 2017  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Storage Formats](#Storage-Formats)  \\n\", \"  - [File Formats](#File-Formats)  \\n\", \"  - [Writing DataFrames](#Writing-DataFrames)  \\n\", \"  - [Reading Files into DataFrames](#Reading-Files-into-DataFrames)  \\n\", \"  - [Practice](#Practice)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Uncomment following line to install on colab\\n\", \"#! pip install qeds\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"import pandas as pd\\n\", \"import numpy as np\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## File Formats\\n\", \"\\n\", \"Data can be saved in a variety of formats\\n\", \"\\n\", \"pandas understands how to write and read DataFrames to and from many of\\n\", \"these formats\\n\", \"\\n\", \"We defer to the [official\\n\", \"documentation](https://pandas.pydata.org/pandas-docs/stable/io.html)\\n\", \"for a full description of how to interact with all the file formats, but\\n\", \"will briefly discuss a few of them here\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## CSV\\n\", \"\\n\", \"**What is it?** CSVs store data as plain text (strings) where each row is a\\n\", \"line and columns are separated by `,`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### CSV: Pros\\n\", \"\\n\", \"- Widely used (you should be familiar with it)  \\n\", \"- Plain text file (can open on any computer, “future proof”)  \\n\", \"- Can be read from and written to by most data software  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### CSV: Cons\\n\", \"\\n\", \"- Not the most efficient way to store or access  \\n\", \"- No formal standard, so there is room for user interpretation on how to\\n\", \"  handle edge cases (e.g. what to do about a data field that itself includes\\n\", \"  a comma)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### CSV: When to use\\n\", \"\\n\", \"- A great default option for most use cases  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## xlsx\\n\", \"\\n\", \"**What is it?** xlsx is a binary file format used as Excel’s default.\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### xlsx: Pros\\n\", \"\\n\", \"- Standard format in many industries  \\n\", \"- Easy to share with colleagues that use Excel  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### xlsx: Cons\\n\", \"\\n\", \"- Quite slow to read/write large amounts of data  \\n\", \"- Stores both data and *metadata* like styling and display information\\n\", \"  and even plots. This metadata is not always portable to other file formats\\n\", \"  or programs.  \\n\", \"- Non-human readable\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### xlsx: When to use\\n\", \"\\n\", \"**When to use**:\\n\", \"\\n\", \"- When sharing data with Excel  \\n\", \"- When you would like special formatting to be applied to the\\n\", \"  spreadsheet when viewed in Excel  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Parquet\\n\", \"\\n\", \"**What is it?** Parquet is a custom binary format designed for efficient reading and\\n\", \"writing of data stored in columns.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Parquet: pros\\n\", \"\\n\", \"- *Very* fast  \\n\", \"- Naturally understands all `dtypes` used by pandas, including\\n\", \"  multi-index DataFrames  \\n\", \"- Very common in “big data” systems like Hadoop or Spark  \\n\", \"- Supports various compression algorithms  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Parquet: Cons\\n\", \"\\n\", \"- Binary storage format that is not human-readable  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Parquet: When to use\\n\", \"\\n\", \"- If you have “not small” amounts (> 100 MB) of unchanging data that\\n\", \"  you want to read quickly  \\n\", \"- If you want to store data in an size-and-time-efficient way that may\\n\", \"  be accessed by external systems  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Feather\\n\", \"\\n\", \"**What is it?** Feather is a custom binary format designed for efficient reading and\\n\", \"writing of data stored in columns.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Feather: Pros\\n\", \"\\n\", \"- *Very* fast – even faster than parquet  \\n\", \"- Naturally understands all `dtypes` used by pandas  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Feather: Cons\\n\", \"\\n\", \"- Can only read and write from Python and a handful of other\\n\", \"  programming languages  \\n\", \"- New file format (introduced in March ‘16), so most files don’t come\\n\", \"  in this format  \\n\", \"- Only supports standard pandas index, so you need to `reset_index`\\n\", \"  before saving and then `set_index` after loading  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Feather: When to use\\n\", \"\\n\", \"- Use as an alternative to Parquet if you need the absolute best read and write\\n\", \"  speeds for unchanging datasets  \\n\", \"- Only use when you will not need to access the data in a programming language\\n\", \"  or software outside of Python, R, and Julia  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## SQL\\n\", \"\\n\", \"**What is it?** SQL is a language used to interact with relational\\n\", \"databases… [more info](https://en.wikipedia.org/wiki/SQL)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### SQL: Pros\\n\", \"\\n\", \"- Well established industry standard for handling data  \\n\", \"- Much of the world’s data is in a SQL database somewhere  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### SQL: Cons\\n\", \"\\n\", \"- Complicated: to have full control you need to learn another language\\n\", \"  (SQL)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### SQL: When to use\\n\", \"\\n\", \"- When reading from or writing to existing SQL databases  \\n\", \"\\n\", \"\\n\", \"**NOTE**: We will likely cover interacting with SQL databases in a later lecture\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## JSON\\n\", \"\\n\", \"TODO: check what the S means, might just be \\\"script\\\"\\n\", \"\\n\", \"**What is is?** JSON is an acronym for \\\"Javascript serialized object notation\\\". It is a very common way to store data, especially when interacting with web services.\\n\", \"\\n\", \"Data is stored almost how we would write a Python `dict` by hand using `{` and `}` to provide  key, value pairs\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### JSON: Pros\\n\", \"\\n\", \"- Very commonly used when interacting with websites and web apis\\n\", \"- Maps naturally into a Python dict\\n\", \"- Human readable\\n\", \"- Can store non-tabular data (each record or row can have different keys)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### JSON: Cons\\n\", \"\\n\", \"- Relatively inefficient when stored as an array of records (must repeat column names for every row)\\n\", \"- Non-binary, so can be slow to read and write\\n\", \"- Ambiguous for how to represent certain data (array of records, record of arrays by column, record of arrays by row, etc.)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### JSON: When to use\\n\", \"\\n\", \"- When recieving data from or preparing data for web APIs\\n\", \"- When you want to store relatively small amounts of non or semi-structured data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Writing DataFrames\\n\", \"\\n\", \"Let’s now talk about saving a DataFrame to a file.\\n\", \"\\n\", \"**Rule of thumb**: To save `df` to a file of type `FOO` use `df.to_FOO`\\n\", \"\\n\", \"We'll test this out for a few of the data formats above using some artificial data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Note**: by default `df2` will be approximately 10 MB. You can change this by adjusting `wanted_mb` in the cell below\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"np.random.seed(42)  # makes sure we get the same random numbers each time\\n\", \"df1 = pd.DataFrame(\\n\", \"    np.random.randint(0, 100, size=(10, 4)),\\n\", \"    columns=[\\\"a\\\", \\\"b\\\", \\\"c\\\", \\\"d\\\"]\\n\", \")\\n\", \"\\n\", \"wanted_mb = 10  # CHANGE THIS LINE\\n\", \"nrow = 100000\\n\", \"ncol = int(((wanted_mb * 1024**2) / 8) / nrow)\\n\", \"df2 = pd.DataFrame(\\n\", \"    np.random.rand(nrow, ncol),\\n\", \"    columns=[\\\"x{}\\\".format(i) for i in range(ncol)]\\n\", \")\\n\", \"\\n\", \"print(\\\"df2.shape = \\\", df2.shape)\\n\", \"print(\\\"df2 is approximately {} MB\\\".format(df2.memory_usage().sum() / (1024**2)))\"], \"outputs\": [], \"metadata\": {\"tags\": [], \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### [df.to_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html)\\n\", \"\\n\", \"Let’s start with `df.to_csv`\\n\", \"\\n\", \"Without any additional arguments, the `df.to_csv` function will return\\n\", \"a string containing the csv form of the DataFrame:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# notice the plain text format -- one row per line, columns separated by `'`\\n\", \"print(df1.to_csv())\"], \"outputs\": [], \"metadata\": {\"tags\": [], \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"If we do pass an argument, the first argument will be used as the file name.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df1.to_csv(\\\"df1.csv\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Run the cell below to verify that the file was created.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import os\\n\", \"os.path.isfile(\\\"df1.csv\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s see how long it takes to save `df2` to a file. (Because of the `%%time` at\\n\", \"the top, Jupyter will report the total time to run all code in\\n\", \"the cell)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%%time\\n\", \"df2.to_csv(\\\"df2.csv\\\")\"], \"outputs\": [], \"metadata\": {\"tags\": [], \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"As we will see below, this isn’t as fastest file format we could choose.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### [df.to_excel](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html)\\n\", \"\\n\", \"The `df.to_excel` method writes a DataFrame to an excel workbook\\n\", \"\\n\", \"The first argument is the name of the file\\n\", \"\\n\", \"The second argument is the name of the sheet in that file (this is optional, and is `Sheet1` by default)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df1.to_excel(\\\"df1.xlsx\\\", \\\"df1\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can write multiple DataFrames to a single workbook, eacn in a different sheet\\n\", \"\\n\", \"To do this we use `pd.ExcelWriter(filename)` and then pass the returned object instead of a file name to `df.to_excel`:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"with pd.ExcelWriter(\\\"df1.xlsx\\\") as writer:\\n\", \"    df1.to_excel(writer, \\\"df1\\\")\\n\", \"    (df1 + 10).to_excel(writer, \\\"df1 plus 10\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The\\n\", \"\\n\", \"```python\\n\", \"with ... as ... :\\n\", \"```\\n\", \"\\n\", \"syntax used above is an example of a *context manager*\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We don’t need to understand all the details behind what this means\\n\", \"(google it if you are curious)\\n\", \"\\n\", \"For our purposes we used it so that Python could:\\n\", \"\\n\", \"1. create the `df1.xlsx` file\\n\", \"2. Ensure that the file remains open while pandas writes the two DataFrames\\n\", \"3. Close the file when finished to finalize storing data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<p style=\\\"color:red;\\\">\\n\", \"\\n\", \"WARNING:\\n\", \"\\n\", \"</p>\\n\", \"\\n\", \"Saving `df2` to an excel file takes a very long time.\\n\", \"\\n\", \"For that reason, we will just show the code and hard-code the output\\n\", \"we saw when we ran the code.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%%time\\n\", \"df2.to_excel(\\\"df2.xlsx\\\")\"], \"outputs\": [], \"metadata\": {\"tags\": [], \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"```python\\n\", \" Wall time: 25.7 s\\n\", \"```\\n\"], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"### [pyarrow.feather.write_feather](https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html#pyarrow.feather.write_feather)\\n\", \"\\n\", \"As noted above, the feather file format was developed for very efficient\\n\", \"reading and writing between Python and your computer\\n\", \"\\n\", \"Support for this format is provided by a separate Python package called `pyarrow`\\n\", \"\\n\", \"This package is not installed by default\\n\", \"\\n\", \"If you do not have it, but would like to install `pyarrow`, uncomment and execute the cell below\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# %pip install pyarrow\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The parameters for `pyarrow.feather.write_feather` are the DataFrame and file name\\n\", \"\\n\", \"Let’s try it out\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pyarrow.feather\\n\", \"pyarrow.feather.write_feather(df1, \\\"df1.feather\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"%%time\\n\", \"pyarrow.feather.write_feather(df2, \\\"df2.feather\\\")\"], \"outputs\": [], \"metadata\": {\"tags\": [], \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"An example timing result:\\n\", \"\\n\", \"|format|time|\\n\", \"|:---------:|:----------------------:|\\n\", \"|csv|2.66 seconds|\\n\", \"|xlsx|25.7 seconds|\\n\", \"|feather|43 milliseconds|\\n\", \"\\n\", \"As you can see, saving this DataFrame in the feather format was far\\n\", \"faster than either CSV or Excel.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Reading Files into DataFrames\\n\", \"\\n\", \"As with the `df.to_FOO` family of methods, there are similar\\n\", \"`pd.read_FOO` functions. (Note: they are in defined pandas, not as\\n\", \"methods on a DataFrame.)\\n\", \"\\n\", \"These methods have many more options because data storage can be messy or wrong\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We will explore these in more detail in a separate lecture\\n\", \"\\n\", \"For now, we just want to highlight the differences in how to read data\\n\", \"from each of the file formats\\n\", \"\\n\", \"Let’s start by reading the files we just created to verify that they\\n\", \"match the data we began with\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# notice that index was specified in the first (0th -- why?) column of the file\\n\", \"df1_csv = pd.read_csv(\\\"df1.csv\\\", index_col=0)\\n\", \"df1_csv.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df1_xlsx = pd.read_excel(\\\"df1.xlsx\\\", \\\"df1\\\", index_col=0)\\n\", \"df1_xlsx.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# notice feather already knows what the index is\\n\", \"df1_feather = pyarrow.feather.read_feather(\\\"df1.feather\\\")\\n\", \"df1_feather.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"`pd.read_FOO`functions can read files stored online\\n\", \"\\n\", \"To do this, we pass a URL as the first argument:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df1_url = \\\"https://storage.googleapis.com/workshop_materials/df1.csv\\\"\\n\", \"df1_web = pd.read_csv(df1_url, index_col=0)\\n\", \"df1_web.head()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Practice\\n\", \"\\n\", \"Now it’s your turn…\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"In the cell below, the variable `url` contains a web address to a csv\\n\", \"file containing the result of all NFL games from September 1920 to\\n\", \"February 2017\\n\", \"\\n\", \"Your task is to do the following:\\n\", \"\\n\", \"- Use `pd.read_csv` to read this file into a DataFrame named `nfl`  \\n\", \"- Print the shape and column names of `nfl`  \\n\", \"- Save the DataFrame to a file named `nfl.xlsx`  \\n\", \"- Open the spreadsheet using Excel on your computer  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"If you finish quickly, do some basic analysis of the data. Try to do\\n\", \"something interesting. If you get stuck, here are some suggestions for\\n\", \"what to try:\\n\", \"\\n\", \"- Compute the average total points in each game (note, you will need to\\n\", \"  sum two of the columns to get total points).  \\n\", \"- Repeat the above calculation, but only for playoff games.  \\n\", \"- Compute the average score for your favorite team (you’ll need to\\n\", \"  consider when they were team1 vs team2).  \\n\", \"- Compute the ratio of “upsets” to total games played. An upset is\\n\", \"  defined as a team with a lower ELO winning the game.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://raw.githubusercontent.com/fivethirtyeight/nfl-elo-game/\\\"\\n\", \"url = url + \\\"3488b7d0b46c5f6583679bc40fb3a42d729abd39/data/nfl_games.csv\\\"\\n\", \"\\n\", \"# your code here --- create more cells if necessary\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Cleanup\\n\", \"\\n\", \"If you want to remove the files we just created, run the following cell.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def try_remove(file):\\n\", \"    if os.path.isfile(file):\\n\", \"        os.remove(file)\\n\", \"\\n\", \"for df in [\\\"df1\\\", \\\"df2\\\"]:\\n\", \"    for extension in [\\\"csv\\\", \\\"feather\\\", \\\"xlsx\\\"]:\\n\", \"        filename = df + \\\".\\\" + extension\\n\", \"        try_remove(filename)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"title\": \"Storage Formats\", \"filename\": \"storage_formats.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"02_storage_formats.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.785Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 7,
    "position": 1,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 8,
    "description": "Organizing Data in Pandas",
    "title": "Lecture 02",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 84,
    "position": 8,
    "content_id": 143,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 143,
    "type": "notebook",
    "description": "Learn about the index of a DataFrame or Series",
    "title": "Pandas index",
    "nb_content": "{\"cells\": [{\"source\": [\"# The Index\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Introduction to pandas](intro.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand how the index is used to align data  \\n\", \"- Know how to set and reset the index  \\n\", \"- Understand how to select subsets of data by slicing on index and columns  \\n\", \"- Understand that for DataFrames, the column names also align data  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [The Index](#The-Index)  \\n\", \"  - [So What is this Index?](#So-What-is-this-Index?)  \\n\", \"  - [Setting the Index](#Setting-the-Index)  \\n\", \"  - [Re-setting the Index](#Re-setting-the-Index)  \\n\", \"  - [Choose the Index Carefully](#Choose-the-Index-Carefully)  \\n\", \"  - [Exercises](#Exercises)  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"try:\\n\", \"    import pyodide_http\\n\", \"    pyodide_http.patch_all()\\n\", \"except:\\n\", \"    print(\\\"Not in pyodide, continuing\\\")\\n\", \"\\n\", \"import pandas as pd\\n\", \"import numpy as np\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## So What is this Index?\\n\", \"\\n\", \"Every Series and DataFrame has an index\\n\", \"\\n\", \"We told you that the index was the “row labels” for the data\\n\", \"\\n\", \"This is true, but an index in pandas does much more than label the rows\\n\", \"\\n\", \"The purpose of this lecture is to understand the importance of the index\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The [pandas\\n\", \"documentation](https://pandas.pydata.org/pandas-docs/stable/dsintro.html)\\n\", \"says\\n\", \"\\n\", \"> Data alignment is intrinsic. The link between labels and data will\\n\", \"not be broken unless done so explicitly by you.\\n\", \"\\n\", \"\\n\", \"In practice, the index and column names are used to make sure the data is\\n\", \"properly aligned when operating on multiple DataFrames\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"This is a somewhat abstract concept that is best understood by\\n\", \"example…\\n\", \"\\n\", \"Let’s begin by loading some data on GDP components that we collected from\\n\", \"the World Bank’s World Development Indicators Dataset\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://datascience.quantecon.org/assets/data/wdi_data.csv\\\"\\n\", \"df = pd.read_csv(url)\\n\", \"df.info()\\n\", \"\\n\", \"# Note data is in trillions USD\\n\", \"df.head()\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We’ll also extract a couple smaller DataFrames we can use in examples\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_small = df.head(5)\\n\", \"df_small\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df_tiny = df.iloc[[0, 3, 2, 4], :]\\n\", \"df_tiny\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"im_ex = df_small[[\\\"Imports\\\", \\\"Exports\\\"]]\\n\", \"im_ex_copy = im_ex.copy()\\n\", \"im_ex_copy\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Observe what happens when we evaluate `im_ex + im_ex_copy`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"im_ex + im_ex_copy\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that this operated *elementwise*, meaning that the `+`\\n\", \"operation was applied to each element of `im_ex` and the corresponding\\n\", \"element of `im_ex_copy`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let’s take a closer look at `df_tiny`:\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_tiny\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Relative to `im_ex` notice a few things:\\n\", \"\\n\", \"- The row labeled `1` appears in `im_ex` but not `df_tiny` \\n\", \"- Some row labels appear in both, but they are not always in the same position\\n\", \"  within each DataFrame\\n\", \"- Certain columns appear only in `df_tiny`\\n\", \"- The `Imports` and `Exports` columns are the 6th and 5th columns of\\n\", \"  `df_tiny` and the 1st and 2nd of `im_ex`, respectively\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Now, let’s see what happens when we try `df_tiny + im_ex`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"im_ex_tiny = df_tiny + im_ex\\n\", \"im_ex_tiny\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Whoa, a lot happened! Let’s break it down piece by piece!\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Automatic Alignment\\n\", \"\\n\", \"For all (row, column) combinations that appear in both DataFrames (e.g.\\n\", \"rows `[1, 3]` and columns `[Imports, Exports]`), the value of `im_ex_tiny`\\n\", \"is equal to `df_tiny.loc[row, col] + im_ex.loc[row, col]`\\n\", \"\\n\", \"This happened even though the rows and columns were not in the same\\n\", \"order\\n\", \"\\n\", \"We refer to this as pandas *aligning* the data for us\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To see how awesome this is, think about how to do something similar in\\n\", \"Excel:\\n\", \"\\n\", \"- `df_tiny` and `im_ex` would be in different sheets\\n\", \"- The index and column names would be the first column and row in each\\n\", \"  sheet\\n\", \"- We would have a third sheet to hold the sum\\n\", \"- For each label in the first row and column of *either* the `df_tiny`\\n\", \"  sheet or the `im_ex` sheet we would have to do a `IFELSE` to check\\n\", \"  if the label exists in the other sheet and then a `VLOOKUP` to\\n\", \"  extract the value\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"In pandas, this happens automatically, behind the scenes, and *very\\n\", \"quickly*\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Handling Missing Data\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"im_ex_tiny = df_tiny + im_ex\\n\", \"im_ex_tiny\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"For all elements in row `1` or columns\\n\", \"`[\\\"country\\\", \\\"year\\\", \\\"GovExpend\\\", \\\"Consumption\\\", \\\"GDP\\\"]`,\\n\", \"the value in `im_ex_tiny` is `NaN`\\n\", \"\\n\", \"This is how pandas represents *missing data*\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"When pandas was trying to look up the values in `df_tiny` and `im_ex`, it could\\n\", \"only find a value in one DataFrame: the other value was missing\\n\", \"\\n\", \"When pandas tries to add a number to something that is missing, it says\\n\", \"that the result is missing (spelled `NaN`)\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-0'></a>\\n\", \"**Exercise 1**\\n\", \"\\n\", \"What happens when you apply the `mean` method to `im_ex_tiny`?\\n\", \"\\n\", \"In particular, what happens to columns that have missing data? (HINT:\\n\", \"also looking at the output of the `sum` method might help)\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Setting the Index\\n\", \"\\n\", \"For a DataFrame `df`, the `df.set_index` method allows us to use one\\n\", \"(or more) of the DataFrame’s columns as the index.\\n\", \"\\n\", \"Here’s an example.\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year = df.set_index([\\\"year\\\"])\\n\", \"df_year.head()\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now that the year is on the index, we can use `.loc` to extract all the\\n\", \"data for a specific year.\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year.loc[2010]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This would be helpful, for example, if we wanted to compute the difference\\n\", \"in the average of all our variables from one year to the next:\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year.loc[2009].mean() - df_year.loc[2008].mean()\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that pandas did a few things for us.\\n\", \"\\n\", \"- After computing `.mean()`, the index was set to the former column names\\n\", \"- These column names were used to align data when we wanted asked pandas to\\n\", \"  compute the difference  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Suppose that someone asked you, “What was the GDP in the US in 2010?”\\n\", \"\\n\", \"To compute that statistic using `df_year` you might do something like this:\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year.loc[df_year[\\\"country\\\"] == \\\"United States\\\", \\\"GDP\\\"].loc[2010]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"That was a lot of work!\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"Now, suppose that after seeing you extract that data, your friend asks you\\n\", \"“What about GDP in Germany and the UK in 2010?”\\n\", \"\\n\", \"To answer that question, you might write\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year.loc[df_year[\\\"country\\\"].isin([\\\"United Kingdom\\\", \\\"Germany\\\"]), \\\"GDP\\\"].loc[2010]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that this code is similar to the code above, but now provides a result\\n\", \"that is ambiguous\\n\", \"\\n\", \"The two elements in the series both have with label 2010\\n\", \"\\n\", \"How do we know which is which?\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"We might think that the first value corresponds to the United Kingdom because\\n\", \"that is what we listed first in the call to `isin`, but we would be wrong!\\n\", \"\\n\", \"Let’s check.\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year.loc[2010]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Setting just the year as the index has one more potential issue: we will\\n\", \"get data alignment only on the year, which may not be sufficient\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To demonstrate this point, suppose now you are asked to use our WDI dataset\\n\", \"to compute an approximation for net exports and investment in in 2009\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"As a seasoned economist, you would remember the expenditure formula for GDP is\\n\", \"written\\n\", \"\\n\", \"$$\\n\", \"\\\\text{GDP} = \\\\text{Consumption} + \\\\text{Investment} + \\\\text{GovExpend} + \\\\text{Net Exports}\\n\", \"$$\\n\", \"\\n\", \"which we can rearrange to compute investment as a function of the variables in\\n\", \"our DataFrame…\\n\", \"\\n\", \"$$\\n\", \"\\\\text{Investment} = \\\\text{GDP} - \\\\text{Consumption} - \\\\text{GovExpend} - \\\\text{Net Exports}\\n\", \"$$\\n\", \"\\n\", \"Note that we can compute NetExports as `Exports - Imports`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"nx = df_year[\\\"Exports\\\"] - df_year[\\\"Imports\\\"]\\n\", \"nx.head(19)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now, suppose that we accidentally had a bug in our code that swapped\\n\", \"the data for Canada and Germany’s net exports in 2017.\\n\", \"\\n\", \">**Note**\\n\", \">\\n\", \">This example is contrived, but if you were getting unclean data from\\n\", \"some resource or doing more complicated operations, this type of mistake\\n\", \"becomes increasingly likely.\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# do the swapping\\n\", \"ca17 = nx.iloc[[0]]\\n\", \"g17 = nx.iloc[[18]]\\n\", \"nx.iloc[[0]] = g17\\n\", \"nx.iloc[[18]] = ca17\\n\", \"\\n\", \"nx.head(19)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that if we now add `nx` to the DataFrame and compute investment\\n\", \"pandas doesn’t complain.\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_year[\\\"NetExports\\\"] = nx\\n\", \"df_year[\\\"Investment\\\"] = df_year.eval(\\\"GDP - Consumption - GovExpend - NetExports\\\")\\n\", \"df_year.loc[2017, :]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"But, because we didn’t also have data alignment on the country, we would have overstated Canada’s investment by 281 billion USD and understated Germany’s by the\\n\", \"same amount.\\n\", \"\\n\", \"To make these types operation easier, we need to include both the year\\n\", \"and country in the index…\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Setting a Hierarchical Index\\n\", \"\\n\", \"Including multiple columns in the index is advantageous in some situations\\n\", \"\\n\", \"These situations might include:\\n\", \"\\n\", \"- When we need more than one piece of information (column) to identify an\\n\", \"  observation (as in the Germany and UK GDP example above)\\n\", \"- When we need data-alignment by more than one column  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To set multiple columns as the index, we pass a list of column\\n\", \"names to `set_index`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi = df.set_index([\\\"country\\\", \\\"year\\\"])\\n\", \"wdi.head(20)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that in the display above, the row labels seem to have two\\n\", \"*levels* now.\\n\", \"\\n\", \"The *outer* (or left-most) level is named `country` and the *inner* (or\\n\", \"right-most) level is named `year`.\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"When a DataFrame’s index has multiple levels, we (and the pandas documentation)\\n\", \"refer to the DataFrame as having a **hierarchical index**\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Slicing a Hierarchical Index\\n\", \"\\n\", \"Now, we can answer our friend’s questions in a much more straightforward way\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df = wdi.loc[([\\\"United Kingdom\\\", \\\"Germany\\\"], list(range(2010, 2015))), [\\\"GDP\\\", \\\"Consumption\\\"]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.loc[('Germany', [2010, 2014]), 'GDP']\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.loc[('Germany', [2010, 2014]), ['GDP']]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.GDP.loc[('Germany', [2010, 2014])]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.GDP.loc['Germany', [2010, 2014]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[(\\\"United States\\\", 2010), \\\"GDP\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[([\\\"United Kingdom\\\", \\\"Germany\\\"], 2010), \\\"GDP\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"As shown above, we can use `wdi.loc` to extract different slices of our\\n\", \"national accounts data\\n\", \"\\n\", \"The rules for using `.loc` with a hierarchically-indexed DataFrame are\\n\", \"similar to the ones we’ve learned for standard DataFrames, but they are a bit\\n\", \"more elaborate as we now have more structure to our data\\n\", \"\\n\", \"We will summarize the main rules, and then work through an exercise that\\n\", \"demonstrates each of them\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Slicing rules**\\n\", \"\\n\", \"When slicing data, using `list`s and `tuple`s lead to different results\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"A `tuple` in row slicing will be used to denote a single hierarchical\\n\", \"index and must include a value for each level \\n\", \"\\n\", \"**Example**: `(\\\"United States\\\", 2010)` in `wdi.loc[(\\\"United States\\\", 2010), \\\"GDP\\\"]`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"`list` in row slicing will be an “or” operation, where it chooses rows\\n\", \"based on whether the index value corresponds to *any* element of the list (e.g. `[\\\"United Kingdom\\\", \\\"Germany\\\"]` above)\\n\", \"\\n\", \"**Example**: `[\\\"United Kingdom\\\", \\\"Germany\\\"]` in `wdi.loc[([\\\"United Kingdom\\\", \\\"Germany\\\"], 2010), \\\"GDP\\\"]` above\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Row slicing examples**\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**1** `wdi.loc[\\\"United States\\\"]`: all rows where the *outer* most index value is\\n\", \"  equal to `United States`  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**2** `wdi.loc[(\\\"United States\\\", 2010)]`: all rows where the *outer-most* index value\\n\", \"  is equal to `\\\"United States` and the second level is equal to `2010`  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**3** `wdi.loc[[\\\"United States\\\", \\\"Canada\\\"]]`: all rows where the *outer-most* index is\\n\", \"  either `\\\"United States\\\"` or `\\\"Canada\\\"`  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**4** `wdi.loc[([\\\"United States\\\", \\\"Canada\\\"], [2010, 2011]), :]`: all rows where the\\n\", \"  *outer-most* index is either `\\\"United States` or `\\\"Canada\\\"` AND where the\\n\", \"  second level index is either `2010` or `2011`  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**5** `wdi.loc[[(\\\"United States\\\", 2010), (\\\"Canada\\\", 2011)], :]`: all rows where the the\\n\", \"  two hierarchical indices are either `(\\\"United States\\\", 2010)` or\\n\", \"  `(\\\"Canada\\\", 2011)`  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We can also restrict `.loc` to extract certain columns by doing:\\n\", \"\\n\", \"1. `wdi.loc[rows, GDP]`: return the rows specified by rows (see rules\\n\", \"  above) and only column named `GDP` (returned object will be a\\n\", \"  Series)  \\n\", \"1. `df.loc[rows, [\\\"GDP\\\", \\\"Consumption\\\"]]`: return the rows specified by rows\\n\", \"  (see rules above) and only columns `GDP` and `Consumption`  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-1'></a>\\n\", \"**Exercise 2**\\n\", \"\\n\", \"For each of the examples below do the following:\\n\", \"\\n\", \"- Determine which of the rules above applies\\n\", \"- Identify the `type` of the returned value  \\n\", \"- Explain why the slicing operation returned the data it did\\n\", \"\\n\", \"Write your answers\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi.loc[[\\\"United States\\\", \\\"Canada\\\"]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[([\\\"United States\\\", \\\"Canada\\\"], [2010, 2011, 2012]), :]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[\\\"United States\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[(\\\"United States\\\", 2010), [\\\"GDP\\\", \\\"Exports\\\"]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[(\\\"United States\\\", 2010)]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[[(\\\"United States\\\", 2010), (\\\"Canada\\\", 2015)]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[[\\\"United States\\\", \\\"Canada\\\"], \\\"GDP\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi.loc[\\\"United States\\\", \\\"GDP\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Alignment with `MultiIndex`\\n\", \"\\n\", \"The data alignment features we talked about above also apply to a\\n\", \"`MultiIndex` DataFrame\\n\", \"\\n\", \"The following exercise gives you a chance to experiment with this\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-2'></a>\\n\", \"**Exercise 3**\\n\", \"\\n\", \"Try setting `my_df` to some subset of the rows in `wdi` (use one of the\\n\", \"`.loc` variations above)\\n\", \"\\n\", \"Then see what happens when you do `wdi / my_df` or `my_df ** wdi`\\n\", \"\\n\", \"Try changing the subset of rows in `my_df` and repeat until you\\n\", \"understand what is happening\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `pd.IndexSlice`\\n\", \"\\n\", \"When we want to extract rows for a few values of the outer index and all\\n\", \"values for an inner index level, we can use the convenient\\n\", \"`df.loc[[id11, id22]]` shorthand\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"For example, we can use this notation to extract all the data for the United States and\\n\", \"Canada:\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi.loc[[\\\"United States\\\", \\\"Canada\\\"]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"However, suppose we wanted to extract the data for all countries, but only the\\n\", \"years 2005, 2007, and 2009\\n\", \"\\n\", \"We cannot do this with what we know about `wdi.loc` because the year is on the second level,\\n\", \"not outer-most level of our index\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To get around this limitation, we can use the `pd.IndexSlice` helper\\n\", \"\\n\", \"Here’s an example\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi.loc[pd.IndexSlice[:, [2005, 2007, 2009]], :]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that the `:` in the first part of `[:, [\\\"A\\\", \\\"D\\\"]]`\\n\", \"instructed pandas to give us rows for all values of the outer most index\\n\", \"level\\n\", \"\\n\", \"The `:` just before the final `]` said grab all the columns\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-3'></a>\\n\", \"**Exercise 4**\\n\", \"\\n\", \"Below, we create `wdi2`, which is the same as `df4` except that the\\n\", \"levels of the index are swapped\\n\", \"\\n\", \"In the cells after `df6` is defined, we have commented out\\n\", \"a few of the slicing examples from the previous exercise\\n\", \"\\n\", \"For each of these examples, use `pd.IndexSlice` to extract the same\\n\", \"data from `df6`\\n\", \"\\n\", \"(HINT: You will need to *swap* the order of the row slicing arguments\\n\", \"within the `pd.IndexSlice`)\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi2 = df.set_index([\\\"year\\\", \\\"country\\\"])\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# wdi.loc[\\\"United States\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# wdi.loc[([\\\"United States\\\", \\\"Canada\\\"], [2010, 2011, 2012]), :]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# wdi.loc[[\\\"United States\\\", \\\"Canada\\\"], \\\"GDP\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Multi-index Columns\\n\", \"\\n\", \"The functionality of `MultiIndex` also applies to the column labels\\n\", \"\\n\", \"Let’s see how it works\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdiT = wdi.T  # .T means \\\"transpose\\\" or \\\"swap rows and columns\\\"\\n\", \"wdiT\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that `wdiT` seems to have two levels of names for the columns\\n\", \"\\n\", \"The same logic laid out in the above row slicing rules applies when we\\n\", \"have a hierarchical index for column names\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdiT.loc[:, \\\"United States\\\"]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdiT.loc[:, [\\\"United States\\\", \\\"Canada\\\"]]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdiT.loc[:, ([\\\"United States\\\", \\\"Canada\\\"], 2010)]\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-4'></a>\\n\", \"**Exercise 5**\\n\", \"\\n\", \"Use `pd.IndexSlice` to extract all data from `wdiT` where the `year`\\n\", \"level of the column names (the second level) is one of 2010, 2012, and 2014\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Re-setting the Index\\n\", \"\\n\", \"The `df.reset_index` method will move one or more level of the index\\n\", \"back into the DataFrame as a normal column\\n\", \"\\n\", \"With no additional arguments, it moves all levels out of the index and\\n\", \"sets the index of the returned DataFrame to the default of\\n\", \"`range(df.shape[0])`\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi.reset_index()\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-5'></a>\\n\", \"**Exercise 6**\\n\", \"\\n\", \"Look up the documentation for the `reset_index` method and study it to\\n\", \"learn how to do the following:\\n\", \"\\n\", \"- Move just the `year` level of the index back as a column\\n\", \"- Completely throw away all levels of the index\\n\", \"- Remove the `country` of the index and *do not* keep it as a column\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# remove just year level and add as column\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# throw away all levels of index\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Remove country from the index -- don't keep it as a column\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Choose the Index Carefully\\n\", \"\\n\", \"So, now that we know that we use index and column names for\\n\", \"aligning data, “how should we pick the index?” is a natural question to ask\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To guide us to the right answer, we will list the first two components\\n\", \"to [Hadley Wickham’s](http://hadley.nz/) description of [tidy\\n\", \"data](http://vita.had.co.nz/papers/tidy-data.html):\\n\", \"\\n\", \"1. Each column should each have one variable\\n\", \"1. Each row should each have one observation\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"If we strive to have our data in a tidy form (we should), then when\\n\", \"choosing the index, we should set:\\n\", \"\\n\", \"- the row labels (index) to be a unique identifier for an observation\\n\", \"  of data  \\n\", \"- the column names to identify one variable  \"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"For example, suppose we are looking data on interest rates\\n\", \"\\n\", \"Each column might represent one bond or asset and each row might\\n\", \"represent the date\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Using hierarchical row and/or column indices allows us to store higher\\n\", \"dimensional data in our (inherently) two dimensional DataFrame\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Know Your Goal\\n\", \"\\n\", \"The correct column(s) to choose for the index often depends on the context of\\n\", \"your analysis\\n\", \"\\n\", \"For example, if I were studying how GDP and consumption evolved over time for\\n\", \"various countries, I would want time (year) and country name on the index\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"On the other hand, if I were trying to look at the differences across countries\\n\", \"and variables within a particular year, I may opt to put the country and\\n\", \"variable on the index and have years be columns\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Following the tidy data rules above and thinking about how you intend to *use*\\n\", \"the data – and a little practice – will enable you to consistently select the\\n\", \"correct index\"], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"title\": \"The Index\", \"@webio\": {\"lastCommId\": null, \"lastKernelId\": null}, \"filename\": \"the_index.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.11.5\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"01_pandas_the_index.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.785Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 7,
    "position": 1,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 8,
    "description": "Organizing Data in Pandas",
    "title": "Lecture 02",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 85,
    "position": 8,
    "content_id": 171,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 171,
    "type": "video",
    "description": "Learn about common data storage formats and how to interact with them in pandas",
    "title": "Pandas: Storage Formats",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=2agIyguTWDA\", \"youtubeVideoId\": \"2agIyguTWDA\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 7,
    "position": 1,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 8,
    "description": "Organizing Data in Pandas",
    "title": "Lecture 02",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 83,
    "position": 8,
    "content_id": 177,
    "lecture_id": 8,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 177,
    "type": "video",
    "description": "Understand the importance and functionality of the index of a DataFrame or Series",
    "title": "The Index",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=Be2z5CKlu18\", \"youtubeVideoId\": \"Be2z5CKlu18\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 9,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 10,
    "description": "Review",
    "title": "Lecture 04",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 93,
    "position": 9,
    "content_id": 168,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 168,
    "type": "video",
    "description": "Review of the pandas material covered so far by analyzing movie ratings data",
    "title": "Pandas Review: MovieLens",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=mQH9kdyXN2c\", \"youtubeVideoId\": \"mQH9kdyXN2c\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 9,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 10,
    "description": "Review",
    "title": "Lecture 04",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 96,
    "position": 10,
    "content_id": 145,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 145,
    "type": "notebook",
    "description": "Explore UN population data with pandas",
    "title": "02_pandas_review_population.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Pandas Review: UN Population Data\\n\", \"\\n\", \"**Attribution**: _This notebook is based on a notebook created by Dave Backus, Chase Coleman, Brian LeBlanc, and Spencer Lyon for the NYU Stern Data Bootcamp course_\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import matplotlib.pyplot as plt\\n\", \"import pandas as pd\\n\", \"\\n\", \"from urllib.parse import urljoin\\n\", \"\\n\", \"%matplotlib inline\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## UN Population Data\\n\", \"\\n\", \"We will look at the UN's [population data](http://esa.un.org/unpd/wpp/Download/Standard/Population/). In this notebook, we will focus specifically the age distribution of the population.\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Projection Variants\\n\", \"\\n\", \"The population numbers for a particular year are reported as corresopnding to 1 July 2020. There will be two types of population numbers that we discuss:\\n\", \"\\n\", \"* *estimates*: Estimates of the population at a point in the past\\n\", \"* *projections*: Forecasts of what the population might be for a given year\\n\", \"\\n\", \"Furthermore, the UN will provide various versions of the projections using different modeling assumptions. We will consider three of their population models:\\n\", \"\\n\", \"1. *Low variant*: Assumes a low fertility with normal mortality and normal migration.\\n\", \"2. *Medium variant*: Assumes a medium fertility with normal mortality and normal migration.\\n\", \"3. *High variant*: Assumes a high fertility with normal mortality and normal migration\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Loading the data \\n\", \"\\n\", \"We start, as usual, by loading the data.  This will take a minute because the file is about 10 MB in size.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"un_base_url = \\\"https://population.un.org/\\\"\\n\", \"path_to_file = \\\"wpp/Download/Files/1_Indicators%20(Standard)/EXCEL_FILES/2_Population/\\\"\\n\", \"filename = (\\\"WPP2022_POP_F02_1_POPULATION_5-YEAR_AGE_GROUPS_BOTH_SEXES.xlsx\\\")\\n\", \"\\n\", \"url = urljoin(un_base_url, path_to_file+filename)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"cols_to_read = [\\n\", \"    \\\"Variant\\\", \\\"Region, subregion, country or area *\\\", \\\"Location code\\\",\\n\", \"    \\\"Type\\\", \\\"Year\\\"\\n\", \"]\\n\", \"\\n\", \"age_cols = [f\\\"{5*i}-{5*i+4}\\\" for i in range(20)]\\n\", \"age_cols.append(\\\"100+\\\")\\n\", \"\\n\", \"cols_to_read.extend(age_cols)\\n\", \"\\n\", \"sheets = pd.read_excel(\\n\", \"    url, sheet_name=[\\\"Estimates\\\", \\\"Low variant\\\", \\\"Medium variant\\\", \\\"High variant\\\"],\\n\", \"    skiprows=16, usecols=cols_to_read, na_values=[\\\"...\\\"]\\n\", \")\\n\", \"\\n\", \"est = sheets[\\\"Estimates\\\"]\\n\", \"low_proj = sheets[\\\"Low variant\\\"]\\n\", \"med_proj = sheets[\\\"Medium variant\\\"]\\n\", \"hgh_proj = sheets[\\\"High variant\\\"]\\n\", \"\\n\", \"# Combine all of the datasets into one\\n\", \"df = pd.concat([est, low_proj, med_proj, hgh_proj], axis=0, ignore_index=True)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"What does our data contain?\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"print(\\\"Dimensions and dtypes of estimates:\\\")\\n\", \"print(df.shape)\\n\", \"print(df.dtypes)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Clean data\\n\", \"\\n\", \"The first step to using this data is to make sure that the data is clean.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Geography types**\\n\", \"\\n\", \"There are currently many different geography types including `World`, `Label/Separator`, `Development Group`, `Income Group`, ..., and `Country/Area`.\\n\", \"\\n\", \"For what we're going to do today, we're going to focus on country level information.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = df.query(\\\"Type == 'Country/Area'\\\")\\n\", \"\\n\", \"df.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Renaming**\\n\", \"\\n\", \"Many column names aren't easily typable or usable. We will resolve this by renaming the columns and will use the conventions below for renaming our columns:\\n\", \"\\n\", \"* Short and memorable (using standard conventions where possible)\\n\", \"* Lower case with underscores to separate words when needed\\n\", \"* Consistent <- Most important\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"renamers = {\\n\", \"    \\\"Variant\\\": \\\"variant\\\",\\n\", \"    \\\"Region, subregion, country or area *\\\": \\\"country\\\",\\n\", \"    \\\"Location code\\\": \\\"alpha3\\\",\\n\", \"    \\\"Type\\\": \\\"type\\\",\\n\", \"    \\\"Year\\\": \\\"year\\\"\\n\", \"}\\n\", \"\\n\", \"# Rename the columns and drop type column (since we've already restricted\\n\", \"# to just countries)\\n\", \"df = df.rename(\\n\", \"    columns=renamers\\n\", \").drop(\\n\", \"    [\\\"type\\\"], axis=1\\n\", \")\\n\", \"\\n\", \"df.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# There were missing years in certain columsn but we've eliminated them\\n\", \"# so now we can move the year back to an integer as it should be\\n\", \"df[\\\"year\\\"] = df[\\\"year\\\"].astype(int)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Reshaping**\\n\", \"\\n\", \"The data currently has one column for each age group. We'll update the data to be a long-form rather than wide-form to make it \\\"tidy\\\"\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = df.melt(id_vars=[\\\"variant\\\", \\\"country\\\", \\\"alpha3\\\", \\\"year\\\"], var_name=\\\"age\\\")\\n\", \"\\n\", \"df.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Population sizes**\\n\", \"\\n\", \"Let's focus on countries that had more than 50,000,000 people in 2010, 2015, and 2020 according to the estimates data.\\n\", \"\\n\", \"In order to do this, we're going to use a pivot table to determine how many people were in each country during each year and then use boolean selection\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Sum up over all age-groups\\n\", \"pop_sizes = (\\n\", \"    df.query(\\\"variant == 'Estimates'\\\")\\n\", \"      .pivot_table(index=\\\"country\\\", columns=\\\"year\\\", values=\\\"value\\\", aggfunc=\\\"sum\\\")\\n\", \")\\n\", \"\\n\", \"# Get list of the countries with more than 50m people\\n\", \"countries_pop_gt_50m_bool = (pop_sizes.loc[:, 2010:2020] > 50_000).all(axis=1)\\n\", \"countries_pop_gt_50m = countries_pop_gt_50m_bool.index[countries_pop_gt_50m_bool].tolist()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"countries_pop_gt_50m\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df_gt_50m = df.query(\\\"country in @countries_pop_gt_50m\\\")\\n\", \"\\n\", \"df_gt_50m.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Visualizations\\n\", \"\\n\", \"Let's now begin visually exploring our data.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Plotting age distribution**\\n\", \"\\n\", \"Below, we'll begin plotting the age distributions of certain countries in different years. Let's start by plotting the age distribution in China for 1960, 1990, 2020, 2050, and 2080.\\n\", \"\\n\", \"To do this, we  will need `year` on the index and `age` as the columns.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"china_sel = \\\"country == 'China' & (variant=='Estimates' | variant=='Medium')\\\"\\n\", \"df_china = (\\n\", \"    df.query(china_sel)\\n\", \"      .pivot_table(index=\\\"age\\\", columns=\\\"year\\\", values=\\\"value\\\")\\n\", \"      .loc[:, range(1960, 2100, 30)]\\n\", \")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"ax = df_china.plot(\\n\", \"    kind=\\\"bar\\\", figsize=(10, 8), subplots=True, sharey=True,\\n\", \"    legend=False\\n\", \")\\n\", \"\\n\", \"fig = ax[0].get_figure()\\n\", \"fig.tight_layout()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"What's wrong with the graph?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"ax = df_china.loc[age_cols, :].plot(\\n\", \"    kind=\\\"bar\\\", figsize=(10, 8), subplots=True, sharey=True,\\n\", \"    legend=False\\n\", \")\\n\", \"\\n\", \"fig = ax[0].get_figure()\\n\", \"fig.tight_layout()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Maybe we want to see the fraction of the population in each bin rather than the number of people\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"ax = (\\n\", \"    df_china.divide(df_china.sum(axis=0), axis=1)\\n\", \"            .loc[age_cols, :]\\n\", \"            .plot(\\n\", \"                kind=\\\"bar\\\", figsize=(10, 8), subplots=True,\\n\", \"                sharey=True, legend=False, ylim=(0.0, 0.2),\\n\", \"                title=\\\"Population Share by Age Bracket\\\"\\n\", \"            )\\n\", \")\\n\", \"\\n\", \"for (i, _ax) in enumerate(ax):\\n\", \"    _population_m = df_china.iloc[:, i].sum()/1000\\n\", \"    _title = f\\\"{df_china.columns[i]}: {_population_m:.2f} million\\\"\\n\", \"    _ax.set_title(\\\"\\\")\\n\", \"    _ax.set_title(_title, loc=\\\"right\\\")\\n\", \"    _ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"    _ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"\\n\", \"fig = ax[0].get_figure()\\n\", \"fig.tight_layout()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"If we wanted a similar graph for another country, we could potentially copy and paste what we wrote, but, if we made minor changes then we could potentially introduce unexpected errors when trying to write them.\\n\", \"\\n\", \"Instead, let's write a function that creates a graph like this.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def age_distribution_graph(data, country, years, forecast_variant=\\\"Medium\\\"):\\n\", \"    # Subset the data\\n\", \"    sel = f\\\"country == '{country}' & (variant=='Estimates' | variant=='{forecast_variant}')\\\"\\n\", \"    plot_df = (\\n\", \"        data.query(sel)\\n\", \"            .pivot_table(index=\\\"age\\\", columns=\\\"year\\\", values=\\\"value\\\")\\n\", \"            .loc[:, years]\\n\", \"    )\\n\", \"    \\n\", \"    # Create the plot\\n\", \"    ax = (\\n\", \"        plot_df.divide(plot_df.sum(axis=0), axis=1)\\n\", \"            .loc[age_cols, :]\\n\", \"            .plot(\\n\", \"                kind=\\\"bar\\\", figsize=(10, 8), subplots=True,\\n\", \"                sharey=True, legend=False, ylim=(0.0, 0.2),\\n\", \"                title=None\\n\", \"            )\\n\", \"    )\\n\", \"\\n\", \"    for (i, _ax) in enumerate(ax):\\n\", \"        _population_m = plot_df.iloc[:, i].sum()/1000\\n\", \"        _title = f\\\"{plot_df.columns[i]}: {_population_m:.2f} million\\\"\\n\", \"        _ax.set_title(\\\"\\\")\\n\", \"        _ax.set_title(_title, loc=\\\"right\\\")\\n\", \"        _ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"        _ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"\\n\", \"    fig = ax[0].get_figure()\\n\", \"    ax[0].set_title(f\\\"Population Share by Age Bracket for {country}\\\", loc=\\\"left\\\")\\n\", \"    fig.tight_layout()\\n\", \"    \\n\", \"    return fig\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Population graphs\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"years_of_interest = [1980, 1990, 2000, 2020, 2050, 2100]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**China**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"age_distribution_graph(df, \\\"China\\\", years_of_interest, \\\"Low\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"age_distribution_graph(df, \\\"China\\\", years_of_interest, \\\"Medium\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"age_distribution_graph(df, \\\"China\\\", years_of_interest, \\\"High\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**India**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"age_distribution_graph(df, \\\"India\\\", years_of_interest, \\\"Low\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"age_distribution_graph(df, \\\"India\\\", years_of_interest, \\\"Medium\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"age_distribution_graph(df, \\\"India\\\", years_of_interest, \\\"High\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**United States of America**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"age_distribution_graph(df, \\\"United States of America\\\", years_of_interest, \\\"Low\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"age_distribution_graph(df, \\\"United States of America\\\", years_of_interest, \\\"Medium\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"age_distribution_graph(df, \\\"United States of America\\\", years_of_interest, \\\"High\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Why do we care about age distribution?\\n\", \"\\n\", \"Lots of reasons, but one that we (as young working-age adults) should be particularly interested in is social security programs.\\n\", \"\\n\", \"\\n\", \"_Definition: **Dependency ratio**_\\n\", \"\\n\", \"> The (old-age) dependency ratio relates the number of persons aged 65 or over per 100 persons aged 15-64.\\n\", \"\\n\", \"We care about this measurement because it tells us roughly how many working age people there are to support those who have stopped working. As the population distribution shifts to the right (and if nothing changes), there will be fewer individuals to support the social security programs that support the old (non-working) population.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Compute old-age dependency ratio for countries with more than\\n\", \"# 50,000,000 people\\n\", \"working_age = [f\\\"{5*i}-{5*i + 4}\\\" for i in range(20) if 5*i >= 15 and 5*i<65]\\n\", \"old_age = [f\\\"{5*i}-{5*i + 4}\\\" for i in range(20) if 5*i>=65] + [\\\"100+\\\"]\\n\", \"\\n\", \"df_gt_50m.loc[:, \\\"age_classification\\\"] = \\\"young\\\"\\n\", \"wa = df_gt_50m[\\\"age\\\"].isin(working_age)\\n\", \"df_gt_50m.loc[wa, \\\"age_classification\\\"] = \\\"work\\\"\\n\", \"oa = df_gt_50m[\\\"age\\\"].isin(old_age)\\n\", \"df_gt_50m.loc[oa, \\\"age_classification\\\"] = \\\"old\\\"\\n\", \"\\n\", \"dr = (\\n\", \"    df_gt_50m.query(\\\"variant == 'Estimates' | variant == 'Medium'\\\")\\n\", \"             .pivot_table(\\n\", \"                 index=[\\\"country\\\", \\\"year\\\"],\\n\", \"                 columns=\\\"age_classification\\\", values=\\\"value\\\"\\n\", \"             )\\n\", \")\\n\", \"\\n\", \"dr.loc[:, \\\"dep_ratio\\\"] = 100 * (dr.loc[:, \\\"old\\\"] / dr.loc[:, \\\"work\\\"])\\n\", \"\\n\", \"dr = dr[\\\"dep_ratio\\\"].unstack(level=\\\"country\\\")\\n\", \"\\n\", \"dr.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Sustainable dependency ratio**\\n\", \"\\n\", \"Obviously the age of retirement could be adjusted which would make the dependency ratio, as we've calculated it, a worse measure of dependence. However, just as a thought experiment, let's suppose that the dependency ratio that is sustainable long term (i.e. the payments to social security are roughly the same as the social security tax collected) is 50 (which means that there are 2 workers for each person over 65).\\n\", \"\\n\", \"Of our countries with over 50 million citizens, how many countries will be sustainable in 2080? In 2100?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots(figsize=(8, 6))\\n\", \"\\n\", \"dr.plot(kind=\\\"line\\\", ax=ax, legend=False)\\n\", \"ax.hlines(50, 1950, 2100, color=\\\"k\\\", linewidth=2.0, linestyle=\\\"--\\\")\\n\", \"\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"\\n\", \"ax.set_title(\\\"Dependency Ratios for countries with 50m+ citizens\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"dr_2080 = dr.loc[2080, :]\\n\", \"dr_2080.index[dr_2080 < 50]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"dr_2100 = dr.loc[2100, :]\\n\", \"dr_2100.index[dr_2100 < 50]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"anaconda-cloud\": {}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"02_pandas_review_population.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.786Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 9,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 10,
    "description": "Review",
    "title": "Lecture 04",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 94,
    "position": 9,
    "content_id": 142,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 142,
    "type": "notebook",
    "description": "Explore a movie rating dataset with pandas",
    "title": "01_pandas_example_movieLens.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"\\n\", \"# Pandas Example: MovieLens Data\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Learn how to download file from internet using the [requests](https://requests.readthedocs.io/en/master/) library\\n\", \"- Know how to operate on a `.zip` file in memory, without writing to hard drive\\n\", \"- Practice merging datasets\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Note: requires internet access to run.**  \\n\", \"\\n\", \"This Jupyter notebook was originally created in 2016 by Dave Backus, Chase Coleman, Brian LeBlanc, and Spencer Lyon for the NYU Stern course [Data Bootcamp](http://databootcamp.nyuecon.com/).\\n\", \"\\n\", \"The notebook has been modified for this course\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%matplotlib inline \\n\", \"\\n\", \"import pandas as pd             # data package\\n\", \"import matplotlib.pyplot as plt # graphics \\n\", \"import datetime as dt           # date tools, used to note current date  \\n\", \"\\n\", \"# these are new \\n\", \"import os                       # operating system tools (check files)\\n\", \"import requests, io             # internet and input tools  \\n\", \"import zipfile as zf            # zip file tools \\n\", \"import shutil                   # file management tools\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## MovieLens data \\n\", \"\\n\", \"The [GroupLens](https://grouplens.org/) team at the University of Minnesota has prepared many datasets\\n\", \"\\n\", \"One is called [MovieLens](https://grouplens.org/datasets/movielens/), and contains millions of movie ratings by viewers and users of the MovieLens website\\n\", \"\\n\", \"We will use a small subset of the data with 100,000 ratings\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"This data comes in a `.zip` file that contains a handful of csv's and a readme\\n\", \"\\n\", \"Here are some details about the zipped files:\\n\", \"\\n\", \"* `ratings.csv`:  each line is an individual film rating with the rater and movie id's and the rating.  Order:  `userId, movieId, rating, timestamp`. \\n\", \"* `tags.csv`:  each line is a tag on a specific film.  Order:  `userId, movieId, tag, timestamp`. \\n\", \"* `movies.csv`:  each line is a movie name, its id, and its genre.  Order:  `movieId, title, genres`.  Multiple genres are separated by \\\"pipes\\\" `|`.   \\n\", \"* `links.csv`:  each line contains the movie id and corresponding id's at [IMBd](http://www.imdb.com/) and [TMDb](https://www.themoviedb.org/).  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We are interested in the `ratings.csv` and `movies.csv` files as pandas DataFrames\\n\", \"\\n\", \"There are at least two approaches to doing this:\\n\", \"\\n\", \"1. Download the file to the hard drive, unzip manually, then come back and use `pd.read_csv`\\n\", \"2. Have Python download the file, learn to use Python to handle zip files, then use `pd.read_csv`\\n\", \"\\n\", \"The first option is likely easier, but the second is more powerful\\n\", \"\\n\", \"We will choose option 2 here as it gives us a chance to learn more \\\"real-world\\\" data+Python skills\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Why do it the hard way?**\\n\", \"\\n\", \"- It builds character\\n\", \"- Entire analysis can be self-contained in our notebook, no external *by hand* steps  needed\\n\", \"- Can be applied to other datasets, as we'll surely see a zip file again in the future\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Automated file download \\n\", \"\\n\", \"**WANT:** create pandas DataFrames from the `ratings.csv` and `movies.csv` files in the zipfile on the GroupLens website\\n\", \"\\n\", \"Let's unpack what steps need to happen:\\n\", \"\\n\", \"1. Download the file: we'll use the [requests](http://docs.python-requests.org/) package\\n\", \"2. Unpack raw downloaded content as file: using built-in [io](https://docs.python.org/3.5/library/io.html) module's `io.BytesIO`\\n\", \"3. Access csv files inside the zip file: using built-in [zipfile](https://docs.python.org/3.5/library/zipfile.html) module to read csv's from zip\\n\", \"4. Read in csvs: easy part -- using `pd.read_csv`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Digression 1\\n\", \"\\n\", \"The `requests` documentation states\\n\", \"\\n\", \">Recreational use of other HTTP libraries may result in dangerous side-effects, including: security vulnerabilities, verbose code, reinventing the wheel, constantly reading documentation, depression, headaches, or even death.\\n\", \"\\n\", \"We whole-heartedly agree\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Digression 2\\n\", \"\\n\", \"We found this [Stack Overflow exchange](http://stackoverflow.com/questions/23419322/download-a-zip-file-and-extract-it-in-memory-using-python3) helpful when creating this solution\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Step 1: Download the file\\n\", \"\\n\", \"Let's get to it!\\n\", \"\\n\", \"Here we download the file and print out some information about the response object\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#  Step 1 -- download the file \\n\", \"url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\\n\", \"res = requests.get(url) \\n\", \"\\n\", \"# (sub-step, see what the response looks like)\\n\", \"print('Response status code:', res.status_code)\\n\", \"print('Response type:', type(res))\\n\", \"print('Response .content:', type(res.content)) \\n\", \"print('Response headers:\\\\n', res.headers, sep='')\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Step 2: read file as bytes\\n\", \"\\n\", \"The ZipFile is a binary file format (not plain text)\\n\", \"\\n\", \"For this reason we need to treat the contents of the file as bytes\\n\", \"\\n\", \"We'll load in `res.content` into an instance of `io.BytesIO\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Step 2 -- read bytes of response contentonvert bytes to zip file  \\n\", \"bytes = io.BytesIO(res.content)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Step 3: Interpret bytes as ZipFile\\n\", \"\\n\", \"Now we have downloaded file, and interpreted as a binary source (`BytesIO`)\\n\", \"\\n\", \"This is not just any binary file, but rather a zip compressed file\\n\", \"\\n\", \"Python knows how to handle these using the built-in `zipfile` module\\n\", \"\\n\", \"Below we tell Python to interpret the `BytesIO` as a ZipFile\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Step 3 -- Interpret bytes as zipfile\\n\", \"zip = zf.ZipFile(bytes)\\n\", \"print('Type of zipfile object:', type(zip))\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now that we have a ZipFile, we need to explore what is inside\\n\", \"\\n\", \"The `ZipFile` class has a handy method named `namelist` \\n\", \"\\n\", \"This method lists all folders and files in the zip archive\\n\", \"\\n\", \"Let's take a look\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# (sub-step, inspect the file)\\n\", \"names = zip.namelist()\\n\", \"names\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that our target `ratings.csv` and `movies.csv` files are there\\n\", \"\\n\", \"However, they are in a folder named `ml-latest-small`\\n\", \"\\n\", \"We could write out the \\\"path\\\" to those files by hand, but instead we'll let Python find them for us\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"movie_fn = [n for n in names if \\\"movies\\\" in n][0]\\n\", \"ratings_fn = [n for n in names if \\\"ratings\\\" in n][0]\\n\", \"\\n\", \"print(\\\"The path to movies.csv is:\\\", movie_fn)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Step 4: `pd.read_csv` the Files\\n\", \"\\n\", \"Now that we have the ZipFile **and** the path to our csvs, let's read them in as DataFrames\\n\", \"\\n\", \"For this we'll use our friend `pd.read_csv`\\n\", \"\\n\", \"We need to call the `.open` method on our zipfile\\n\", \"\\n\", \"This method expects the path to the file we need to open, these are saved in `movie_fn` and `ratings_fn` above\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# extract and read csv's\\n\", \"movies  = pd.read_csv(zip.open(movie_fn))\\n\", \"ratings = pd.read_csv(zip.open(ratings_fn))\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let's take a look at the data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"movies.info()\\n\", \"movies.head(3)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false, \"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"ratings.info()\\n\", \"ratings.head(3)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Data Organization\\n\", \"\\n\", \"The movie ratings in the dataframe `ratings` give us individual opinions about movies, but they don't include the name of the movie.  \\n\", \"\\n\", \"**Why not?**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Normalization\\n\", \"\\n\", \"Storing movie names in rating DataFrame would cause movie name to be repeated many times\\n\", \"\\n\", \"The string \\\"Grumpier Old Men (1995)\\\" takes up more space in a file than the integer `3`\\n\", \"\\n\", \"So, the GroupLens team decided to store the movie name in `movies.csv`, and a `movidId` column that is an integer\\n\", \"\\n\", \"In other files, they can use just the `movieId` column\\n\", \"\\n\", \"This is an eample of a relational database (think SQL) technique known as [normalization](https://en.wikipedia.org/wiki/Database_normalization)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Why normalize?**\\n\", \"\\n\", \"There are many benefits to normalization, but two that immediately come to mind are:\\n\", \"\\n\", \"1. Save storage space: movie title (and genres in this example) are not repeated for each rating\\n\", \"2. Easier to maintain/update\\n\", \"\\n\", \"For the second point, suppose the GroupLens team wanted to include the movie's director\\n\", \"\\n\", \"In the current, normalized format they would add a `director` column to `movies.csv` and have one row to update per movie\\n\", \"\\n\", \"If they instead chose to put the movie title inside the `ratings.csv` they would have to find all occurances of each movie and add the director\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Combining ratings and movies\\n\", \"\\n\", \"We **want** to be able to analyze the ratings for movies, and associate those ratings with a movie name\\n\", \"\\n\", \"We need a way to bring in the movie title information into the `ratings` DataFrame\\n\", \"\\n\", \"This is a perfect use case for `merge`!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's start with a small example, just the first three rows of ratings:\\n\", \"\\n\", \"Here's what this looks like\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"ratings.head(3)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now let's see what happens when we `merge` this with the `movies` DataFrame\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"ratings.head(3).merge(movies, on=\\\"movieId\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Here's a breakdown of what happened:\\n\", \"\\n\", \"- For each row in `ratings.head(3)` pandas found the `movieId`\\n\", \"- It then looked for row(s) in `movies` that had a matching `movieId`\\n\", \"- It then added columns `title` and `genres` alongside existing columns from `ratings` (`userId`, `rating`, `timestamp`) to form combined DataFrame\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's now apply this `merge` to the whole `ratings` dataset\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"combo = pd.merge(ratings, movies,   # left and right df's\\n\", \"                 how='left',        # add to left \\n\", \"                 on='movieId'       # link with this variable/column \\n\", \"                ) \\n\", \"\\n\", \"print('Dimensions of ratings:', ratings.shape)\\n\", \"print('Dimensions of movies:', movies.shape)\\n\", \"print('Dimensions of new df:', combo.shape)\\n\", \"\\n\", \"combo.head(20)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise.** Some of these we know how to do, the others we don't.  For the ones we know, what is the answer?  For the others, what (in loose terms) do we need to be able to do to come up with an answer?  \\n\", \"\\n\", \"* What is the overall average rating?  \\n\", \"* What is the overall distribution of ratings?  \\n\", \"* What is the average rating of each movie?  \\n\", \"* How many ratings does each movie get?  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Your code/idea here -- average rating\\n\", \"combo['rating'].mean()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Your code/idea here -- distribution of ratings\\n\", \"fig, ax = plt.subplots()\\n\", \"bins = [bin/100 for bin in list(range(25, 550, 50))]\\n\", \"print(bins)\\n\", \"combo['rating'].plot(kind='hist', ax=ax, bins=bins, color='blue', alpha=0.5)\\n\", \"ax.set_xlim(0,5.5)\\n\", \"ax.set_ylabel('Number')\\n\", \"ax.set_xlabel('Rating')\\n\", \"plt.show()\\n\", \"\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"import plotly.graph_objects as go \\n\", \"\\n\", \"trace = go.Histogram(\\n\", \"    x=combo['rating'],\\n\", \"    name='control',\\n\", \"    autobinx=False,\\n\", \"    xbins=dict(\\n\", \"        start=.5,\\n\", \"        end=5.0,\\n\", \"        size=0.5\\n\", \"    ),\\n\", \"    marker_color='Blue',\\n\", \"    opacity=0.75\\n\", \")\\n\", \"\\n\", \"layout = go.Layout(\\n\", \"    title='Distribution of ratings',\\n\", \"    xaxis_title='Rating value',\\n\", \"    yaxis_title='Count',\\n\", \"    bargap=0.01,\\n\", \"    bargroupgap=0.1,\\n\", \")\\n\", \"\\n\", \"fig = go.Figure(data=[trace], layout=layout)\\n\", \"fig.show()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Your code/idea here -- average rating of each movie\\n\", \"\\n\", \"# average for a specific movie \\n\", \"combo[combo['movieId']==31]['rating'].mean()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"%%time\\n\", \"# average for all movies (spoiler we will learn this next time!!)\\n\", \"ave_mov = combo['rating'].groupby(combo['movieId']).mean()\\n\", \"ave_mov.reset_index() \"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Your code/idea here -- number of ratings for each movie\\n\", \"\\n\", \"# number of ratings for a single movie\\n\", \"combo[combo['movieId']==31]['rating'].count()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# number of ratings for all movies (another spoiler!)\\n\", \"combo['rating'].groupby(combo['movieId']).count().reset_index()\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"collapsed\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Resources \\n\", \"\\n\", \"The [Pandas docs](http://pandas.pydata.org/pandas-docs/stable/merging.html) are ok, but we prefer the Data Carpentry [guide](http://www.datacarpentry.org/python-ecology-lesson/04-merging-data)\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"widgets\": {\"application/vnd.jupyter.widget-state+json\": {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}}, \"kernelspec\": {\"name\": \"css\", \"language\": \"python\", \"display_name\": \"css\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.2\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"anaconda-cloud\": {}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"01_pandas_example_movieLens.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.785Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 16,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 17,
    "description": "Data Manipulation in Pandas",
    "title": "Lecture 03",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 92,
    "position": 11,
    "content_id": 192,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 192,
    "type": "notebook",
    "description": "Learn how to use pandas methods like concat, join, and merge to combine multiple datasets",
    "title": "v3_pandas_data_merge.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Merge\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Reshape](./v2_pandas_data_reshape.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Know the different pandas routines for combining datasets  \\n\", \"- Know when to use `pd.concat` vs `pd.merge` vs `pd.join`  \\n\", \"- Be able to apply the three main combining routines  \\n\", \"\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- WDI data on GDP components, population, and square miles of countries  \\n\", \"- Book ratings: 6,000,000 ratings for the 10,000 most rated books on\\n\", \"  [Goodreads](https://www.goodreads.com/)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pandas as pd\\n\", \"\\n\", \"%matplotlib inline\\n\", \"\\n\", \"from IPython.display import display\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Merge](#Merge)  \\n\", \"  - [Combining Datasets](#Combining-Datasets)  \\n\", \"  - [`pd.concat`](#`pd.concat`)  \\n\", \"  - [`pd.merge`](#`pd.merge`)  \\n\", \"  - [Arguments to `merge`](#Arguments-to-`merge`)  \\n\", \"  - [`df.join`](#`df.join`)  \\n\", \"  - [Case Study](#Case-Study)  \\n\", \"  - [Visualizing Merge Operations](#Visualizing-Merge-Operations)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Combining Datasets\\n\", \"\\n\", \"Often, we will want perform joint analysis on data from different sources.\\n\", \"\\n\", \"For example, when analyzing the regional sales for a company, we might\\n\", \"want to include industry aggregates or demographic information for each\\n\", \"region.\\n\", \"\\n\", \"Or perhaps we are working with product-level data, have a list of\\n\", \"product groups in a separate dataset, and want to compute aggregate\\n\", \"statistics for each group.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# from WDI. Units trillions of 2010 USD\\n\", \"url = \\\"https://datascience.quantecon.org/assets/data/wdi_data.csv\\\"\\n\", \"wdi = pd.read_csv(url).set_index([\\\"country\\\", \\\"year\\\"])\\n\", \"wdi.info()\\n\", \"\\n\", \"wdi2017 = wdi.xs(2017, level=\\\"year\\\")\\n\", \"wdi2017\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi2016_17 = wdi.loc[pd.IndexSlice[:, [2016, 2017]],: ]\\n\", \"wdi2016_17\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Data from https://www.nationmaster.com/country-info/stats/Geography/Land-area/Square-miles\\n\", \"# units -- millions of square miles\\n\", \"sq_miles = pd.Series({\\n\", \"   \\\"United States\\\": 3.8,\\n\", \"   \\\"Canada\\\": 3.8,\\n\", \"   \\\"Germany\\\": 0.137,\\n\", \"   \\\"United Kingdom\\\": 0.0936,\\n\", \"   \\\"Russia\\\": 6.6,\\n\", \"}, name=\\\"sq_miles\\\").to_frame()\\n\", \"sq_miles.index.name = \\\"country\\\"\\n\", \"sq_miles\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# from WDI. Units millions of people\\n\", \"pop_url = \\\"https://datascience.quantecon.org/assets/data/wdi_population.csv\\\"\\n\", \"pop = pd.read_csv(pop_url).set_index([\\\"country\\\", \\\"year\\\"])\\n\", \"pop.info()\\n\", \"pop.head(10)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Suppose that we were asked to compute a number of statistics with the data above:\\n\", \"\\n\", \"- As a measure of land usage or productivity, what is Consumption per square mile?  \\n\", \"- What is GDP per capita (per person) for each country in each year? How about\\n\", \"  Consumption per person?  \\n\", \"- What is the population density of each country? How much does it change over time?  \\n\", \"\\n\", \"\\n\", \"Notice that to answer any of the questions from above, we will have to use data\\n\", \"from more than one of our DataFrames.\\n\", \"\\n\", \"In this lecture, we will learn many techniques for combining datasets that\\n\", \"originate from different sources, careful to ensure that data is properly\\n\", \"aligned.\\n\", \"\\n\", \"In pandas three main methods can combine datasets:\\n\", \"\\n\", \"1. `pd.concat([dfs...])`  \\n\", \"1. `pd.merge(df1, df2)`  \\n\", \"1. `df1.join(df2)`  \\n\", \"\\n\", \"\\n\", \"We’ll look at each one.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## `pd.concat`\\n\", \"\\n\", \"The `pd.concat` function is used to stack two or more DataFrames\\n\", \"together.\\n\", \"\\n\", \"An example of when you might want to do this is if you have monthly data\\n\", \"in separate files on your computer and would like to have 1 year of data\\n\", \"in a single DataFrame.\\n\", \"\\n\", \"The first argument to `pd.concat` is a list of DataFrames to be\\n\", \"stitched together.\\n\", \"\\n\", \"The other commonly used argument is named `axis`.\\n\", \"\\n\", \"As we have seen before, many pandas functions have an `axis` argument\\n\", \"that specifies whether a particular operation should happen down rows\\n\", \"(`axis=0`) or along columns (`axis=1`).\\n\", \"\\n\", \"In the context of `pd.concat`, setting `axis=0` (the default case)\\n\", \"will stack DataFrames on top of one another while `axis=1` stacks them\\n\", \"side by side.\\n\", \"\\n\", \"We’ll look at each case separately.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `axis=0`\\n\", \"\\n\", \"When we call `pd.concat` and set `axis=0`, the list of DataFrames\\n\", \"passed in the first argument will be stacked on top of one another.\\n\", \"\\n\", \"Let’s try it out here.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# equivalent to pd.concat([wdi2017, sq_miles]) -- axis=0 is default\\n\", \"pd.concat([wdi2017, sq_miles], axis=0)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice a few things:\\n\", \"\\n\", \"- \\n\", \"  <dl style='margin: 20px 0;'>\\n\", \"  <dt>The number of rows in the output is the total number</dt>\\n\", \"  <dd>\\n\", \"  of rows in all inputs. The labels are all from the original\\n\", \"  DataFrames.  \\n\", \"  </dd>\\n\", \"  \\n\", \"  </dl>\\n\", \"  \\n\", \"- The column labels are all the distinct column labels from all the inputs.  \\n\", \"- For columns that appeared only in one input, the value for all row labels\\n\", \"  originating from a different input is equal to `NaN` (marked as missing).  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `axis=1`\\n\", \"\\n\", \"In this example, concatenating by stacking\\n\", \"side-by-side makes more sense.\\n\", \"\\n\", \"We accomplish this by passing `axis=1` to `pd.concat`:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.concat([wdi2017, sq_miles], axis=1)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice here that\\n\", \"\\n\", \"- The index entries are all unique index entries that appeared in any DataFrame.  \\n\", \"- The column labels are all column labels from the inputs.  \\n\", \"- As `wdi2017` didn’t have a `Russia` row, the value for all of its columns\\n\", \"  is `NaN`.  \\n\", \"\\n\", \"\\n\", \"Now we can answer one of our questions from above: What is\\n\", \"Consumption per square mile?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"temp = pd.concat([wdi2017, sq_miles], axis=1)\\n\", \"temp[\\\"Consumption\\\"] / temp[\\\"sq_miles\\\"]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## `pd.merge`\\n\", \"\\n\", \"`pd.merge` operates on two DataFrames at a time and is primarily used\\n\", \"to bring columns from one DataFrame into another, *aligning* data based\\n\", \"on one or more “key” columns.\\n\", \"\\n\", \"This is a somewhat difficult concept to grasp by reading, so let’s look at some\\n\", \"examples.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2017, sq_miles, on=\\\"country\\\")\"], \"outputs\": [], \"metadata\": {\"scrolled\": true, \"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The output here looks very similar to what we saw with `concat` and\\n\", \"`axis=1`, except that the row for `Russia` does not appear.\\n\", \"\\n\", \"We will talk more about why this happened soon.\\n\", \"\\n\", \"For now, let’s look at a slightly more intriguing example:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2016_17, sq_miles, on=\\\"country\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Here’s how we think about what happened:\\n\", \"\\n\", \"- The data in `wdi2016_17` is copied over exactly as is.  \\n\", \"- Because `country` was on the index for both DataFrames, it is on the\\n\", \"  index of the output.  \\n\", \"- We lost the year on the index – we’ll work on getting it back below.  \\n\", \"- The additional column in `sq_miles` was added to column labels for the\\n\", \"  output.  \\n\", \"- The data from the `sq_miles` column was added to the output by looking up\\n\", \"  rows where the `country` in the two DataFrames lined up.\\n\", \"  -  Note that all the countries appeared twice, and the data in `sq_miles` was repeated. This is because `wdi2016_17` had two rows for each country.\\n\", \"  -  Also note that because `Russia` did not appear in `wdi2016_17`, the value `sq_miles.loc[\\\"Russia\\\"]` (i.e. `6.6`) is not used the output.  \\n\", \"\\n\", \"\\n\", \"How do we get the year back?\\n\", \"\\n\", \"We must first call `reset_index` on `wdi2016_17` so\\n\", \"that in the first step when all columns are copied over, `year` is included.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2016_17.reset_index(), sq_miles, on=\\\"country\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Multiple Columns\\n\", \"\\n\", \"Sometimes, we need to merge multiple columns.\\n\", \"\\n\", \"For example our `pop` and `wdi2016_17` DataFrames both have observations\\n\", \"organized by country and year.\\n\", \"\\n\", \"To properly merge these datasets, we would need to align the data by\\n\", \"both country and year.\\n\", \"\\n\", \"We pass a list to the `on` argument to accomplish this:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2016_17.reset_index(), pop, on=[\\\"country\\\", \\\"year\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now, we can answer more of our questions from above: What is GDP per capita (per\\n\", \"person) for each country in each year? How about Consumption per person?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi_pop = pd.merge(wdi2016_17, pop, on=[\\\"country\\\", \\\"year\\\"])\\n\", \"wdi_pop[\\\"GDP\\\"] / wdi_pop[\\\"Population\\\"]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi_pop[\\\"Consumption\\\"] / wdi_pop[\\\"Population\\\"]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise 1**\\n\", \"\\n\", \"Use your new `merge` skills to answer the final question from above: What\\n\", \"is the population density of each country? How much does it change over\\n\", \"time?\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Arguments to `merge`\\n\", \"\\n\", \"The `pd.merge` function can take many optional arguments.\\n\", \"\\n\", \"We’ll talk about a few of the most commonly-used ones here and refer you\\n\", \"to the\\n\", \"[documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html#pandas.merge)\\n\", \"for more details.\\n\", \"\\n\", \"We’ll follow the pandas convention and refer to the first argument to\\n\", \"`pd.merge` as `left` and call the second `right`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `on`\\n\", \"\\n\", \"We have already seen this one used before, but we want to point out that on\\n\", \"is optional.\\n\", \"\\n\", \"If nothing is given for this argument, pandas will use **all** columns\\n\", \"in `left` and `right` with the same name.\\n\", \"\\n\", \"In our example, `country` is the only column that appears in both\\n\", \"DataFrames, so it is used for `on` if we don’t pass anything.\\n\", \"\\n\", \"The following two are equivalent.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2017, sq_miles, on=\\\"country\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# if we move index back to columns, the `on` is un-necessary\\n\", \"pd.merge(wdi2017.reset_index(), sq_miles.reset_index())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `left_on`, `right_on`\\n\", \"\\n\", \"Above, we used the `on` argument to identify a column in both `left`\\n\", \"and `right` that was used to align data.\\n\", \"\\n\", \"Sometimes, both DataFrames don’t have the same name for this column.\\n\", \"\\n\", \"In that case, we use the `left_on` and `right_on` arguments, passing\\n\", \"the proper column name(s) to align the data.\\n\", \"\\n\", \"We’ll show you an example below, but it is somewhat silly as our\\n\", \"DataFrames do both have the `country` column.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2017, sq_miles, left_on=\\\"country\\\", right_on=\\\"country\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `left_index`, `right_index`\\n\", \"\\n\", \"Sometimes, as in our example, the key used to align data is actually in the\\n\", \"index instead of one of the columns.\\n\", \"\\n\", \"In this case, we can use the `left_index` or `right_index` arguments.\\n\", \"\\n\", \"We should only set these values to a boolean (`True` or `False`).\\n\", \"\\n\", \"Let’s practice with this.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(wdi2017, sq_miles, left_on=\\\"country\\\", right_index=True)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `how`\\n\", \"\\n\", \"The `how` is perhaps the most powerful, but most conceptually\\n\", \"difficult of the arguments we will cover.\\n\", \"\\n\", \"This argument controls which values from the key column(s) appear in the\\n\", \"output.\\n\", \"\\n\", \"The 4 possible options for this argument are summarized in\\n\", \"the image below.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/merge_venns.png\\\" alt=\\\"merge\\\\_venns.png\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \\n\", \"In words, we have:\\n\", \"\\n\", \"- `left`: Default and what we described above. It uses\\n\", \"  the keys from the `left` DataFrame.  \\n\", \"- `right`: Output will contain all keys from `right`.  \\n\", \"- `inner`: The output will only contain keys that appear in *both*\\n\", \"  `left` and `right`.  \\n\", \"- `outer`: The output will contain any key found in either `left`\\n\", \"  or `right`.  \\n\", \"\\n\", \"\\n\", \"In addition to the above, we will use the following two DataFrames to\\n\", \"illustrate the `how` option.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi2017_no_US = wdi2017.drop(\\\"United States\\\")\\n\", \"wdi2017_no_US\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"sq_miles_no_germany = sq_miles.drop(\\\"Germany\\\")\\n\", \"sq_miles_no_germany\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now, let’s see all the possible `how` options.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# default\\n\", \"pd.merge(wdi2017_no_US, sq_miles, on=\\\"country\\\", how=\\\"left\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# notice ``Russia`` is included\\n\", \"pd.merge(wdi2017, sq_miles, on=\\\"country\\\", how=\\\"right\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# notice no United States or Russia\\n\", \"pd.merge(wdi2017, sq_miles, on=\\\"country\\\", how=\\\"inner\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# includes all 5, even though they don't all appear in either DataFrame\\n\", \"pd.merge(wdi2017_no_US, sq_miles_no_germany, on=\\\"country\\\", how=\\\"outer\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `df.merge(df2)`\\n\", \"\\n\", \"Note that the DataFrame type has a `merge` *method*.\\n\", \"\\n\", \"It is the same as the function we have been working with, but passes the\\n\", \"DataFrame before the period as `left`.\\n\", \"\\n\", \"Thus `df.merge(other)` is equivalent to `pd.merge(df, other)`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi2017.merge(sq_miles, on=\\\"country\\\", how=\\\"right\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## `df.join`\\n\", \"\\n\", \"The `join` method for a DataFrame is very similar to the `merge`\\n\", \"method described above, but only allows you to use the index of the\\n\", \"`right` DataFrame as the join key.\\n\", \"\\n\", \"Thus, `left.join(right, on=\\\"country\\\")` is equivalent to calling\\n\", \"`pd.merge(left, right, left_on=\\\"country\\\", right_index=True)`.\\n\", \"\\n\", \"The implementation of the `join` method calls `merge` internally,\\n\", \"but sets the `left_on` and `right_index` arguments for you.\\n\", \"\\n\", \"You can do anything with `df.join` that you can do with\\n\", \"`df.merge`, but df.join` is more convenient to use if the keys of `right`\\n\", \"are in the index.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"wdi2017.join(sq_miles, on=\\\"country\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"wdi2017.merge(sq_miles, left_on=\\\"country\\\", right_index=True)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Case Study\\n\", \"\\n\", \"Let’s put these tools to practice by loading some real datasets and\\n\", \"seeing how these functions can be applied.\\n\", \"\\n\", \"We’ll analyze ratings of books from the website [Goodreads](https://www.goodreads.com/).\\n\", \"\\n\", \"We accessed the data [here](https://github.com/zygmuntz/goodbooks-10k).\\n\", \"\\n\", \"Let’s load it up.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://datascience.quantecon.org/assets/data/goodreads_ratings.csv.zip\\\"\\n\", \"ratings = pd.read_csv(url)\\n\", \"display(ratings.head())\\n\", \"ratings.info()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can already do some interesting things with just the ratings data.\\n\", \"\\n\", \"Let’s see how many ratings of each number are in our dataset.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"ratings[\\\"rating\\\"].value_counts().sort_index().plot(kind=\\\"bar\\\");\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s also see how many users have rated `N` books, for all `N`\\n\", \"possible.\\n\", \"\\n\", \"To do this, we will use `value_counts` twice (can you think of why?).\\n\", \"\\n\", \"We will see a more flexible way of performing similar grouped operations in\\n\", \"a future lecture.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"ratings.head()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"ratings[\\\"user_id\\\"].value_counts()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"users_by_n = (\\n\", \"    ratings[\\\"user_id\\\"]\\n\", \"    .value_counts()  # Series. Index: user_id, value: n ratings by user\\n\", \"    .value_counts()  # Series. Index: n_ratings by user, value: N_users with this many ratings\\n\", \"    .sort_index()    # Sort our Series by the index (number of ratings)\\n\", \"    .reset_index()   # Dataframe with columns `index` (from above) and `user_id`\\n\", \"    .rename(columns={\\\"index\\\": \\\"N_ratings\\\", \\\"user_id\\\": \\\"N_users\\\"})\\n\", \")\\n\", \"users_by_n.head(10)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s look at some statistics on that dataset.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"users_by_n.describe()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can see the same data visually in a box plot.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"users_by_n.plot(kind=\\\"box\\\", subplots=True)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s practice applying the want operator…\\n\", \"\\n\", \"**Want**: Determine whether a relationship between the number of\\n\", \"ratings a user has written and the distribution of the ratings exists. (Maybe we\\n\", \"are an author hoping to inflate our ratings and wonder if we should\\n\", \"target “more experienced” Goodreads users, or focus on newcomers.)\\n\", \"\\n\", \"Let’s start from the result and work our way backwards:\\n\", \"\\n\", \"1. We can answer our question if we have two similar DataFrames:  \\n\", \"  - All ratings by the `N` (e.g. 25) users with the most ratings  \\n\", \"  - All ratings by the `N` users with the least number of\\n\", \"    ratings  \\n\", \"1. To get that, we will need to extract rows of `ratings` with\\n\", \"  `user_id` associated with the `N` most and least prolific raters  \\n\", \"1. For that, we need the most and least active `user_id`s  \\n\", \"1. To get that info, we need a count of how many ratings each user left.  \\n\", \"  - We can get that with `df[\\\"user_id\\\"].value_counts()`, so let’s\\n\", \"    start there.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# step 4\\n\", \"n_ratings = ratings[\\\"user_id\\\"].value_counts()\\n\", \"n_ratings.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# step 3\\n\", \"N = 25\\n\", \"most_prolific_users = n_ratings.nlargest(N).index.tolist()\\n\", \"least_prolific_users = n_ratings.nsmallest(N).index.tolist()\\n\", \"\\n\", \"print(most_prolific_users)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# step 2\\n\", \"active_ratings = ratings.loc[ratings[\\\"user_id\\\"].isin(most_prolific_users), :]\\n\", \"inactive_ratings = ratings.loc[ratings[\\\"user_id\\\"].isin(least_prolific_users), :]\\n\", \"\\n\", \"active_ratings.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# step 1 -- get the answer!\\n\", \"active_ratings[\\\"rating\\\"].value_counts().sort_index().plot(\\n\", \"    kind=\\\"bar\\\", title=\\\"Distribution of ratings by most active users\\\"\\n\", \")\\n\", \"print(active_ratings[\\\"rating\\\"].mean())\\n\", \"print(active_ratings[\\\"rating\\\"].median())\\n\", \"print(active_ratings[\\\"rating\\\"].std())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"inactive_ratings[\\\"rating\\\"].value_counts().sort_index().plot(\\n\", \"    kind=\\\"bar\\\", title=\\\"Distribution of ratings by least active users\\\"\\n\", \")\\n\", \"print(inactive_ratings[\\\"rating\\\"].mean())\\n\", \"print(inactive_ratings[\\\"rating\\\"].median())\\n\", \"print(inactive_ratings[\\\"rating\\\"].std())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Nice! From the picture above, the new users look much more\\n\", \"likely to leave 5 star ratings than more experienced users.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Book Data\\n\", \"\\n\", \"We know what you are probably thinking: “Isn’t this a lecture on merging?\\n\", \"Why are we only using one dataset?”\\n\", \"\\n\", \"We hear you.\\n\", \"\\n\", \"Let’s also load a dataset containing information on the actual books.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://datascience.quantecon.org/assets/data/goodreads_books.csv\\\"\\n\", \"books = pd.read_csv(url)\\n\", \"\\n\", \"# we only need a few of the columns\\n\", \"books = books[[\\\"book_id\\\", \\\"authors\\\", \\\"title\\\"]]\\n\", \"print(\\\"shape: \\\", books.shape)\\n\", \"print(\\\"dtypes:\\\\n\\\", books.dtypes, sep=\\\"\\\")\\n\", \"books.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We could do similar interesting things with just the books dataset,\\n\", \"but we will skip it for now and merge them together.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"rated_books = pd.merge(ratings, books, on=\\\"book_id\\\", how=\\\"left\\\")\\n\", \"\\n\", \"rated_books.shape\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now, let’s see which books have been most often rated.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"most_rated_books_id = rated_books[\\\"book_id\\\"].value_counts().nlargest(10).index\\n\", \"most_rated_books = rated_books.loc[rated_books[\\\"book_id\\\"].isin(most_rated_books_id), :]\\n\", \"list(most_rated_books[\\\"title\\\"].unique())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s use our `pivot_table` knowledge to compute the average rating\\n\", \"for each of these books.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"most_rated_books.pivot_table(values=\\\"rating\\\", index=\\\"title\\\").sort_values(\\\"rating\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s compute the average rating for each book in our sample.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"average_ratings = (\\n\", \"    rated_books\\n\", \"    .pivot_table(values=\\\"rating\\\", index=\\\"title\\\")\\n\", \"    .sort_values(by=\\\"rating\\\", ascending=False)\\n\", \")\\n\", \"average_ratings.head(10)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"What does the overall distribution of average ratings look like?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# plot a kernel density estimate of average ratings\\n\", \"average_ratings.plot.density(xlim=(1, 5))\\n\", \"\\n\", \"# or a histogram\\n\", \"average_ratings.plot.hist(bins=30, xlim=(1, 5))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"It looks like most books have an average rating of just below 4.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Visualizing Merge Operations\\n\", \"\\n\", \"As we did in the [reshape lecture](reshape.ipynb), we will visualize the\\n\", \"various merge operations using artificial DataFrames.\\n\", \"\\n\", \"First, we create some dummy DataFrames.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"dfL = pd.DataFrame(\\n\", \"    {\\\"Key\\\": [\\\"A\\\", \\\"B\\\", \\\"A\\\", \\\"C\\\"], \\\"C1\\\":[1, 2, 3, 4], \\\"C2\\\": [10, 20, 30, 40]},\\n\", \"    index=[\\\"L1\\\", \\\"L2\\\", \\\"L3\\\", \\\"L4\\\"]\\n\", \")[[\\\"Key\\\", \\\"C1\\\", \\\"C2\\\"]]\\n\", \"\\n\", \"print(\\\"This is dfL: \\\")\\n\", \"display(dfL)\\n\", \"\\n\", \"dfR = pd.DataFrame(\\n\", \"    {\\\"Key\\\": [\\\"A\\\", \\\"B\\\", \\\"C\\\", \\\"D\\\"], \\\"C3\\\": [100, 200, 300, 400]},\\n\", \"    index=[\\\"R1\\\", \\\"R2\\\", \\\"R3\\\", \\\"R4\\\"]\\n\", \")[[\\\"Key\\\", \\\"C3\\\"]]\\n\", \"\\n\", \"print(\\\"This is dfR:\\\")\\n\", \"display(dfR)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `pd.concat`\\n\", \"\\n\", \"Recall that calling `pd.concat(..., axis=0)` will stack DataFrames on top of\\n\", \"one another:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.concat([dfL, dfR], axis=0)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Here’s how we might visualize that.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/concat_axis0.gif\\\" alt=\\\"concat\\\\_axis0.gif\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We can also set `axis=1` to stack side by side.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.concat([dfL, dfR], axis=1)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Here’s how we might visualize that.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/concat_axis1.gif\\\" alt=\\\"concat\\\\_axis1.gif\\\" style=\\\"\\\">\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `pd.merge`\\n\", \"\\n\", \"The animation below shows a visualization of what happens when we call\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(dfL, dfR, on=\\\"Key\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"<img src=\\\"https://datascience.quantecon.org/_images/left_merge.gif\\\" alt=\\\"left\\\\_merge.gif\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \\n\", \"Now, let’s focus on what happens when we set `how=\\\"right\\\"`.\\n\", \"\\n\", \"Pay special attention to what happens when filling the output value for\\n\", \"the key `A`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.merge(dfL, dfR, on=\\\"Key\\\", how=\\\"right\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"<img src=\\\"https://datascience.quantecon.org/_images/right_merge.gif\\\" alt=\\\"right\\\\_merge.gif\\\" style=\\\"\\\">\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"date\": 1595352472.696203, \"title\": \"Merge\", \"filename\": \"merge.rst\", \"kernelspec\": {\"name\": \"python3\", \"display_name\": \"Python 3.9.2 64-bit ('css': conda)\"}, \"celltoolbar\": \"Slideshow\", \"download_nb\": false, \"interpreter\": {\"hash\": \"e55df22e38eb6f84ed485cdc18bfaacd9fb4774a6d0c2caf3f6700c08b998f77\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.2\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"pandas/merge\"}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v3_pandas_data_merge.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.785Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 16,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 17,
    "description": "Data Manipulation in Pandas",
    "title": "Lecture 03",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 91,
    "position": 11,
    "content_id": 167,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 167,
    "type": "video",
    "description": "Learn about pandas' powerful merge and join functionality for combining multiple datasets",
    "title": "Merging Datasets",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=47cvISFv9iw\", \"youtubeVideoId\": \"47cvISFv9iw\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 16,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 17,
    "description": "Data Manipulation in Pandas",
    "title": "Lecture 03",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 89,
    "position": 9,
    "content_id": 174,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 174,
    "type": "video",
    "description": "Learn the fundamental reshaping operations in pandas to get the data in just the right format",
    "title": "Reshaping Data",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=svXlCneXIKc\", \"youtubeVideoId\": \"svXlCneXIKc\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 16,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 17,
    "description": "Data Manipulation in Pandas",
    "title": "Lecture 03",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 90,
    "position": 10,
    "content_id": 190,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 190,
    "type": "notebook",
    "description": "Learn the fundamental reshaping operations in pandas to get the data in just the right format",
    "title": "v2_pandas_data_reshape.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Reshape\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Intro](../p01_pandas_intro/v01_pandas_intro.ipynb)  \\n\", \"- [pandas basics](../p01_pandas_intro/v02_pandas_basics.ipynb)  \\n\", \"- [Importance of index](../p02_organizing_data_with_pandas_1/01_the_index.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand tidy data  \\n\", \"- Understand and be able to apply the `melt`/`stack`/`unstack`/`pivot` methods  \\n\", \"- Practice transformations of indices  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import numpy as np\\n\", \"import pandas as pd\\n\", \"\\n\", \"%matplotlib inline\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Reshape](#Reshape)  \\n\", \"  - [Tidy Data](#Tidy-Data)  \\n\", \"  - [Reshaping your Data](#Reshaping-your-Data)  \\n\", \"  - [Long vs Wide](#Long-vs-Wide)  \\n\", \"  - [`set_index`, `reset_index`, and Transpose](#`set_index`,-`reset_index`,-and-Transpose)  \\n\", \"  - [`stack` and `unstack`](#`stack`-and-`unstack`)  \\n\", \"  - [`melt`](#`melt`)  \\n\", \"  - [`pivot` and `pivot_table`](#`pivot`-and-`pivot_table`)  \\n\", \"  - [Visualizing Reshaping](#Visualizing-Reshaping)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Tidy Data\\n\", \"\\n\", \"While pushed more generally in the `R` language, the concept of “[tidy data](https://en.wikipedia.org/wiki/Tidy_data)” is helpful in understanding the\\n\", \"objectives for reshaping data, which in turn makes advanced features like\\n\", \"[GroupBy](groupby.ipynb) more seamless.\\n\", \"\\n\", \"Hadley Wickham gives a terminology slightly better-adapted for the experimental\\n\", \"sciences, but nevertheless useful for the social sciences.\\n\", \"\\n\", \"> A dataset is a collection of values, usually either numbers (if\\n\", \"quantitative) or strings (if qualitative). Values are organized in two\\n\", \"ways. Every value belongs to a variable and an observation. A variable\\n\", \"contains all values that measure the same underlying attribute (like\\n\", \"height, temperature, duration) across units. An observation contains all\\n\", \"values measured on the same unit (like a person, or a day, or a race)\\n\", \"across attributes. – [Tidy Data (Journal of Statistical Software 2013)](https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"With this framing,\\n\", \"\\n\", \"> A dataset is messy or tidy depending on how rows, columns and tables are\\n\", \"matched with observations, variables, and types. In tidy data:\\n\", \"1.  Each variable forms a column.\\n\", \"2.  Each observation forms a row.\\n\", \"3.  Each type of observational unit forms a table.\\n\", \"\\n\", \"The “column” and “row” terms map directly to pandas columns and rows, while the\\n\", \"“table” maps to a pandas DataFrame.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The question that should come to mind anytime you're introduced to a dataset is, \\\"What uniquely identifies an “observation” in your data?\\\"\\n\", \"\\n\", \"Is it a country? A year? A combination of country and year?\\n\", \"\\n\", \"These will become the indices of your DataFrame.\\n\", \"\\n\", \"The concept of an \\\"observation\\\" may  not be unique to a dataset. For example, consider a time-series of county level GDP data.\\n\", \"\\n\", \"* The most \\\"pure\\\" form of tidy data would probably classify the year/country as the identifier and have a single variable of GDP\\n\", \"* You could also consider the year to be the unique identifier and have each country's GDP be the variable\\n\", \"* Or, a variable might even be the GDP in a given year with the countries being the unique identifiers\\n\", \"\\n\", \"What you consider to be an observation will depend on the question you're asking.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Reshaping your Data\\n\", \"\\n\", \"The data you receive is not always in a “shape” that makes it easy to analyze.\\n\", \"\\n\", \"What do we mean by shape? The number of rows and columns in a\\n\", \"DataFrame and how information is stored in the index and column names.\\n\", \"\\n\", \"This lecture will teach you the basic concepts of reshaping data.\\n\", \"\\n\", \"As with other topics, we recommend reviewing the [pandas\\n\", \"documentation](https://pandas.pydata.org/pandas-docs/stable/reshaping.html)\\n\", \"on this subject for additional information.\\n\", \"\\n\", \"We will keep our discussion here as brief and simple as possible because\\n\", \"these tools will reappear in subsequent lectures.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://datascience.quantecon.org/assets/data/bball.csv\\\"\\n\", \"bball = pd.read_csv(url)\\n\", \"bball.info()\\n\", \"\\n\", \"bball\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Long vs Wide\\n\", \"\\n\", \"Many of the operations discussed change between long and wide DataFrames.\\n\", \"\\n\", \"What does it mean for a DataFrame to be long or wide?\\n\", \"\\n\", \"Here is long possible long-form representation of our basketball data.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Don't worry about what this command does -- We'll see it soon\\n\", \"bball_long = bball.melt(id_vars=[\\\"Year\\\", \\\"Player\\\", \\\"Team\\\", \\\"TeamName\\\"])\\n\", \"\\n\", \"bball_long\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"And here is a wide-form version.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Again, don't worry about this command... We'll see it soon too\\n\", \"bball_wide = bball_long.pivot_table(\\n\", \"    index=\\\"Year\\\",\\n\", \"    columns=[\\\"Player\\\", \\\"variable\\\", \\\"Team\\\"],\\n\", \"    values=\\\"value\\\"\\n\", \")\\n\", \"bball_wide\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## `set_index`, `reset_index`, and Transpose\\n\", \"\\n\", \"We have already seen a few basic methods for reshaping a\\n\", \"DataFrame.\\n\", \"\\n\", \"- `set_index`: Move one or more columns into the index.  \\n\", \"- `reset_index`: Move one or more index levels out of the index and make\\n\", \"  them either columns or drop from DataFrame.  \\n\", \"- `T`: Swap row and column labels.  \\n\", \"\\n\", \"\\n\", \"Sometimes, the simplest approach is the right approach.\\n\", \"\\n\", \"Let’s review them briefly.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball2 = bball.set_index([\\\"Player\\\", \\\"Year\\\"])\\n\", \"bball2.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"bball3 = bball2.T\\n\", \"bball3.head()\"], \"outputs\": [], \"metadata\": {\"scrolled\": true, \"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## `stack` and `unstack`\\n\", \"\\n\", \"The `stack` and `unstack` methods operate directly on the index\\n\", \"and/or column labels.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `stack`\\n\", \"\\n\", \"`stack` is used to move certain levels of the column labels into the\\n\", \"index (i.e. moving from wide to long)\\n\", \"\\n\", \"Let’s take `ball_wide` as an example.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball_wide\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Suppose that we want to be able to use the `mean` method to compute the\\n\", \"average value of each stat for each player, regardless of year or team.\\n\", \"\\n\", \"To do that, we need two column levels: one for the player and one for the variable.\\n\", \"\\n\", \"We can achieve this using the `stack` method.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball_wide.stack()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now, we can compute the statistic we are after.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"player_stats = bball_wide.stack().mean()\\n\", \"player_stats\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now suppose instead of that we wanted to compute the average for each team and\\n\", \"stat, averaging over years and players.\\n\", \"\\n\", \"We’d need to move the `Player` level down into the index so we are\\n\", \"left with column levels for Team and variable.\\n\", \"\\n\", \"We can ask pandas do this using the `level` keyword argument.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball_wide.stack(level=\\\"Player\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now we can compute the mean.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball_wide.stack(level=\\\"Player\\\").mean()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice a few features of the `stack` method:\\n\", \"\\n\", \"- Without any arguments, the `stack` arguments move the level of column\\n\", \"  labels closest to the data (also called inner-most or bottom level of labels)\\n\", \"  to become the index level closest to the data (also called the inner-most or\\n\", \"  right-most level of the index). In our example, this moved `Team` down from\\n\", \"  columns to the index.  \\n\", \"- When we do pass a level, that level of column labels is moved down to the\\n\", \"  right-most level of the index and all other column labels stay in their\\n\", \"  relative position.  \\n\", \"\\n\", \"\\n\", \"Note that we can also move multiple levels at a time in one call to `stack`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball_wide.stack(level=[\\\"Player\\\", \\\"Team\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"In the example above, we started with one level on the index (just the year) and\\n\", \"stacked two levels to end up with a three-level index.\\n\", \"\\n\", \"Notice that the two new index levels went closer to the data than the existing\\n\", \"level and that their order matched the order we passed in our list argument to\\n\", \"`level`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `unstack`\\n\", \"\\n\", \"Now suppose that we wanted to see a bar chart of each player’s stats.\\n\", \"\\n\", \"This chart should have one “section” for each player and a different colored\\n\", \"bar for each variable.\\n\", \"\\n\", \"As we’ll learn in more detail in a later lecture,  we will\\n\", \"need to have the player’s name on the index and the variables as columns to do this.\\n\", \"\\n\", \">**Note**\\n\", \">\\n\", \">In general, for a DataFrame, calling the `plot` method will put the index\\n\", \"on the horizontal (x) axis and make a new line/bar/etc. for each column.\\n\", \"\\n\", \"Notice that we are close to that with the `player_stats` variable.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"player_stats\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We now need to rotate the variable level of the index up to be column layers.\\n\", \"\\n\", \"We use the `unstack` method for this.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"player_stats.unstack()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"And we can make our plot!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"player_stats.unstack().plot.bar()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This particular visualization would be helpful if we wanted to see which stats\\n\", \"for which each player is strongest.\\n\", \"\\n\", \"For example, we can see that Steph Curry scores far more points than he does\\n\", \"rebound, but Serge Ibaka is a bit more balanced.\\n\", \"\\n\", \"What if we wanted to be able to compare all players for each statistic?\\n\", \"\\n\", \"This would be easier to do if the bars were grouped by variable, with a\\n\", \"different bar for each player.\\n\", \"\\n\", \"To plot this, we need to have the variables on the index and the player\\n\", \"name as column names.\\n\", \"\\n\", \"We can get this DataFrame by setting `level=\\\"Player\\\"` when calling `unstack`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"player_stats.unstack(level=\\\"Player\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"player_stats.unstack(level=\\\"Player\\\").plot.bar()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now we can use the chart to make a number of statements about players:\\n\", \"\\n\", \"- Ibaka does not get many assists, compared to Curry and Durant.  \\n\", \"- Steph and Kevin Durant are both high scorers.  \\n\", \"\\n\", \"\\n\", \"Based on the examples above, notice a few things about `unstack`:\\n\", \"\\n\", \"- It is the *inverse* of `stack`; `stack` will move labels down\\n\", \"  from columns to index, while `unstack` moves them up from index to columns.  \\n\", \"- By default, `unstack` will move the level of the index closest to the data\\n\", \"  and place it in the column labels closest to the data.  \\n\", \"\\n\", \"\\n\", \">**Note**\\n\", \">\\n\", \">Just as we can pass multiple levels to `stack`, we can also pass multiple\\n\", \"levels to `unstack`.\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Summary\\n\", \"\\n\", \"In some ways `set_index`, `reset_index`, `stack`, and `unstack`\\n\", \"are the “most fundamental” reshaping operations…\\n\", \"\\n\", \"The other operations we discuss can be formulated with these\\n\", \"four operations (and, in fact, some of them are exactly written as these\\n\", \"operations in `pandas`’s code base).\\n\", \"\\n\", \"*Pro tip*: We remember stack vs unstack with a mnemonic: **U**nstack moves index\\n\", \"levels **U**p\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## `melt`\\n\", \"\\n\", \"The `melt` method is used to move from wide to long form.\\n\", \"\\n\", \"It can be used to move all of the “values” stored in your DataFrame to a\\n\", \"single column with all other columns being used to contain identifying\\n\", \"information.\\n\", \"\\n\", \"**Warning**: When you use `melt`, any index that you currently have\\n\", \"will be deleted.\\n\", \"\\n\", \"We saw used `melt` above when we constructed `bball_long`:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# this is how we made ``bball_long``\\n\", \"bball.melt(id_vars=[\\\"Year\\\", \\\"Player\\\", \\\"Team\\\", \\\"TeamName\\\"])]]\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that the columns we specified as `id_vars` remained columns, but all\\n\", \"other columns were put into two new columns:\\n\", \"\\n\", \"1. `variable`: This has dtype string and contains the former column names.\\n\", \"  as values  \\n\", \"1. `value`: This has the former values.  \\n\", \"\\n\", \"\\n\", \"Using this method is an effective way to get our data in *tidy* form as noted\\n\", \"above.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Exercise 2**\\n\", \"\\n\", \"- What do you think would happen if we wrote `bball.melt(id_vars=[\\\"Year\\\", \\\"Player\\\"])`\\n\", \"  rather than `bball.melt(id_vars=[\\\"Year\\\", \\\"Player\\\", \\\"Team\\\", \\\"TeamName\\\"])`?\\n\", \"  Were you right? Write your thoughts.  \\n\", \"- Read the documentation and focus on the argument `value_vars`. How\\n\", \"  does `bball.melt(id_vars=[\\\"Year\\\", \\\"Player\\\"], value_vars=[\\\"Pts\\\", \\\"Rebound\\\"])`\\n\", \"  differ from `bball.melt(id_vars=[\\\"Year\\\", \\\"Player\\\"])`?  \\n\", \"- Consider the differences between `bball.stack` and `bball.melt`.\\n\", \"  Is there a way to make them generate the same output?\\n\", \"  (Hint: you might need to use both `stack` and another method from\\n\", \"  above)? Write your thoughts.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## `pivot` and `pivot_table`\\n\", \"\\n\", \"The next two reshaping methods that we will use are closely related.\\n\", \"\\n\", \"Some of you might even already be familiar with these ideas because you\\n\", \"have previously used *pivot tables* in Excel.\\n\", \"\\n\", \"- If so, good news. We think this is even more powerful than Excel\\n\", \"  and easier to use!  \\n\", \"- If not, good news. You are about to learn a very powerful and user-friendly tool.  \\n\", \"\\n\", \"\\n\", \"We will begin with `pivot`.\\n\", \"\\n\", \"The `pivot` method:\\n\", \"\\n\", \"- Takes the unique values of one column and places them along the index.  \\n\", \"- Takes the unique values of another column and places them along the\\n\", \"  columns.  \\n\", \"- Takes the values that correspond to a third column and fills in the\\n\", \"  DataFrame values that correspond to that index/column pair.  \\n\", \"\\n\", \"\\n\", \"We’ll illustrate with an example.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# .head 8 excludes Ibaka -- will discuss why later\\n\", \"bball.head(6).pivot(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can replicate `pivot` using three of the fundamental operations\\n\", \"from above:\\n\", \"\\n\", \"1. Call `set_index` with the `index` and `columns` arguments  \\n\", \"1. Extract the `values` column  \\n\", \"1. `unstack` the columns level of the new index  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#  1---------------------------------------  2---  3----------------------\\n\", \"bball.head(6).set_index([\\\"Year\\\", \\\"Player\\\"])[\\\"Pts\\\"].unstack(level=\\\"Player\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"One important thing to be aware of is that in order for `pivot` to\\n\", \"work, the index/column pairs must be *unique*!\\n\", \"\\n\", \"Below, we demonstrate the error that occurs when they are not unique.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Ibaka shows up twice in 2016 because he was traded mid-season from\\n\", \"# the Orlando Magic to the Toronto Raptors\\n\", \"bball.pivot(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `pivot_table`\\n\", \"\\n\", \"The `pivot_table` method is a generalization of `pivot`.\\n\", \"\\n\", \"It overcomes two limitations of `pivot`:\\n\", \"\\n\", \"1. It allows you to choose multiple columns for the index/columns/values\\n\", \"  arguments.  \\n\", \"1. It allows you to deal with duplicate entries by\\n\", \"  having you choose how to combine them.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that we can replicate the functionality of `pivot` if we pass\\n\", \"the same arguments.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball.head(6).pivot_table(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"But we can also choose multiple columns to be used in\\n\", \"index/columns/values.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball.pivot_table(index=[\\\"Year\\\", \\\"Team\\\"], columns=\\\"Player\\\", values=\\\"Pts\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"bball.pivot_table(index=\\\"Year\\\", columns=[\\\"Player\\\", \\\"Team\\\"], values=\\\"Pts\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"AND we can deal with duplicated index/column pairs.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# This produced an error\\n\", \"# bball.pivot(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\")\\n\", \"\\n\", \"# This doesn't!\\n\", \"bball_pivoted = bball.pivot_table(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\")\\n\", \"bball_pivoted\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"`pivot_table` handles duplicate index/column pairs using an aggregation.\\n\", \"\\n\", \"By default, the aggregation is the mean.\\n\", \"\\n\", \"For example, our duplicated index/column pair is `(\\\"x\\\", 1)` and had\\n\", \"associated values of 2 and 5.\\n\", \"\\n\", \"Notice that `bball_pivoted.loc[2016, \\\"Ibaka\\\"]` is `(15.1 + 14.2)/2 = 14.65`.\\n\", \"\\n\", \"We can choose how `pandas` aggregates all of the values.\\n\", \"\\n\", \"For example, here’s how we would keep the max.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball.pivot_table(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\", aggfunc=max)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Maybe we wanted to count how many values there were.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"bball.pivot_table(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\", aggfunc=len)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can even pass multiple aggregation functions!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"bball.pivot_table(index=\\\"Year\\\", columns=\\\"Player\\\", values=\\\"Pts\\\", aggfunc=[max, len])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise 3**\\n\", \"\\n\", \"- First, take a breath... That was a lot to take in.  \\n\", \"- Can you think of a reason to ever use `pivot` rather than\\n\", \"  `pivot_table`? Write your thoughts.  \\n\", \"- Create a pivot table with column `Player` as the index, `TeamName` as the\\n\", \"  columns, and `[Rebound, Assist]` as the values. What happens when you use\\n\", \"  `aggfunc=[np.max, np.min, len]`? Describe how Python produced\\n\", \"  each of the values in the resultant pivot table.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Visualizing Reshaping\\n\", \"\\n\", \"Now that you have learned the basics and had a chance to experiment,\\n\", \"we will use some generic data to provide a visualization of what the above\\n\", \"reshape operations do.\\n\", \"\\n\", \"The data we will use is:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# made up\\n\", \"# columns A and B are \\\"identifiers\\\" while C, D, and E are variables.\\n\", \"df = pd.DataFrame({\\n\", \"    \\\"A\\\": [0, 0, 1, 1],\\n\", \"    \\\"B\\\": \\\"x y x z\\\".split(),\\n\", \"    \\\"C\\\": [1, 2, 1, 4],\\n\", \"    \\\"D\\\": [10, 20, 30, 20,],\\n\", \"    \\\"E\\\": [2, 1, 5, 4,]\\n\", \"})\\n\", \"\\n\", \"df.info()\\n\", \"df\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df2 = df.set_index([\\\"A\\\", \\\"B\\\"])\\n\", \"df2.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df3 = df2.T\\n\", \"df3.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `stack` and `unstack`\\n\", \"\\n\", \"Below is an animation that shows how stacking works.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/stack.gif\\\" alt=\\\"stack.gif\\\" style=\\\"\\\">\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df2_stack = df2.stack()\\n\", \"df2_stack\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"And here is an animation that shows how unstacking works.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/unstack_level0.gif\\\" alt=\\\"unstack\\\\_level0.gif\\\" style=\\\"\\\">\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df2.unstack()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `melt`\\n\", \"\\n\", \"As noted above, the `melt` method transforms data from wide to long in form.\\n\", \"\\n\", \"Here’s a visualization of that operation.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/melt.gif\\\" alt=\\\"melt.gif\\\" style=\\\"\\\">\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df_melted = df.melt(id_vars=[\\\"A\\\", \\\"B\\\"])\\n\", \"df_melted\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"date\": 1595352472.897125, \"title\": \"Reshape\", \"filename\": \"reshape.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"download_nb\": false, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"pandas/reshape\"}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v2_pandas_data_reshape.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.785Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 16,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 17,
    "description": "Data Manipulation in Pandas",
    "title": "Lecture 03",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 87,
    "position": 9,
    "content_id": 152,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 152,
    "type": "video",
    "description": "Learn how to deal with messy data in pandas",
    "title": "Data Cleaning in Pandas",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=PFumg74dIso\", \"youtubeVideoId\": \"PFumg74dIso\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 16,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 17,
    "description": "Data Manipulation in Pandas",
    "title": "Lecture 03",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 88,
    "position": 9,
    "content_id": 188,
    "lecture_id": 17,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 188,
    "type": "notebook",
    "description": "Learn how to deal with messy data in pandas",
    "title": "v1_pandas_data_clean.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Cleaning Data\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Intro](../p01_pandas_intro/v01_pandas_intro.ipynb)  \\n\", \"- [Boolean selection](../p01_pandas_intro/v02_pandas_basics.ipynb)  \\n\", \"- [Indexing](../p02_organizing_data_with_pandas_1/01_the_index.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Be able to use string methods to clean data that comes as a string  \\n\", \"- Be able to drop missing data  \\n\", \"- Use cleaning methods to prepare and analyze a real dataset  \\n\", \"\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- Item information from about 3,000 Chipotle meals from about 1,800\\n\", \"  Grubhub orders  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pandas as pd\\n\", \"import numpy as np\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Cleaning Data](#Cleaning-Data)  \\n\", \"  - [Cleaning Data](#Cleaning-Data)  \\n\", \"  - [String Methods](#String-Methods)  \\n\", \"  - [Type Conversions](#Type-Conversions)  \\n\", \"  - [Missing Data](#Missing-Data)  \\n\", \"  - [Case Study](#Case-Study)  \\n\", \"  - [Appendix: Performance of `.str` Methods](#Appendix:-Performance-of-`.str`-Methods)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Cleaning Data\\n\", \"\\n\", \"For many data projects, a [significant proportion of\\n\", \"time](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#74d447456f63)\\n\", \"is spent collecting and cleaning the data — not performing the analysis.\\n\", \"\\n\", \"This non-analysis work is often called “data cleaning”.\\n\", \"\\n\", \"pandas provides very powerful data cleaning tools, which we\\n\", \"will demonstrate using the following dataset.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = pd.DataFrame({\\\"numbers\\\": [\\\"#23\\\", \\\"#24\\\", \\\"#18\\\", \\\"#14\\\", \\\"#12\\\", \\\"#10\\\", \\\"#35\\\"],\\n\", \"                   \\\"nums\\\": [\\\"23\\\", \\\"24\\\", \\\"18\\\", \\\"14\\\", np.nan, \\\"XYZ\\\", \\\"35\\\"],\\n\", \"                   \\\"colors\\\": [\\\"green\\\", \\\"red\\\", \\\"yellow\\\", \\\"orange\\\", \\\"purple\\\", \\\"blue\\\", \\\"pink\\\"],\\n\", \"                   \\\"other_column\\\": [0, 1, 0, 2, 1, 0, 2]})\\n\", \"df\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"What would happen if we wanted to try and compute the mean of\\n\", \"`numbers`?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df[\\\"numbers\\\"].mean()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"It throws an error!\\n\", \"\\n\", \"Can you figure out why?\\n\", \"\\n\", \"Hint: When looking at error messages, start at the very\\n\", \"bottom.\\n\", \"\\n\", \"The final error says, `TypeError: Could not convert #23#24... to numeric`.\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The problem is that if we look at the `dtypes` of the DataFrame that the elements of the `numbers` column are strings!\\n\", \"\\n\", \"We learned how to modify strings in one of the lectures about Python fundamentals. Let's modify one of the strings contained in the `numbers` column as a reminder:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"numbers_str = \\\"#35\\\"\\n\", \"\\n\", \"numbers_num = int(numbers_str.replace(\\\"#\\\", \\\"\\\"))\\n\", \"\\n\", \"print(numbers_str)\\n\", \"print(numbers_num)\\n\", \"print(type(numbers_num))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## String Methods\\n\", \"\\n\", \"Our solution to the previous exercise was to remove the `#` by using\\n\", \"the `replace` string method: `int(numbers_str.replace(\\\"#\\\", \\\"\\\"))`.\\n\", \"\\n\", \"One way to make this change to every element of a column would be to\\n\", \"loop through all elements of the column and apply the desired string\\n\", \"methods…\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%%time\\n\", \"\\n\", \"# Iterate over all rows\\n\", \"for row in df.iterrows():\\n\", \"\\n\", \"    # `iterrows` method produces a tuple with two elements...\\n\", \"    # The first element is an index and the second is a Series with the data from that row\\n\", \"    index_value, column_values = row\\n\", \"\\n\", \"    # Apply string method\\n\", \"    clean_number = int(column_values[\\\"numbers\\\"].replace(\\\"#\\\", \\\"\\\"))\\n\", \"\\n\", \"    # The `at` method is very similar to the `loc` method, but it is specialized\\n\", \"    # for accessing single elements at a time... We wanted to use it here to give\\n\", \"    # the loop the best chance to beat a faster method which we show you next.\\n\", \"    df.at[index_value, \\\"numbers_loop\\\"] = clean_number\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"While this is fast for a small dataset like this, this method slows for larger datasets.\\n\", \"\\n\", \"One *significantly* faster (and easier) method is to apply a string\\n\", \"method to an entire column of data.\\n\", \"\\n\", \"Most methods that are available to a Python string (we learned a\\n\", \"few of them in the [strings lecture](../python_fundamentals/basics.ipynb)) are\\n\", \"also available to a pandas Series that has `dtype` object.\\n\", \"\\n\", \"We access them by doing `s.str.method_name` where `method_name` is\\n\", \"the name of the method.\\n\", \"\\n\", \"When we apply the method to a Series, it is applied to all rows in the\\n\", \"Series in one shot!\\n\", \"\\n\", \"Let’s redo our previous example using a pandas `.str` method.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%%time\\n\", \"\\n\", \"# ~2x faster than loop... However, speed gain increases with size of DataFrame. The\\n\", \"# speedup can be in the ballpark of ~100-500x faster for big DataFrames.\\n\", \"# See appendix at the end of the lecture for an application on a larger DataFrame\\n\", \"df[\\\"numbers_str\\\"] = df[\\\"numbers\\\"].str.replace(\\\"#\\\", \\\"\\\")\\n\", \"\\n\", \"df.dtypes\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can use `.str` to access almost any string method that works on\\n\", \"normal strings. (See the [official\\n\", \"documentation](https://pandas.pydata.org/pandas-docs/stable/text.html)\\n\", \"for more information.)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df[\\\"colors\\\"].str.contains(\\\"p\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df[\\\"colors\\\"].str.capitalize()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Type Conversions\\n\", \"\\n\", \"In our example above, the `dtype` of the `numbers_str` column shows that pandas still treats\\n\", \"it as a string even after we have removed the `\\\"#\\\"`.\\n\", \"\\n\", \"We need to convert this column to numbers.\\n\", \"\\n\", \"The best way to do this is using the `pd.to_numeric` function.\\n\", \"\\n\", \"This method attempts to convert whatever is stored in a Series into\\n\", \"numeric values\\n\", \"\\n\", \"For example, after the `\\\"#\\\"` removed, the numbers of column\\n\", \"`\\\"numbers\\\"` are ready to be converted to actual numbers.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df[\\\"numbers_numeric\\\"] = pd.to_numeric(df[\\\"numbers_str\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.dtypes\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can convert to other types well.\\n\", \"\\n\", \"Using the `astype` method, we can convert to any of the supported\\n\", \"pandas `dtypes` (recall the [intro lecture](intro.ipynb)).\\n\", \"\\n\", \"Below are some examples. (Pay attention to the reported `dtype`)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df[\\\"numbers_numeric\\\"].astype(str)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df[\\\"numbers_numeric\\\"].astype(float)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Missing Data\\n\", \"\\n\", \"Many datasets have missing data.\\n\", \"\\n\", \"In our example, we are missing an element from the `\\\"nums\\\"` column.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can find missing data by using the `isnull` method.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df.isnull()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We might want to know whether particular rows or columns have any\\n\", \"missing data.\\n\", \"\\n\", \"To do this we can use the `.any` method on the boolean DataFrame\\n\", \"`df.isnull()`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df.isnull().any(axis=0)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.isnull().any(axis=1)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Many approaches have been developed to deal with missing data, but the two most commonly used (and the corresponding DataFrame method) are:\\n\", \"\\n\", \"- Exclusion: Ignore any data that is missing (`.dropna`).  \\n\", \"- Imputation: Compute “predicted” values for the data that is missing\\n\", \"  (`.fillna`).  \\n\", \"\\n\", \"\\n\", \"For the advantages and disadvantages of these (and other) approaches,\\n\", \"consider reading the [Wikipedia\\n\", \"article](https://en.wikipedia.org/wiki/Missing_data).\\n\", \"\\n\", \"For now, let’s see some examples.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# drop all rows containing a missing observation\\n\", \"df.dropna(axis=1)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# fill the missing values with a specific value\\n\", \"df.fillna(value=100)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# use the _next_ valid observation to fill the missing data\\n\", \"df.fillna(method=\\\"bfill\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# use the _previous_ valid observation to fill missing data\\n\", \"df.fillna(method=\\\"ffill\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We will see more examples of dealing with missing data in future\\n\", \"chapters.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Case Study\\n\", \"\\n\", \"We will now use data from an\\n\", \"[article](https://www.nytimes.com/interactive/2015/02/17/upshot/what-do-people-actually-order-at-chipotle.html)\\n\", \"written by The Upshot at the NYTimes.\\n\", \"\\n\", \"This data has order information from almost 2,000 Chipotle orders and\\n\", \"includes information on what was ordered and how much it cost.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://datascience.quantecon.org/assets/data/chipotle_raw.csv.zip\\\"\\n\", \"chipotle = pd.read_csv(url)\\n\", \"chipotle.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"chipotle.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise**\\n\", \"\\n\", \"We'd like you to use this data to answer the following questions.\\n\", \"\\n\", \"- What is the average price of an item with chicken?  \\n\", \"- What is the average price of an item with steak?  \\n\", \"- Did chicken or steak produce more revenue (total)?  \\n\", \"- How many missing items are there in this dataset? How many missing\\n\", \"  items in each column?  \\n\", \"\\n\", \"\\n\", \"Hint: before you will be able to do any of these things you will need to\\n\", \"make sure the `item_price` column has a numeric `dtype` (probably\\n\", \"float)\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Appendix: Performance of `.str` Methods\\n\", \"\\n\", \"Let’s repeat the “remove the `#`” example from above, but this time on\\n\", \"a much larger dataset.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import numpy as np\\n\", \"test = pd.DataFrame({\\\"floats\\\": np.round(100*np.random.rand(100_000), 2)})\\n\", \"test[\\\"strings\\\"] = test[\\\"floats\\\"].astype(str) + \\\"%\\\"\\n\", \"test.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"%%time\\n\", \"\\n\", \"for row in test.iterrows():\\n\", \"    index_value, column_values = row\\n\", \"    clean_number = column_values[\\\"strings\\\"].replace(\\\"%\\\", \\\"\\\")\\n\", \"    test.at[index_value, \\\"numbers_loop\\\"] = clean_number\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"%%time\\n\", \"test[\\\"numbers_str_method\\\"] = test[\\\"strings\\\"].str.replace(\\\"%\\\", \\\"\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"test[\\\"numbers_str_method\\\"].equals(test[\\\"numbers_loop\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We got the exact same result in a fraction of the time!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"date\": 1595352471.783975, \"title\": \"Cleaning Data\", \"filename\": \"data_clean.rst\", \"kernelspec\": {\"name\": \"python3\", \"display_name\": \"Python 3.9.2 64-bit ('css': conda)\"}, \"celltoolbar\": \"Slideshow\", \"download_nb\": false, \"interpreter\": {\"hash\": \"e55df22e38eb6f84ed485cdc18bfaacd9fb4774a6d0c2caf3f6700c08b998f77\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.2\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"pandas/data_clean\"}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v1_pandas_data_clean.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.785Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 9,
    "position": 2,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 10,
    "description": "Review",
    "title": "Lecture 04",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 95,
    "position": 9,
    "content_id": 169,
    "lecture_id": 10,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 169,
    "type": "video",
    "description": "Review of the pandas material covered so far using UN population data",
    "title": "Pandas Review: UN Population",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=zlZ77INU3n8\", \"youtubeVideoId\": \"zlZ77INU3n8\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 17,
    "position": 4,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 18,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 18,
    "description": "Groupby operations with Pandas",
    "title": "Lecture 05",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 98,
    "position": 9,
    "content_id": 141,
    "lecture_id": 18,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 141,
    "type": "notebook",
    "description": "Learn the split-aggregate-combine pattern for data analysis",
    "title": "01_groupby.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# GroupBy\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Functions](../python_fundamentals/functions.ipynb)  \\n\", \"- pandas introduction [1](intro.ipynb) and [2](basics.ipynb)  \\n\", \"- [Reshape](reshape.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand the split-apply-combine strategy for aggregate\\n\", \"  computations on groups of data  \\n\", \"- Be able use basic aggregation methods on `df.groupby` to compute\\n\", \"  within group statistics  \\n\", \"- Understand how to group by multiple keys at once  \\n\", \"\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- Details for all delayed US domestic flights in December 2016,\\n\", \"  obtained from the [Bureau of Transportation\\n\", \"  Statistics](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import random\\n\", \"import numpy as np\\n\", \"import pandas as pd\\n\", \"import matplotlib.pyplot as plt\\n\", \"\\n\", \"%matplotlib inline\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Split-Apply-Combine\\n\", \"\\n\", \"One powerful paradigm for analyzing data is the “Split-Apply-Combine”\\n\", \"strategy\\n\", \"\\n\", \"This strategy has three steps:\\n\", \"\\n\", \"1. `Split`: split the data into groups based on values in one or more columns.  \\n\", \"1. `Apply`: apply a function or routine to each group separately.  \\n\", \"1. `Combine`: combine the output of the apply step into a DataFrame,\\n\", \"  using the group identifiers as the index  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We will cover the core concepts here\\n\", \"\\n\", \"We **strongly** encourage you\\n\", \"to also study the [official\\n\", \"documentation](https://pandas.pydata.org/pandas-docs/stable/groupby.html)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To describe the concepts, we will need some data\\n\", \"\\n\", \"We'll start with artificial data and then use a real-world dataset\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"C = np.arange(1, 7, dtype=float)\\n\", \"C[[3, 5]] = np.nan\\n\", \"df = pd.DataFrame({\\n\", \"    \\\"A\\\" : [1, 1, 1, 2, 2, 2],\\n\", \"    \\\"B\\\" : [1, 1, 2, 2, 1, 1],\\n\", \"    \\\"C\\\": C,\\n\", \"})\\n\", \"df\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### First Example\\n\", \"\\n\", \"\\n\", \"To perform the **Split** step, we call the `groupby` method on our\\n\", \"DataFrame\\n\", \"\\n\", \"First argument to `groupby` is how we want to form group\\n\", \"\\n\", \"The most basic form of splitting is to use a single column\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"gbA = df.groupby(\\\"A\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"`gbA` has type `DataFrameGroupBy`:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"type(gbA)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We usually refer to this type as `GroupBy` for short\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We use `gb.get_group(group_name)` for the group with value `group_name`:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"gbA.get_group(1)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"gbA.get_group(2)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Note that we used the *values* in the `A` column to access groups\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-0'></a>\\n\", \"**Exercise 1**\\n\", \"\\n\", \"We can *apply* some of our favorite aggregation functions directly on the\\n\", \"`GroupBy` object\\n\", \"\\n\", \"Look closely at the output of the cells below\\n\", \"\\n\", \"How did pandas compute the sum of `gbA`? What happened to the `NaN`\\n\", \"entries in column `C`?\\n\", \"\\n\", \"Write your thoughts\\n\", \"\\n\", \"Hint: try `gbA.count()` or `gbA.mean()` if you can’t decide what\\n\", \"happened to the `NaN`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"gbA.sum()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-1'></a>\\n\", \"**Exercise 2**\\n\", \"\\n\", \"Use introspection (tab completion) to see what other aggregations are\\n\", \"defined for GroupBy objects.\\n\", \"\\n\", \"Pick three and evaluate them in the cells below.\\n\", \"\\n\", \"Does the output of each of these commands have the same features as the\\n\", \"output of `gbA.sum()` from above? If not, what is different?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# method 1\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# method 2\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# method 3\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can also group by multiple columns\\n\", \"\\n\", \"How?  pass a list of strings to `groupby`\\n\", \"\\n\", \"DataFrame will be split into collections of rows with unique combinations of requested columns\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"gbAB = df.groupby([\\\"A\\\", \\\"B\\\"])\\n\", \"type(gbAB)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# all rows below have A = 1 AND B = 1\\n\", \"gbAB.get_group((1, 1))\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that we still have a GroupBy object, so we can apply our favorite\\n\", \"aggregations.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"gbAB.count()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that the output is a DataFrame with two levels on the index\\n\", \"and a single column `C`. (Quiz: how do we know it is a DataFrame with\\n\", \"one column and not a Series?)\\n\", \"\\n\", \"This highlights a principle of how pandas handles the *Combine* part of\\n\", \"the strategy:\\n\", \"\\n\", \"> The index of the combined DataFrame will be the group identifiers,\\n\", \"with one index level per group key\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Custom Aggregate Functions\\n\", \"\\n\", \"So far, we have been applying built-in aggregations to our GroupBy object\\n\", \"\\n\", \"We can also apply custom aggregations to each group of a GroupBy in two\\n\", \"steps:\\n\", \"\\n\", \"1. Write our custom aggregation as a Python function\\n\", \"1. Passing our function as an argument to the `.agg` method of a GroupBy  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's try it!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def num_missing(df):\\n\", \"    \\\"Return the number of missing items in each column of df\\\"\\n\", \"    return df.isnull().sum()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can call this function on our original DataFrame to get the number of\\n\", \"missing items in each column\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"num_missing(df)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can also apply it to a GroupBy object to get the number of missing\\n\", \"items in each column *for each group*\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"gbA.agg(num_missing)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The function we write should either\\n\", \"\\n\", \"- Consume `DataFrame` and return `Series`\\n\", \"- Consume `Series`  and return `scalar`\\n\", \"\\n\", \"Pandas calls the function for each group\\n\", \"\\n\", \"For DataFrames, the function is called separately for each column\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Transforms: The `apply` Method\\n\", \"\\n\", \"As we saw in the [basics lecture](basics.ipynb), we can apply transforms to DataFrames\\n\", \"\\n\", \"We can do the same with GroupBy objects using the `.apply` method\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let’s see an example\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"def smallest_by_b(df):\\n\", \"    return df.nsmallest(2, \\\"B\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"gbA.apply(smallest_by_b)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**NOTE**: The return value above has a two-level index\\n\", \"\\n\", \"1. The value of `A`\\n\", \"2. The index from the original DataFrame\\n\", \"\\n\", \"The second layer carried the original DataFrames's index because `smallest_by_b` kept the original index in its return value\\n\", \"\\n\", \"If `smallest_by_b` returned a different index, that would have shown up in `gbA.apply(smallest_by_b)`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-2'></a>\\n\", \"**Exercise 3**\\n\", \"\\n\", \"This exercise has a few steps:\\n\", \"\\n\", \"1. Write a function that, given a DataFrame, computes each entry’s deviation from the mean of its column\\n\", \"2. Apply the function to `gbA`\\n\", \"3. With your neighbor describe what the index and and columns are? Where are the group keys (the `A` column)?\\n\", \"4. Determine the correct way to add these results back into `df` as new columns (Hint: remember the [merge](merge.ipynb) lecture)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# write function here\\n\", \"def deviation_from_mean(x):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Compute the deviation of each value of x from its mean\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    x: pd.Series, pd.DataFrame\\n\", \"        The Series or DataFrame for which to do the computation\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    x_hat: type(x)\\n\", \"        The transformed version of x\\n\", \"    \\\"\\\"\\\"\\n\", \"    return x - x.mean()\\n\", \"    \\n\", \"\\n\", \"\\n\", \"# apply function here\\n\", \"deviations = gbA.apply(deviation_from_mean)\\n\", \"deviations\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# add output of function as new columns to df here...\\n\", \"df.merge(\\n\", \"    deviations, \\n\", \"    left_index=True, \\n\", \"    right_index=True,\\n\", \"    suffixes=(\\\"\\\", \\\"_deviation\\\")\\n\", \")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### `pd.Grouper`\\n\", \"\\n\", \"Columns don't always contain desired groups\\n\", \"\\n\", \"Some examples are:\\n\", \"\\n\", \"- Grouping by a column and a level of the index  \\n\", \"- Grouping time series data at a particular frequency  \\n\", \"\\n\", \"pandas lets you do this through the `pd.Grouper` type\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To see it in action, let’s make a copy of `df` with `A` moved to the\\n\", \"index and a `Date` column added\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2 = df.copy()\\n\", \"df2[\\\"Date\\\"] = pd.date_range(\\n\", \"    start=pd.datetime.today().strftime(\\\"%m/%d/%Y\\\"),\\n\", \"    freq=\\\"BQ\\\",\\n\", \"    periods=df.shape[0]\\n\", \")\\n\", \"df2 = df2.set_index(\\\"A\\\")\\n\", \"df2\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can group by year\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2.groupby(pd.Grouper(key=\\\"Date\\\", freq=\\\"A\\\")).count()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can group by the `A` level of the index\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2.groupby(pd.Grouper(level=\\\"A\\\")).count()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can combine these to group by both\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2.groupby([pd.Grouper(key=\\\"Date\\\", freq=\\\"A\\\"), pd.Grouper(level=\\\"A\\\")]).count()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"And we can combine `pd.Grouper` with a string, where the string\\n\", \"denotes a column name\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df2.groupby([pd.Grouper(key=\\\"Date\\\", freq=\\\"A\\\"), \\\"B\\\"]).count()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Case Study: Airline Delays\\n\", \"\\n\", \"Let's practice on some real data!\\n\", \"\\n\", \"We'll revisit the airline dataset from the [merge](merge.ipynb) lecture\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url = \\\"https://datascience.quantecon.org/assets/data/airline_performance_dec16.csv.zip\\\"\\n\", \"air_dec = pd.read_csv(url, parse_dates = ['Date'])\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"First, we compute the average delay in arrival time for all carriers\\n\", \"each week\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"weekly_delays = (\\n\", \"    air_dec\\n\", \"    .groupby([pd.Grouper(key=\\\"Date\\\", freq=\\\"W\\\"), \\\"Carrier\\\"])\\n\", \"    [\\\"ArrDelay\\\"]               # extract one column\\n\", \"    .mean()                    # take average\\n\", \"    .unstack(level=\\\"Carrier\\\")  # Flip carrier up as column names\\n\", \")\\n\", \"weekly_delays\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s also plot this data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# plot\\n\", \"axs = weekly_delays.plot.bar(\\n\", \"    figsize=(10, 8), subplots=True, legend=False, sharex=True,\\n\", \"    sharey=True, layout=(4, 3), grid=False\\n\", \")\\n\", \"\\n\", \"# tweak spacing between subplots and xaxis   labels\\n\", \"axs[0,0].get_figure().tight_layout()\\n\", \"for ax in axs[-1, :]:\\n\", \"    ax.set_xticklabels(weekly_delays.index.strftime(\\\"%a, %b. %d'\\\"))\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Most delayed week ended on Sunday December (except for Frontier, who did *worse* on week of 25th)\\n\", \"\\n\", \"Let’s see why...\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"The `air_dec` DataFrame has information on the minutes of delay\\n\", \"attributed to 5 different categories:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"delay_cols = [\\n\", \"    'CarrierDelay',\\n\", \"    'WeatherDelay',\\n\", \"    'NASDelay',\\n\", \"    'SecurityDelay',\\n\", \"    'LateAircraftDelay'\\n\", \"]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s take a quick look at each of those delay categories for the week ending December 18, 2016\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pre_christmas = air_dec.loc[\\n\", \"    (air_dec[\\\"Date\\\"] >= \\\"2016-12-12\\\") & (air_dec[\\\"Date\\\"] <= \\\"2016-12-18\\\")\\n\", \"]\\n\", \"\\n\", \"# custom agg function\\n\", \"def positive(df):\\n\", \"    return (df > 0).sum()\\n\", \"\\n\", \"delay_totals = pre_christmas.groupby(\\\"Carrier\\\")[delay_cols].agg([\\\"sum\\\", \\\"mean\\\", positive])\\n\", \"delay_totals\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Want**: plot total, average, and number of each type of delay by\\n\", \"carrier\\n\", \"\\n\", \"To do this, we need to have a DataFrame with:\\n\", \"\\n\", \"- Delay type in index (so it is on horizontal-axis)  \\n\", \"- Aggregation method on *outer* most level of columns (so we can do\\n\", \"  `data[\\\"mean\\\"]` to get averages)  \\n\", \"- Carrier name on inner level of columns  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Many sequences of the reshaping commands can accomplish this\\n\", \"\\n\", \"We show one example below\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"reshaped_delays = (\\n\", \"    delay_totals\\n\", \"    .stack()             # move aggregation method into index (with Carrier)\\n\", \"    .T                   # put delay type in index and Carrier+agg in column\\n\", \"    .swaplevel(axis=1)   # make agg method outer level of column label\\n\", \"    .sort_index(axis=1)  # sort column labels so it prints nicely\\n\", \")\\n\", \"reshaped_delays\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"for agg in [\\\"mean\\\", \\\"sum\\\", \\\"positive\\\"]:\\n\", \"    axs = reshaped_delays[agg].plot(\\n\", \"        kind=\\\"bar\\\", subplots=True, layout=(4, 3), figsize=(10, 8), legend=False,\\n\", \"        sharex=True, sharey=True\\n\", \"    )\\n\", \"    fig = axs[0, 0].get_figure()\\n\", \"    fig.suptitle(agg)\\n\", \"#     fig.tight_layout();\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-3'></a>\\n\", \"**Exercise 4**\\n\", \"\\n\", \"Think about what is shown in the the plots above\\n\", \"\\n\", \"Answer questions like:\\n\", \"\\n\", \"- Which type of delay was the most common?  \\n\", \"- Which one caused the largest average delay?  \\n\", \"- Does that vary by airline?  \\n\", \"\\n\", \"\\n\", \"Write your thoughts\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# your code here if needed\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s summarize what we did:\\n\", \"\\n\", \"- Computed average flight delay for each airline for each week  \\n\", \"- Noticed that one week had more delays for all airlines\\n\", \"- Studied the flights in that week to determine the *cause* of the\\n\", \"  delays in that week\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Suppose now that we want to repeat that analysis, but at a daily\\n\", \"frequency instead of weekly\\n\", \"\\n\", \"We could copy/paste the code from above and change the `W` to a `D`,\\n\", \"but there’s a better way…\\n\", \"\\n\", \"Let’s convert the steps above into two functions:\\n\", \"\\n\", \"1. Produce the set of bar charts for average delays at each frequency \\n\", \"1. Produce the second set of charts for the total, average, and number of occurrences of each type of delay\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def mean_delay_plot(df, freq, figsize=(10, 8)):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Make a bar chart of average flight delays for each carrier at\\n\", \"    a given frequency.\\n\", \"    \\\"\\\"\\\"\\n\", \"    mean_delays = (\\n\", \"        df\\n\", \"        .groupby([pd.Grouper(key=\\\"Date\\\", freq=freq), \\\"Carrier\\\"])\\n\", \"        [\\\"ArrDelay\\\"]               # extract one column\\n\", \"        .mean()                    # take average\\n\", \"        .unstack(level=\\\"Carrier\\\")  # Flip carrier up as column names\\n\", \"    )\\n\", \"\\n\", \"    # plot\\n\", \"    axs = mean_delays.plot.bar(\\n\", \"        figsize=figsize, subplots=True, legend=False, sharex=True,\\n\", \"        sharey=True, layout=(4, 3), grid=False\\n\", \"    )\\n\", \"\\n\", \"    # tweak spacing between subplots and x-axis labels\\n\", \"    axs[0, 0].get_figure().tight_layout()\\n\", \"    for ax in axs[-1, :]:\\n\", \"        ax.set_xticklabels(mean_delays.index.strftime(\\\"%a, %b. %d'\\\"))\\n\", \"\\n\", \"    # return the axes in case we want to further tweak the plot outside the function\\n\", \"    return axs\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"def delay_type_plot(df, start, end):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Make bar charts for total minutes, average minutes, and number of\\n\", \"    occurrences for each delay type, for all flights that were scheduled\\n\", \"    between `start` date and `end` date\\n\", \"    \\\"\\\"\\\"\\n\", \"    sub_df = df.loc[\\n\", \"        (df[\\\"Date\\\"] >= start) & (df[\\\"Date\\\"] <= end)\\n\", \"    ]\\n\", \"\\n\", \"    def positive(df):\\n\", \"        return (df > 0).sum()\\n\", \"\\n\", \"    aggs = sub_df.groupby(\\\"Carrier\\\")[delay_cols].agg([\\\"sum\\\", \\\"mean\\\", positive])\\n\", \"\\n\", \"    reshaped = aggs.stack().T.swaplevel(axis=1).sort_index(axis=1)\\n\", \"\\n\", \"    for agg in [\\\"mean\\\", \\\"sum\\\", \\\"positive\\\"]:\\n\", \"        axs = reshaped[agg].plot(\\n\", \"            kind=\\\"bar\\\", subplots=True, layout=(4, 3), figsize=(10, 8), legend=False,\\n\", \"            sharex=True, sharey=True\\n\", \"        )\\n\", \"        fig = axs[0, 0].get_figure()\\n\", \"        fig.suptitle(agg)\\n\", \"#         fig.tight_layout();\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"\\n\", \"<a id='exercise-4'></a>\\n\", \"**Exercise 5**\\n\", \"\\n\", \"Verify that we wrote the functions properly by setting the arguments to\\n\", \"appropriate values to replicate the plots from above.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# call mean_delay_plot here\\n\", \"mean_delay_plot(air_dec, \\\"W\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# call delay_type_plot here\\n\", \"delay_type_plot(air_dec, \\\"2016-12-12\\\", \\\"2016-12-18\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now let’s look at that plot at a daily frequency\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# figure needs to be a bit wider to see all the dates\\n\", \"mean_delay_plot(air_dec, \\\"D\\\", figsize=(16, 8));\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"As we expected given our analysis above, the longest average delays\\n\", \"seemed to happen in the third week\\n\", \"\\n\", \"In particular, it looks like December 17th and 18th had — on average —\\n\", \"higher delays than other days in December\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let’s use the `delay_type_plot` function to determine the cause of the\\n\", \"delays on those two days\\n\", \"\\n\", \"Because our analysis is captured in a single function, we can look at\\n\", \"the days together and separately without much effort\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# both days\\n\", \"delay_type_plot(air_dec, \\\"12-17-16\\\", \\\"12-18-16\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# only the 17th\\n\", \"delay_type_plot(air_dec, \\\"12-17-16\\\", \\\"12-17-16\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# only the 18th\\n\", \"delay_type_plot(air_dec, \\\"12-18-16\\\", \\\"12-18-16\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"- The purpose of this exercise was to drive home the ability to *automate* tasks\\n\", \"- We wrote a pair of `functions` that allow us to easily repeat the exact same analysis on different subsets of the data, or different datasets entirely (e.g. we could do the same analysis on November 2016 data, with two lines of code)\\n\", \"- These principles can be applied in many settings\\n\", \"- Keep that in mind as we work through the rest of the materials\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Exercise: Cohort Analysis using Shopify Data\\n\", \"\\n\", \"The `qeds` library includes routines to simulate data sets in the\\n\", \"format of common sources\\n\", \"\\n\", \"One of these sources is [Shopify](https://www.shopify.com/) — an\\n\", \"e-commerce platform used by many retail companies for online sales\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The code below will simulate a fairly large data set that has the\\n\", \"properties of a order-detail report from Shopify\\n\", \"\\n\", \"We’ll first look at the data, and then describe the exercise\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Set the \\\"randomness\\\" seeds\\n\", \"random.seed(42)\\n\", \"np.random.seed(42)\\n\", \"\\n\", \"url = \\\"https://datascience.quantecon.org/assets/data/shopify_orders.csv.zip\\\"\\n\", \"orders = pd.read_csv(url)\\n\", \"orders.info()\\n\", \"\\n\", \"orders.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Definition:** A customer’s cohort is the month in which a customer placed\\n\", \"their first order\\n\", \"\\n\", \"The customer type column indicates whether order was placed by a new or returning customer\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We now describe the *want* for the exercise, which we ask you to complete\\n\", \"\\n\", \"**Want**: Compute the monthly total number of orders, total sales, and\\n\", \"total quantity separated by customer cohort and customer type\\n\", \"\\n\", \"Read that carefully one more time…\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Extended Exercise\\n\", \"\\n\", \"Using the reshape and `groupby` tools you have learned, apply the want\\n\", \"operator described above\\n\", \"\\n\", \"See below for advice on how to proceed\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"When you are finished, you should have something that looks like this:\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/_images/groupby_cohort_analysis_exercise_output.png\\\" alt=\\\"groupby\\\\_cohort\\\\_analysis\\\\_exercise\\\\_output.png\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Two notes on the table above:\\n\", \"\\n\", \"1. Your actual output will be much bigger. This is just to give you an idea of what it might look like\\n\", \"1. The numbers you produce should actually be the same as what are included in this table… Index into your answer and compare what you have with this table to verify your progress  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Now, how to do it?\\n\", \"\\n\", \"There is more than one way to code this, but here are some suggested\\n\", \"steps.\\n\", \"\\n\", \"1. Convert the `Day` column to have a `datetime` `dtype` instead of object (Hint: use the `pd.to_datetime` function)\\n\", \"1. Add a new column that specifies the date associated with each\\n\", \"  customer’s `\\\"First-time\\\"` order\\n\", \"  - Hint 1: You can do this with a combination of `groupby` and\\n\", \"    `join`\\n\", \"  - Hint 2: `customer_type` is always one of `Returning` and\\n\", \"    `First-time`  \\n\", \"  - Hint 3: Some customers don’t have a\\n\", \"    `customer_type == \\\"First-time\\\"` entry. You will need to set the\\n\", \"    value for these users to some date that precedes the dates in the\\n\", \"    sample. After adding valid data back into `orders` DataFrame,\\n\", \"    you can identify which customers don’t have a `\\\"First-Time\\\"`\\n\", \"    entry by checking for missing data in the new column.  \\n\", \"1. You’ll need to group by 3 things  \\n\", \"1. You can apply one of the built-in aggregation functions to the GroupBy\\n\", \"1. After doing the aggregation, you’ll need to use your reshaping skills to\\n\", \"  move things to the right place in rows and columns\\n\", \"\\n\", \"\\n\", \"Good luck!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"date\": 1596736608.073952, \"title\": \"GroupBy\", \"filename\": \"groupby.rst\", \"kernelspec\": {\"name\": \"css\", \"language\": \"python\", \"display_name\": \"css\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.9.2\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"01_groupby.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.786Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 17,
    "position": 4,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 18,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 18,
    "description": "Groupby operations with Pandas",
    "title": "Lecture 05",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 97,
    "position": 9,
    "content_id": 154,
    "lecture_id": 18,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 154,
    "type": "video",
    "description": "Learn how to use the groupby method to perform powerful data aggregations",
    "title": "Groupby Operations",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=73R5pesuTbY\", \"youtubeVideoId\": \"73R5pesuTbY\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 104,
    "position": 11,
    "content_id": 148,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 148,
    "type": "notebook",
    "description": "Learn how to use the BLS API to download labor statistics data",
    "title": "03_bls_api.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# BLS API\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- APIs\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Register for an API key to access data from US Bureau of Labor Statistics (BLS)\\n\", \"- Write Python code that integrates with the BLS API\\n\", \"- Construct a dataset of unemployment data as reporeted by the BLS\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- All data from monthly unemployment reports from US Bureau of Labor Statistics\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## The BLS API\\n\", \"\\n\", \"The US Bureau of Labor statistics collects data on employment statistics for the US\\n\", \"\\n\", \"They provide this data to the public via an API\\n\", \"\\n\", \"The data is available without the need for an API key\\n\", \"\\n\", \"However, if we supply an API key we can get additional information in the response\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Registering for an API key\\n\", \"\\n\", \"Let's open the [landing page](https://www.bls.gov/developers/) for the BLS API\\n\", \"\\n\", \"While there we will look for how to request an API key\\n\", \"\\n\", \"Once we recieve the API key via email, we will store it somewhere safe \\n\", \"\\n\", \"Treat it like a password -- in the world of APIs that's what it is!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Building the request\\n\", \"\\n\", \"We now need to use the API documentation to learn how to build a request\\n\", \"\\n\", \"Recall that a request has the following components:\\n\", \"\\n\", \"- Endpoint\\n\", \"- Request type\\n\", \"- Query Parameters\\n\", \"- Payload\\n\", \"- Headers\\n\", \"- Authentication\\n\", \"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We'll find these one at a time below\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Endpoint\\n\", \"\\n\", \"Let's go to the API documentation\\n\", \"\\n\", \"Our goal here is to find the endpoint where we can access the data\\n\", \"\\n\", \"We find the docs by clicking the [\\\"BLS API Signatures\\\"](https://www.bls.gov/developers/api_signature_v2.htm) link on the [landing page](https://www.bls.gov/developers/)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"As shown in the docs, our endpoint is `https://api.bls.gov/publicAPI/v2/timeseries/data`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Request Type and Query Parameters\\n\", \"\\n\", \"Let's find the request type\\n\", \"\\n\", \"The docs say \\n\", \"\\n\", \"> \\\"HTTP Type: POST\\\", \\n\", \"\\n\", \"meaning we need to use a `POST` request\\n\", \"\\n\", \"We can specify all information in the *body* of the request, so no query parameters are needed\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"###  Payload\\n\", \"\\n\", \"The docs indicate the structure of the payload\\n\", \"\\n\", \"They say\\n\", \"\\n\", \"```json\\n\", \"{\\n\", \"    \\\"seriesid\\\":[\\\"Series1\\\",..., \\\"SeriesN\\\"],\\n\", \"    \\\"startyear\\\":\\\"yearX\\\", \\n\", \"    \\\"endyear\\\":\\\"yearY\\\",\\n\", \"    \\\"catalog\\\":true|false,\\n\", \"    \\\"calculations\\\":true|false,\\n\", \"    \\\"annualaverage\\\":true|false,\\n\", \"    \\\"aspects\\\":true|false,\\n\", \"    \\\"registrationkey\\\":\\\"995f4e779f204473aa565256e8afe73e\\\"\\n\", \"}\\n\", \"```\\n\", \"\\n\", \"The required parameters are `seriesid`, `startyear`, and `endyear`\\n\", \"\\n\", \"We will also pass `catalog: true` so we can get extra information about each variable\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Headers\\n\", \"\\n\", \"The only header we need in our request is `\\\"Content-Type\\\": \\\"application/json\\\"`\\n\", \"\\n\", \"This tells the api provider that the payload or body is JSON\\n\", \"\\n\", \"> Note: recall JSON is web-speak for a Python dict\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Authentication\\n\", \"\\n\", \"To identify ourselves to the API provider, we use our API key\\n\", \"\\n\", \"As noted in the payload section above, we set the `registrationkey` field in our payload to the API Key\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Summary\\n\", \"\\n\", \"Let's summarize:\\n\", \"\\n\", \"- We will make `POST` requests to `https://api.bls.gov/publicAPI/v2/timeseries/data/`\\n\", \"- We don't have any query parameters\\n\", \"- We pass `registrationkey`, `seriesid`, `startyear`, `endyear`, and `catalog` in the payload\\n\", \"- And we set headers to `{\\\"Content-Type\\\": \\\"application/json\\\"}` note our payload is JSON\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Python requests\\n\", \"\\n\", \"Let's give it a shot!\\n\", \"\\n\", \"We'll use the python `requests` package to make the HTTP requests\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import requests\\n\", \"import json\\n\", \"import pandas as pd\\n\", \"from typing import List, Union, Optional\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Making the request\\n\", \"\\n\", \"Below we define a Python function that makes the request\\n\", \"\\n\", \"Let's study it together\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def request_for_series(\\n\", \"        series_ids: List[str], \\n\", \"        startyear: Union[int,str], \\n\", \"        endyear: Union[int,str],\\n\", \"        apikey:Optional[str]=None,\\n\", \"    ):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Request data for all ``series_ids`` between ``startyear`` and ``endyear``\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    series_ids: List[str]\\n\", \"        A list of all BLS series IDs for which to request data\\n\", \"    \\n\", \"    startyear, endyear: Union[int,str]\\n\", \"        Starting and ending years for period of data. All intervals between\\n\", \"        these two years (inclusive) will be reported\\n\", \"    \\n\", \"    apikey: Optional[str]\\n\", \"        A registration or API key to enable more extensive use of the\\n\", \"        api and more detailed results\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    responses: List[requests.Response]\\n\", \"        A list of `Response` objects from the requests library\\n\", \"    \\n\", \"    Notes\\n\", \"    -----\\n\", \"    The BLS API only allows 25 series to be requested in a single call to the API\\n\", \"    This function allows an arbitrary number of series. The function first checks how \\n\", \"    many series_ids are reuqested, and then makes two recursive calls to this function:\\n\", \"    (1) the first 25 series IDs are fetched and (2) the rest of the series ids. If the\\n\", \"    second request contains more than 25 series, another split is made and a pair of \\n\", \"    recursive function calls are issued.\\n\", \"    \\n\", \"    Also note that the response objects from ``requests`` are not processed or validated\\n\", \"    in any way -- this is up to the caller of this routine.\\n\", \"    \\n\", \"    Finally, if an apikey is given, then a catalog of series metadata will be requested\\n\", \"    and returned from this function. This is necessary for getting the metadata DataFrame\\n\", \"    from the functions ``make_dfs_from_series``, ``unpack_response``, and \\n\", \"    ``unpack_all_responses`` functions below.\\n\", \"    \\\"\\\"\\\"\\n\", \"    n_series = len(series_ids)\\n\", \"    if n_series > 25:\\n\", \"        parts = []\\n\", \"        # make common keyword arguments so we don't have to type twice below\\n\", \"        kw = dict(endyear=endyear, startyear=startyear, apikey=apikey)\\n\", \"        parts.extend(request_for_series(series_ids[:25], **kw))\\n\", \"        parts.extend(request_for_series(series_ids[25:], **kw))\\n\", \"        return parts\\n\", \"    else:\\n\", \"        headers = {'Content-type': 'application/json'}\\n\", \"        params = {\\n\", \"            \\\"seriesid\\\": series_ids, \\n\", \"            \\\"startyear\\\":startyear, \\n\", \"            \\\"endyear\\\": endyear,\\n\", \"        }\\n\", \"        if apikey is not None:\\n\", \"            params[\\\"catalog\\\"] = True\\n\", \"            params[\\\"registrationkey\\\"] = apikey\\n\", \"        \\n\", \"        # convert params dictionary to json string\\n\", \"        data = json.dumps(params)\\n\", \"        p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\\n\", \"        return [p]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let's make a request\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# These lines are to keep my api key private, they likely won't work for you!\\n\", \"# you should replace right hand side of `apikey = ` with a string containing\\n\", \"# your api key!\\n\", \"import os\\n\", \"apikey = \\\"b1c735cea7314e1db2523669dad8bdf9\\\"\\n\", \"# os.environ.get(\\\"BLS_KEY\\\", None)\\n\", \"\\n\", \"# make the request\\n\", \"responses = request_for_series([\\\"LAUCN040010000000005\\\"], 2010, 2022, apikey)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"type(responses[0])\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Validate responses\\n\", \"\\n\", \"Our `request_for_series` function will make the `POST` requests and return a list of responses\\n\", \"\\n\", \"It does not, however, check to see if the requests were successful\\n\", \"\\n\", \"We'll now write some code to verify that requests were successful\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### HTTP codes\\n\", \"\\n\", \"Part of the HTTP standard is that a response will include an integer that is the response code\\n\", \"\\n\", \"Codes are three digits\\n\", \"\\n\", \"Successful resposnes are between 200 and 299\\n\", \"\\n\", \"The next function checks that the request code is no more than 299\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def check_response(res: requests.Response):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Check a response from the BLS API for success\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    res: requests.Response\\n\", \"        The requests object returned from iteracting with BLS API\\n\", \"        \\n\", \"    Notes\\n\", \"    -----\\n\", \"    Right now we just check for success at the http protocol level\\n\", \"    and don't do any checking specific to the BLS api\\n\", \"    \\\"\\\"\\\"\\n\", \"    code = res.status_code\\n\", \"    if code > 299:\\n\", \"        raise ValueError(f\\\"Response error with code {code}\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can apply our function to the response we recieved\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"check_response(responses[0])\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"It did not return or do anything, which is great!\\n\", \"\\n\", \"The function would have raised an error if there was a problem\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Unpacking the data\\n\", \"\\n\", \"Let's take a closer look at the data that was returned to us\\n\", \"\\n\", \"The `requests.Response` type has a `.content` field that includes the data returned from the API provider:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"responses[0].headers\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"responses[0].content \"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"It appears that this is JSON\\n\", \"\\n\", \"We can call the `.json()` method to have `requests` read the `content` into a dict\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"js = responses[0].json()\\n\", \"js\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice the structure of the response:\\n\", \"\\n\", \"```json\\n\", \"{\\n\", \"    'status': 'REQUEST_SUCCEEDED',\\n\", \"    'responseTime': 190,\\n\", \"    'message': [],\\n\", \"    'Results': {\\n\", \"        'series': [\\n\", \"            {\\n\", \"                'seriesID': 'LAUCN040010000000005',\\n\", \"                'catalog': {\\n\", \"                    'series_title': 'Employment: Apache County, AZ (U)',\\n\", \"                    'series_id': 'LAUCN040010000000005',\\n\", \"                    ...\\n\", \"                },\\n\", \"                'data': [\\n\", \"                    ...\\n\", \"                ]\\n\", \"            }\\n\", \"        ]\\n\", \"\\n\", \"    }\\n\", \"}\\n\", \"```\\n\", \"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"A few points:\\n\", \"\\n\", \"- The results are in `js[\\\"Results\\\"][\\\"series\\\"]`\\n\", \"- This contains a list of dictionaries, each one having\\n\", \"    - `seriesID`: BLS ID for series\\n\", \"    - `catalog`: more information about the series (only if you used a valid apikey)\\n\", \"    - `data`: a list of dictionaries with actual data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We'll need a Python function to operate on one of these series results\\n\", \"\\n\", \"We'd like it to return a DataFrame with data and Series with metadata\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def make_dfs_from_series(series_results):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Unpack a series response object into data and metadata pandas objects\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    series_results: dict\\n\", \"        A dictionary returned from the `timeseries/data` endpoint of the\\n\", \"        BLS api. An example object for this parameter would be found at\\n\", \"        ``res.json()[\\\"Results\\\"][\\\"series\\\"][0]`` where ``res`` is the \\n\", \"        ``requests.Response`` obtained from interacting with the API endpoint.\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    data: pd.DataFrame\\n\", \"        A pandas DataFrame containing the actual observations of the data series\\n\", \"    \\n\", \"    metadata: Optional[pd.Series]\\n\", \"        If the ``\\\"catgalog\\\"`` key exists in ``series_results``, then ``metadata``\\n\", \"        is a pandas Series containing the catalog information. If ``\\\"catalog\\\"``\\n\", \"        is found, then this is None    \\n\", \"    \\\"\\\"\\\"\\n\", \"    series_id = series_results[\\\"seriesID\\\"]\\n\", \"    data = pd.DataFrame(series_results[\\\"data\\\"]).assign(series_id=series_id)\\n\", \"    if \\\"catalog\\\" in series_results:\\n\", \"        metadata = pd.Series(series_results[\\\"catalog\\\"])\\n\", \"    else:\\n\", \"        metadata = None\\n\", \"    return data, metadata\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let's test it out!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"data, meta = make_dfs_from_series(js[\\\"Results\\\"][\\\"series\\\"][0])\\n\", \"data\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"meta\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Many DataFrames\\n\", \"\\n\", \"We're getting closer!\\n\", \"\\n\", \"Notice how the `js[\\\"Results\\\"][\\\"series\\\"]` was a list?\\n\", \"\\n\", \"There will be one of the series results per series requested from the API\\n\", \"\\n\", \"We need a function to take a single request `Response` and process the whole list of series data inside\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def unpack_response(res: requests.Response):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Unpack the response for requesting one or more timeseries \\n\", \"    from the BLS api\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    res: requests.Response\\n\", \"        The object returned from interacting with the ``timeseries/data``\\n\", \"        BLS API endpoint via the reuqests library\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    datasets: List[Tuple[pd.DataFrame, pd.Series]]\\n\", \"        For each BLS series contained in ``res``, a tuple with the \\n\", \"        timeseries observations and series metadata will be returned.\\n\", \"        The observations are a pandas DataFrame and the metadata is a\\n\", \"        pandas Series. These pairs of (data, metadata) are returned\\n\", \"        in a list\\n\", \"    \\n\", \"    See Also\\n\", \"    --------\\n\", \"    See ``make_dfs_from_series`` for more information on content\\n\", \"    of output.\\n\", \"    \\\"\\\"\\\"\\n\", \"    js = res.json()\\n\", \"    return list(map(make_dfs_from_series, js[\\\"Results\\\"][\\\"series\\\"]))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let's test it out\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"parsed = unpack_response(responses[0])\\n\", \"print(f\\\"Parsed is a {type(parsed)}\\\")\\n\", \"print(f\\\"Parsed has len: {len(parsed)}\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"type(parsed[0])\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"parsed[0][0]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"parsed[0][1]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Many responses\\n\", \"\\n\", \"On to the last step...\\n\", \"\\n\", \"The `request_for_series` function returns a list of `Response`s\\n\", \"\\n\", \"If we pass more than 25 series ids, this list will have more than one item\\n\", \"\\n\", \"We need to unpack all the responses, then concatenate the DataFrames and metadata Series\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def unpack_all_responses(all_res: List[requests.Response]):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Given a list of responses from the BLS API, extract and \\n\", \"    return all data and metadata\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    all_res: List[requests.Response])\\n\", \"        Each item in this list is the result of using ``requests`` to \\n\", \"        fetch data from ``timeseries/data`` endpoint of the BLS API.\\n\", \"        \\n\", \"    Returns\\n\", \"    -------\\n\", \"    data: pd.DataFrame\\n\", \"        A pandas DataFrame containing all timeseries observations included\\n\", \"        in any of the responses in ``all_res``\\n\", \"    \\n\", \"    metadata: pd.DataFrame\\n\", \"        Detailed metadata about each series, if such metadata exists in the\\n\", \"        response objects\\n\", \"    \\n\", \"    See Also\\n\", \"    --------\\n\", \"    See ``unpack_response`` and ``make_dfs_from_series`` functions\\n\", \"        \\n\", \"    \\\"\\\"\\\"\\n\", \"    unpacked = []\\n\", \"    for res in all_res:\\n\", \"        unpacked.extend(unpack_response(res))\\n\", \"    \\n\", \"    data_dfs, metadata_series = list(zip(*unpacked))\\n\", \"    data = pd.concat(data_dfs, ignore_index=True)\\n\", \"    metadata = pd.concat([x for x in metadata_series if x is not None], axis=1).T\\n\", \"    return data, metadata\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Unemployment Data\\n\", \"\\n\", \"With our functions in place, it is time to get the data for our analysis!\\n\", \"\\n\", \"Below we have a list of all series IDs from the BLS report on national unemployment\\n\", \"\\n\", \"There are 108 total series\\n\", \"\\n\", \"We'll request them all\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# all series ids from the BLS unemployment report...\\n\", \"series_ids = [\\n\", \"    \\\"LNU02000000\\\",\\n\", \"    \\\"LNU02000001\\\",\\n\", \"    \\\"LNU02000002\\\",\\n\", \"    \\\"LNS12000000\\\",\\n\", \"    \\\"LNS12000001\\\",\\n\", \"    \\\"LNS12000002\\\",\\n\", \"    \\\"LNU07000000\\\",\\n\", \"    \\\"LNU07000001\\\",\\n\", \"    \\\"LNU07000002\\\",\\n\", \"    \\\"LNS17000000\\\",\\n\", \"    \\\"LNS17000001\\\",\\n\", \"    \\\"LNS17000002\\\",\\n\", \"    \\\"LNU07100000\\\",\\n\", \"    \\\"LNU07100001\\\",\\n\", \"    \\\"LNU07100002\\\",\\n\", \"    \\\"LNS17100000\\\",\\n\", \"    \\\"LNS17100001\\\",\\n\", \"    \\\"LNS17100002\\\",\\n\", \"    \\\"LNU07200000\\\",\\n\", \"    \\\"LNU07200001\\\",\\n\", \"    \\\"LNU07200002\\\",\\n\", \"    \\\"LNS17200000\\\",\\n\", \"    \\\"LNS17200001\\\",\\n\", \"    \\\"LNS17200002\\\",\\n\", \"    \\\"LNU07300000\\\",\\n\", \"    \\\"LNU07300001\\\",\\n\", \"    \\\"LNU07300002\\\",\\n\", \"    \\\"LNS17300000\\\",\\n\", \"    \\\"LNS17300001\\\",\\n\", \"    \\\"LNS17300002\\\",\\n\", \"    \\\"LNU03000000\\\",\\n\", \"    \\\"LNU03000001\\\",\\n\", \"    \\\"LNU03000002\\\",\\n\", \"    \\\"LNS13000000\\\",\\n\", \"    \\\"LNS13000001\\\",\\n\", \"    \\\"LNS13000002\\\",\\n\", \"    \\\"LNU07400000\\\",\\n\", \"    \\\"LNU07400001\\\",\\n\", \"    \\\"LNU07400002\\\",\\n\", \"    \\\"LNS17400000\\\",\\n\", \"    \\\"LNS17400001\\\",\\n\", \"    \\\"LNS17400002\\\",\\n\", \"    \\\"LNU07500000\\\",\\n\", \"    \\\"LNU07500001\\\",\\n\", \"    \\\"LNU07500002\\\",\\n\", \"    \\\"LNS17500000\\\",\\n\", \"    \\\"LNS17500001\\\",\\n\", \"    \\\"LNS17500002\\\",\\n\", \"    \\\"LNU07600000\\\",\\n\", \"    \\\"LNU07600001\\\",\\n\", \"    \\\"LNU07600002\\\",\\n\", \"    \\\"LNS17600000\\\",\\n\", \"    \\\"LNS17600001\\\",\\n\", \"    \\\"LNS17600002\\\",\\n\", \"    \\\"LNU07700000\\\",\\n\", \"    \\\"LNU07700001\\\",\\n\", \"    \\\"LNU07700002\\\",\\n\", \"    \\\"LNS17700000\\\",\\n\", \"    \\\"LNS17700001\\\",\\n\", \"    \\\"LNS17700002\\\",\\n\", \"    \\\"LNU05000000\\\",\\n\", \"    \\\"LNU05000001\\\",\\n\", \"    \\\"LNU05000002\\\",\\n\", \"    \\\"LNS15000000\\\",\\n\", \"    \\\"LNS15000001\\\",\\n\", \"    \\\"LNS15000002\\\",\\n\", \"    \\\"LNU07800000\\\",\\n\", \"    \\\"LNU07800001\\\",\\n\", \"    \\\"LNU07800002\\\",\\n\", \"    \\\"LNS17800000\\\",\\n\", \"    \\\"LNS17800001\\\",\\n\", \"    \\\"LNS17800002\\\",\\n\", \"    \\\"LNU07900000\\\",\\n\", \"    \\\"LNU07900001\\\",\\n\", \"    \\\"LNU07900002\\\",\\n\", \"    \\\"LNS17900000\\\",\\n\", \"    \\\"LNS17900001\\\",\\n\", \"    \\\"LNS17900002\\\",\\n\", \"    \\\"LNU08000000\\\",\\n\", \"    \\\"LNU08000001\\\",\\n\", \"    \\\"LNU08000002\\\",\\n\", \"    \\\"LNS18000000\\\",\\n\", \"    \\\"LNS18000001\\\",\\n\", \"    \\\"LNS18000002\\\",\\n\", \"    \\\"LNU08100000\\\",\\n\", \"    \\\"LNU08100001\\\",\\n\", \"    \\\"LNU08100002\\\",\\n\", \"    \\\"LNS18100000\\\",\\n\", \"    \\\"LNS18100001\\\",\\n\", \"    \\\"LNS18100002\\\",\\n\", \"    \\\"LNU08200000\\\",\\n\", \"    \\\"LNU08200001\\\",\\n\", \"    \\\"LNU08200002\\\",\\n\", \"    \\\"LNS18200000\\\",\\n\", \"    \\\"LNS18200001\\\",\\n\", \"    \\\"LNS18200002\\\",\\n\", \"    \\\"LNU08300000\\\",\\n\", \"    \\\"LNU08300001\\\",\\n\", \"    \\\"LNU08300002\\\",\\n\", \"    \\\"LNS18300000\\\",\\n\", \"    \\\"LNS18300001\\\",\\n\", \"    \\\"LNS18300002\\\",\\n\", \"    \\\"LNU08400000\\\",\\n\", \"    \\\"LNU08400001\\\",\\n\", \"    \\\"LNU08400002\\\",\\n\", \"    \\\"LNS18400000\\\",\\n\", \"    \\\"LNS18400001\\\",\\n\", \"    \\\"LNS18400002\\\",\\n\", \"]\\n\", \"\\n\", \"# make requests, get responses\\n\", \"responses = request_for_series(series_ids, \\\"2002\\\", \\\"2021\\\", apikey=apikey)\\n\", \"\\n\", \"# validate the responses\\n\", \"[check_response(r) for r in responses]\\n\", \"\\n\", \"# extract data and metadata from responses\\n\", \"df, metadata = unpack_all_responses(responses)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let's see what we have\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df[\\\"year\\\"].unique()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"metadata\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"And save the data for later usage\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"metadata.to_parquet(\\\"bls_metadata.parquet\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df.drop([\\\"footnotes\\\"], axis=1).to_parquet(\\\"bls_data.parquet\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"03_bls_api.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.786Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 106,
    "position": 11,
    "content_id": 149,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 149,
    "type": "notebook",
    "description": "Explore the labor statistics data using pandas",
    "title": "04_bls_exploration.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# BLS Labor Statistics\\n\", \"\\n\", \"We will now use the data that we downloaded to explore the labor market reactions during times of economic turbulence\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import matplotlib.pyplot as plt\\n\", \"import numpy as np\\n\", \"import pandas as pd\\n\", \"\\n\", \"%matplotlib inline\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Clean the data\\n\", \"\\n\", \"We are going to assume that the data is stored in the same directory as you are in right now under `bls_metadata.parquet` and `bls_data.parquet`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"data = pd.read_parquet(\\\"bls_data.parquet\\\")\\n\", \"metadata = pd.read_parquet(\\\"bls_metadata.parquet\\\")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Convert to numeric data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"data[\\\"value\\\"] = pd.to_numeric(data[\\\"value\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Convert date information to datetime\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"data.head()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"data[\\\"dt\\\"] = pd.to_datetime(\\n\", \"    data.apply(lambda x: f\\\"1 {x['periodName']} {x['year']}\\\", axis=1)\\n\", \")\\n\", \"\\n\", \"# Drop unneeded date info\\n\", \"data = data.drop(\\n\", \"    [\\\"year\\\", \\\"periodName\\\", \\\"period\\\"], axis=1\\n\", \").sort_values(\\n\", \"    [\\\"series_id\\\", \\\"dt\\\"]\\n\", \")[[\\\"series_id\\\", \\\"dt\\\", \\\"value\\\"]]\\n\", \"\\n\", \"data.head(3)\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Rename `cps_labor_force_status`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"lfs_status = {\\n\", \"    \\\"Employed\\\": \\\"employed\\\",\\n\", \"    \\\"Unemployed\\\": \\\"unemployed\\\",\\n\", \"    \\\"Labor Force Flows Employed to Employed\\\": \\\"ee\\\",\\n\", \"    \\\"Labor Force Flows Employed to Unemployed\\\": \\\"eu\\\",\\n\", \"    \\\"Labor Force Flows Unemployed to Employed\\\": \\\"ue\\\",\\n\", \"    \\\"Labor Force Flows Unemployed to Unemployed\\\": \\\"uu\\\",                \\n\", \"}\\n\", \"\\n\", \"metadata = metadata.replace({\\\"cps_labor_force_status\\\": lfs_status})\\n\", \"\\n\", \"metadata.head(2)\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Only keep subset of metadata\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"and_filters = [\\n\", \"    \\\"(commerce_industry == 'All Industries')\\\",\\n\", \"    \\\"(occupation == 'All Occupations')\\\",\\n\", \"    \\\"(seasonality == 'Not Seasonally Adjusted')\\\",\\n\", \"    \\\"(demographic_race == 'All Races')\\\",\\n\", \"    \\\"(demographic_gender == 'Both Sexes')\\\",\\n\", \"]\\n\", \"\\n\", \"metadata_lf = metadata.query(\\n\", \"    \\\"&\\\".join(and_filters)\\n\", \").loc[metadata[\\\"cps_labor_force_status\\\"].isin(lfs_status.values()), :]\\n\", \"\\n\", \"metadata_lf.head(2)\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Merge variable names to `data`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = pd.merge(\\n\", \"    data, metadata_lf[[\\\"series_id\\\", \\\"cps_labor_force_status\\\"]],\\n\", \"    on=\\\"series_id\\\", how=\\\"right\\\"\\n\", \").rename(\\n\", \"    columns={\\\"cps_labor_force_status\\\": \\\"variable\\\"}\\n\", \")[[\\\"variable\\\", \\\"dt\\\", \\\"value\\\"]]\\n\", \"\\n\", \"df.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Generate series from dates\\n\", \"\\n\", \"Create a function that will give us a series starting from a particular date and going forward a certain number of days\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def filter_days_from(data, start_date, ndays=3*365):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Creates a history of data starting at a particular date and running\\n\", \"    forward for `ndays` days.\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    data : pd.DataFrame\\n\", \"        A DataFrame with relevant data that we'd like to  consider\\n\", \"        starting from a particular date. Must have a column `dt` with\\n\", \"        datetime data\\n\", \"    start_date : str or datetime\\n\", \"        The moment we would like our history to start\\n\", \"    ndays : int\\n\", \"        The number of days that we would like to keep in our history\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    out : pd.DataFrame\\n\", \"        A copy of `data` with only data from `start_date` to\\n\", \"        `start_date + f'{ndays} days'` and a new column that counts\\n\", \"        the number of days since `start_date`\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Determine number of days from certain date\\n\", \"    days_from = (data[\\\"dt\\\"] - pd.to_datetime(start_date)).dt.days\\n\", \"    \\n\", \"    # Check for >=0 days and <ndays\\n\", \"    dates_to_keep = (days_from >= 0) & (days_from < ndays)\\n\", \"    \\n\", \"    # Create copy of DataFrame that we'll use as the return\\n\", \"    out = data.copy()\\n\", \"    out[\\\"days_from\\\"] = days_from\\n\", \"    out = out.loc[dates_to_keep, :]\\n\", \"\\n\", \"    return out\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### What does this function do?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_nt = filter_days_from(df, \\\"2012-01-01\\\", ndays=5*365)\\n\", \"\\n\", \"df_nt.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## UE and EU during 'normal times'\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pt_nt = df_nt.pivot(index=\\\"dt\\\", columns=\\\"variable\\\", values=\\\"value\\\")\\n\", \"\\n\", \"pt_nt[\\\"laborforce\\\"] = pt_nt.eval(\\\"employed + unemployed\\\")\\n\", \"\\n\", \"for col in [\\\"ee\\\", \\\"eu\\\"]:\\n\", \"    pt_nt.loc[:, col] = pt_nt.eval(f\\\"{col} / employed\\\")\\n\", \"\\n\", \"for col in [\\\"ue\\\", \\\"uu\\\"]:\\n\", \"    pt_nt.loc[:, col] = pt_nt.eval(f\\\"{col} / unemployed\\\")\\n\", \"\\n\", \"pt_nt.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"fig, ax = plt.subplots(figsize=(10, 8))\\n\", \"\\n\", \"pt_nt.plot(\\n\", \"    y=\\\"eu\\\", ax=ax,\\n\", \"    color=\\\"Red\\\", legend=False, linewidth=2.0\\n\", \")\\n\", \"pt_nt.plot(\\n\", \"    y=\\\"ue\\\", ax=ax,\\n\", \"    color=\\\"Green\\\", legend=False, linewidth=2.0\\n\", \")\\n\", \"\\n\", \"ax.hlines(\\n\", \"    pt_nt[\\\"eu\\\"].mean(), pt_nt.index[0], pt_nt.index[-1],\\n\", \"    linewidth=1.0, linestyle=\\\"--\\\", color=\\\"Red\\\"\\n\", \")\\n\", \"ax.hlines(\\n\", \"    pt_nt[\\\"ue\\\"].mean(), pt_nt.index[0], pt_nt.index[-1],\\n\", \"    linewidth=1.0, linestyle=\\\"--\\\", color=\\\"Green\\\"\\n\", \")\\n\", \"\\n\", \"xs, xe = ax.get_xlim()\\n\", \"ax.annotate(\\n\", \"    \\\"EU\\\", xy=(xs + 0.33*(xe-xs), 0.020),\\n\", \"    color=\\\"Red\\\", size=16\\n\", \")\\n\", \"ax.annotate(\\n\", \"    \\\"UE\\\", xy=(xs + 0.15*(xe-xs), 0.22),\\n\", \"    color=\\\"Green\\\", size=16\\n\", \")\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"\\n\", \"ax.set_title(\\\"EU and UE during 'normal times'\\\", size=18)\"], \"outputs\": [], \"metadata\": {\"scrolled\": false, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Labor statistics during 'turbulent times'\\n\", \"\\n\", \"We are going to start our plots of \\\"turbulent times\\\" one year prior to the trough of unemployment...\\n\", \"\\n\", \"\\n\", \"Note that we did this approximately, but writing code to do this would be a great exercise if you're looking for practice!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_covid = filter_days_from(\\n\", \"    df, \\\"2019-03-01\\\", 5*365\\n\", \").pivot(\\n\", \"    index=\\\"days_from\\\", columns=\\\"variable\\\", values=\\\"value\\\"\\n\", \").assign(\\n\", \"    laborforce= lambda x: x.eval(\\\"employed + unemployed\\\"),\\n\", \"    ee=lambda x: x.eval(\\\"ee / employed\\\"),\\n\", \"    eu=lambda x: x.eval(\\\"eu / employed\\\"),\\n\", \"    ue=lambda x: x.eval(\\\"ue / unemployed\\\"),\\n\", \"    uu=lambda x: x.eval(\\\"uu / unemployed\\\"),\\n\", \")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df_gr = filter_days_from(\\n\", \"    df, \\\"2007-02-01\\\", 5*365\\n\", \").pivot(\\n\", \"    index=\\\"days_from\\\", columns=\\\"variable\\\", values=\\\"value\\\"\\n\", \").assign(\\n\", \"    laborforce= lambda x: x.eval(\\\"employed + unemployed\\\"),\\n\", \"    ee=lambda x: x.eval(\\\"ee / employed\\\"),\\n\", \"    eu=lambda x: x.eval(\\\"eu / employed\\\"),\\n\", \"    ue=lambda x: x.eval(\\\"ue / unemployed\\\"),\\n\", \"    uu=lambda x: x.eval(\\\"uu / unemployed\\\"),\\n\", \")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Changes in employment and unemployment\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\\n\", \"\\n\", \"df_covid.plot(\\n\", \"    y=\\\"employed\\\", ax=ax[0],\\n\", \"    color=\\\"k\\\", linewidth=1.0, legend=False\\n\", \")\\n\", \"df_gr.plot(\\n\", \"    y=\\\"employed\\\", ax=ax[0],\\n\", \"    color=\\\"k\\\", alpha=0.5, linestyle=\\\"--\\\", linewidth=0.5,\\n\", \"    legend=False\\n\", \")\\n\", \"ax[0].set_title(\\\"Number of employed\\\")\\n\", \"ax[0].set_ylabel(\\\"People in thousands\\\")\\n\", \"\\n\", \"df_covid.plot(\\n\", \"    y=\\\"unemployed\\\", ax=ax[1],\\n\", \"    color=\\\"k\\\", linewidth=1.0, legend=False\\n\", \")\\n\", \"df_gr.plot(\\n\", \"    y=\\\"unemployed\\\", ax=ax[1],\\n\", \"    color=\\\"k\\\", alpha=0.5, linestyle=\\\"--\\\", linewidth=0.5,\\n\", \"    legend=False\\n\", \")\\n\", \"ax[1].set_title(\\\"Number of unemployed\\\")\\n\", \"ax[1].set_xlabel(\\\"Days since one year before trough\\\")\\n\", \"ax[1].set_ylabel(\\\"People in thousands\\\")\\n\", \"\\n\", \"for _ax in ax:\\n\", \"    _ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"    _ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"\\n\", \"ax[0].annotate(\\\"COVID\\\", (385, 155_000))\\n\", \"ax[0].annotate(\\\"Great Recession\\\", (1_250, 142_000), alpha=0.5)\\n\", \"\\n\", \"fig.tight_layout()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Changes in EU and UE\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\\n\", \"\\n\", \"df_covid.plot(\\n\", \"    y=\\\"ue\\\", ax=ax[0],\\n\", \"    color=\\\"k\\\", linewidth=1.0, legend=False\\n\", \")\\n\", \"df_gr.plot(\\n\", \"    y=\\\"ue\\\", ax=ax[0],\\n\", \"    color=\\\"k\\\", alpha=0.5, linestyle=\\\"--\\\", linewidth=0.5,\\n\", \"    legend=False\\n\", \")\\n\", \"ax[0].set_title(\\\"Unemployed to Employed\\\")\\n\", \"ax[0].set_ylabel(\\\"Percent of unemployed\\\")\\n\", \"\\n\", \"df_covid.plot(\\n\", \"    y=\\\"eu\\\", ax=ax[1],\\n\", \"    color=\\\"k\\\", linewidth=1.0, legend=False\\n\", \")\\n\", \"df_gr.plot(\\n\", \"    y=\\\"eu\\\", ax=ax[1],\\n\", \"    color=\\\"k\\\", alpha=0.5, linestyle=\\\"--\\\", linewidth=0.5,\\n\", \"    legend=False\\n\", \")\\n\", \"ax[1].set_title(\\\"Employed to Unmployed\\\")\\n\", \"ax[1].set_xlabel(\\\"Days since one year before trough\\\")\\n\", \"ax[1].set_ylabel(\\\"Percent of employed\\\")\\n\", \"\\n\", \"for _ax in ax:\\n\", \"    _ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"    _ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"\\n\", \"ax[0].hlines(pt_nt[\\\"ue\\\"].mean(), 0, 5*365, color=\\\"Green\\\", linewidth=0.75, alpha=0.75)\\n\", \"ax[1].hlines(pt_nt[\\\"eu\\\"].mean(), 0, 5*365, color=\\\"Green\\\", linewidth=0.75, alpha=0.75)\\n\", \"\\n\", \"ax[0].annotate(\\\"COVID\\\", (550, 0.35))\\n\", \"ax[0].annotate(\\\"Great Recession\\\", (750, 0.10), alpha=0.5)\\n\", \"ax[0].annotate(\\\"Normal times\\\", (1000, 0.24), color=\\\"Green\\\", alpha=0.75)\\n\", \"\\n\", \"fig.tight_layout()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"04_bls_exploration.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.786Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 100,
    "position": 9,
    "content_id": 139,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 139,
    "type": "notebook",
    "description": "Learn the basics of working with web APIs to obtain data",
    "title": "01_apis.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# APIs and Web Services\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand the roles of an API\\n\", \"- Be familiar with how to find an API from a service provider's website\\n\", \"- Understand core parts of an API including endpoints, parameters, and authentication\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## What is an API?\\n\", \"\\n\", \"API stands for *application programming interface*\\n\", \"\\n\", \"An API a way for one computer program or service to communicate with and use another program\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### API particpants\\n\", \"\\n\", \"There are two main participants in an API request, each with a specific role:\\n\", \"\\n\", \"1. Producer: this party sets up a service for others to consume. They dictate behaviors and permissions for access and usage\\n\", \"2. Consumer: this party makes a request to the producer's service. They adhere to structure set up by producer\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### How are APIs used?\\n\", \"\\n\", \"APIs can be used for many purposes:\\n\", \"\\n\", \"- Performing actions on a service: e.g. posting to twitter\\n\", \"- Communicating between services inside a company: e.g. running machine learning tasks on user uploaded photos\\n\", \"- Controlled distribution of data: e.g. access to price feeds for a financial asset\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"In this class we will primarily use an API to request data from a third party provider\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Identifying an API\\n\", \"\\n\", \"Many (most?) platforms or companies provide an API for developers to leverage\\n\", \"\\n\", \"I typically search for text like \\\"API\\\", \\\"Developers\\\", or \\\"Tool\\\" on a website\\n\", \"\\n\", \"Examples:\\n\", \"\\n\", \"- [GitHub](https://github.com): \\\"API\\\"\\n\", \"- [Huboi](https://www.huobi.com/en-us/): \\\"API\\\"\\n\", \"- [World Bank](https://data.worldbank.org/): \\\"Tool\\\" -> \\\"API\\\"\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## API Mechanics\\n\", \"\\n\", \"There are a few core components for interacting with most modern APIs\\n\", \"\\n\", \"These are:\\n\", \"\\n\", \"- Endpoints: The URLs needed to access a particular part of the API\\n\", \"- Request type: `GET` type for getting data (usually) `POST` for publishing data (usually)\\n\", \"- Query Parameters: part of a URL that follows a `?`\\n\", \"- Payload: Data contained in *body* of request\\n\", \"- Headers: extra data to provide context for request\\n\", \"- Authentication: how to prove your identity and access privledged content\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Endpoints\\n\", \"\\n\", \"APIs are often represented as URLs, or web links\\n\", \"\\n\", \"Within an API a URL is called an endpoint\\n\", \"\\n\", \"A url has the form `scheme://host/path?query`\\n\", \"\\n\", \"- `scheme` is usually `http` or `https`\\n\", \"- `host` is something like `github.com`\\n\", \"- `path` specifies which part of an API or website you are accessing\\n\", \"- `query` has additional options or parameters associated with the request\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Example \\n\", \"\\n\", \"Example url: `https://api.covidcountydata.org/covid_us?location=eq.12045&variable=eq.tests_total`\\n\", \"\\n\", \"- Scheme: https\\n\", \"- host: `api.covidcountydata.org`\\n\", \"- path: `covid_us`\\n\", \"- query parameters: `location=eq.12045` and `variable=eq.tests_total`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Request Type\\n\", \"\\n\", \"When one computer sends a message to another computer over http, it is called a request\\n\", \"\\n\", \"Requests have a type associated with them\\n\", \"\\n\", \"The two most common request types are\\n\", \"\\n\", \"1. `GET`: this is used when the consumer would like to **get** data from the provider\\n\", \"2. `POST`: this is used when the consumer would like to **post** or store data with the provider\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Query Parameters\\n\", \"\\n\", \"Query parameters come after the path and a `?`\\n\", \"\\n\", \"They provide additional details for *what* is being requested\\n\", \"\\n\", \"Multiple parameters are separated by `&`\\n\", \"\\n\", \"In example for this url: `https://api.covidcountydata.org/covid_us?location=eq.12045&variable=eq.tests_total`, query parameters are:\\n\", \"\\n\", \"1. `location`: with value of `eq.12045`\\n\", \"2. `variable`: with value of `eq.tests_total`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"###  Payload\\n\", \"\\n\", \"A payload is not found in the URL, but is attached to a request\\n\", \"\\n\", \"Sometimes referred to as the *body* of the request\\n\", \"\\n\", \"A payload is used to attach data to a request\\n\", \"\\n\", \"Let's see an example...\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Example \\n\", \"\\n\", \"Suppose we want to \\\"like\\\" a post on a social media platform\\n\", \"\\n\", \"We might make a `POST` request to `https://api.mysocialmedia.com/likes`\\n\", \"\\n\", \"The body of the request might then look like:\\n\", \"\\n\", \"```json\\n\", \"{\\n\", \"    \\\"user\\\": \\\"spencer_lyon\\\",\\n\", \"    \\\"post_id\\\": 42\\n\", \"}\\n\", \"```\\n\", \"\\n\", \"Here `user` is my username on the platform and `42` is the identifier for the post I am liking\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Headers\\n\", \"\\n\", \"Headers are additional options passed with network request\\n\", \"\\n\", \"They are passed as a key-value pair, like a dictionary in Python\\n\", \"\\n\", \"Thier role is to provide context or metadata about the request\\n\", \"\\n\", \"They do **not** typically specify data\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"####  Common Headers\\n\", \"\\n\", \"- `Content-Type`: Specifies what type of data is in the Payload. Example `text/plain` or `application/json`\\n\", \"- `APIKey`: passing the api key\\n\", \"- `Authorization`: A different name some services use for passing api key\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Authentication\\n\", \"\\n\", \"The most common forms of authentication are:\\n\", \"\\n\", \"- OAuth: use a third party provider (like a social media platform) to prove identity\\n\", \"- API keys: API consumer (us) registers directly with provider and is given a key\\n\", \"\\n\", \"We'll use API keys in our examples\\n\", \"\\n\", \"Sometimes API keys are part of query parameters, other times part of headers\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.8.6\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"01_apis.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.786Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 102,
    "position": 10,
    "content_id": 144,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 144,
    "type": "notebook",
    "description": "Learn about a few key variables that describe the labor market",
    "title": "02_labor_statistics.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Labor Statistics\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Current Population Survey\\n\", \"\\n\", \"The [Current Population Survey](https://en.wikipedia.org/wiki/Current_Population_Survey) (CPS) is one of the oldest, largest, and most well-recognized surveys in the United States.\\n\", \"\\n\", \"* A version of this survey has been administered since the late 1930s\\n\", \"* The data is used for many major demographic and economic reports\\n\", \"* Considered reliable at the state level and for the 12 largest metropolitan statistical areas\\n\", \"* Administered to ~60,000 households per month with an average response rate of ~90%\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### CPS: How is it administered?\\n\", \"\\n\", \"One of the strengths of the CPS is it's panel nature:\\n\", \"\\n\", \"* When a household is selected for the CPS, they are interviewed for 4 consecutive months\\n\", \"* The household is then interviewed for 4 consecutive months again 8 months later\\n\", \"\\n\", \"Consider an example:\\n\", \"\\n\", \"Imagine you were selected for the CPS with your first interview starting in February 2020, then you are interviewed in February 2020, March 2020, April 2020, May 2020, February 2021, March 2021, April 2021, and May 2021.\\n\", \"\\n\", \"Details on their methodology can be found [online](https://www.census.gov/programs-surveys/cps/technical-documentation/methodology.html)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### CPS: What is asked?\\n\", \"\\n\", \"The CPS collects lots of information on demographics and labor status:\\n\", \"\\n\", \"* Number of people in the household? Sex? Age? Race? Relationship between each person?\\n\", \"* Working? Full-time or part-time? Hourly pay? Hours worked?\\n\", \"* Looking for work? How? Why?\\n\", \"* Retired?\\n\", \"* Disabilities?\\n\", \"\\n\", \"You can find their exact questionaires [online](https://www.census.gov/programs-surveys/cps/technical-documentation/questionnaires.html)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### CPS: Who uses it?\\n\", \"\\n\", \"* Government organizations: Bureau of Labor Statistics (BLS) issues a jobs report once a month based on these numbers\\n\", \"* Researchers: Anyone can access the microdata behind the CPS and use it for their own research\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Labor Statistics:\\n\", \"\\n\", \"Below are definitions for common terms in labor statistics:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"* **Employed**: Individual who did any work at all as a paid employee, worked in their own business or profession, worked on their own farm, worked 15 or more hours as unpaid workers in a family member's business, or were temporarily absent from their jobs or businesses.\\n\", \"  * (Usually) **Part-time**: Individual who usually works less than 35 hours per week (at all jobs combined)\\n\", \"  * (Usually) **Full-time**: Individual who usually works 35 or more hours per week (at all jobs combined)\\n\", \"* **Unemployed**: Individual who is not employed, was available to work, AND made an active effort to find employment sometime during the 4-week period ending with the survey reference week.\\n\", \"* **Labor force**: The sum of employed and unemployed individuals\\n\", \"* **Unemployment rate**: The number of unemployed people as a percentage of the labor force\\n\", \"* **Not in the labor force**: An individual (aged 16 or older) who was neither employed nor unemployed.\\n\", \"\\n\", \"Can find more detail in the [BLS documentation](https://www.bls.gov/opub/hom/cps/pdf/cps.pdf)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Transition Rates\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"One key statistic that people often analyze are the transition rates between different states of employment:\\n\", \"\\n\", \"* *EE*: The fraction of people who were employed during the current month and in the previous month\\n\", \"* *EU*: The fraction of people who were unemployed during the current month and employed in the previous month\\n\", \"* *UE*: The fraction of people who were employed during the current month and unemployed in the previous month\\n\", \"* *UU*: The fraction of people who were unemployed during the current month and in the previous month\\n\", \"\\n\", \"Note, `EE + EU + UE + UU = 1` if we hold the labor force constant\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### What drives unemployment levels?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Two possible ways to increase unemployment**\\n\", \"\\n\", \"1. A decrease in UE\\n\", \"2. An increase in EU\\n\", \"\\n\", \"In the back of your mind, we'd like you to think about which of these channels will have a larger effect on unemployment levels. We will ask you to investigate this formally next class.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Great Recession vs COVID-19\\n\", \"\\n\", \"We will use this class to explore how the changes in the transition rates changed during two periods of economic turbulence in the United States.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.8.3\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"02_labor_statistics.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.786Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 99,
    "position": 9,
    "content_id": 156,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 156,
    "type": "video",
    "description": "Learn what an API is and how to interact with one using Python",
    "title": "Introduction to APIs",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=8z9MhmPWQIA\", \"youtubeVideoId\": \"8z9MhmPWQIA\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 101,
    "position": 9,
    "content_id": 159,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 159,
    "type": "video",
    "description": "Learn about the key variables that describe the labor market",
    "title": "Labor statistics",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=i94OQY8vvxI\", \"youtubeVideoId\": \"i94OQY8vvxI\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 103,
    "position": 11,
    "content_id": 151,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 151,
    "type": "video",
    "description": "Learn how to use the BLS API to download labor statistics data",
    "title": "BLS API",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=MiUSZr9_poQ\", \"youtubeVideoId\": \"MiUSZr9_poQ\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 6,
    "position": 5,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 7,
    "description": "APIs",
    "title": "Lecture 06",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 105,
    "position": 11,
    "content_id": 158,
    "lecture_id": 7,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 158,
    "type": "video",
    "description": "Explore the labor statistics data using pandas",
    "title": "Labor Stats Exploration",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=LcOY1C1Kn4U\", \"youtubeVideoId\": \"LcOY1C1Kn4U\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 108,
    "position": 9,
    "content_id": 189,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 189,
    "type": "notebook",
    "description": "Learn the fundamentals of data visualization",
    "title": "v1_plotting_intro.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Plotting\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Introduction to Numpy](numpy_arrays.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand components of matplotlib plots  \\n\", \"- Make basic plots  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Plotting](#Plotting)  \\n\", \"  - [Visualization](#Visualization)  \\n\", \"  - [`matplotlib`](#`matplotlib`)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Visualization\\n\", \"\\n\", \"One of the most important outputs of your analysis will be the visualizations that you choose to\\n\", \"communicate what you’ve discovered.\\n\", \"\\n\", \"Here are what some people – whom we think have earned the right to an opinion on this\\n\", \"material – have said with respect to data visualizations.\\n\", \"\\n\", \"> Above all else, show the data – Edward Tufte\\n\", \"\\n\", \"> By visualizing information, we turn it into a landscape that you can explore with your eyes. A sort of information map. And when you’re lost in information, an information map is kind of useful – David McCandless\\n\", \"\\n\", \"> I spend hours thinking about how to get the story across in my visualizations. I don’t mind taking that long because it’s that five minutes of presenting it or someone getting it that can make or break a deal – Goldman Sachs executive\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We begin by focusing on the basics of creating visualizations in Python.\\n\", \"\\n\", \"This will be a fast introduction, but this material appears in almost every\\n\", \"lecture going forward, which will help the concepts sink in.\\n\", \"\\n\", \"In almost any profession that you pursue, much of what you do involves communicating ideas to others.\\n\", \"\\n\", \"Data visualization can help you communicate these ideas effectively, and we encourage you to learn\\n\", \"more about what makes a useful visualization.\\n\", \"\\n\", \"We include some references that we have found useful below.\\n\", \"\\n\", \"- [The Functional Art: An introduction to information graphics and visualization](https://www.amazon.com/The-Functional-Art-introduction-visualization/dp/0321834739/) by Alberto Cairo  \\n\", \"- [The Visual Display of Quantitative Information](https://www.amazon.com/Visual-Display-Quantitative-Information/dp/1930824130) by Edward Tufte  \\n\", \"- [The Wall Street Journal Guide to Information Graphics: The Dos and Don’ts of Presenting Data, Facts, and Figures](https://www.amazon.com/Street-Journal-Guide-Information-Graphics/dp/0393347281) by Dona M Wong  \\n\", \"- [Introduction to Data Visualization](http://paldhous.github.io/ucb/2016/dataviz/index.html)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## `matplotlib`\\n\", \"\\n\", \"The most widely used plotting package in Python is matplotlib.\\n\", \"\\n\", \"The standard import alias is\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import matplotlib.pyplot as plt\\n\", \"import numpy as np\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Note above that we are using `matplotlib.pyplot` rather than just `matplotlib`.\\n\", \"\\n\", \"`pyplot` is a sub-module found in some large packages to further organize functions and types. We are able to give the `plt` alias to this sub-module.\\n\", \"\\n\", \"Additionally, when we are working in the notebook, we need tell matplotlib to display our images\\n\", \"inside of the notebook itself instead of creating new windows with the image.\\n\", \"\\n\", \"This is done by\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%matplotlib inline\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The commands with `%` before them are called [Magics](https://ipython.readthedocs.io/en/stable/interactive/magics.html).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### First Plot\\n\", \"\\n\", \"Let’s create our first plot!\\n\", \"\\n\", \"After creating it, we will walk through the steps one-by-one to understand what they do.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Step 1\\n\", \"fig, ax = plt.subplots()\\n\", \"\\n\", \"# Step 2\\n\", \"x = np.linspace(0, 2*np.pi, 100)\\n\", \"y = np.sin(x)\\n\", \"\\n\", \"# Step 3\\n\", \"ax.plot(x, y)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"1. Create a figure and axis object which stores the information from our graph.  \\n\", \"1. Generate data that we will plot.  \\n\", \"1. Use the `x` and `y` data, and make a line plot on our axis, `ax`, by calling the `plot` method.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Difference between Figure and Axis\\n\", \"\\n\", \"We’ve found that the easiest way for us to distinguish between the figure and axis objects is to\\n\", \"think about them as a framed painting.\\n\", \"\\n\", \"The axis is the canvas; it is where we “draw” our plots.\\n\", \"\\n\", \"The figure is the entire framed painting (which inclues the axis itself!).\\n\", \"\\n\", \"We can also see this by setting certain elements of the figure to different colors.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"fig.set_facecolor(\\\"red\\\")\\n\", \"ax.set_facecolor(\\\"blue\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This difference also means that you can place more than one axis on a figure.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# We specified the shape of the axes -- It means we will have two rows and three columns\\n\", \"# of axes on our figure\\n\", \"fig, axes = plt.subplots(2, 3)\\n\", \"\\n\", \"fig.set_facecolor(\\\"gray\\\")\\n\", \"\\n\", \"# Can choose hex colors\\n\", \"colors = [\\\"#065535\\\", \\\"#89ecda\\\", \\\"#ffd1dc\\\", \\\"#ff0000\\\", \\\"#6897bb\\\", \\\"#9400d3\\\"]\\n\", \"\\n\", \"# axes is a numpy array and we want to iterate over a flat version of it\\n\", \"for (ax, c) in zip(axes.flat, colors):\\n\", \"    ax.set_facecolor(c)\\n\", \"\\n\", \"fig.tight_layout()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Functionality\\n\", \"\\n\", \"The matplotlib library is versatile and very flexible.\\n\", \"\\n\", \"You can see various examples of what it can do on the\\n\", \"[matplotlib example gallery](https://matplotlib.org/gallery.html).\\n\", \"\\n\", \"We work though a few examples to quickly introduce some possibilities.\\n\", \"\\n\", \"**Bar**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"countries = [\\\"CAN\\\", \\\"MEX\\\", \\\"USA\\\"]\\n\", \"populations = [36.7, 129.2, 325.700]\\n\", \"land_area = [3.850, 0.761, 3.790]\\n\", \"\\n\", \"fig, ax = plt.subplots(2, 1)\\n\", \"\\n\", \"ax[0].bar(countries, populations, align=\\\"center\\\")\\n\", \"ax[0].set_title(\\\"Populations (in millions)\\\")\\n\", \"\\n\", \"ax[1].bar(countries, land_area, align=\\\"center\\\")\\n\", \"ax[1].set_title(\\\"Land area (in millions miles squared)\\\")\\n\", \"\\n\", \"fig.tight_layout()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Scatter and annotation**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"N = 50\\n\", \"\\n\", \"np.random.seed(42)\\n\", \"\\n\", \"x = np.random.rand(N)\\n\", \"y = np.random.rand(N)\\n\", \"colors = np.random.rand(N)\\n\", \"area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radii\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"\\n\", \"ax.scatter(x, y, s=area, c=colors, alpha=0.5)\\n\", \"\\n\", \"ax.annotate(\\n\", \"    \\\"First point\\\", xy=(x[0], y[0]), xycoords=\\\"data\\\",\\n\", \"    xytext=(25, -25), textcoords=\\\"offset points\\\",\\n\", \"    arrowprops=dict(arrowstyle=\\\"->\\\", connectionstyle=\\\"arc3,rad=0.6\\\")\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Fill between**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"x = np.linspace(0, 1, 500)\\n\", \"y = np.sin(4 * np.pi * x) * np.exp(-5 * x)\\n\", \"y2 = np.sin(4 * np.pi * x) * np.exp(-10 * x)\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"\\n\", \"ax.grid(True)\\n\", \"ax.fill_between(x, y, y2)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"date\": 1595397099.882795, \"title\": \"Plotting\", \"filename\": \"plotting.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}, \"celltoolbar\": \"Slideshow\", \"download_nb\": false, \"language_info\": {\"name\": \"python\", \"version\": \"3.8.8\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"scientific/plotting\"}, \"nbformat\": 4, \"nbformat_minor\": 2}",
    "md_content": null,
    "properties": "{\"fileName\": \"v1_plotting_intro.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.787Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 13,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 14,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 14,
    "description": "Time Series Data with Pandas",
    "title": "Lecture 08",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 116,
    "position": 9,
    "content_id": 184,
    "lecture_id": 14,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 184,
    "type": "notebook",
    "description": "Learn the key time series funtionality provided by pandas",
    "title": "timeseries.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Temporal Data\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- Python functions\\n\", \"- GroupBy\\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Know how pandas handles dates  \\n\", \"- Understand how to parse strings into `datetime` objects  \\n\", \"- Know how to write dates as custom formatted strings  \\n\", \"- Be able to access day, month, year, etc. for a `DateTimeIndex` and\\n\", \"  a column with `dtype` `datetime`  \\n\", \"- Understand both rolling and re-sampling operations and the difference\\n\", \"  between the two  \\n\", \"\\n\", \"\\n\", \"**Data**\\n\", \"\\n\", \"- Bitcoin to USD exchange rates from March 2014 to the present\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Time series](#Time-series)  \\n\", \"  - [Intro](#Intro)  \\n\", \"  - [Parsing Strings as Dates](#Parsing-Strings-as-Dates)  \\n\", \"  - [Date Formatting](#Date-Formatting)  \\n\", \"  - [Extracting Data](#Extracting-Data)  \\n\", \"  - [Accessing Date Properties](#Accessing-Date-Properties)  \\n\", \"  - [Leads and Lags: `df.shift`](#Leads-and-Lags:-`df.shift`)  \\n\", \"  - [Rolling Computations: `.rolling`](#Rolling-Computations:-`.rolling`)  \\n\", \"  - [Changing Frequencies: `.resample`](#Changing-Frequencies:-`.resample`)  \\n\", \"  - [Optional: API keys](#Optional:-API-keys)  \\n\", \"  - [Exercises](#Exercises)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Uncomment following line to install on colab\\n\", \"#  %pip install quandl\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:36:59.277048Z\", \"iopub.status.idle\": \"2020-11-04T20:36:59.279723Z\", \"iopub.execute_input\": \"2020-11-04T20:36:59.277314Z\", \"shell.execute_reply\": \"2020-11-04T20:36:59.279178Z\", \"shell.execute_reply.started\": \"2020-11-04T20:36:59.277289Z\"}, \"slideshow\": {\"slide_type\": \"subslide\"}, \"incorrectly_encoded_metadata\": \"hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"import os\\n\", \"import pandas as pd\\n\", \"import matplotlib.pyplot as plt\\n\", \"import quandl\\n\", \"\\n\", \"# see section on API keys at end of lecture!\\n\", \"quandl.ApiConfig.api_key = os.environ.get(\\\"QUANDL_AUTH\\\", \\\"Dn6BtVoBhzuKTuyo6hbp\\\")\\n\", \"start_date = \\\"2014-05-01\\\"\\n\", \"\\n\", \"%matplotlib inline\\n\", \"# activate plot theme\\n\", \"plt.style.use(\\\"ggplot\\\")\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T21:04:22.959815Z\", \"iopub.status.idle\": \"2020-11-04T21:04:23.251939Z\", \"iopub.execute_input\": \"2020-11-04T21:04:22.960063Z\", \"shell.execute_reply\": \"2020-11-04T21:04:23.251582Z\", \"shell.execute_reply.started\": \"2020-11-04T21:04:22.960041Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.status.idle\\\": \\\"2020-11-03T15:03:50.165336Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:03:50.164644Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:03:49.755777Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Intro\\n\", \"\\n\", \"pandas has extensive support for handling dates and times\\n\", \"\\n\", \"We will loosely refer to data with date or time information as time series data\\n\", \"\\n\", \"Today we'll explore pandas timeseries capabilities\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Among these topics are:\\n\", \"\\n\", \"- Parsing strings as dates  \\n\", \"- Writing `datetime` objects as (inverse operation of previous point)  \\n\", \"- Extracting data from a DataFrame or Series with date information in\\n\", \"  the index  \\n\", \"- Shifting data through time (taking leads or lags)  \\n\", \"- Re-sampling data to a different frequency and rolling operations  \\n\", \"\\n\", \"**NOTE:** here, even more than with previous topics, we will skip a lot of the\\n\", \"functionality pandas offers, and we urge you to refer to the [official\\n\", \"documentation](https://pandas.pydata.org/pandas-docs/stable/timeseries.html) for\\n\", \"more information\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Parsing Strings as Dates\\n\", \"\\n\", \"Dates often come to us as strings\\n\", \"\\n\", \"Hopefully, the date strings follow a structured format or pattern\\n\", \"\\n\", \"One common pattern is `YYYY-MM-DD`: 4 numbers for the year, 2 for the month, and\\n\", \"2 for the day with each section separated by a `-`\\n\", \"\\n\", \"For example, we write Christmas day 2020 in this format as\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"christmas_str = \\\"2020-12-25\\\"\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:00.979626Z\", \"iopub.status.idle\": \"2020-11-04T20:37:00.982485Z\", \"iopub.execute_input\": \"2020-11-04T20:37:00.979912Z\", \"shell.execute_reply\": \"2020-11-04T20:37:00.982008Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:00.979887Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:03:50.166529Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:03:50.166397Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:03:50.169284Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:03:50.168585Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:03:50.166507Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"To convert a string into a time-aware object, we use the `pd.to_datetime`\\n\", \"function.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"christmas = pd.to_datetime(christmas_str)\\n\", \"print(\\\"The type of christmas is\\\", type(christmas))\\n\", \"christmas\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:01.290809Z\", \"iopub.status.idle\": \"2020-11-04T20:37:01.297164Z\", \"iopub.execute_input\": \"2020-11-04T20:37:01.291062Z\", \"shell.execute_reply\": \"2020-11-04T20:37:01.296601Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:01.291041Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:03:50.170326Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:03:50.170214Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:03:50.178824Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:03:50.178350Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:03:50.170306Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The `pd.to_datetime` function is pretty smart at guessing the format of the\\n\", \"date…\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"for date in [\\\"December 25, 2020\\\", \\\"Dec. 25, 2020\\\",\\n\", \"             \\\"Monday, Dec. 25, 2020\\\", \\\"25 Dec. 2020\\\", \\\"25th Dec. 2020\\\"]:\\n\", \"    print(\\\"pandas interprets {} as {}\\\".format(date, pd.to_datetime(date)))\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:01.619752Z\", \"iopub.status.idle\": \"2020-11-04T20:37:01.626223Z\", \"iopub.execute_input\": \"2020-11-04T20:37:01.620005Z\", \"shell.execute_reply\": \"2020-11-04T20:37:01.625775Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:01.619982Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:03:50.179551Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:03:50.179433Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:03:50.188770Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:03:50.188275Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:03:50.179531Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"However, sometimes we will need to give pandas a hint\\n\", \"\\n\", \"For example, that same time (midnight on Christmas) would be reported on an\\n\", \"Amazon transaction report as\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"christmas_amzn = \\\"2020-12-25T00:00:00+ 00 :00\\\"\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:01.937042Z\", \"iopub.status.idle\": \"2020-11-04T20:37:01.939746Z\", \"iopub.execute_input\": \"2020-11-04T20:37:01.937297Z\", \"shell.execute_reply\": \"2020-11-04T20:37:01.939268Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:01.937274Z\"}, \"slideshow\": {\"slide_type\": \"-\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:03:50.189489Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:03:50.189362Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:03:50.194258Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:03:50.193489Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:03:50.189467Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"If we try to pass this to `pd.to_datetime`, it will fail\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.to_datetime(christmas_amzn)\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:02.242058Z\", \"iopub.status.idle\": \"2020-11-04T20:37:02.268888Z\", \"iopub.execute_input\": \"2020-11-04T20:37:02.242309Z\", \"shell.execute_reply\": \"2020-11-04T20:37:02.268282Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:02.242287Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:03:50.194912Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:03:50.194797Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:03:50.261732Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:03:50.260199Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:03:50.194892Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"To parse a date with this format, we need to specify the `format` argument for\\n\", \"`pd.to_datetime`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"christmas_amzn\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"amzn_strftime = \\\"%Y-%m-%dT%H:%M:%S+ 00 :00\\\"\\n\", \"pd.to_datetime(christmas_amzn, format=amzn_strftime)\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:02.558185Z\", \"iopub.status.idle\": \"2020-11-04T20:37:02.563324Z\", \"iopub.execute_input\": \"2020-11-04T20:37:02.558468Z\", \"shell.execute_reply\": \"2020-11-04T20:37:02.562796Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:02.558443Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:01.438918Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:01.438595Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:01.445655Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:01.444984Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:01.438874Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Can you guess what `amzn_strftime` represents?\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let’s take a closer look at `amzn_strftime` and `christmas_amzn`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"print(amzn_strftime)\\n\", \"print(christmas_amzn)\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:04.694045Z\", \"iopub.status.idle\": \"2020-11-04T20:37:04.697803Z\", \"iopub.execute_input\": \"2020-11-04T20:37:04.694329Z\", \"shell.execute_reply\": \"2020-11-04T20:37:04.697233Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:04.694304Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:02.402508Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:02.402187Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:02.406241Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:02.405635Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:02.402446Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that both of the strings have a similar form, but that instead of actual\\n\", \"numerical values, `amzn_strftime` has *placeholders*\\n\", \"\\n\", \"Specifically, anywhere the `%` shows up is a signal to the `pd.to_datetime`\\n\", \"function that it is where relevant information is stored\\n\", \"\\n\", \"For example, the `%Y` is a stand-in for a four digit year, `%m` is for 2 a digit\\n\", \"month, and so on…\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The official [Python\\n\", \"documentation](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)\\n\", \"contains a complete list of possible `%`something patterns that are accepted in\\n\", \"the `format` argument\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"<a id='exercise-0'></a> **Exercise 1**\\n\", \"\\n\", \"By referring to table found at the link above, figure out the correct argument\\n\", \"to pass as `format` in order to parse the dates in the next three cells below.\\n\", \"\\n\", \"Test your work by passing your format string to `pd.to_datetime`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"christmas_str2 = \\\"2020:12:25\\\"\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:05.434940Z\", \"iopub.status.idle\": \"2020-11-04T20:37:05.437735Z\", \"iopub.execute_input\": \"2020-11-04T20:37:05.435191Z\", \"shell.execute_reply\": \"2020-11-04T20:37:05.437252Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:05.435169Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:02.969574Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:02.969283Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:02.972905Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:02.972288Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:02.969532Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"dbacks_win = \\\"M:11 D:4 Y:2001 9:15 PM\\\"\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:05.616152Z\", \"iopub.status.idle\": \"2020-11-04T20:37:05.618910Z\", \"iopub.execute_input\": \"2020-11-04T20:37:05.616403Z\", \"shell.execute_reply\": \"2020-11-04T20:37:05.618419Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:05.616381Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:03.221062Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:03.220804Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:03.223985Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:03.223401Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:03.221021Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"america_bday = \\\"America was born on July 4, 1776\\\"\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:05.789867Z\", \"iopub.status.idle\": \"2020-11-04T20:37:05.792659Z\", \"iopub.execute_input\": \"2020-11-04T20:37:05.790119Z\", \"shell.execute_reply\": \"2020-11-04T20:37:05.792128Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:05.790096Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:03.487985Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:03.487672Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:03.491166Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:03.490559Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:03.487942Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Multiple Dates\\n\", \"\\n\", \"`pd.to_datetime` can convert a collection (e.g. list, tuple, Series) of date\\n\", \"strings to dates in one go\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.to_datetime([\\\"2020-12-25\\\", \\\"2020-12-31\\\"])\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:56.029287Z\", \"iopub.status.idle\": \"2020-11-04T20:37:56.033823Z\", \"iopub.execute_input\": \"2020-11-04T20:37:56.029535Z\", \"shell.execute_reply\": \"2020-11-04T20:37:56.033407Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:56.029514Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:03.801679Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:03.801430Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:03.807852Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:03.807311Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:03.801643Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We'll just do the one example b/c everything for one date applies to a\\n\", \"collection of dates\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `pd.date_range`\\n\", \"\\n\", \"Often when working with multiple dates, they will be regularly spaced\\n\", \"\\n\", \"Pandas provides the `date_range` function to help construct sequences of regularly spaced dates\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"There are two basic forms of calling `pd.date_range`:\\n\", \"\\n\", \"```python\\n\", \"# first approach\\n\", \"pd.date_range(START, END, freq=FREQUENCY)\\n\", \"\\n\", \"\\n\", \"pd.date_range(START, periods=N, freq=FREQUENCY)\\n\", \"```\\n\", \"\\n\", \"In the above examples \\n\", \"\\n\", \"- `START` and `END` represent anything `pd.to_datetime` can recognize as a date\\n\", \"- `FREQUENCY` is a frequency string like `\\\"D\\\"` for daily and `\\\"A\\\"` for annual\\n\", \"- `N` is an integer\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's see how it works\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"pd.date_range(\\\"2020-03-13\\\", \\\"2024-03-13\\\", freq=\\\"A\\\")\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T21:04:36.505462Z\", \"iopub.status.idle\": \"2020-11-04T21:04:36.509479Z\", \"iopub.execute_input\": \"2020-11-04T21:04:36.505675Z\", \"shell.execute_reply\": \"2020-11-04T21:04:36.508993Z\", \"shell.execute_reply.started\": \"2020-11-04T21:04:36.505656Z\"}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"pd.date_range(\\\"2020-04-01\\\", \\\"2020-04-25\\\")  # default freq is \\\"D\\\"\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T21:06:34.486823Z\", \"iopub.status.idle\": \"2020-11-04T21:06:34.491630Z\", \"iopub.execute_input\": \"2020-11-04T21:06:34.487074Z\", \"shell.execute_reply\": \"2020-11-04T21:06:34.491094Z\", \"shell.execute_reply.started\": \"2020-11-04T21:06:34.487052Z\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"pd.date_range(\\\"2020-04-01\\\", periods=25, freq=\\\"m\\\")  # same as above\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T21:06:50.445554Z\", \"iopub.status.idle\": \"2020-11-04T21:06:50.450295Z\", \"iopub.execute_input\": \"2020-11-04T21:06:50.445807Z\", \"shell.execute_reply\": \"2020-11-04T21:06:50.449823Z\", \"shell.execute_reply.started\": \"2020-11-04T21:06:50.445785Z\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Offset Aliases\\n\", \"\\n\", \"The frequency arguments we've been passing are known as \\\"offset aliases\\\" in the pandas documentation\\n\", \"\\n\", \"There are  *many* of them\\n\", \"\\n\", \"Here we include a table (taken from [their documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases))\\n\", \"\\n\", \"| Alias    | Description                                      |\\n\", \"| -------- | ------------------------------------------------ |\\n\", \"| B        | business day frequency                           |\\n\", \"| C        | custom business day frequency                    |\\n\", \"| D        | calendar day frequency                           |\\n\", \"| W        | weekly frequency                                 |\\n\", \"| M        | month end frequency                              |\\n\", \"| SM       | semi-month end frequency (15th and end of month) |\\n\", \"| BM       | business month end frequency                     |\\n\", \"| CBM      | custom business month end frequency              |\\n\", \"| MS       | month start frequency                            |\\n\", \"| SMS      | semi-month start frequency (1st and 15th)        |\\n\", \"| BMS      | business month start frequency                   |\\n\", \"| CBMS     | custom business month start frequency            |\\n\", \"| Q        | quarter end frequency                            |\\n\", \"| BQ       | business quarter end frequency                   |\\n\", \"| QS       | quarter start frequency                          |\\n\", \"| BQS      | business quarter start frequency                 |\\n\", \"| A, Y     | year end frequency                               |\\n\", \"| BA, BY   | business year end frequency                      |\\n\", \"| AS, YS   | year start frequency                             |\\n\", \"| BAS, BYS | business year start frequency                    |\\n\", \"| BH       | business hour frequency                          |\\n\", \"| H        | hourly frequency                                 |\\n\", \"| T, min   | minutely frequency                               |\\n\", \"| S        | secondly frequency                               |\\n\", \"| L, ms    | milliseconds                                     |\\n\", \"| U, us    | microseconds                                     |\\n\", \"| N        | nanoseconds                                      |\\n\", \"\\n\", \"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Pandas also allows for \\\"anchored offsets\\\"\\n\", \"\\n\", \"This are represented as a suffix on the offset alias that ties the date offset to a particular end of a range\\n\", \"\\n\", \"For example `A` would return the last day of the year, while `A-MAR` would return the last day of March of each year\\n\", \"\\n\", \"The list of anchored offsets is even longer, so we'll just provide a [link to the docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#anchored-offsets)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## TimeDelta\\n\", \"\\n\", \"Often we need to do arithmatic computations with datetime objects themselves\\n\", \"\\n\", \"For example, we can subtract one date from another...\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"new_year = pd.to_datetime(\\\"2020-01-01\\\")\\n\", \"diff = christmas - new_year\\n\", \"print(\\\"diff is type\\\", type(diff))\\n\", \"diff\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"`TimeDelta` objects represent the difference between two timestamps\\n\", \"\\n\", \"They have various properties like days and seconds\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"diff.days\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"diff.seconds\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that `diff.seconds` was `0`\\n\", \"\\n\", \"The difference was *exactly* 359 days, so no seconds were needed\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"If a `Timedelta` is added to a `Timestamp`, the result is a `Timestamp`:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"new_year + diff  # diff = christmas - new_year\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"`Timedelta`s can also do arithmetic with integers:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"10*diff\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The interpretation of `N*diff` is the duration `diff` repeated `N` times\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We often construct `Timedelta` by hand\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"diff2 = pd.Timedelta(days=1, hours=4, minutes=3)\\n\", \"diff2\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"diff2 + christmas\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"A common technique is to construct a variable representing a \\\"standard\\\" Timedelta of interest and then use scalar multiplication and addition to adjust dates\\n\", \"\\n\", \"For example\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"oneday = pd.Timedelta(days=1)\\n\", \"onehour = pd.Timedelta(hours=1)\\n\", \"\\n\", \"oneday + onehour\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"christmas + 365*oneday\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"christmas + 365*oneday - onehour\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Date Formatting\\n\", \"\\n\", \"We can use the `%`pattern format to have pandas write `datetime` objects as\\n\", \"specially formatted strings using the `strftime` (string format time) method\\n\", \"\\n\", \"For example,\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"christmas\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"christmas.strftime(\\\"We love %A %B %d (also written %c)\\\")\"], \"outputs\": [], \"metadata\": {\"execution\": {\"iopub.status.busy\": \"2020-11-04T20:37:07.196885Z\", \"iopub.status.idle\": \"2020-11-04T20:37:07.200568Z\", \"iopub.execute_input\": \"2020-11-04T20:37:07.197138Z\", \"shell.execute_reply\": \"2020-11-04T20:37:07.200117Z\", \"shell.execute_reply.started\": \"2020-11-04T20:37:07.197114Z\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:04.334081Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:04.333768Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:04.338132Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:04.337608Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:04.334041Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"<a id='exercise-1'></a> **Exercise 2**\\n\", \"\\n\", \"Use `pd.to_datetime` to express the birthday of one of your friends or family\\n\", \"members as a `datetime` object.\\n\", \"\\n\", \"Then use the `strftime` method to write a message of the format:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"```python\\n\", \"NAME's birthday is June 10, 1989 (a Saturday)\\n\", \"```\"], \"metadata\": {\"incorrectly_encoded_metadata\": \"hide-output=false\"}, \"cell_type\": \"markdown\"}, {\"source\": [\"(where the name and date are replaced by the appropriate values)\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Extracting Data\\n\", \"\\n\", \"Pandas is aware of when DataFrames or Series have dates on the index\\n\", \"\\n\", \"In these cases it uses a special `DateTimeIndex` for the index\\n\", \"\\n\", \"The special index comes with many convenient data access features\\n\", \"\\n\", \"We will explore these now\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"It's easiest to understand how they work by example, so we'll load in some real\\n\", \"world data:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# get data from quandl\\n\", \"btc_usd = quandl.get(\\\"BCHARTS/BITSTAMPUSD\\\", start_date=start_date)\\n\", \"\\n\", \"# ... or if you have network issues read from a file\\n\", \"# btc_usd = pd.read_csv(\\\"btc_usd.csv\\\", parse_date=[\\\"Date\\\"])\\n\", \"\\n\", \"btc_usd.info()\\n\", \"btc_usd.head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:05.631582Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:05.631264Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:06.434325Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:06.433489Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:05.631540Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Here, we have the Bitcoin (BTC) to US dollar (USD) exchange rate from March 2014\\n\", \"until today.\\n\", \"\\n\", \"Notice that the type of index is `DateTimeIndex`.\\n\", \"\\n\", \"This is the key that enables things like…\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"Extracting all data for the year 2015 by passing `\\\"2015\\\"` to `.loc`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_usd.loc[\\\"2015\\\"]\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:06.435232Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:06.435102Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:06.447076Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:06.446439Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:06.435209Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can also narrow down to specific months.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# By month's name\\n\", \"btc_usd.loc[\\\"August 2017\\\"]\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:06.447972Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:06.447848Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:06.466242Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:06.465635Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:06.447950Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# By month's number\\n\", \"btc_usd.loc[\\\"08/2017\\\"]\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:06.617487Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:06.617199Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:06.631272Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:06.630612Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:06.617442Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Or even a day…\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# By date name\\n\", \"btc_usd.loc[\\\"August 1, 2020\\\"]\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:06.886742Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:06.886460Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:06.894147Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:06.893428Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:06.886699Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# By date number\\n\", \"btc_usd.loc[\\\"08-01-2020\\\"]\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:07.126029Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:07.125691Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:07.132160Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:07.131562Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:07.125985Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Question:** What can we pass as the `.loc` argument when we have a\\n\", \"`DateTimeIndex`?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Answer:** Anything that can be converted to a `datetime` using\\n\", \"`pd.to_datetime`, *without* having to specify the format argument.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"When we pass date-like objects to `.loc`, pandas returns *all* rows whose date\\n\", \"in the index \\\"belong\\\" to that date or period\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We can also use the range shorthand notation to give a start and end date for\\n\", \"selection\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_usd.loc[\\\"April 1, 2015\\\":\\\"April 10, 2015\\\"]\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:08.228029Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:08.227713Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:08.246260Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:08.245574Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:08.227992Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"<a id='exercise-2'></a> **Exercise 3**\\n\", \"\\n\", \"For each item in the list, extract the specified data from `btc_usd`:\\n\", \"\\n\", \"- July 2017 through August 2017 (inclusive)  \\n\", \"- April 25, 2015 to June 10, 2016  \\n\", \"- October 31, 2017\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Accessing Date Properties\\n\", \"\\n\", \"\\n\", \"We can access parts of a date: e.g. the month, minute, second, or hour\\n\", \"\\n\", \"When the date/time information is in the index, we can to `df.index.XX` where\\n\", \"`XX` is replaced by `year`, `month`, or whatever we would like to access.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_usd.index.year\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:09.111812Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:09.111537Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:09.118239Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:09.117419Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:09.111771Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_usd.index.day\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:09.391577Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:09.391316Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:09.397238Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:09.396515Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:09.391536Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can also do the same if the date/time information is stored in a column, but\\n\", \"we have to use a slightly different syntax.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"```python\\n\", \"df[\\\"column_name\\\"].dt.XX\\n\", \"```\"], \"metadata\": {\"incorrectly_encoded_metadata\": \"hide-output=false\"}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's see it in action:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_date_column = btc_usd.reset_index()\\n\", \"btc_date_column.head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:10.766966Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:10.766684Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:10.783280Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:10.782715Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:10.766925Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_date_column[\\\"Date\\\"].dt.year.head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:11.317843Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:11.317593Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:11.324494Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:11.323821Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:11.317808Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_date_column[\\\"Date\\\"].dt.month.head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:11.612597Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:11.612330Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:11.618288Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:11.617642Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:11.612555Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Leads and Lags: `df.shift`\\n\", \"\\n\", \"When doing time series analysis, we often want to compare data at one date\\n\", \"against data at another date\\n\", \"\\n\", \"pandas can help us with this if we leverage the `shift` method\\n\", \"\\n\", \"Without any additional arguments, `shift()` will move all data *forward* one\\n\", \"period, filling the first row with missing data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# so we can see the result of shift clearly\\n\", \"btc_usd.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:11.937883Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:11.937629Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:11.951140Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:11.950456Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:11.937841Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_usd.shift().head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:12.601740Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:12.601479Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:12.615234Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:12.614585Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:12.601700Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can use this to compute the percent change from one day to the next\\n\", \"\\n\", \"(Quiz: Why does that work? Remember how pandas uses the index to *align* data.)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"((btc_usd - btc_usd.shift()) / btc_usd.shift()).head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:13.634746Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:13.634470Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:13.650366Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:13.649626Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:13.634704Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Setting the first argument to `n` tells pandas to shift the data down `n` rows\\n\", \"(apply an `n` period lag).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_usd.shift(3).head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:14.597307Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:14.597047Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:14.612143Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:14.611465Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:14.597265Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"A negative value will shift the data *up* or apply a lead.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_usd.shift(-2).head()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:15.193837Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:15.193578Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:15.208540Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:15.207953Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:15.193799Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_usd.shift(-2).tail()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:15.482609Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:15.482345Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:15.497365Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:15.496643Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:15.482567Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"<a id='exercise-3'></a> **Exercise 4**\\n\", \"\\n\", \"Using the `shift` function, determine the week with the largest percent change\\n\", \"in the volume of trades (the `\\\"Volume (BTC)\\\"` column).\\n\", \"\\n\", \"Repeat the analysis at the bi-weekly and monthly frequencies.\\n\", \"\\n\", \"Hint 1: We have data at a *daily* frequency and one week is `7` days.\\n\", \"\\n\", \"Hint 2: Approximate a month by 30 days.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# your code here\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:16.101157Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:16.100895Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:16.104426Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:16.103643Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:16.101116Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Rolling Computations: `.rolling`\\n\", \"\\n\", \"pandas has facilities that enable easy computation of *rolling statistics*.\\n\", \"\\n\", \"These are best understood by example, so we will dive right in.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# first take only the first 6 rows so we can easily see what is going on\\n\", \"btc_small = btc_usd.head(6)\\n\", \"btc_small\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:17.140415Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:17.140155Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:17.154281Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:17.153595Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:17.140377Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Below, we compute the 2 day moving average (for all columns).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"btc_small\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_small.rolling(\\\"2d\\\").mean()\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:18.083366Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:18.083046Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:18.100437Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:18.099748Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:18.083315Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"To do this operation, pandas starts at each row (date) then looks *backwards*\\n\", \"the specified number of periods (here 2 days) and then applies some aggregation\\n\", \"function (`mean`) on all the data in that window.\\n\", \"\\n\", \"If pandas cannot look back the full length of the window (e.g. when working on\\n\", \"the first row), it fills as much of the window as possible and then does the\\n\", \"operation. Notice that the value at 2014-05-01 is the same in both DataFrames.\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"Below, we see a visual depiction of the rolling maximum on a 21 day window for\\n\", \"the whole dataset.\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots(figsize=(14, 8))\\n\", \"btc_usd[\\\"Open\\\"].plot(ax=ax, linestyle=\\\"--\\\", alpha=0.8)\\n\", \"btc_usd.rolling(\\\"21d\\\").max()[\\\"Open\\\"].plot(ax=ax, alpha=0.8, linewidth=3)\\n\", \"ax.legend([\\\"Original\\\", \\\"21 day max\\\"]);\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:19.342720Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:19.342404Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:19.596998Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:19.596327Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:19.342678Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can also ask pandas to `apply` custom functions, similar to what we saw when\\n\", \"studying GroupBy.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def is_volatile(x):\\n\", \"    \\\"Returns a 1 if the variance is greater than 1, otherwise returns 0\\\"\\n\", \"    if x.var() > 1.0:\\n\", \"        return 1.0\\n\", \"    else:\\n\", \"        return 0.0\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:19.715642Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:19.715377Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:19.718816Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:19.718284Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:19.715600Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"btc_small.rolling(\\\"2d\\\").apply(is_volatile)\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:20.231293Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:20.231028Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:20.259778Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:20.259092Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:20.231252Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise 5**\\n\", \"\\n\", \"Imagine that you have access to the [TARDIS time\\n\", \"machine](https://en.wikipedia.org/wiki/TARDIS) from “Dr. Who”.\\n\", \"\\n\", \"You are allowed to use the TARDIS only once, subject to the following\\n\", \"conditions:\\n\", \"\\n\", \"- You may travel back to any day in the past.  \\n\", \"- On that day, you may purchase one bitcoin *at market open*.  \\n\", \"- You can then take the time machine 30 days into the future and sell your bitcoin *at market close*.  \\n\", \"- Then you return to the present, pocketing the profits.  \\n\", \"\\n\", \"How would you pick the day?\\n\", \"\\n\", \"Think carefully about what you would need to compute to make the optimal choice.\\n\", \"Try writing it out in the markdown cell below so you have a clear description of\\n\", \"the *want* operator that we will apply after the exercise.\\n\", \"\\n\", \"(Note: **Don’t** look too far below, because in the next non-empty cell we have\\n\", \"written out our answer.)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To make this decision, we want to know …\\n\", \"\\n\", \"**Your answer here**\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"To make the optimal decision, we need to know the maximum difference between the\\n\", \"close price at the end of the window and the open price at the start of the\\n\", \"window.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Exercise 6**\\n\", \"\\n\", \"Do the following:\\n\", \"\\n\", \"1. Write a pandas function that implements your strategy.  \\n\", \"1. Pass it to the `agg` method of `rolling_btc`.  \\n\", \"1. Extract the `\\\"Open\\\"` column from the result.  \\n\", \"1. Find the date associated with the maximum value in that column.  \\n\", \"\\n\", \"\\n\", \"How much money did you make?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def daily_value(df):\\n\", \"    # DELETE `pass` below and replace it with your code\\n\", \"    pass\\n\", \"\\n\", \"rolling_btc = btc_usd.rolling(\\\"30d\\\")\\n\", \"\\n\", \"# do steps 2-4 here\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:22.209004Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:22.208717Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:22.213121Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:22.212374Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:22.208960Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Changing Frequencies: `.resample`\\n\", \"\\n\", \"In addition to computing rolling statistics, we can also change the frequency of\\n\", \"the data.\\n\", \"\\n\", \"For example, instead of a monthly moving average, suppose that we wanted to\\n\", \"compute the average *within* each calendar month.\\n\", \"\\n\", \"We will use the `resample` method to do this.\\n\", \"\\n\", \"Below are some examples.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# business quarter\\n\", \"btc_usd.resample(\\\"BQ\\\").mean()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}, \"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:22.851543Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:22.851229Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:22.878342Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:22.877566Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:22.851496Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Note that unlike with `rolling`, a single number is returned for each column for\\n\", \"each quarter.\\n\", \"\\n\", \"The `resample` method will alter the frequency of the data and the number of\\n\", \"rows in the result will be different from the number of rows in the input.\\n\", \"\\n\", \"On the other hand, with `rolling`, the size and frequency of the result are the\\n\", \"same as the input.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We can sample at other frequencies and aggregate with multiple aggregations\\n\", \"function at once.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# multiple functions at 2 start-of-quarter frequency\\n\", \"btc_usd.resample(\\\"2BQS\\\").agg([\\\"min\\\", \\\"max\\\"])\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:23.800434Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:23.800127Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:23.899100Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:23.898419Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:23.800383Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"As with `groupby` and `rolling`, you can also provide custom functions to\\n\", \"`.resample(...).agg` and `.resample(...).apply`\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Exercise 7**\\n\", \"\\n\", \"Now suppose you still have access to the TARDIS, but the conditions are slightly\\n\", \"different.\\n\", \"\\n\", \"You may now:\\n\", \"\\n\", \"- Travel back to the *first day* of any month in the past.  \\n\", \"- On that day, you may purchase one bitcoin *at market open*.  \\n\", \"- You can then travel to any day *in that month* and sell the bitcoin *at market close*.  \\n\", \"- Then return to the present, pocketing the profits.  \\n\", \"\\n\", \"To which month would you travel? On which day of that month would you return to\\n\", \"sell the bitcoin?\\n\", \"\\n\", \"Determine what you would need to compute to make the optimal choice. Try writing\\n\", \"it out in the markdown cell below so you have a clear description of the *want*\\n\", \"operator that we will apply after the exercise.\\n\", \"\\n\", \"(Note: **Don’t** look too many cells below, because we have written out our\\n\", \"answer.)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To make the optimal decision we need …\\n\", \"\\n\", \"**Your answer here**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"To make the optimal decision we need to, for each month, compute the maximum\\n\", \"value of the close price on any day minus the open price on the first day of the\\n\", \"month.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Exercise 8**\\n\", \"\\n\", \"Do the following:\\n\", \"\\n\", \"1. Write a pandas function that implements your strategy.  \\n\", \"1. Pass it to the `agg` method of `resampled_btc`.  \\n\", \"1. Extract the `\\\"Open\\\"` column from the result.  \\n\", \"1. Find the date associated with the maximum value in that column.  \\n\", \"\\n\", \"\\n\", \"How much money did you make? Compare with your neighbor.\\n\", \"\\n\", \"Was this strategy more profitable than the previous one? By how much?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def monthly_value(df):\\n\", \"    # DELETE `pass` below and replace it with your code\\n\", \"    pass\\n\", \"\\n\", \"resampled_btc = btc_usd.resample(\\\"MS\\\")\\n\", \"\\n\", \"# Do steps 2-4 here\"], \"outputs\": [], \"metadata\": {\"incorrectly_encoded_metadata\": \"execution={\\\"iopub.execute_input\\\": \\\"2020-11-03T15:04:26.014554Z\\\", \\\"iopub.status.busy\\\": \\\"2020-11-03T15:04:26.014237Z\\\", \\\"iopub.status.idle\\\": \\\"2020-11-03T15:04:26.018787Z\\\", \\\"shell.execute_reply\\\": \\\"2020-11-03T15:04:26.018126Z\\\", \\\"shell.execute_reply.started\\\": \\\"2020-11-03T15:04:26.014504Z\\\"} hide-output=false\"}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Optional: API keys\\n\", \"\\n\", \"Recall above that we had the line of code:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"```python\\n\", \"quandl.ApiConfig.api_key = \\\"Dn6BtVoBhzuKTuyo6hbp\\\"\\n\", \"```\"], \"metadata\": {\"incorrectly_encoded_metadata\": \"hide-output=false\"}, \"cell_type\": \"markdown\"}, {\"source\": [\"This line told the `quandl` library that when obtaining making requests for\\n\", \"data, it should use the *API key* `Dn6BtVoBhzuKTuyo6hbp`.\\n\", \"\\n\", \"An API key is a sort of password that web services (like the Quandl API) require\\n\", \"you to provide when you make requests.\\n\", \"\\n\", \"Using this password, we were able to make a request to Quandl to obtain data\\n\", \"directly from them.\\n\", \"\\n\", \"The API key used here is one that we requested on behalf of this course.\\n\", \"\\n\", \"If you plan to use Quandl more extensively, you should obtain your own personal\\n\", \"API key from [their\\n\", \"website](https://docs.quandl.com/docs#section-authentication) and re-run the\\n\", \"`quandl.ApiConfig.api_key...` line of code with your new API key on the\\n\", \"right-hand side.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"date\": 1596755784.16782, \"title\": \"Time series\", \"widgets\": {\"application/vnd.jupyter.widget-state+json\": {\"state\": {}, \"version_major\": 2, \"version_minor\": 0}}, \"filename\": \"timeseries.rst\", \"jupytext\": {\"formats\": \"ipynb,md:myst\"}, \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"timeseries.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.788Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 13,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 14,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 14,
    "description": "Time Series Data with Pandas",
    "title": "Lecture 08",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 115,
    "position": 9,
    "content_id": 178,
    "lecture_id": 14,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 178,
    "type": "video",
    "description": "Learn how to work with time series data in pandas",
    "title": "Time Series Data",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=Ej6HN9oD1gk\", \"youtubeVideoId\": \"Ej6HN9oD1gk\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 112,
    "position": 11,
    "content_id": 193,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 193,
    "type": "notebook",
    "description": "Hands on practice using plotting libraries in Python",
    "title": "v3_plotting_libraries.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Plotting Libraries\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- Pandas\\n\", \"- Matplotlib\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Be exposed to the large variety of plotting packages in Python\\n\", \"- Understand the benefits of web based plotting libraries\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Plotting in Python\\n\", \"\\n\", \"There are *many* packages for plotting in Python\\n\", \"\\n\", \"[Jake Vanderplas](http://vanderplas.com/) summarized the Python plotting landscape in a [2017 talk](https://www.youtube.com/watch?v=FytuB8nFHPQ)\\n\", \"\\n\", \"This graphic was one of his slides:\\n\", \"\\n\", \"![python plotting landscape](https://phbs-css.s3-ap-southeast-1.amazonaws.com/python_plotting_landscape.png)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Matplotlib\\n\", \"\\n\", \"Matplotlib started as one man's mission to make MATLAB quality plots in Python\\n\", \"\\n\", \"To date it is the most widely used plotting package in Python\\n\", \"\\n\", \"It excels at creating publication quality plots and is integrated in to many other libraries:\\n\", \"\\n\", \"- [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) `.plot` method for Series and DataFrame uses matplotlib\\n\", \"- [Seaborn](https://seaborn.pydata.org/) is built on matplotlib\\n\", \"- [sklearn](https://scikit-learn.org/stable/visualizations.html) uses matplotlib for its built-in visualizaitons\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Static vs. Interactive\\n\", \"\\n\", \"Matplotlib is excellent at doing its job of creating print-worthy charts\\n\", \"\\n\", \"However, since matplotlib was created (2003), many huge techonology advances have occured:\\n\", \"\\n\", \"- Internet connected devices are widespread: smartphones, tablets, laptops\\n\", \"- Websites have evolved into interactive applicatoins, rather than digital version of printed content\\n\", \"- Computers and computing devices are more powerful\\n\", \"\\n\", \"These (and other) phenomena have led to a rise of web-based plotting tools\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Web based plotting\\n\", \"\\n\", \"Web based plotting libraries are written in another programming language: javascript\\n\", \"\\n\", \"Javascript is the language of the internet and what powers the vast majority of online applications\\n\", \"\\n\", \"Javascript based plots have a few distinct advantages over static plots (like those produced by matplotlib):\\n\", \"\\n\", \"- Interactivity: zoom, pan, toggle, rotate, tooltips, etc.\\n\", \"- Responsive: (can) look great on devices of all sizes\\n\", \"- Sharable: embed on a website, in social media post, mobile application\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### `d3`\\n\", \"\\n\", \"One of the original and most powerful visualization libraries for javascript is `d3`\\n\", \"\\n\", \"`d3` stands for **d**ata **d**riven **d**ocuments\\n\", \"\\n\", \"From the `d3` [homepage](https://d3js.org/)\\n\", \"\\n\", \"> D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation. \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"With `d3` you can create just about any visualization\\n\", \"\\n\", \"Let's see some [examples](https://observablehq.com/@d3/gallery)\\n\", \"\\n\", \"- [Candlestick Chart](https://observablehq.com/@d3/candlestick-chart)\\n\", \"- [Line Charts](https://observablehq.com/@mbostock/inequality-in-american-cities)\\n\", \"- [Heatmaps](https://observablehq.com/@mbostock/electric-usage-2019)\\n\", \"- [Calandar Views](https://observablehq.com/@d3/calendar-view)\\n\", \"- [Arc Diagram](https://observablehq.com/@d3/arc-diagram)\\n\", \"- [Hexbin Map](https://observablehq.com/@d3/hexbin-map)\\n\", \"- [Bar Charts](https://observablehq.com/@d3/bar-chart-race)\\n\", \"- [Map (Choropleth)](https://observablehq.com/@d3/world-choropleth)\\n\", \"- [Map (custom)](https://observablehq.com/@d3/spike-map?collection=@d3/d3-geo)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"This power comes at a cost...\\n\", \"\\n\", \"d3 by itself is rather low-level, meaning you write more code than you would with a higher level package\\n\", \"\\n\", \"For this reason, many libraries have been created that build on `d3` to make things easier for the user\\n\", \"\\n\", \"Two of these are Vega and plotly\\n\", \"\\n\", \"Both Vega and plotly have corresponding Python packages, but we'll first talk about the native javascript version to understand their unique features\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Vega and Vega-Lite\\n\", \"\\n\", \"Vega is a research product out of the University of Washington's Interactive Data Lab (UW IDL)\\n\", \"\\n\", \"From the [Vega homepage](https://vega.github.io/vega/)\\n\", \"\\n\", \"> Vega is a visualization grammar, a declarative language for creating, saving, and sharing interactive visualization designs. With Vega, you can describe the visual appearance and interactive behavior of a visualization in a JSON format, and generate web-based views using Canvas or SVG.\\n\", \">\\n\", \"> Vega provides basic building blocks for a wide variety of visualization designs: data loading and transformation, scales, map projections, axes, legends, and graphical marks such as rectangles, lines, plotting symbols, etc. Interaction techniques can be specified using reactive signals that dynamically modify a visualization in response to input event streams.\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"### A Visualization Grammar\\n\", \"\\n\", \"The main goal of the Vega team is to provide a *grammar* for specifying visualizations\\n\", \"\\n\", \"This grammar is used to build a specification of a chart\\n\", \"\\n\", \"The specification is then applied to a dataset to generate a visualization\\n\", \"\\n\", \"With Vega rather complicated charts can be created from a set of primitives\\n\", \"\\n\", \"This is similar to how written or spoken language can be constructed from a core set of concepts: noun, adjective, verb, punctuation, etc.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Vega-Lite\\n\", \"\\n\", \"The Vega grammar seeks to be very flexible, and as such can also be thought of as low level\\n\", \"\\n\", \"To make things even easier for the user, the UW IDL team created a second grammar called Vega Lite\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Example\\n\", \"\\n\", \"The following bar chart was created with Vega Lite\\n\", \"\\n\", \"![horizontal stacked bar](https://phbs-css.s3-ap-southeast-1.amazonaws.com/vega_lite_horizontal_stacked_bar.png)\\n\", \"\\n\", \"\\n\", \"The *specification* for this chart is:\\n\", \"\\n\", \"```json\\n\", \"{\\n\", \"  \\\"$schema\\\": \\\"https://vega.github.io/schema/vega-lite/v4.json\\\",\\n\", \"  \\\"data\\\": {\\\"url\\\": \\\"data/barley.json\\\"},\\n\", \"  \\\"mark\\\": \\\"bar\\\",\\n\", \"  \\\"encoding\\\": {\\n\", \"    \\\"x\\\": {\\\"aggregate\\\": \\\"sum\\\", \\\"field\\\": \\\"yield\\\"},\\n\", \"    \\\"y\\\": {\\\"field\\\": \\\"variety\\\"},\\n\", \"    \\\"color\\\": {\\\"field\\\": \\\"site\\\"}\\n\", \"  }\\n\", \"}\\n\", \"```\\n\", \"\\n\", \"Notice we didn't specify *how* or *when* to add chart elements, only what the end result should look like...\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Declarative vs Imperative\\n\", \"\\n\", \"The Vega team notes that their library is *declarative*\\n\", \"\\n\", \"This is a programming paradigm where the user specifies *what* should be done, but not *how* or *when*\\n\", \"\\n\", \"The paradigm we are used to working with is called *imperative*\\n\", \"\\n\", \"With *imperative* programming we provide the computer a sequence of tasks to perform, we are building up our product one instruction at a time\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Example\\n\", \"\\n\", \"As a non-programming example, consider the task of pouring a bowl of cold cereal with milk\\n\", \"\\n\", \"A declarative way of specifying the end result might be:\\n\", \"\\n\", \"```json\\n\", \"{\\n\", \"    \\\"bowl\\\": {\\n\", \"        \\\"containing\\\": [\\n\", \"            \\\"cereal\\\",\\n\", \"            \\\"milk\\\",\\n\", \"            \\\"spoon\\\"\\n\", \"        ]\\n\", \"    }\\n\", \"}\\n\", \"```\\n\", \"\\n\", \"An imperative set of instructions for achieving the end result might be:\\n\", \"\\n\", \"```json\\n\", \"[\\n\", \"    \\\"Get bowl from cupboard\\\",\\n\", \"    \\\"Get cereal from pantry\\\",\\n\", \"    \\\"Pour cereal into bowl\\\",\\n\", \"    \\\"Get milk from fridge\\\",\\n\", \"    \\\"Pour milk into bowl\\\",\\n\", \"    \\\"Get spoon from drawer\\\",\\n\", \"    \\\"Put spoon in bowl\\\",\\n\", \"]\\n\", \"```\\n\", \"\\n\", \"Notice that we had to list out *what* we wanted as well as *how* and *when* to do things\\n\", \"\\n\", \"We'll see more examples of this later on today...\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Plotly\\n\", \"\\n\", \"Plotly is another javascript based plotting library that builds on d3\\n\", \"\\n\", \"From their [plotly.js homepage](https://plotly.com/javascript/):\\n\", \"\\n\", \"> Built on top of d3.js and stack.gl, Plotly.js is a high-level, declarative charting library. plotly.js ships with over 40 chart types, including 3D charts, statistical graphs, and SVG maps. \\n\", \"\\n\", \"Plotly has a diverse set of chart types such as basic charts, statistical charts, finance charts, scientific charts, maps, 3d charts, and more\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Plotly Features\\n\", \"\\n\", \"Plotly is declarative like Vega-Lite, but does not attempt to create a grammar for visualizations\\n\", \"\\n\", \"Instead, plotly charts are created by directly setting features or attributes of the chart\\n\", \"\\n\", \"Plotly will feel more familiar than Vega for users of matplotlib or other imperative plotting libraries\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Example\\n\", \"\\n\", \"See the [example candlestick chart](https://plotly.com/javascript/candlestick-charts/#candlestick-chart-without-rangeslider) in the plotly documentation\\n\", \"\\n\", \"The (javascript) code below shows how to create this chart:\\n\", \"\\n\", \"```js\\n\", \"Plotly.d3.csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv', function(err, rows){\\n\", \"\\n\", \"function unpack(rows, key) {\\n\", \"  return rows.map(function(row) {\\n\", \"    return row[key];\\n\", \"  });\\n\", \"}\\n\", \"\\n\", \"var trace = {\\n\", \"  x: unpack(rows, 'Date'),\\n\", \"  close: unpack(rows, 'AAPL.Close'),\\n\", \"  high: unpack(rows, 'AAPL.High'),\\n\", \"  low: unpack(rows, 'AAPL.Low'),\\n\", \"  open: unpack(rows, 'AAPL.Open'),\\n\", \"\\n\", \"  // cutomise colors\\n\", \"  increasing: {line: {color: 'black'}},\\n\", \"  decreasing: {line: {color: 'red'}},\\n\", \"\\n\", \"  type: 'candlestick',\\n\", \"  xaxis: 'x',\\n\", \"  yaxis: 'y'\\n\", \"};\\n\", \"\\n\", \"var data = [trace];\\n\", \"\\n\", \"var layout = {\\n\", \"  dragmode: 'zoom',\\n\", \"  showlegend: false,\\n\", \"  xaxis: {\\n\", \"    rangeslider: {\\n\", \"\\t\\t visible: false\\n\", \"\\t }\\n\", \"  }\\n\", \"};\\n\", \"\\n\", \"Plotly.newPlot('myDiv', data, layout);\\n\", \"});\\n\", \"```\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Web in Python\\n\", \"\\n\", \"As shown above, both Vega and Plotly are based on javascript\\n\", \"\\n\", \"For both Vega and Plotly, there are Python packages that allow you to write Python code and leverage the javascript libraries for rendering the plots\\n\", \"\\n\", \"In this way we can have the best of both worlds: we keep our Python tools and packages, but get the modern interactivity from web-based visualization libraries\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.8.8\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v3_plotting_libraries.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.787Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 113,
    "position": 11,
    "content_id": 150,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 150,
    "type": "video",
    "description": "Learn the basics of using the altair library for visualizing data",
    "title": "Altair Library",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=MBiEcNWuy-M\", \"youtubeVideoId\": \"MBiEcNWuy-M\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 114,
    "position": 11,
    "content_id": 194,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 194,
    "type": "notebook",
    "description": "Learn and practice using the Altair library to create web-friendly visualizations",
    "title": "v4_altair.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Altair: Vega-Lite in Python\\n\", \"\\n\", \"In this notebook we will learn about Vega-Lite in Python\\n\", \"\\n\", \"There are multiple Vega and Vega-Lite wrappers in Python\\n\", \"\\n\", \"The one we will learn about (which is also the most popular) is called `altair`\\n\", \"\\n\", \"The purpose of this notebook is to introduce the core concepts of Altair\\n\", \"\\n\", \"Further exploration and experimentation will be left as an exercise\\n\", \"\\n\", \"> Note: we borrow heavily from the [official documentation](https://altair-viz.github.io/getting_started/overview.html) in this notebook. We strongly encourage you to review the documentation yourself for more examples and details on how to utilize altair in your workflow\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# uncomment the line below and evaluate this cell to install altair\\n\", \"# %pip install --user altair vega_datasets\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Overview\\n\", \"\\n\", \"Core concepts:\\n\", \"\\n\", \"- `alt.Chart`: The *container* for your chart specification. Typically all charts start with `alt.Chart(df: DataFrame)`\\n\", \"- `marks`: A *type* of visual element -- perhaps a line, circle, star, bar, etc.\\n\", \"- `encodings`: A *mapping* between the columns in your dataset and \\\"visual encoding channels\\\" -- perhaps x, y, color, etc.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Example: cars scatter\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import altair as alt\\n\", \"\\n\", \"# load a simple dataset as a pandas DataFrame\\n\", \"from vega_datasets import data\\n\", \"cars = data.cars()\\n\", \"cars.head()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"c_cars = alt.Chart(cars).mark_point().encode(\\n\", \"    x='Horsepower',\\n\", \"    y='Miles_per_Gallon',\\n\", \"    color='Origin',\\n\", \").interactive()\\n\", \"c_cars\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Marks and Encodings\\n\", \"\\n\", \"Let's dive into more detail about marks and encodings\\n\", \"\\n\", \"We'll use the following dataset as an example\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pandas as pd\\n\", \"data = pd.DataFrame(\\n\", \"    {\\n\", \"        \\\"a\\\": list(\\\"CCCDDDEEE\\\"),\\n\", \"        \\\"b\\\": [2, 7, 4, 1, 2, 6, 8, 4, 7],\\n\", \"        \\\"c\\\": [1, 2, 3]*3,\\n\", \"    }\\n\", \")\\n\", \"\\n\", \"data\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Marks\\n\", \"\\n\", \"The mark property is how altair tells Vega what *type* of element to draw\\n\", \"\\n\", \"These are set on the `Chart` object using a method named `.mark_TYPE` where `TYPE` is the type of the mark:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"alt.Chart(data).mark_point()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"alt.Chart(data).mark_rect()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"c2 = alt.Chart(data).mark_circle()\\n\", \"print(type(c2))\\n\", \"c2\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"In these examples there is actually one mark per row in the dataset (9 marks)\\n\", \"\\n\", \"However, all the marks are plotted on top of one another because we haven't specified where they should be plotted\\n\", \"\\n\", \"To fix this we need to *encode* variables (columns) of our dataset as visual channels\\n\", \"\\n\", \"To do this we use the `Chart.encode` method (notice that `Chart.mark_TYPE` returns `Chart`, so we can chain the `.encode` method call)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Below we'll instruct altair to map the column named `a` to the `x` channel, which controls the horizontal or x position of the mark\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"alt.Chart(data).mark_point().encode(x=\\\"a\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"In this chart we can see three distinct marks\\n\", \"\\n\", \"There are actually three points at each of `C`, `D`, and `E`\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"To see all 9 points we also need to encode the `y` channel:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"c2 = (\\n\", \"    alt.Chart(data)\\n\", \"    .mark_point()\\n\", \"    .encode(\\n\", \"        x=\\\"a\\\",\\n\", \"        y=\\\"b\\\"\\n\", \"    )\\n\", \")\\n\", \"c2\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We can now map the `c` column to another channel...\\n\", \"\\n\", \"> Note: `.encode` also returns a `Chart` so we can call `.encode` again to add more mappings\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"c2.encode(color=\\\"c\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"c2.encode(size=\\\"c\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Aggregations\\n\", \"\\n\", \"When specifying the encoding for the chart, we mapped keyword arguments (like `x` and `y`) into strings\\n\", \"\\n\", \"Above we used strings that mapped into column names\\n\", \"\\n\", \"Altair has a mini-language for expressing other types of operations in the strings\\n\", \"\\n\", \"We'll demonstrate this via examples\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Example: Plotting mean over `a`\\n\", \"\\n\", \"**Want**: Plot the mean of the values in column `b`, for each value in column `a`\\n\", \"\\n\", \"Being pandas experts, we might first think to do a groupby then plot:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"(\\n\", \"    alt.Chart(data.groupby(\\\"a\\\").mean().reset_index())\\n\", \"    .mark_point()\\n\", \"    .encode(x=\\\"a\\\", y=\\\"b\\\")\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This certainly works, but we can actually let altair do the aggregation for us:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"c3 = (\\n\", \"    alt.Chart(data)\\n\", \"    .mark_point()\\n\", \"    .encode(x=\\\"a\\\", y=\\\"average(b)\\\")\\n\", \")\\n\", \"\\n\", \"c3\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"There are a few benefits to doing things this way:\\n\", \"\\n\", \"1. The y-axis label was set to \\\"Average of b\\\" instead of just \\\"b\\\"\\n\", \"2. We can leverage further Altair operations that might not be as straightforward with raw pandas then altair\\n\", \"3. The aggregations or transformations happen in a context that is aware of the rest of the chart, allowing for other optimizations or conveniences (similar to setting y-axis title)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Our chart above did what we said we wanted, but looks a bit odd...\\n\", \"\\n\", \"Usually an aggregation like an average is represented via bars instead of points\\n\", \"\\n\", \"To make a bar chart instead we need to use the `rect` mark:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"c3.mark_bar()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Another tweak we might make to this chart would be to make a horizontal bar chart\\n\", \"\\n\", \"To do this we need only swap the map for the `x` and `y` channels:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"c4 = (\\n\", \"    alt.Chart(data)\\n\", \"    .mark_bar()\\n\", \"    .encode(y=\\\"a\\\", x=\\\"average(b)\\\")\\n\", \")\\n\", \"\\n\", \"c4\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Viewing Chart JSON\\n\", \"\\n\", \"The main purpose of the altair library is to make it convenient for Python users to create Vega-Lite compliant JSON specifications from pandas DataFrames\\n\", \"\\n\", \"Altair can report back the JSON that it generated using the `to_json` method\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"print(c4.to_json())\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Viewing the chart JSON can be a useful debugging tool when trying to learn from the Altair or Vega-Lite documentation\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Data in Altair\\n\", \"\\n\", \"Let's take a closer look at the `encoding` section of the Vega-Lite JSON for `c4` from above:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"print(c4.encoding.to_json())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that both \\\"x\\\" and \\\"y\\\" have a `type` field\\n\", \"\\n\", \"Vega-Lite requires that all encoding channels have a type\\n\", \"\\n\", \"Altair took care of these for us based on the dtype of the DataFrame column\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"There are 5 core types of encoding, summarized in the table below:\\n\", \"\\n\", \"<table class=\\\"docutils\\\" border=\\\"1\\\">\\n\", \"<colgroup>\\n\", \"<col width=\\\"16%\\\">\\n\", \"<col width=\\\"19%\\\">\\n\", \"<col width=\\\"65%\\\">\\n\", \"</colgroup>\\n\", \"<thead valign=\\\"bottom\\\">\\n\", \"<tr class=\\\"row-odd\\\"><th class=\\\"head\\\">Data Type</th>\\n\", \"<th class=\\\"head\\\">Shorthand Code</th>\\n\", \"<th class=\\\"head\\\">Description</th>\\n\", \"</tr>\\n\", \"</thead>\\n\", \"<tbody valign=\\\"top\\\">\\n\", \"<tr class=\\\"row-even\\\"><td>quantitative</td>\\n\", \"<td><code class=\\\"docutils literal\\\"><span class=\\\"pre\\\">Q</span></code></td>\\n\", \"<td>a continuous real-valued quantity</td>\\n\", \"</tr>\\n\", \"<tr class=\\\"row-odd\\\"><td>ordinal</td>\\n\", \"<td><code class=\\\"docutils literal\\\"><span class=\\\"pre\\\">O</span></code></td>\\n\", \"<td>a discrete ordered quantity</td>\\n\", \"</tr>\\n\", \"<tr class=\\\"row-even\\\"><td>nominal</td>\\n\", \"<td><code class=\\\"docutils literal\\\"><span class=\\\"pre\\\">N</span></code></td>\\n\", \"<td>a discrete unordered category</td>\\n\", \"</tr>\\n\", \"<tr class=\\\"row-odd\\\"><td>temporal</td>\\n\", \"<td><code class=\\\"docutils literal\\\"><span class=\\\"pre\\\">T</span></code></td>\\n\", \"<td>a time or date value</td>\\n\", \"</tr>\\n\", \"<tr class=\\\"row-even\\\"><td>geojson</td>\\n\", \"<td><code class=\\\"docutils literal\\\"><span class=\\\"pre\\\">G</span></code></td>\\n\", \"<td>a geographic shape</td>\\n\", \"</tr>\\n\", \"</tbody>\\n\", \"</table>\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Using the shorthand code we can give altair a hint about the type of our columns\\n\", \"\\n\", \"We'll see that the type has significant implications for some encoding channels...\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"c2.encode(color=\\\"c:Q\\\").mark_bar()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"c2.encode(color=\\\"c:O\\\").mark_bar()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"c2.encode(color=\\\"c:N\\\").mark_bar()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The shorthand for specifying the type of an encoding also works when using an aggregation:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"(\\n\", \"    alt.Chart(data)\\n\", \"    .mark_bar()\\n\", \"    .encode(x=\\\"a\\\", y=\\\"average(b):Q\\\")\\n\", \")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"(\\n\", \"    alt.Chart(data)\\n\", \"    .mark_bar()\\n\", \"    .encode(x=\\\"a\\\", y=\\\"average(b):N\\\")\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"(\\n\", \"    alt.Chart(data)\\n\", \"    .mark_bar()\\n\", \"    .encode(x=\\\"a\\\", y=\\\"average(b):O\\\")\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Sometimes the `keyword=STRING` shorthand isn't flexible enough for a particular application\\n\", \"\\n\", \"Altair also lets you construct the encoding channels using `alt.CHANNEL` types\\n\", \"\\n\", \"These types are passed as unordered positional arguments before any keyword arguments\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"(\\n\", \"    alt.Chart(data)\\n\", \"    .mark_bar()\\n\", \"    .encode(\\n\", \"        alt.X(\\\"a\\\"), # x = \\\"a\\\"  \\n\", \"        alt.Y(\\\"b\\\", aggregate=\\\"average\\\", type=\\\"quantitative\\\"),  # y=\\\"average(b):Q\\\"\\n\", \"#         alt.Color(\\\"c\\\"),\\n\", \"        color=\\\"c\\\",\\n\", \"    )\\n\", \")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Data from files\\n\", \"\\n\", \"In addition to setting data for our charts by passing in a DataFrame, we could also pass a url to a remote dataset\\n\", \"\\n\", \"> Note when not using a DataFrame, we **must** specify column types\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"url_cars = \\\"https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/cars.json\\\"\\n\", \"\\n\", \"c_cars_url = alt.Chart(url_cars).mark_point().encode(\\n\", \"    x='Horsepower:Q',\\n\", \"    y='Miles_per_Gallon:Q',\\n\", \"    color='Origin:N',\\n\", \").interactive()\\n\", \"\\n\", \"c_cars_url\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"url_aapl = \\\"https://raw.githubusercontent.com/plotly/datasets/master/2014_apple_stock.csv\\\"\\n\", \"\\n\", \"c_aapl = (\\n\", \"    alt.Chart(url_aapl)\\n\", \"    .mark_line()\\n\", \"    .encode(x=\\\"AAPL_x:T\\\", y=\\\"AAPL_y:Q\\\")\\n\", \").interactive()\\n\", \"\\n\", \"c_aapl\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This is not much easier than generating a DataFrame using `pd.read_csv(url_aapl)` and then passing the DataFrame to Altair\\n\", \"\\n\", \"So, why would we do it?\\n\", \"\\n\", \"The benefit here is that the JSON spec for the chart can actually contain a URL which will be handled by the Vega-Lite runtime when rendering the chart\\n\", \"\\n\", \"With a DataFrame, all the data is written out/hard-coded into the JSON spec before Vega-Lite sees it\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"print(len(c_cars.to_json()))  # not going to print the whole thing... too long\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"print(len(c_cars_url.to_json()))\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"print(c_cars_url.to_json())\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"print(c_cars.to_json())\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The smaller spec size makes a Vega-Lite chart more suitable for sharing, loading into websites, or tracking with version control systems\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Other features\\n\", \"\\n\", \"There are many other features we didn't cover:\\n\", \"\\n\", \"- Chart Types: maps, candlesticks, compound chart types (multiple marks), heatmaps, area chart, scatter charts, etc...\\n\", \"- Compound Charts: multiple subplots in one figure\\n\", \"- Interactivity: linked brushing\\n\", \"- Customization: colors, labels\\n\", \"\\n\", \"The best way to learn these concepts is by practice and study of the documentation\\n\", \"\\n\", \"We'll provide opportunity for both on the upcoming homework\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Saving to webpage\\n\", \"\\n\", \"The last thing we will show is how straightforward it is to include an Altair chart on a webpage\\n\", \"\\n\", \"The `Chart` type has a `to_html` method that will generate an html document\\n\", \"\\n\", \"This can be used directly as a standalone webpage, or parts can be copied and pasted into an existing page\\n\", \"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"print(c_aapl.to_html())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"with open(\\\"aapl_altair_chart.html\\\", \\\"w\\\") as f:\\n\", \"    f.write(c_aapl.to_html())\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.8.8\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v4_altair.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.787Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 109,
    "position": 9,
    "content_id": 153,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 153,
    "type": "video",
    "description": "Learn rules for creating effective data visualizations",
    "title": "Data Viz Rules",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=pJsjOBm6PVI\", \"youtubeVideoId\": \"pJsjOBm6PVI\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 110,
    "position": 10,
    "content_id": 191,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 191,
    "type": "notebook",
    "description": "Practice using best pratices for data viz",
    "title": "v2_visualization_rules.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Data Visualization: Rules and Guidelines\\n\", \"\\n\", \"**Co-authored with** [Paul Schrimpf (*UBC*)](https://economics.ubc.ca/faculty-and-staff/paul-schrimpf/)\\n\", \"\\n\", \"\\n\", \"**Prerequisites**\\n\", \"\\n\", \"- [Introduction to Plotting](../scientific/plotting.ipynb)  \\n\", \"\\n\", \"\\n\", \"**Outcomes**\\n\", \"\\n\", \"- Understand steps of creating a visualization  \\n\", \"- Know when to use each of the core plots  \\n\", \"- Introductory ability to make effective visualizations  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Outline\\n\", \"\\n\", \"- [Data Visualization: Rules and Guidelines](#Data-Visualization:-Rules-and-Guidelines)  \\n\", \"  - [Introduction](#Introduction)  \\n\", \"  - [Steps to Creating Effective Charts](#Steps-to-Creating-Effective-Charts)  \\n\", \"  - [Visualization Types](#Visualization-Types)  \\n\", \"  - [Color in Plots](#Color-in-Plots)  \\n\", \"  - [Visualization Rules](#Visualization-Rules)  \\n\", \"  - [References](#References)  \\n\", \"  - [Exercises](#Exercises)  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import matplotlib.colors as mplc\\n\", \"import matplotlib.patches as patches\\n\", \"import matplotlib.pyplot as plt\\n\", \"import numpy as np\\n\", \"import pandas as pd\\n\", \"import statsmodels.formula.api as sm\\n\", \"\\n\", \"from pandas_datareader import DataReader\\n\", \"\\n\", \"%matplotlib inline\\n\", \"\\n\", \"# activate plot theme\\n\", \"import qeds\\n\", \"qeds.themes.mpl_style();\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Introduction\\n\", \"\\n\", \"An economist’s (or data scientist’s) job goes beyond simply learning new things using data and\\n\", \"theory.\\n\", \"\\n\", \"Learning these new things is meant to help economists or data scientists communicate these ideas to others,\\n\", \"whether the medium is an academic paper or a business meeting.\\n\", \"\\n\", \"One of the most effective mediums of communication is visualization.\\n\", \"\\n\", \"Well-done visualizations can help your audience remember your message.\\n\", \"\\n\", \"They accomplish this through at least two main channels:\\n\", \"\\n\", \"1. Psychology researchers have observed [*picture superiority*](https://en.wikipedia.org/wiki/Picture_superiority_effect): the fact that images are more likely to be remembered than\\n\", \"words. While the reasons and extent of the effect are debated, the consensus view is that the effect exists.\\n\", \"How large might this effect be? In a paper by Defeyter, Russo, and McPartlin (2009), the authors found that participants were able to identify pictures they had previously studied for approximately 500 ms each with 75-85% accuracy but words with only a 55-65% accuracy.\\n\", \"\\n\", \"1. Data visualizations help people walk through the logic you used to build the chart,\\n\", \"  allowing them to reason through the argument and thus makes your claim more convincing.  \\n\", \"\\n\", \"\\n\", \"In this lecture, we will discuss the process of creating a data visualization, ways to best ensure\\n\", \"successfully communication, and some general design guidelines.\\n\", \"\\n\", \"Along the way, you will be introduced to many features of `matplotlib` that were not discussed in the\\n\", \"introductory lecture.\\n\", \"\\n\", \"We won’t discuss most of these features, but we encourage you to read more in the\\n\", \"[online documentation](https://matplotlib.org/contents.html) if you have questions.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Steps to Creating Effective Charts\\n\", \"\\n\", \"Before we begin discussions of specific recommendations, it is helpful to agree on\\n\", \"the goal of a data visualization and a process that can be used to accomplish the goal.\\n\", \"\\n\", \"As mentioned in the introduction, the purpose of a visualization is to facilitate the communication\\n\", \"of a message or idea.\\n\", \"\\n\", \"We have found the following steps to be useful for achieving this goal:\\n\", \"\\n\", \"1. Identify the message.  \\n\", \"1. Describe your visualization.  \\n\", \"1. Create a draft of the visualization (and verify your data!).  \\n\", \"1. Fine tune the visualization details.  \\n\", \"\\n\", \"\\n\", \"After discussing the role research plays in data visualization, we will use an example to provide\\n\", \"deeper context for the remaining steps.\\n\", \"\\n\", \"**Step 0: Research**\\n\", \"\\n\", \"Before we proceed, note that prior to reaching this process, you will have spent\\n\", \"a significant amount of time exploring the data and thinking about your overall message.\\n\", \"\\n\", \"In fact, during the research step, you will almost certainly produce visualizations to help yourself\\n\", \"understand the data.\\n\", \"\\n\", \"These visualizations can highlight outliers or unlikely combinations of variables.\\n\", \"\\n\", \"For example, NYC publishes data on taxi pickups and dropoffs.\\n\", \"\\n\", \"One might expect tips to be somewhat independent of whether someone pays cash or credit, but\\n\", \"the 75th percentile tip for cash payers is 0!\\n\", \"\\n\", \"Because it’s unlikely that cash payers choose not to tip, one likely explanation\\n\", \"is a reporting issue in the data collection.\\n\", \"\\n\", \"The steps and guidelines that follow may be helpful in the research process, but refining each of your exploratory visualizations to a publishable version is not practical.\\n\", \"Some of these recommendations will be specific to creating a visualization after you understand the story you’re telling.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Example\\n\", \"\\n\", \"The output of this example will be a reproduction of a visualization from\\n\", \"[this NYT article](https://www.nytimes.com/2019/01/11/upshot/big-cities-low-skilled-workers-wages.html).\\n\", \"\\n\", \"This NYT article is based on research done by David Autor <sup>[1](#ely)</sup> , an\\n\", \"economist at MIT, and his co-authors.\\n\", \"\\n\", \"Autor’s research investigates the observable changes over time between work opportunities\\n\", \"in rural and urban locations in the United States.\\n\", \"\\n\", \"This particular graph explores how both college-educated workers and non-college-educated workers were able\\n\", \"to find higher-paying jobs by working in urban areas in the 20th century.\\n\", \"\\n\", \"More recently, this *city wage premium* is still apparent for those\\n\", \"with college educations. However, for those without a college education, it has\\n\", \"disappeared.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Identify the Message\\n\", \"\\n\", \"The first step to creating a visualization might feel a little obvious, but is\\n\", \"the most important step.\\n\", \"\\n\", \"If you fail to choose a concise message, then you won’t be able to clearly communicate the idea.\\n\", \"\\n\", \"In this example, we want people to be able to answer the question, “What happened to the\\n\", \"rural/urban wage gap for non-college-educated workers since the 1950s?”.\\n\", \"\\n\", \"Part of what makes the answer interesting is that the wage gap changes are unique to non-college-educated workers; we will want to display changes in the wage gap for college-educated workers as well.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Visualize\\n\", \"\\n\", \"Choosing the type of visualization that best illustrates your point is an important\\n\", \"skill to develop.\\n\", \"\\n\", \"Using the wrong type of visualization can inhibit the flow of information from the\\n\", \"graph to your audience’s brain.\\n\", \"\\n\", \"In our case, we need to display the relationship between population density (our measure of\\n\", \"rural/urban) and wages for different years and education levels.\\n\", \"\\n\", \"Since the wage gap will be the main focus, we want to choose a visualization that highlights\\n\", \"this aspect of the data.\\n\", \"\\n\", \"Scatter plots are one of the most effective ways to demonstrate the relationship of two\\n\", \"variables.\\n\", \"\\n\", \"We will place the log of population density on the x-axis and the log of wages on the y-axis.\\n\", \"\\n\", \"We will then need to find a way to demonstrate this for different years and education levels.\\n\", \"\\n\", \"One natural solution is to demonstrate one of these variables using color and the other using\\n\", \"different subplots.\\n\", \"\\n\", \"In the original article, they chose to highlight differences in college\\n\", \"education using color and time using subplots.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Visualization Draft\\n\", \"\\n\", \"Drafting an early version of your visualization without concerning yourself about its aesthetics allows you to think about whether it is able to answer the proposed question.\\n\", \"\\n\", \"Sometimes you’ll get to this step and realize that you need to go back to one of the previous\\n\", \"steps…\\n\", \"\\n\", \"It’s ok to scrap what you have and restart at square one.\\n\", \"\\n\", \"In fact, you will frequently do this, no matter how much experience you’ve developed.\\n\", \"\\n\", \"In our own work, we’ve found that it’s not uncommon to discard ten or more versions of a graph before\\n\", \"settling on a draft we are happy with.\\n\", \"\\n\", \"Below, we create a draft of our plot.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Read in data\\n\", \"df = pd.read_csv(\\\"https://datascience.quantecon.org/assets/data/density_wage_data.csv\\\")\\n\", \"df[\\\"year\\\"] = df.year.astype(int)  # Convert year to int\\n\", \"\\n\", \"\\n\", \"def single_scatter_plot(df, year, educ, ax, color):\\n\", \"    \\\"\\\"\\\"\\n\", \"    This function creates a single year's and education level's\\n\", \"    log density to log wage plot\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Filter data to keep only the data of interest\\n\", \"    _df = df.query(\\\"(year == @year) & (group == @educ)\\\")\\n\", \"    _df.plot(\\n\", \"        kind=\\\"scatter\\\", x=\\\"density_log\\\", y=\\\"wages_logs\\\", ax=ax, color=color\\n\", \"    )\\n\", \"\\n\", \"    return ax\\n\", \"\\n\", \"# Create initial plot\\n\", \"fig, ax = plt.subplots(1, 4, figsize=(16, 6), sharey=True)\\n\", \"\\n\", \"for (i, year) in enumerate(df.year.unique()):\\n\", \"    single_scatter_plot(df, year, \\\"college\\\", ax[i], \\\"b\\\")\\n\", \"    single_scatter_plot(df, year, \\\"noncollege\\\", ax[i], \\\"r\\\")\\n\", \"    ax[i].set_title(str(year))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Exercise 1**\\n\", \"\\n\", \"Create a draft of the alternative way to organize time and education -- that is, have two subplots (one for each education level) and four groups of points (one for each year).\\n\", \"\\n\", \"Why do you think they chose to organize the information as they did rather than this way?\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Your code here\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Fine-tune\\n\", \"\\n\", \"Great! We have now confirmed that our decisions up until this point have made sense and that a\\n\", \"version of this graphic can successfully convey our message.\\n\", \"\\n\", \"The last step is to clean the graph. We want to ensure that no features\\n\", \"detract or distract from our message.\\n\", \"\\n\", \"Much of the remaining lecture will be dedicated to this fine-tuning, so we will post-pone\\n\", \"presenting the details.  However, the code we use to create the best version of this graphic is included below.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Read in data\\n\", \"df = pd.read_csv(\\\"https://datascience.quantecon.org/assets/data/density_wage_data.csv\\\")\\n\", \"df[\\\"year\\\"] = df.year.astype(int)  # Convert year to int\\n\", \"\\n\", \"\\n\", \"def single_scatter_plot(df, year, educ, ax, color):\\n\", \"    \\\"\\\"\\\"\\n\", \"    This function creates a single year's and education level's\\n\", \"    log density to log wage plot\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Filter data to keep only the data of interest\\n\", \"    _df = df.query(\\\"(year == @year) & (group == @educ)\\\")\\n\", \"    _df.plot(\\n\", \"        kind=\\\"scatter\\\", x=\\\"density_log\\\", y=\\\"wages_logs\\\", ax=ax, color=color\\n\", \"    )\\n\", \"\\n\", \"    return ax\\n\", \"\\n\", \"# Create initial plot\\n\", \"fig, ax = plt.subplots(1, 4, figsize=(16, 6))\\n\", \"colors = {\\\"college\\\": \\\"#1385ff\\\", \\\"noncollege\\\": \\\"#ff6d13\\\"}\\n\", \"\\n\", \"for (i, year) in enumerate(df.year.unique()):\\n\", \"    single_scatter_plot(df, year, \\\"college\\\", ax[i], colors[\\\"college\\\"])\\n\", \"    single_scatter_plot(df, year, \\\"noncollege\\\", ax[i], colors[\\\"noncollege\\\"])\\n\", \"    ax[i].set_title(str(year))\\n\", \"\\n\", \"bgcolor = (250/255, 250/255, 250/255)\\n\", \"fig.set_facecolor(bgcolor)\\n\", \"for (i, _ax) in enumerate(ax):\\n\", \"    # Label with words\\n\", \"    if i == 0:\\n\", \"        _ax.set_xlabel(\\\"Population Density\\\")\\n\", \"    else:\\n\", \"        _ax.set_xlabel(\\\"\\\")\\n\", \"\\n\", \"    # Turn off right and top axis lines\\n\", \"    _ax.spines['right'].set_visible(False)\\n\", \"    _ax.spines['top'].set_visible(False)\\n\", \"\\n\", \"    # Don't use such a white background color\\n\", \"    _ax.set_facecolor(bgcolor)\\n\", \"\\n\", \"    # Change bounds\\n\", \"    _ax.set_ylim((np.log(4), np.log(30)))\\n\", \"    _ax.set_xlim((0, 10))\\n\", \"\\n\", \"    # Change ticks\\n\", \"    xticks = [10, 100, 1000, 10000]\\n\", \"    _ax.set_xticks([np.log(xi) for xi in xticks])\\n\", \"    _ax.set_xticklabels([str(xi) for xi in xticks])\\n\", \"\\n\", \"    yticks = list(range(5, 32, 5))\\n\", \"    _ax.set_yticks([np.log(yi) for yi in yticks])\\n\", \"    if i == 0:\\n\", \"        _ax.set_yticklabels([str(yi) for yi in yticks])\\n\", \"        _ax.set_ylabel(\\\"Average Wage\\\")\\n\", \"    else:\\n\", \"        _ax.set_yticklabels([])\\n\", \"        _ax.set_ylabel(\\\"\\\")\\n\", \"\\n\", \"ax[0].annotate(\\\"College Educated Workers\\\", (np.log(75), np.log(14.0)), color=colors[\\\"college\\\"])\\n\", \"ax[0].annotate(\\\"Non-College Educated Workers\\\", (np.log(10), np.log(5.25)), color=colors[\\\"noncollege\\\"]);\\n\", \"ax[0].set_zorder(1)\"], \"outputs\": [], \"metadata\": {\"scrolled\": true, \"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Visualization Types\\n\", \"\\n\", \"You have seen many kinds of visualizations throughout your life.\\n\", \"\\n\", \"We discuss a few of the most frequently used visualization types and how they describe data below.\\n\", \"\\n\", \"For a more complete list of visualization types, see the Duke library’s\\n\", \"[data visualization guide](https://guides.library.duke.edu/datavis/vis_types).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Scatter Plots\\n\", \"\\n\", \"Scatter plots can be used in various ways.\\n\", \"\\n\", \"They are frequently used to show how two variables are related to one another or compare\\n\", \"various observations based on two variables.\\n\", \"\\n\", \"[This article](https://qz.com/1235712/the-origins-of-the-scatter-plot-data-visualizations-greatest-invention/)\\n\", \"about the scatter plot is a good read. One piece of this article that stuck out to us was that, \\\"Edward Tuft once estimated that more than 70% of all charts in scientific publications are scatter plots.\\\"\\n\", \"\\n\", \"One strength of a scatter plot is that its simplicity allows the data to speak for itself.\\n\", \"A plot of two variables allows viewers to immediately see whether the variables are\\n\", \"linearly related, quadratically related, or maybe not related at all.\\n\", \"\\n\", \"We have already seen an example of a scatter plot which shows the relationship between two\\n\", \"variables.\\n\", \"\\n\", \"Below, we demonstrate how it can be used to compare.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"cities = [\\n\", \"    \\\"San Francisco\\\", \\\"Austin\\\", \\\"Las Vegas\\\", \\\"New York\\\", \\\"Seattle\\\", \\\"Pittsburgh\\\",\\n\", \"    \\\"Detroit\\\", \\\"Fresno\\\", \\\"Phoenix\\\", \\\"Orlando\\\", \\\"Atlanta\\\", \\\"Madison\\\"\\n\", \"]\\n\", \"unemp_wage = np.array([\\n\", \"    [2.6, 39.89], [2.9, 29.97], [4.6, 24.38], [3.9, 33.09], [3.9, 40.11], [4.2, 27.98],\\n\", \"    [4.1, 28.41], [7.1, 22.96], [4.5, 27.42], [3.0, 21.47], [3.6, 25.19], [2.2, 29.48]\\n\", \"])\\n\", \"df = pd.DataFrame(unemp_wage, index=cities, columns=[\\\"Unemployment\\\", \\\"Wage\\\"])\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"df.plot(kind=\\\"scatter\\\", x=\\\"Unemployment\\\", y=\\\"Wage\\\", ax=ax, s=25, color=\\\"#c90000\\\")\\n\", \"\\n\", \"# Add annotations\\n\", \"for (i, row) in df.iterrows():\\n\", \"    city = row.name\\n\", \"\\n\", \"    if city in [\\\"San Francisco\\\", \\\"Madison\\\"]:\\n\", \"        offset = (-35, -10.5)\\n\", \"    elif city in [\\\"Atlanta\\\", \\\"Phoenix\\\", \\\"Madison\\\"]:\\n\", \"        offset = (-25, -12.5)\\n\", \"    elif city in [\\\"Detroit\\\"]:\\n\", \"        offset = (-38, 0)\\n\", \"    elif city in [\\\"Pittsburgh\\\"]:\\n\", \"        offset = (5, 0)\\n\", \"    else:\\n\", \"        offset = (5, 2.5)\\n\", \"    ax.annotate(\\n\", \"        city, xy=(row[\\\"Unemployment\\\"], row[\\\"Wage\\\"]),\\n\", \"        xytext=offset, textcoords=\\\"offset points\\\"\\n\", \"    )\\n\", \"\\n\", \"bgcolor = (250/255, 250/255, 250/255)\\n\", \"fig.set_facecolor(bgcolor)\\n\", \"ax.set_facecolor(bgcolor)\\n\", \"ax.set_xlim(0, 10)\\n\", \"ax.set_ylim(20, 45)\\n\", \"ax.spines['right'].set_visible(True)\\n\", \"ax.spines['top'].set_visible(True)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Line Plots\\n\", \"\\n\", \"Line plots are best used to either show how a variable evolves over time or to demonstrate the\\n\", \"relationship between variables.\\n\", \"\\n\", \"Note: it differs from scatter plots in the way it displays relationships between variables.\\n\", \"\\n\", \"A line plot is restricted to displaying a line, so you cannot just draw a line between all of your\\n\", \"datapoints.\\n\", \"\\n\", \"Instead, before drawing the line, you must fit some kind of statistical model that can\\n\", \"show how one variable changes as the other changes.\\n\", \"\\n\", \"Below, we add regression lines which estimate the relationship between population density and wages to\\n\", \"our college/non-college urban wage premium plot.\\n\", \"\\n\", \"In fact, Dr. Autor’s original slides contain regression lines, but the New York Times\\n\", \"chose to remove them.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"from sklearn.linear_model import LinearRegression\\n\", \"\\n\", \"# Read in data\\n\", \"df = pd.read_csv(\\\"https://datascience.quantecon.org/assets/data/density_wage_data.csv\\\")\\n\", \"df[\\\"year\\\"] = df.year.astype(int)  # Convert year to int\\n\", \"\\n\", \"\\n\", \"def single_scatter_plot(df, year, educ, ax, color):\\n\", \"    \\\"\\\"\\\"\\n\", \"    This function creates a single year's and education level's\\n\", \"    log density to log wage plot\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Filter data to keep only the data of interest\\n\", \"    _df = df.query(\\\"(year == @year) & (group == @educ)\\\")\\n\", \"    _df.plot(\\n\", \"        kind=\\\"scatter\\\", x=\\\"density_log\\\", y=\\\"wages_logs\\\", ax=ax, color=color\\n\", \"    )\\n\", \"\\n\", \"    lr = LinearRegression()\\n\", \"    X = _df[\\\"density_log\\\"].values.reshape(-1, 1)\\n\", \"    y = _df[\\\"wages_logs\\\"].values.reshape(-1, 1)\\n\", \"    lr.fit(X, y)\\n\", \"\\n\", \"    x = np.linspace(2.0, 9.0).reshape(-1, 1)\\n\", \"    y_pred = lr.predict(x)\\n\", \"    ax.plot(x, y_pred, color=color)\\n\", \"\\n\", \"    return ax\\n\", \"\\n\", \"# Create initial plot\\n\", \"fig, ax = plt.subplots(1, 4, figsize=(16, 6))\\n\", \"colors = {\\\"college\\\": \\\"#1385ff\\\", \\\"noncollege\\\": \\\"#ff6d13\\\"}\\n\", \"\\n\", \"for (i, year) in enumerate(df.year.unique()):\\n\", \"    single_scatter_plot(df, year, \\\"college\\\", ax[i], colors[\\\"college\\\"])\\n\", \"    single_scatter_plot(df, year, \\\"noncollege\\\", ax[i], colors[\\\"noncollege\\\"])\\n\", \"    ax[i].set_title(str(year))\\n\", \"\\n\", \"bgcolor = (250/255, 250/255, 250/255)\\n\", \"fig.set_facecolor(bgcolor)\\n\", \"for (i, _ax) in enumerate(ax):\\n\", \"    # Label with words\\n\", \"    if i == 0:\\n\", \"        _ax.set_xlabel(\\\"Population Density\\\")\\n\", \"    else:\\n\", \"        _ax.set_xlabel(\\\"\\\")\\n\", \"\\n\", \"    # Turn off right and top axis lines\\n\", \"    _ax.spines['right'].set_visible(False)\\n\", \"    _ax.spines['top'].set_visible(False)\\n\", \"\\n\", \"    # Don't use such a white background color\\n\", \"    _ax.set_facecolor(bgcolor)\\n\", \"\\n\", \"    # Change bounds\\n\", \"    _ax.set_ylim((np.log(4), np.log(30)))\\n\", \"    _ax.set_xlim((0, 10))\\n\", \"\\n\", \"    # Change ticks\\n\", \"    xticks = [10, 100, 1000, 10000]\\n\", \"    _ax.set_xticks([np.log(xi) for xi in xticks])\\n\", \"    _ax.set_xticklabels([str(xi) for xi in xticks])\\n\", \"\\n\", \"    yticks = list(range(5, 32, 5))\\n\", \"    _ax.set_yticks([np.log(yi) for yi in yticks])\\n\", \"    if i == 0:\\n\", \"        _ax.set_yticklabels([str(yi) for yi in yticks])\\n\", \"        _ax.set_ylabel(\\\"Average Wage\\\")\\n\", \"    else:\\n\", \"        _ax.set_yticklabels([])\\n\", \"        _ax.set_ylabel(\\\"\\\")\\n\", \"\\n\", \"ax[0].annotate(\\\"College Educated Workers\\\", (np.log(75), np.log(14.0)), color=colors[\\\"college\\\"])\\n\", \"ax[0].annotate(\\\"Non-College Educated Workers\\\", (np.log(10), np.log(5.25)), color=colors[\\\"noncollege\\\"])\\n\", \"ax[0].set_zorder(1)\"], \"outputs\": [], \"metadata\": {\"scrolled\": true, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Bar Charts\\n\", \"\\n\", \"Bar charts are mostly used to display differences for a variable between groups though they can also\\n\", \"be used to show how a variable changes over time (which in some ways, is just showing a difference as grouped by time…).\\n\", \"\\n\", \"Bar charts show the differences between these groups using the length of each bar, so that comparing the different groups is straightforward.\\n\", \"\\n\", \"In the example below, we show a bar chart of how the unemployment rate differs across several cities\\n\", \"in the United States.\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"cities = [\\n\", \"    \\\"San Francisco\\\", \\\"Austin\\\", \\\"Las Vegas\\\", \\\"New York\\\", \\\"Seattle\\\", \\\"Pittsburgh\\\",\\n\", \"    \\\"Detroit\\\", \\\"Fresno\\\", \\\"Phoenix\\\", \\\"Orlando\\\", \\\"Atlanta\\\", \\\"Madison\\\"\\n\", \"]\\n\", \"unemp_wage = np.array([\\n\", \"    [2.6, 39.89], [2.9, 29.97], [4.6, 24.38], [3.9, 33.09], [3.9, 40.11], [4.2, 27.98],\\n\", \"    [4.1, 28.41], [7.1, 22.96], [4.5, 27.42], [3.0, 21.47], [3.6, 25.19], [2.2, 29.48]\\n\", \"])\\n\", \"df = pd.DataFrame(unemp_wage, index=cities, columns=[\\\"Unemployment\\\", \\\"Wage\\\"])\\n\", \"df = df.sort_values([\\\"Unemployment\\\"], ascending=False)\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"\\n\", \"df[\\\"Unemployment\\\"].plot(kind=\\\"barh\\\", ax=ax, color=\\\"#1b48fc\\\")\\n\", \"ax.spines['right'].set_visible(False)\\n\", \"ax.spines['top'].set_visible(False)\\n\", \"ax.set_title(\\\"Unemployment Rate in US Cities\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Histograms\\n\", \"\\n\", \"Histograms display the approximate distribution of a single variable.\\n\", \"\\n\", \"They can be particularly important when your variables are not distributed normally\\n\", \"since we typically think of means and variances in terms of the normal distribution.\\n\", \"\\n\", \"In the example below, we show a histogram of GDP growth rates over the period 1948 - 2019.\\n\", \"\\n\", \"Our histogram indicates this variable is approximately normally distributed.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# GDP quarterly growth\\n\", \"gdp = DataReader(\\\"GDP\\\", \\\"fred\\\", 1948, 2019).pct_change().dropna()\\n\", \"gdp = gdp * 100\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"gdp.plot(\\n\", \"    kind=\\\"hist\\\", y=\\\"GDP\\\", color=(244/255, 77/255, 24/255),\\n\", \"    bins=23, legend=False, density=True, ax=ax\\n\", \")\\n\", \"ax.set_facecolor((0.96, 0.96, 0.96))\\n\", \"fig.set_facecolor((0.96, 0.96, 0.96))\\n\", \"ax.spines['right'].set_visible(False)\\n\", \"ax.spines['top'].set_visible(False)\\n\", \"ax.set_title(\\\"US GDP Growth from 1948-2019\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Color in Plots\\n\", \"\\n\", \"Choosing colors for your plots is not always a straightforward task.\\n\", \"\\n\", \"Visualization expert Edward Tufte <https://www.edwardtufte.com/tufte/> wrote,\\n\", \"\\n\", \"> … Avoiding catastrophe becomes the first principle in bringing color to information: Above\\n\", \"all, do no harm ([*Envisioning Information*](https://www.edwardtufte.com/tufte/books_ei) by Edward Tufte)\\n\", \"\\n\", \"\\n\", \"So how do we “do no harm”?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Hue Saturation Lightness\\n\", \"\\n\", \"We will use the [Hue Saturation Value](https://en.wikipedia.org/wiki/HSL_and_HSV) (HSV) paradigm as a way to formalize our discussion of colors.\\n\", \"\\n\", \"- **Hue**: This represents the share of each of the primary colors (red, green, blue)\\n\", \"  as angles around a circle. The hue begins with red at 0 degrees, green at 120\\n\", \"  degrees, and blue at 240 degrees (Note: matplotlib converts these back into numbers between 0 and\\n\", \"  1 by dividing by 360). Angles between these colors are mixes of the primary colors.  \\n\", \"- **Saturation**: Denotes how rich the color is using numbers between 0 and 1. At full saturation\\n\", \"  (saturation is 1), the color is as rich as possible. At saturation 0, the color has no\\n\", \"  color and is approximately a projection of the color into grayscale (Note that this is not\\n\", \"  exactly true).  \\n\", \"- **Value**: Denotes how dark the color is using numbers between 0 and 1. We view this as how much black\\n\", \"  has been added to a color. If a color has value 0, then it is as dark as possible (the\\n\", \"  color black). If the color has value 1 then it has no black and is just the original color.  \\n\", \"\\n\", \"\\n\", \"The way in which HSV covers the color space is demonstrated in the following figure.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/assets/_static/visualization_files/HSV_color_solid_cylinder_saturation_gray.png\\\" alt=\\\"HSL_cylinder.png\\\" style=\\\"\\\" width=\\\"500\\\">\\n\", \"\\n\", \"  \\n\", \"Image attribution: By [SharkD](https://commons.wikimedia.org/w/index.php?curid=9801673).\\n\", \"\\n\", \"Below, we demonstrate how colors change as we move hue/saturation/value one at a time.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def color_swatches(colors):\\n\", \"\\n\", \"    ncolors = len(colors)\\n\", \"    fig, ax = plt.subplots(figsize=(ncolors*2, 2))\\n\", \"\\n\", \"    for (start_x, color) in enumerate(colors):\\n\", \"        color_rect = patches.Rectangle((start_x, 0), 1, 1, color=color)\\n\", \"        ax.add_patch(color_rect)\\n\", \"\\n\", \"    ax.set_xlim(0, len(colors))\\n\", \"    ax.set_ylim(0, 1)\\n\", \"    ax.set_yticks([])\\n\", \"    ax.set_yticklabels([])\\n\", \"    ax.set_xticks([])\\n\", \"    ax.set_xticklabels([])\\n\", \"\\n\", \"    return fig\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Vary hue\\n\", \"colors = [mplc.hsv_to_rgb((i/360, 1, 1)) for i in np.linspace(0, 360, 6)]\\n\", \"fig = color_swatches(colors)\\n\", \"fig.suptitle(\\\"Varying Hue\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Vary saturation\\n\", \"colors = [mplc.hsv_to_rgb((0, i, 1)) for i in np.linspace(0, 1, 5)]\\n\", \"fig = color_swatches(colors)\\n\", \"fig.suptitle(\\\"Varying Saturation\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Vary value\\n\", \"colors = [mplc.hsv_to_rgb((0.0, 1, i)) for i in np.linspace(0, 1, 5)]\\n\", \"fig = color_swatches(colors)\\n\", \"fig.suptitle(\\\"Varying Value\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Color Palettes\\n\", \"\\n\", \"A good color palette will exploit aspects of hue, saturation, and value to emphasize the information\\n\", \"in the data visualization.\\n\", \"\\n\", \"For example, for qualitatively different groups (where we just want to identify separate groups\\n\", \"which have no quantitative relationships between them), one could fix\\n\", \"the saturation and value then draw $ N $ evenly spaced values from hue space.\\n\", \"\\n\", \"However, creating a good color palette sometimes requires more nuance than can be attributed to\\n\", \"rules of thumb.\\n\", \"\\n\", \"Luckily, matplotlib and other Python packages can help us choose good color\\n\", \"palettes. Often, relying on these pre-built color palettes and\\n\", \"themes is better than than creating your own.\\n\", \"\\n\", \"We can get a list of all of the color palettes (referred to as colormaps by matplotlib) included\\n\", \"with matplotlib by doing:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"print(plt.colormaps())\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"The [matplotlib documentation](https://matplotlib.org/tutorials/colors/colormaps.html)\\n\", \"differentiates between colormaps used for varying purposes.\\n\", \"\\n\", \"Colormaps are often split into several categories based on their function (see, e.g., [Moreland]):\\n\", \"\\n\", \"- Sequential: incrementally change lightness and often saturation of color,\\n\", \"  generally using a single hue; should be used for representing information that has ordering.  \\n\", \"- Diverging: change lightness and possibly saturation of two different\\n\", \"  colors that meet in the middle at an unsaturated color; should be used when the\\n\", \"  information being plotted has a critical middle value, such as topography or\\n\", \"  when the data deviates around zero.  \\n\", \"- Cyclic: change lightness of two different colors that meet in\\n\", \"  the middle and beginning/end at an unsaturated color; should be used for\\n\", \"  values that wrap around at the endpoints, such as phase angle, wind direction, or time of day.  \\n\", \"- Qualitative: often are miscellaneous colors; should be used to represent\\n\", \"  information which does not have ordering or relationships.  \\n\", \"\\n\", \"\\n\", \"Most of the examples we have used so far can use qualitative colormaps because they are simply\\n\", \"meant to distinguish between different variables/observations and not say something about how they\\n\", \"differ.\\n\", \"\\n\", \"Additionally, three other sources of information on colors and color palettes are:\\n\", \"\\n\", \"- The [seaborn documentation](https://seaborn.pydata.org/tutorial/color_palettes.html).  \\n\", \"- A [talk](https://www.youtube.com/watch?v=xAoljeRJ3lU) given at the Scipy conference in 2015 by\\n\", \"  Nathaniel Smith.  \\n\", \"- A [website](https://colorusage.arc.nasa.gov/graphics_page_design.php) literally put together by\\n\", \"  “rocket scientists” at NASA.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Do No Harm\\n\", \"\\n\", \"Now that we have a little background that we can use as a common language, we can proceed with\\n\", \"discussing how we can use color effectively.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Sometimes Value is More Effective than Hue\\n\", \"\\n\", \"Sometimes, in a graph with many lines, using the same color with different values is a more effective way to highlight differences than using different colors.\\n\", \"\\n\", \"Compare the following example, which is a modification of an example by Larry Arend, Alex Logan, and\\n\", \"Galina Havin’s [graphics website](https://colorusage.arc.nasa.gov/graphics_page_design.php) (the NASA one we linked above).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def confusing_plot(colors):\\n\", \"\\n\", \"    c1, c2, c3 = colors\\n\", \"\\n\", \"    fig, ax = plt.subplots()\\n\", \"\\n\", \"    x1 = np.linspace(0.2, 0.9, 5)\\n\", \"    x2 = np.linspace(0.3, 0.8, 5)\\n\", \"\\n\", \"    ax.text(0.4, 0.10, \\\"Not Important\\\", color=c3, fontsize=15)\\n\", \"    ax.text(0.25, 0.25, \\\"Not Important\\\", color=c3, fontsize=15)\\n\", \"    ax.text(0.5, 0.70, \\\"Not Important\\\", color=c3, fontsize=15)\\n\", \"    ax.plot(x1, 1.25*x1 - 0.2, color=c3, linewidth=2)\\n\", \"    ax.plot(x1, 1.25*x1 + 0.1, color=c3, linewidth=2)\\n\", \"    ax.plot(x1, 0*x1 + 0.3, color=c3, linewidth=2)\\n\", \"    ax.plot(x2, 0.15*x1 + 0.4, color=c2, linewidth=3)\\n\", \"    ax.plot(x1, -x1 + 1.2, color=c2, linewidth=3)\\n\", \"    ax.plot(x1, -x1 + 1.25, color=c2, linewidth=3)\\n\", \"    ax.text(0.10, 0.5, \\\"Second order\\\", color=c2, fontsize=22)\\n\", \"    ax.text(0.5, 0.35, \\\"Second order\\\", color=c2, fontsize=22)\\n\", \"    ax.text(0.40, 0.65, \\\"Second order\\\", color=c2, fontsize=22)\\n\", \"    ax.plot(x2, 0.25*x1 + 0.1, color=c1, linewidth=5)\\n\", \"    ax.text(0.05, 0.4, \\\"Important\\\", color=c1, fontsize=34)\\n\", \"\\n\", \"    ax.set_xlim(0, 1)\\n\", \"    ax.set_ylim(0, 1)\\n\", \"\\n\", \"    return fig\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# All black\\n\", \"colors = [mplc.hsv_to_rgb((0, 1, x)) for x in [0.0, 0.0, 0.0]]\\n\", \"fig = confusing_plot(colors)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Vary the hues\\n\", \"colors = [mplc.hsv_to_rgb((x, 1, 1)) for x in [0.0, 0.33, 0.66]]\\n\", \"fig = confusing_plot(colors)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Vary the values\\n\", \"colors = [mplc.hsv_to_rgb((0, 0, x)) for x in [0.00, 0.35, 0.7]]\\n\", \"fig = confusing_plot(colors)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"In our opinion, the last one with no color is actually the most readable.\\n\", \"\\n\", \"The point of this exercise is **not** to not use color in your plots, but rather to\\n\", \"encourage you to think about whether hue or value will be more effective in\\n\", \"communicating your message.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Carelessness with Value Can Make Grayscale Impossible to Read\\n\", \"\\n\", \"Recall that driving the saturation to 0 is approximately equivalent to projecting the colors\\n\", \"onto grayscale.\\n\", \"\\n\", \"Well, if you aren’t careful in choosing your colors, then they may have the same projected\\n\", \"values and become unidentifiable once converted to grayscale.\\n\", \"\\n\", \"This code is based on an [example](https://matplotlib.org/gallery/statistics/barchart_demo.html#barchart-demo)\\n\", \"from the matplotlib documentation.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"n_groups = 5\\n\", \"\\n\", \"means_men = (20, 35, 30, 35, 27)\\n\", \"means_women = (25, 32, 34, 20, 25)\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"\\n\", \"index = np.arange(n_groups)\\n\", \"bar_width = 0.35\\n\", \"\\n\", \"color_men = mplc.hsv_to_rgb((0.66, 0.35, 0.9))\\n\", \"rects1 = ax.bar(\\n\", \"    index, means_men, bar_width, color=color_men, label='men'\\n\", \")\\n\", \"\\n\", \"color_women = mplc.hsv_to_rgb((0.10, 0.65, 0.85))\\n\", \"rects2 = ax.bar(\\n\", \"    index + bar_width, means_women, bar_width, color=color_women, label='women'\\n\", \")\\n\", \"\\n\", \"ax.set_xlabel('group')\\n\", \"ax.set_ylabel('scores')\\n\", \"ax.set_title('scores by group and gender')\\n\", \"ax.set_xticks(index + bar_width / 2)\\n\", \"ax.set_xticklabels(('a', 'b', 'c', 'd', 'e'))\\n\", \"ax.legend()\\n\", \"\\n\", \"fig.tight_layout()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"And here is the same image converted to grayscale.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/assets/_static/visualization_files/bar_grayscale.png\\\" alt=\\\"bar_grayscale.png\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \\n\", \"The image below, from [this flowingdata blog entry](https://flowingdata.com/2012/11/09/incredibly-divided-nation-in-a-map),\\n\", \"shows what happens when you don’t check your colors… Don’t do this.\\n\", \"\\n\", \"<img src=\\\"https://datascience.quantecon.org/assets/_static/visualization_files/Divided-nation.jpg\\\" alt=\\\"Divided-nation.jpg\\\" style=\\\"\\\">\\n\", \"\\n\", \"  \\n\", \"Warm colors (colors like red, yellow, and orange) often appear lighter than cool colors (colors\\n\", \"like blue, green and purple) when converted to grayscale even when they have similar values.\\n\", \"Sometimes to know whether colors are different enough, you just have to test it out.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Use Color to Draw Attention\\n\", \"\\n\", \"If you are displaying information about various groups but are really only interested in how one\\n\", \"group differs from the others, then you should choose several close-together hues to represent the less\\n\", \"important groups and a distinct color to display the group of interest.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"npts = 50\\n\", \"x = np.linspace(0, 1, npts)\\n\", \"\\n\", \"np.random.seed(42)  # Set seed for reproducibility\\n\", \"y1 = 1.20 + 0.75*x + 0.25*np.random.randn(npts)\\n\", \"y2 = 1.35 + 0.50*x + 0.25*np.random.randn(npts)\\n\", \"y3 = 1.40 + 0.65*x + 0.25*np.random.randn(npts)\\n\", \"y4 = 0.15 + 3.0*x + 0.15*np.random.randn(npts)  # Group of interest\\n\", \"\\n\", \"colors = [mplc.hsv_to_rgb((x, 0.4, 0.85)) for x in [0.40, 0.50, 0.60]]\\n\", \"colors.append(mplc.hsv_to_rgb((0.0, 0.85, 1.0)))\\n\", \"\\n\", \"for (y, c) in zip([y1, y2, y3, y4], colors):\\n\", \"    ax.scatter(x=x, y=y, color=c, s=36)\\n\", \"\\n\", \"ax.text(0.25, 0.5, \\\"Group of Interest\\\", color=colors[-1])\\n\", \"ax.spines['right'].set_visible(False)\\n\", \"ax.spines['top'].set_visible(False)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Don’t Use Color to Differentiate Small Objects\\n\", \"\\n\", \"Color is a great differentiator when there is enough of the colored object to see… However, when the objects\\n\", \"become too small, differentiating between colors, no matter how distinct, becomes quite difficult.\\n\", \"\\n\", \"Below is the same plot as we had above, but we have made the scatter plot’s points smaller.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"npts = 50\\n\", \"x = np.linspace(0, 1, npts)\\n\", \"\\n\", \"np.random.seed(42)  # Set seed for reproducibility\\n\", \"y1 = 1.20 + 0.75*x + 0.25*np.random.randn(npts)\\n\", \"y2 = 1.35 + 0.50*x + 0.25*np.random.randn(npts)\\n\", \"y3 = 1.40 + 0.65*x + 0.25*np.random.randn(npts)\\n\", \"y4 = 0.15 + 3.0*x + 0.15*np.random.randn(npts)  # Group of interest\\n\", \"\\n\", \"colors = [mplc.hsv_to_rgb((x, 0.4, 0.85)) for x in [0.40, 0.50, 0.60]]\\n\", \"colors.append(mplc.hsv_to_rgb((0.0, 0.85, 1.0)))\\n\", \"\\n\", \"for (y, c) in zip([y1, y2, y3, y4], colors):\\n\", \"    ax.scatter(x=x, y=y, color=c, s=1)\\n\", \"\\n\", \"ax.text(0.25, 0.5, \\\"Group of Interest\\\", color=colors[-1])\\n\", \"ax.spines['right'].set_visible(False)\\n\", \"ax.spines['top'].set_visible(False)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"It becomes harder to read, but because the red is so much darker than some of the other colors,\\n\", \"finding the group of interest is still possible (a lesson to be learned here!).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Colors’ Connotations\\n\", \"\\n\", \"Some colors have connotations.\\n\", \"\\n\", \"Using colors to mean the opposite of what they’re usually used for can be confusing.\\n\", \"\\n\", \"For example, using red to denote positive profits and black to denote negative profits would be\\n\", \"a poor color choice because red is often associated with losses and black is associated with profits.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = pd.DataFrame(\\n\", \"    {\\\"profits\\\": [1.5, 2.5, 3.5, -6.7, -2.0, 1.0]},\\n\", \"    index=[2005, 2006, 2007, 2008, 2009, 2010]\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"colors = [\\\"k\\\" if x < 0 else \\\"r\\\" for x in df[\\\"profits\\\"].values]\\n\", \"bars = ax.bar(np.arange(len(colors)), df[\\\"profits\\\"].values, color=colors, alpha=0.8)\\n\", \"ax.hlines(0, -1.0, 6.0)\\n\", \"\\n\", \"ax.set_xticks([0, 1, 2, 3, 4, 5])\\n\", \"ax.set_xticklabels([x for x in df.index])\\n\", \"\\n\", \"ax.set_xlim(-0.5, 5.5)\\n\", \"ax.set_title(\\\"Profits for Company X\\\")\\n\", \"\\n\", \"ax.xaxis.set_ticks_position('none')\\n\", \"ax.yaxis.set_ticks_position('none')\\n\", \"\\n\", \"for _spine in [\\\"right\\\", \\\"top\\\", \\\"left\\\", \\\"bottom\\\"]:\\n\", \"    ax.spines[_spine].set_visible(False)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This plot becomes much more intuitive by using red for negative values.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"colors = [\\\"r\\\" if x < 0 else \\\"k\\\" for x in df[\\\"profits\\\"].values]\\n\", \"bars = ax.bar(np.arange(len(colors)), df[\\\"profits\\\"].values, color=colors, alpha=0.8)\\n\", \"ax.hlines(0, -1.0, 6.0)\\n\", \"ax.set_xticks([0, 1, 2, 3, 4, 5])\\n\", \"ax.set_xticklabels(df.index)\\n\", \"\\n\", \"ax.set_xlim(-0.5, 5.5)\\n\", \"ax.set_title(\\\"Profits for Company X\\\")\\n\", \"\\n\", \"ax.xaxis.set_ticks_position('none')\\n\", \"ax.yaxis.set_ticks_position('none')\\n\", \"\\n\", \"for _spine in [\\\"right\\\", \\\"top\\\", \\\"left\\\", \\\"bottom\\\"]:\\n\", \"    ax.spines[_spine].set_visible(False)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Accounting for Color Blindness\\n\", \"\\n\", \"Nearly 1 in 10 men have some form of color blindness.\\n\", \"\\n\", \"The most prevalent form makes differentiating between red\\n\", \"and green difficult.\\n\", \"\\n\", \"So, besides making your plots feel “Christmas-themed”, using both red and green to illustrate differences in a plot can often make your visualization difficult for some to follow.\\n\", \"\\n\", \"Some Python libraries allow you to simulate different forms of color blindness or choose\\n\", \"sensible defaults for colors.\\n\", \"\\n\", \"We recommend viewing the documentation for\\n\", \"[colorspacious](https://colorspacious.readthedocs.io/en/latest/tutorial.html#simulating-colorblindness)\\n\", \"and [viscm](https://github.com/matplotlib/viscm).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Visualization Rules\\n\", \"\\n\", \"We have already discussed some guidelines for color.\\n\", \"\\n\", \"We will now discuss some guidelines for which elements to include and how to structure your graphs.\\n\", \"\\n\", \"Violating each of these may make sense in particular situations, but please have a\\n\", \"good reason (and one you can explain when someone points out what you’ve done).\\n\", \"\\n\", \"The main theme for these guidelines will be to keep the plot as simple as possible so that your\\n\", \"readers can get the clearest understanding of your story.\\n\", \"\\n\", \"Many people try too hard to make their plot eye-catching, and in the process, they destroy the\\n\", \"message in the graph.\\n\", \"\\n\", \"Graphs should be a simple as possible, but not simpler.\\n\", \"\\n\", \"We will discuss some guidelines that we feel are most abused, but many very good books have\\n\", \"been written on this subject.\\n\", \"\\n\", \"Some books that we have found extremely instructive are:\\n\", \"\\n\", \"1. *Visual Display of Quantitative Information* by Edward Tufte.  \\n\", \"1. *The Wall Street Journal Guide to Information Graphics: The Dos and Don’ts of Presenting Data,\\n\", \"  Facts, and Figures* by Dona M Wong.  \\n\", \"1. *The Functional Art: An introduction to information graphics and visualization* by Alberto Cairo.  \\n\", \"\\n\", \"\\n\", \"Some blogs that we think are useful for seeing well-done visualizations are:\\n\", \"\\n\", \"1. Flowing Data: [https://flowingdata.com/](https://flowingdata.com/)  \\n\", \"1. Story Telling with Data: [http://www.storytellingwithdata.com/](http://www.storytellingwithdata.com/)  \\n\", \"1. Visualizing Data: [http://www.visualisingdata.com/](http://www.visualisingdata.com/)  \\n\", \"1. Junk Charts: [https://junkcharts.typepad.com/](https://junkcharts.typepad.com/)  \\n\", \"\\n\", \"\\n\", \"As you begin to create more visualizations in your work, we recommend reading these books and blogs.\\n\", \"\\n\", \"Seeing how others display their information will ensure that when you run into interesting\\n\", \"problems in the future, you’ll have a well of knowledge that you can call upon.\\n\", \"\\n\", \"In fact, one friend of ours takes this very seriously.\\n\", \"\\n\", \"He keeps an organized binder of graphics that he has seen and likes.\\n\", \"\\n\", \"He reads this binder, sometimes for hours, when he is thinking about how to communicate messages\\n\", \"for his presentations.\\n\", \"\\n\", \"A couple last links to specific articles we enjoyed:\\n\", \"\\n\", \"- [This Financial Times article](https://ig.ft.com/science-of-charts) is a great exercise to\\n\", \"  demonstrate how choice of graph type can affect a visualizations interpretability.  \\n\", \"- [This article](https://towardsdatascience.com/data-visualization-best-practices-less-is-more-and-people-dont-read-ba41b8f29e7b)\\n\", \"  does an exceptional job at redesigning graphics that were originally poorly done.  \\n\", \"- [Duke library data visualization guide](https://guides.library.duke.edu/datavis/topten) has a\\n\", \"  few concise rules worth reviewing.  \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Bar Plot Recommendations\\n\", \"\\n\", \"In Dona Wong’s book, she advises against using *zebra patterns*.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = pd.DataFrame(\\n\", \"    {\\n\", \"        \\\"Unemployment Rate\\\": [5.20, 5.67, 9.20, 4.03, 3.80],\\n\", \"        \\\"Pension Expenditure (% of GDP)\\\": [4.18, 4.70, 13.90, 6.24, 7.06],\\n\", \"        \\\"Social Welfare Expenditure (% of GDP)\\\": [7.42, 9.84, 19.72, 12.98, 14.50],\\n\", \"        \\\"Highest Tax Rate\\\": [47, 33, 59.6, 50, 39.6]\\n\", \"    },\\n\", \"    index = [\\\"Australia\\\", \\\"Canada\\\", \\\"France\\\", \\\"UK\\\", \\\"USA\\\"]\\n\", \")\\n\", \"\\n\", \"def create_barplot(df, colors):\\n\", \"\\n\", \"    fig, ax = plt.subplots(figsize=(14, 6))\\n\", \"\\n\", \"    df.T.plot(kind=\\\"bar\\\", color=colors, ax=ax, edgecolor=\\\"k\\\", rot=0)\\n\", \"    ax.legend(bbox_to_anchor=(0, 1.02, 1.0, 1.02), loc=3, mode=\\\"expand\\\", ncol=5)\\n\", \"    ax.set_xticklabels(df.columns, fontsize=6)\\n\", \"\\n\", \"    return fig\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Instead, she proposes using different shades of the same color (ordered from lightest to darkest!).\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"colors = [\\n\", \"    (0.902, 0.902, 0.997), (0.695, 0.695, 0.993), (0.488, 0.488, 0.989),\\n\", \"    (0.282, 0.282, 0.985), (0.078, 0.078, 0.980)\\n\", \"]\\n\", \"\\n\", \"create_barplot(df, colors);\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that we put a legend at the top and maintain the same order as kept in the bars.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Additionally, the general consensus is that starting bar plots at any number besides 0 is a\\n\", \"misrepresentation of the data.\\n\", \"\\n\", \"Always start your bar plots at 0!\\n\", \"\\n\", \"An example of how starting at a non-zero number is misleading can be seen below and was originally from the\\n\", \"[flowingdata blog](https://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence).\\n\", \"\\n\", \"First, we look at a reproduction of the originally displayed image.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"ax.bar([0, 1], [35, 39.6], color=\\\"orange\\\")\\n\", \"\\n\", \"ax.set_xticks([0, 1])\\n\", \"ax.set_xticklabels([\\\"Now\\\", \\\"Jan 1, 2013\\\"])\\n\", \"ax.set_ylim(34, 42)\\n\", \"\\n\", \"ax.xaxis.set_ticks_position('none')\\n\", \"ax.yaxis.set_ticks_position('none')\\n\", \"\\n\", \"for _spine in [\\\"right\\\", \\\"top\\\", \\\"left\\\", \\\"bottom\\\"]:\\n\", \"    ax.spines[_spine].set_visible(False)\\n\", \"\\n\", \"ax.set_title(\\\"IF BUSH TAX CUTS EXPIRE\\\\nTop Tax Rate\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This looks like a big difference!\\n\", \"\\n\", \"In fact, your eyes are telling you that taxes will increase by a factor of 5 if the tax cuts expire.\\n\", \"\\n\", \"If we start this same bar plot at 0, the chart becomes much less striking and tells you that the percentage\\n\", \"increase in the top tax rate is only 5-10 percent.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"ax.bar([0, 1], [35, 39.6], color=\\\"orange\\\")\\n\", \"\\n\", \"ax.set_xticks([0, 1])\\n\", \"ax.set_xticklabels([\\\"Now\\\", \\\"Jan 1, 2013\\\"])\\n\", \"ax.set_ylim(0, 42)\\n\", \"\\n\", \"ax.xaxis.set_ticks_position('none')\\n\", \"ax.yaxis.set_ticks_position('none')\\n\", \"\\n\", \"for _spine in [\\\"right\\\", \\\"top\\\", \\\"left\\\", \\\"bottom\\\"]:\\n\", \"    ax.spines[_spine].set_visible(False)\\n\", \"\\n\", \"ax.set_title(\\\"IF BUSH TAX CUTS EXPIRE\\\\nTop Tax Rate\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We also have opinions about what type of person uses all caps, but we’ll keep that to ourselves for\\n\", \"now.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Pie Plots\\n\", \"\\n\", \"As a general rule, you should avoid pie plots.\\n\", \"\\n\", \"When comparing groups, your reader can more easily measure the heights on a\\n\", \"bar graph than determine the size of the angles in a pie chart.\\n\", \"\\n\", \"Let’s look at an example of this below.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df = pd.DataFrame(\\n\", \"    {\\\"values\\\": [5.5, 4.5, 8.4, 4.75, 2.5]},\\n\", \"    index=[\\\"Bob\\\", \\\"Alice\\\", \\\"Charlie\\\", \\\"Susan\\\", \\\"Jessie\\\"]\\n\", \")\\n\", \"\\n\", \"colors = [mplc.hsv_to_rgb((0.66, 0.8, 0.9))]*2\\n\", \"colors += [mplc.hsv_to_rgb((0.05, 0.6, 0.9))]\\n\", \"colors += [mplc.hsv_to_rgb((0.66, 0.8, 0.9))]*2\\n\", \"\\n\", \"fig, ax = plt.subplots(1, 2)\\n\", \"\\n\", \"df.plot(kind=\\\"barh\\\", y=\\\"values\\\", ax=ax[0], legend=False, color=colors)\\n\", \"df.plot(kind=\\\"pie\\\", y=\\\"values\\\", ax=ax[1], legend=False, colors=colors, startangle=0)\\n\", \"\\n\", \"ax[0].spines['right'].set_visible(False)\\n\", \"ax[0].spines['top'].set_visible(False)\\n\", \"ax[1].set_ylabel(\\\"\\\")\\n\", \"fig.suptitle(\\\"How many pieces of pie eaten\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Using the pie chart, can you tell who ate more pie Alice or Susan? How about with the bar chart?\\n\", \"\\n\", \"The pie chart can sometimes be used to illustrate whether one or two groups is much larger than the\\n\", \"others.\\n\", \"\\n\", \"If you were making a case that Charlie ate too much of the pie and should pay more than an equal split,\\n\", \"then a pie chart works (though a bar plot also works…).\\n\", \"\\n\", \"If you wanted to make a more precise point, then you might consider going with a bar plot instead.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Simplify Line Plots\\n\", \"\\n\", \"We’ve tried to emphasize repeatedly that simplifying your visualizations is essential to being able\\n\", \"to communicate your message.\\n\", \"\\n\", \"We do it again here and will do it a few more times after this…\\n\", \"\\n\", \"Don’t try and fit too much information into a single line plot.\\n\", \"\\n\", \"We see people do this very frequently – remember that a visualization should have ONE main message.\\n\", \"\\n\", \"Do not pollute your message with extra information.\\n\", \"\\n\", \"In our example using World Bank data below, we will show that Japan’s population is aging faster\\n\", \"than that of many other economically successful countries.\\n\", \"\\n\", \"We show this using the age dependency ratio, which is the number of individuals aged 65+ divided by the number of individuals who are 15-64, for each country over time.\\n\", \"\\n\", \"A high age dependency ratio means that the government will have a smaller tax base to collect from\\n\", \"but have relatively higher health and pension expenditures to pay to the old.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"download_url = \\\"https://datascience.quantecon.org/assets/data/WorldBank_AgeDependencyRatio.csv\\\"\\n\", \"df = pd.read_csv(download_url, na_values=\\\"..\\\")\\n\", \"df = df[[\\\"Country Name\\\", \\\"1960\\\", \\\"1970\\\", \\\"1980\\\", \\\"1990\\\", \\\"2000\\\", \\\"2010\\\", \\\"2017\\\"]]\\n\", \"df = df.set_index(\\\"Country Name\\\").T\\n\", \"df.index = df.index.values.astype(int)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Let’s visualize these variables for a collection of many developed countries.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"df.plot(ax=ax, legend=False)\\n\", \"ax.text(2007, 38, \\\"Japan\\\")\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.set_title(\\\"Japan's Aging Population\\\")\\n\", \"ax.set_ylabel(\\\"Age Dependency Ratio\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice that with so many lines, the message about Japan is hidden or polluted by noise.\\n\", \"\\n\", \"If we did want to demonstrate that Japan is significantly different than many other developed countries,\\n\", \"we might try a plot like this:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"not_japan = list(df.columns)\\n\", \"not_japan.remove(\\\"Japan\\\")\\n\", \"\\n\", \"df[not_japan].plot(ax=ax, color=[(0.8, 0.8, 0.8)], lw=0.4, legend=False)\\n\", \"ax.text(1970, 29, \\\"Other Developed Countries\\\")\\n\", \"\\n\", \"df[\\\"Japan\\\"].plot(ax=ax, color=(0.95, 0.05, 0.05), lw=2.5, legend=False)\\n\", \"ax.text(2006.5, 38, \\\"Japan\\\")\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.set_title(\\\"Japan's Aging Population\\\")\\n\", \"ax.set_ylabel(\\\"Age Dependency Ratio\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"However, placing this many lines on a single plot is definitely an exception, and we encourage you\\n\", \"to do so sparingly.\\n\", \"\\n\", \"Generally, you should only have a few informative lines for each plot.\\n\", \"\\n\", \"We now will focus our graph on a few countries of interest.\\n\", \"\\n\", \"To do so, the plot below uses many different line styles.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"df[\\\"Japan\\\"].plot(ax=ax, legend=False, linestyle=\\\"solid\\\")\\n\", \"ax.text(2002, 35, \\\"Japan\\\")\\n\", \"df[\\\"United Kingdom\\\"].plot(ax=ax, legend=False, linestyle=\\\"dashed\\\")\\n\", \"ax.text(1975, 24, \\\"UK\\\")\\n\", \"df[\\\"United States\\\"].plot(ax=ax, legend=False, linestyle=\\\"dashed\\\")\\n\", \"ax.text(1980, 19, \\\"US\\\")\\n\", \"df[\\\"China\\\"].plot(ax=ax, legend=False, linestyle=\\\"dotted\\\")\\n\", \"ax.text(1990, 10, \\\"China\\\")\\n\", \"df[\\\"India\\\"].plot(ax=ax, legend=False, linestyle=\\\"dotted\\\")\\n\", \"ax.text(2005, 5, \\\"India\\\")\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.set_title(\\\"Japan's Aging Population\\\")\\n\", \"ax.set_ylabel(\\\"Age Dependency Ratio\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"There are some good-use cases for using line styles to distinguish between different pieces\\n\", \"of data, but not many.\\n\", \"\\n\", \"In particular, having this many different styles and colors makes it difficult to figure out what is going on.\\n\", \"\\n\", \"Instead, we recommend using color and line width instead of line styles to highlight certain pieces of\\n\", \"information, as seen below.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"emph_color = (0.95, 0.05, 0.05)\\n\", \"sec_color = [(0.05, 0.05+0.075*x, 0.95) for x in range(4)]\\n\", \"df[\\\"Japan\\\"].plot(ax=ax, legend=False, color=emph_color, linewidth=2.5)\\n\", \"ax.text(2002, 35, \\\"Japan\\\")\\n\", \"df[\\\"United Kingdom\\\"].plot(ax=ax, legend=False, color=sec_color[0], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(1975, 24, \\\"UK\\\")\\n\", \"df[\\\"United States\\\"].plot(ax=ax, legend=False, color=sec_color[1], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(1980, 19, \\\"US\\\")\\n\", \"df[\\\"China\\\"].plot(ax=ax, legend=False, color=sec_color[2], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(1990, 10, \\\"China\\\")\\n\", \"df[\\\"India\\\"].plot(ax=ax, legend=False, color=sec_color[3], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(2005, 5, \\\"India\\\")\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.set_title(\\\"Japan's Aging Population\\\")\\n\", \"ax.set_ylabel(\\\"Age Dependency Ratio\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Tick Steps\\n\", \"\\n\", \"Use easy to interpret increments such as multiples of 2, 5, 10, 25 etc…\\n\", \"\\n\", \"Using increments like `0, 3, 6, 9, 12, ...` make it more difficult for your reader to do mentally\\n\", \"determine what the values between the lines are:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots(1, 2, figsize=(14, 6))\\n\", \"\\n\", \"x = np.linspace(0, 26, 50)\\n\", \"\\n\", \"ax[0].plot(x, np.sqrt(x))\\n\", \"ax[1].plot(x, np.sqrt(x))\\n\", \"\\n\", \"ax[0].set_xticks(np.arange(0, 27, 3))\\n\", \"ax[0].set_xticklabels(np.arange(0, 27, 3))\\n\", \"ax[1].set_xticks(np.arange(0, 27, 5))\\n\", \"ax[1].set_xticklabels(np.arange(0, 27, 5))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### No Background Colors\\n\", \"\\n\", \"There are no reasons to use background colors in your visualizations.\\n\", \"\\n\", \"Research has shown that white or very light grays provide the best contrast as a background.\\n\", \"\\n\", \"Compare the following graphs and think about which feels better.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"ax.bar([0, 1], [35, 39.6], color=\\\"orange\\\")\\n\", \"\\n\", \"ax.set_xticks([0, 1])\\n\", \"ax.set_xticklabels([\\\"Now\\\", \\\"Jan 1, 2013\\\"])\\n\", \"ax.set_ylim(0, 42)\\n\", \"\\n\", \"bgcolor = \\\"blue\\\"\\n\", \"fig.set_facecolor(bgcolor)\\n\", \"ax.set_facecolor(bgcolor)\\n\", \"ax.xaxis.set_ticks_position('none')\\n\", \"ax.yaxis.set_ticks_position('none')\\n\", \"\\n\", \"for _spine in [\\\"right\\\", \\\"top\\\", \\\"left\\\", \\\"bottom\\\"]:\\n\", \"    ax.spines[_spine].set_visible(False)\\n\", \"\\n\", \"ax.set_title(\\\"IF BUSH TAX CUTS EXPIRE\\\\nTop Tax Rate\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"versus\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"ax.bar([0, 1], [35, 39.6], color=\\\"orange\\\")\\n\", \"\\n\", \"ax.set_xticks([0, 1])\\n\", \"ax.set_xticklabels([\\\"Now\\\", \\\"Jan 1, 2013\\\"])\\n\", \"ax.set_ylim(0, 42)\\n\", \"\\n\", \"bgcolor = (0.99, 0.99, 0.99)\\n\", \"fig.set_facecolor(bgcolor)\\n\", \"ax.set_facecolor(bgcolor)\\n\", \"ax.xaxis.set_ticks_position('none')\\n\", \"ax.yaxis.set_ticks_position('none')\\n\", \"\\n\", \"for _spine in [\\\"right\\\", \\\"top\\\", \\\"left\\\", \\\"bottom\\\"]:\\n\", \"    ax.spines[_spine].set_visible(False)\\n\", \"\\n\", \"ax.set_title(\\\"IF BUSH TAX CUTS EXPIRE\\\\nTop Tax Rate\\\")\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Legends\\n\", \"\\n\", \"Legends are quite common in charts, but many visualization experts advise against using them.\\n\", \"\\n\", \"Legends have several weaknesses:\\n\", \"\\n\", \"1. Relying solely on line color often makes a black and white version of your plot effectively\\n\", \"  useless, since you don’t know whether the colors will be distinguishable in grayscale.  \\n\", \"1. Legends require people to distinguish between small samples of colors. For\\n\", \"  someone with weak eyesight or color blindness, this can make interpreting graphs nearly\\n\", \"  impossible.  \\n\", \"1. They add distance between the data and its description. This requires peoples’ eyes to go back\\n\", \"  and forth between the lines and the legend when trying to understand the story being told. This\\n\", \"  distracts from the ability to digest the story quickly and succinctly.  \\n\", \"\\n\", \"\\n\", \"To demonstrate this, we revisit our age dependency ratio example from earlier.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"download_url = \\\"https://datascience.quantecon.org/assets/data/WorldBank_AgeDependencyRatio.csv\\\"\\n\", \"df = pd.read_csv(download_url, na_values=\\\"..\\\")\\n\", \"df = df[[\\\"Country Name\\\", \\\"1960\\\", \\\"1970\\\", \\\"1980\\\", \\\"1990\\\", \\\"2000\\\", \\\"2010\\\", \\\"2017\\\"]]\\n\", \"df = df.set_index(\\\"Country Name\\\").T\\n\", \"df.index = df.index.values.astype(int)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"With a legend:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"emph_color = (0.95, 0.05, 0.05)\\n\", \"sec_color = [(0.05, 0.05+0.075*x, 0.95) for x in range(4)]\\n\", \"df[\\\"Japan\\\"].plot(ax=ax, legend=True, color=emph_color, linewidth=2.5)\\n\", \"df[\\\"United Kingdom\\\"].plot(ax=ax, legend=True, color=sec_color[0], alpha=0.4, linewidth=0.75)\\n\", \"df[\\\"United States\\\"].plot(ax=ax, legend=True, color=sec_color[1], alpha=0.4, linewidth=0.75)\\n\", \"df[\\\"China\\\"].plot(ax=ax, legend=True, color=sec_color[2], alpha=0.4, linewidth=0.75)\\n\", \"df[\\\"India\\\"].plot(ax=ax, legend=True, color=sec_color[3], alpha=0.4, linewidth=0.75)\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.set_title(\\\"Japan's Aging Population\\\")\\n\", \"ax.set_ylabel(\\\"Age Dependency Ratio\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"With labels:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"fig, ax = plt.subplots()\\n\", \"\\n\", \"emph_color = (0.95, 0.05, 0.05)\\n\", \"sec_color = [(0.05, 0.05+0.075*x, 0.95) for x in range(4)]\\n\", \"df[\\\"Japan\\\"].plot(ax=ax, legend=False, color=emph_color, linewidth=2.5)\\n\", \"ax.text(2002, 35, \\\"Japan\\\")\\n\", \"df[\\\"United Kingdom\\\"].plot(ax=ax, legend=False, color=sec_color[0], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(1975, 24, \\\"UK\\\")\\n\", \"df[\\\"United States\\\"].plot(ax=ax, legend=False, color=sec_color[1], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(1980, 19, \\\"US\\\")\\n\", \"df[\\\"China\\\"].plot(ax=ax, legend=False, color=sec_color[2], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(1990, 10, \\\"China\\\")\\n\", \"df[\\\"India\\\"].plot(ax=ax, legend=False, color=sec_color[3], alpha=0.4, linewidth=0.75)\\n\", \"ax.text(2005, 5, \\\"India\\\")\\n\", \"\\n\", \"ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"ax.set_title(\\\"Japan's Aging Population\\\")\\n\", \"ax.set_ylabel(\\\"Age Dependency Ratio\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}, \"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Most people find the example with labels to be a more readable graph.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Limit the Information in a Single Plot\\n\", \"\\n\", \"Don’t try to put too much information in a single plot!\\n\", \"\\n\", \"We have tried to emphasize this point throughout this lecture, but it is so important that\\n\", \"we are emphasizing it again!\\n\", \"\\n\", \"Don’t information overload your audience!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Talk to Other People\\n\", \"\\n\", \"Our last guideline: talk with others about your visualization.\\n\", \"\\n\", \"The best way to determine whether other people understand your message is to show it to them.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## References\\n\", \"\\n\", \"<a id='ely'></a>\\n\", \"**[1]** In particular, it is based on [this lecture](https://www.aeaweb.org/webcasts/2019/aea-ely-lecture-work-of-the-past-work-of-the-future)\\n\", \"by Autor presented at the annual AEA meeting in January, 2019. This\\n\", \"is a prestigious invited lecture with a large audience, so it is a more\\n\", \"“polished” than the typical academic lecture. It is worth\\n\", \"watching. Notice how almost every slide includes data\\n\", \"visualizations, and very few consist solely of text. Also, notice\\n\", \"the ways that the NYT modified Autor’s figures and think about\\n\", \"whether these changes improved the figures.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"date\": 1595352471.115743, \"title\": \"Data Visualization: Rules and Guidelines\", \"filename\": \"visualization_rules.rst\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3\"}, \"celltoolbar\": \"Slideshow\", \"download_nb\": false, \"language_info\": {\"name\": \"python\", \"version\": \"3.8.8\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}, \"filename_with_path\": \"applications/visualization_rules\"}, \"nbformat\": 4, \"nbformat_minor\": 2}",
    "md_content": null,
    "properties": "{\"fileName\": \"v2_visualization_rules.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.787Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 111,
    "position": 11,
    "content_id": 173,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 173,
    "type": "video",
    "description": "Learn about the most popular libraries for creating data visualizations in Python",
    "title": "Python Plotting Libraries",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=L50TEMAE54U\", \"youtubeVideoId\": \"L50TEMAE54U\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 15,
    "position": 7,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 16,
    "description": "Data Visualization",
    "title": "Lecture 07",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 107,
    "position": 9,
    "content_id": 172,
    "lecture_id": 16,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 172,
    "type": "video",
    "description": "Learn the fundamentals of data visualization",
    "title": "Plotting Introduction",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=u9w-qp-_MWE\", \"youtubeVideoId\": \"u9w-qp-_MWE\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 10,
    "position": 8,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 11,
    "description": "SQL Introduction",
    "title": "Lecture 09",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 119,
    "position": 9,
    "content_id": 175,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 175,
    "type": "video",
    "description": "Use the Instacart dataset to learn the basics of SQL",
    "title": "SQL By Example",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=-mpsQYhRdfc\", \"youtubeVideoId\": \"-mpsQYhRdfc\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 10,
    "position": 8,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 11,
    "description": "SQL Introduction",
    "title": "Lecture 09",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 118,
    "position": 9,
    "content_id": 140,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 140,
    "type": "notebook",
    "description": "Understand the structure of a grocery shopping dataset from Instacart",
    "title": "01_data_description.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Data Description: Instacart Dataset\\n\", \"\\n\", \"[Instacart](https://www.instacart.com/) is an online grocery retailer that sells and deliver grocery products. The data that we will use today comes from the an open sourced subset of 3 million Instacart orders and contains data on what was ordered and which customer ordered it.\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pandas as pd\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Data files\\n\", \"\\n\", \"The dataset was released as a part of a Kaggle competition. Instacart described the dataset in the competition description by saying,\\n\", \"\\n\", \"> The dataset for this competition is a relational set of files describing customers' orders over time. The goal of the competition is to predict which products will be in a user's next order. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, we provide between 4 and 100 of their orders, with the sequence of products purchased in each order. We also provide the week and hour of day the order was placed, and a relative measure of time between orders. For more information, see the blog post accompanying its [public release](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2).\\n\", \"\\n\", \"“The Instacart Online Grocery Shopping Dataset 2017”, Accessed from https://www.kaggle.com/c/instacart-market-basket-analysis/data on 2020-12-27\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Aisles: `aisles.csv`\\n\", \"\\n\", \"This file contains metadata about the aisles:\\n\", \"\\n\", \"* `aisle_id`: The aisle identifier\\n\", \"* `aisle`: A string name describing the aisle\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"aisle = pd.read_csv(\\\"https://compsosci-resources.s3.amazonaws.com/instacart/aisles.csv\\\")\\n\", \"aisle.to_parquet(\\\"aisles.parquet\\\")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"aisle.tail()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Departments: `department.csv`\\n\", \"\\n\", \"This file contains metadata about the departments:\\n\", \"\\n\", \"* `department_id`: The department identifier\\n\", \"* `department`: A string name describing the department\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"department = pd.read_csv(\\\"https://compsosci-resources.s3.amazonaws.com/instacart/departments.csv\\\")\\n\", \"department.to_parquet(\\\"departments.parquet\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"department.tail()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Products: `products.csv`\\n\", \"\\n\", \"This file contains metadata about each of the products:\\n\", \"\\n\", \"* `product_id`: An identifier for the product that was purchased\\n\", \"* `product_name`: A string name for the product's description\\n\", \"* `aisle_id`: The aisle identifier for the \\\"aisle\\\" that the item is from\\n\", \"* `department_id`: The department identifier for the department that the item is from\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"products = pd.read_csv(\\\"https://compsosci-resources.s3.amazonaws.com/instacart/products.csv\\\")\\n\", \"products.to_parquet(\\\"products.parquet\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"products.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Count number of items with a particular aisle/department\\n\", \"products.groupby(\\n\", \"    [\\\"aisle_id\\\", \\\"department_id\\\"]\\n\", \")[\\\"product_name\\\"].count().sort_values(\\n\", \"    ascending=False\\n\", \").reset_index().merge(\\n\", \"    aisle, on=\\\"aisle_id\\\", how=\\\"left\\\"\\n\", \").merge(\\n\", \"    department, on=\\\"department_id\\\", how=\\\"left\\\"\\n\", \").sort_values(\\n\", \"    [\\\"aisle_id\\\", \\\"department_id\\\"]\\n\", \").head(25)\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Orders: `orders.csv`\\n\", \"\\n\", \"This file contains meta information about each of the 3 million orders that are covered in the dataset.\\n\", \"\\n\", \"* `order_id`: A unique identifier for each order made\\n\", \"* `user_id`: A unique identifier for each consumer that made one of the 3 million orders\\n\", \"* `eval_set`: Instacart released this data for machine learning and classified orders into `prior`, `train`, and `test`\\n\", \"* `order_number`: The order in which the individual made the given orders\\n\", \"* `order_dow`: An integer between 0 and 6 denoting the day of the week the order was made\\n\", \"* `order_hour_of_day`: An integer between 0 and 23 denoting the hour of the day that the order was made\\n\", \"* `days_since_prior_order`: An integer that represents how many days it has been since a customers previous order\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**What is not included?**\\n\", \"\\n\", \"Instacart does not disclose the exact date of the transactions or the geography in which the transaction took place.\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"orders = pd.read_csv(\\\"https://compsosci-resources.s3.amazonaws.com/instacart/orders.csv\\\")\\n\", \"orders.to_parquet(\\\"orders.parquet\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"orders.describe()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"orders.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Order Products: `order_products__prior.csv` and `order_products__train.csv`\\n\", \"\\n\", \"This file contains detailed information about each of the orders:\\n\", \"\\n\", \"* `order_id`: A unique identifier that describes which order the purchase belonged to\\n\", \"* `product_id`: An identifier for the product that was purchased\\n\", \"* `add_to_cart_order`: The order in which the items were placed in the cart\\n\", \"* `reordered`: Was the item reordered\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**What is not included?**\\n\", \"\\n\", \"Instacart does not disclose two pieces of information that would typically be important:\\n\", \"\\n\", \"1. Price paid for a product\\n\", \"2. Quantity purchased\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"prior_orders = pd.read_csv(\\\"https://compsosci-resources.s3.amazonaws.com/instacart/order_products__prior.csv\\\")\\n\", \"\\n\", \"prior_orders.describe()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"train_orders = pd.read_csv(\\\"https://compsosci-resources.s3.amazonaws.com/instacart/order_products__train.csv\\\")\\n\", \"\\n\", \"train_orders.describe()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"order_products = pd.concat([prior_orders, train_orders], axis=0, ignore_index=True)\\n\", \"order_products.to_parquet(\\\"order_products_all.parquet\\\")\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Relational nature of the files\\n\", \"\\n\", \"These files are organized in a way that each one contains information that either references or is referenced by one of the other files.\\n\", \"\\n\", \"* `aisles.csv` and `departments.csv` are used to provide additional information and context for `products.csv`.\\n\", \"* `products.csv` describes the products that appear in `order_products__prior.csv` and `order_products__train.csv`.\\n\", \"* `orders.csv` contains information about when and who made the orders that show up in `order_products__prior.csv` and `order_products__train.csv`.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\" Let's explicitly write down these relationships to set the stage for the coming lecture about SQL.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Most frequently reordered?\\n\", \"\\n\", \"Instacart was particularly interested in determining whether certain items (or groups of items) were reordered more than others.\\n\", \"\\n\", \"Let's explore what items/groups were the most reordered -- We will do this by computing the fraction of \\\"reorders\\\" for a particular item/group ($j$) by computing:\\n\", \"\\n\", \"$$\\\\gamma_j = \\\\frac{\\\\sum_i \\\\sum_{t_i} \\\\mathbb{1}_{\\\\text{reordered } j}}{\\\\sum_i \\\\sum_{t_i} \\\\mathbb{1}_{\\\\text{ordered } j}}$$\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"order_product_user = order_products.merge(\\n\", \"    orders.loc[:, [\\\"order_id\\\", \\\"user_id\\\", \\\"order_number\\\", \\\"days_since_prior_order\\\"]],\\n\", \"    how=\\\"left\\\", on=\\\"order_id\\\"\\n\", \")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"order_product_user.sort_values([\\\"user_id\\\", \\\"order_number\\\", \\\"add_to_cart_order\\\"])\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Most reordered products**\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"mrp = order_product_user.loc[\\n\", \"    ~order_product_user[\\\"days_since_prior_order\\\"].isna(), :\\n\", \"].groupby(\\n\", \"    \\\"product_id\\\"\\n\", \").agg(\\n\", \"    {\\\"add_to_cart_order\\\": \\\"count\\\", \\\"reordered\\\": \\\"sum\\\"}\\n\", \").query(\\n\", \"    \\\"reordered > 10\\\"\\n\", \").rename(\\n\", \"    columns={\\\"add_to_cart_order\\\": \\\"norder\\\", \\\"reordered\\\": \\\"nreorder\\\"}\\n\", \").assign(\\n\", \"    frac_reorder=lambda x: x[\\\"nreorder\\\"] / x[\\\"norder\\\"]\\n\", \").sort_values(\\n\", \"    \\\"frac_reorder\\\", ascending=False\\n\", \").merge(products, on=\\\"product_id\\\", how=\\\"left\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"mrp.describe()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"mrp.head()\"], \"outputs\": [], \"metadata\": {\"scrolled\": true, \"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"order_product_user.query(\\\"product_id == 43553\\\").sort_values([\\\"user_id\\\", \\\"order_number\\\"])\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"interpreter\": {\"hash\": \"06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"01_data_description.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.788Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 10,
    "position": 8,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 11,
    "description": "SQL Introduction",
    "title": "Lecture 09",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 120,
    "position": 10,
    "content_id": 146,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 146,
    "type": "notebook",
    "description": "Use the Instacart dataset to learn the basics of SQL",
    "title": "02_sql.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# SQL Introduction\\n\", \"\\n\", \"We will learn how to use SQL today (mostly by example)\\n\", \"\\n\", \"Our example data comes from the [Instacart dataset](./01_data_description.ipynb) we discussed previously\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import os\\n\", \"import pandas as pd\\n\", \"import sqlalchemy as sa\\n\", \"import zipfile\\n\", \"import requests\\n\", \"from io import BytesIO\\n\", \"\\n\", \"from sqlalchemy.ext.declarative import declarative_base\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## SQL (structured query language)\\n\", \"\\n\", \"SQL is a \\\"query language\\\" that can be used to communicate with (relational) databases.\\n\", \"\\n\", \"SQL itself is more of a standard for a language to communicate with databases rather than an implemented programming language which means that each database creates their own implementation of how SQL commands get translated into queries.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**What problem does SQL solve?**\\n\", \"\\n\", \"1. Straightforward way to ingest data from a database\\n\", \"2. Industry standard to make database code/requirements (nearly) compatible\\n\", \"3. The implementations often provide great ways to provide multiple levels of \\\"access\\\" to a dataset\\n\", \"  - Some users will be \\\"data users\\\" and will use the data in their projects -- These users can get away with \\\"read only access\\\" to the database\\n\", \"  - Other users will be \\\"data creators\\\" and will maintain and update the data stored in the database -- These users will need to be able to either add data or participate through other administration roles\\n\", \"4. Allows administrators to impose strict requirements across the data -- For example, could impose a uniqueness constraint if we did not want an email to correspond to more than one user etc...\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Our focus today**\\n\", \"\\n\", \"Our main focus for this class will be on introducing how to be \\\"database users\\\" rather than \\\"database administrators\\\"\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## SQL and SQLAlchemy\\n\", \"\\n\", \"We'll now discuss a few details of SQL and SQLAlchemy:\\n\", \"\\n\", \"`sqlalchemy` is a Python package that allows one to generically interface with many different \\\"flavors\\\" of SQL (PostgreSQL, MySQL, SQLite, etc...) using Python code.\\n\", \"\\n\", \"We will only discuss it briefly today because it isn't the focus of this lecture.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### SQL Tables and Types\\n\", \"\\n\", \"As we mentioned, one of the benefits of SQL is that it allows those who are creating the databases to impose tight requirements on what is contained in the data:\\n\", \"\\n\", \"* **Tables**: SQL allows one to specify a table with pre-defined columns, cross-table restrictions, and more\\n\", \"* **Types**: Each column in a SQL table must have a specified type. These types are mostly the \\\"usual suspects\\\"\\n\", \"  - Boolean\\n\", \"  - Date\\n\", \"  - Numeric (Float, Integer, ...)\\n\", \"  - String\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Declaring table structures\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Base = declarative_base()\\n\", \"\\n\", \"\\n\", \"class Aisles(Base):\\n\", \"    __tablename__ = \\\"aisles\\\"\\n\", \"    aisle_id = sa.Column(sa.Integer, primary_key=True)\\n\", \"    aisle = sa.Column(sa.String)\\n\", \"\\n\", \"\\n\", \"class Departments(Base):\\n\", \"    __tablename__ = \\\"departments\\\"\\n\", \"    department_id = sa.Column(sa.Integer, primary_key=True)\\n\", \"    department = sa.Column(sa.String)\\n\", \"\\n\", \"\\n\", \"class Products(Base):\\n\", \"    __tablename__ = \\\"products\\\"\\n\", \"    product_id = sa.Column(sa.Integer, primary_key=True)\\n\", \"    product_name = sa.Column(sa.String)\\n\", \"    aisle_id = sa.Column(sa.Integer)  # One can set these to reference the aisles/departments tables\\n\", \"    department_id = sa.Column(sa.Integer)\\n\", \"\\n\", \"\\n\", \"class Orders(Base):\\n\", \"    __tablename__ = \\\"orders\\\"\\n\", \"    order_id = sa.Column(sa.Integer, primary_key=True)\\n\", \"    user_id = sa.Column(sa.Integer)\\n\", \"    eval_set = sa.Column(sa.String)\\n\", \"    order_number = sa.Column(sa.Integer)\\n\", \"    order_dow = sa.Column(sa.Integer)\\n\", \"    order_hour_of_day = sa.Column(sa.Integer)\\n\", \"    days_since_prior_order = sa.Column(sa.Integer)\\n\", \"\\n\", \"\\n\", \"class ProductsOrdered(Base):\\n\", \"    __tablename__ = \\\"products_ordered\\\"\\n\", \"    order_id = sa.Column(sa.Integer, primary_key=True)\\n\", \"    product_id = sa.Column(sa.Integer, primary_key=True)\\n\", \"    add_to_cart_order = sa.Column(sa.Integer)\\n\", \"    reordered = sa.Column(sa.Boolean)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Dump data into database\\n\", \"\\n\", \"We're going to postpone a detailed discussion on what happens next for now...\\n\", \"\\n\", \"The tl;dr is that the code cell below is completely commented out. That cell takes the csv files that we previously saw and loads them in to a SQLite database\\n\", \"\\n\", \"It isn't a very effiicent operation, so we've done it for you and uploaded the results.\\n\", \"\\n\", \"The code cell two beneath this one will check the database already exists, if not it will download it for you.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# %%time\\n\", \"# # Uncomment if the data needs to be fixed or updated\\n\", \"# # Create a SQL alchemy engine and add table information to the engine\\n\", \"# os.remove(\\\"~/Data/instacart/instacart.db\\\")\\n\", \"# eng = sa.create_engine(\\\"sqlite:///instacart.db\\\")\\n\", \"# Base.metadata.create_all(eng)\\n\", \"\\n\", \"# Session = sa.orm.sessionmaker(bind=eng)\\n\", \"\\n\", \"# # Create table -> filename pairs\\n\", \"# table_to_file = [\\n\", \"#     (Aisles, \\\"~/Data/instacart/aisles.parquet\\\"),\\n\", \"#     (Departments, \\\"~/Data/instacart/departments.parquet\\\"),\\n\", \"#     (Products, \\\"~/Data/instacart/products.parquet\\\"),\\n\", \"#     (Orders, \\\"~/Data/instacart/orders.parquet\\\"),\\n\", \"#     (ProductsOrdered,  \\\"~/Data/instacart/order_products_all.parquet\\\"),\\n\", \"# ]\\n\", \"\\n\", \"# session = Session()\\n\", \"# # Delete any data from previous inserts\\n\", \"# for (_t, _csv) in table_to_file:\\n\", \"#     session.execute(_t.__table__.delete())\\n\", \"#     session.commit()\\n\", \"\\n\", \"# # Insert data\\n\", \"# for (_t, _f) in table_to_file:\\n\", \"#     # Read parquet file and put into the list of dictionaries\\n\", \"#     _rows = pd.read_parquet(_f).to_sql(\\n\", \"#         _t.__tablename__, eng, if_exists=\\\"append\\\", index=False\\n\", \"#     )\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"def download_db():\\n\", \"    if os.path.exists(\\\"instacart.db\\\"):\\n\", \"        print(\\\"Already have file\\\")\\n\", \"        return\\n\", \"    url = \\\"https://compsosci-resources.s3.amazonaws.com/instacart/instacart.db.zip\\\"\\n\", \"    res = requests.get(url)\\n\", \"    if not res.ok:\\n\", \"        raise Exception(\\\"Could not download database\\\")\\n\", \"    \\n\", \"    with zipfile.ZipFile(BytesIO(res.content)) as z:\\n\", \"        z.extract(\\\"instacart.db\\\")\\n\", \"    \\n\", \"download_db()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### A SQLAlchemy Engine\\n\", \"\\n\", \"In order to access the data in the database, we need a sqlalchemy engine\\n\", \"\\n\", \"This is a type provided by sqlalchemy that (1) knows how to interact with a database and (2) abstracts over the type of database so we can use the same Python code to interact with multiple database types.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Create a SQL alchemy engine and add table information to the engine\\n\", \"eng = sa.create_engine(\\\"sqlite:///instacart.db\\\")\\n\", \"\\n\", \"Session = sa.orm.sessionmaker(bind=eng)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Reading data from a SQL database\\n\", \"\\n\", \"Unless you end up becoming a data engineer, you will spend most of your time interacting with an already created database that others manage...\\n\", \"\\n\", \"Because of this, we will spend most of our time focused on reading data from a database\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### SQL Read Commands\\n\", \"\\n\", \"We will run the raw SQL commands into the SQLAlchemy engine, but you could interact with the engine using SQLAlchemy\\n\", \"\\n\", \"**Note**: It is good practice to capitalize the SQL keywords -- For example, rather than write `select` or `from`, you should write `SELECT` and `FROM`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def run_query(eng, query, str_length=30):\\n\", \"    with eng.connect() as conn:\\n\", \"        result = conn.execute(query)\\n\", \"        cols = result.keys()\\n\", \"        vals = result.fetchmany(5)\\n\", \"\\n\", \"        fmter = (\\\"{\\\" + f\\\":<{str_length}\\\" + \\\"}\\\") * len(cols)\\n\", \"        print(fmter.format(*cols))\\n\", \"        for _vals in vals:\\n\", \"            _pvals = map(lambda x: str(x)[:str_length], _vals)\\n\", \"            print(fmter.format(*_pvals))\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### SELECT/FROM\\n\", \"\\n\", \"The most fundamental read command in SQL combines the `SELECT` statement with the `FROM` statement.\\n\", \"\\n\", \"* `SELECT` specifies what data to read (and what to call it)\\n\", \"* `FROM` specifies where that data can be read from\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Select all columns from a single table**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT *\\n\", \"        FROM products\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Select certain columns**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT product_id, aisle_id, department_id\\n\", \"        FROM products\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Select and rename certain columns**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT product_id AS pid, aisle_id AS aid, department_id AS did\\n\", \"        FROM products\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Reference table using abbreviation**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_id AS pid, p.aisle_id, p.department_id\\n\", \"        FROM products p\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Select functions of columns**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT product_id AS pid, aisle_id, department_id, aisle_id + department_id AS a_d_id\\n\", \"        FROM products p\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### JOIN\\n\", \"\\n\", \"SQL is a relational database which means that\\n\", \"\\n\", \"1. We will typically store data in multiple tables\\n\", \"2. We'd like to be able to combine and manipulate data from multiple tables\\n\", \"\\n\", \"`JOIN` allows us bring together two (or more) datasets into a single query\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Select all columns from two tables**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT *\\n\", \"        FROM products p\\n\", \"        JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 18)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Select subset of columns from each table**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_name, p.aisle_id, p.department_id, a.aisle\\n\", \"        FROM products p\\n\", \"        JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 30)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Select data with different joins**\\n\", \"\\n\", \"The merges that we've done using pandas use the same notation as SQL joins:\\n\", \"\\n\", \"- `LEFT`: Use values from the left table to merge datasets\\n\", \"- `RIGHT`: Use values from the right table to merge datasets\\n\", \"- `INNER`: Only keep values contained in both the left and right datasets\\n\", \"- `OUTER`: Keep all values contained in either the left or right dataset.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_name, p.aisle_id, p.department_id, a.aisle\\n\", \"        FROM products p\\n\", \"        INNER JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"# In this case they're all the same because there is no\\n\", \"# missing data...\\n\", \"run_query(eng, query, 30)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Select data with multiple joins**\\n\", \"\\n\", \"We don't have to restrict ourselves to only combining two datasets -- We can combine as many as we'd like!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_name, a.aisle, d.department\\n\", \"        FROM products p\\n\", \"        LEFT JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        LEFT JOIN departments d ON (p.department_id=d.department_id)\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"# In this case they're all the same because there is no\\n\", \"# missing data...\\n\", \"run_query(eng, query, 30)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### WHERE\\n\", \"\\n\", \"We are often interested in working with subsets of the data rather than selecting all of the rows.\\n\", \"\\n\", \"SQL allows us to specify certain conditions to restrict the set of observations that are returned using the `WHERE` clause.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Retrieve certain groups** (compare  strings)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_name, a.aisle, d.department\\n\", \"        FROM products p\\n\", \"        LEFT JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        LEFT JOIN departments d ON (p.department_id=d.department_id)\\n\", \"        WHERE d.department = 'snacks'\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 30)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Retrieve certain groups** (compare numbers)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_name, a.aisle, d.department, a.aisle_id\\n\", \"        FROM products p\\n\", \"        LEFT JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        LEFT JOIN departments d ON (p.department_id=d.department_id)\\n\", \"        WHERE a.aisle_id > 132\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 30)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Multiple conditions**\\n\", \"\\n\", \"We use `AND` and `OR` to specify the boolean condition\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT p.product_name, a.aisle, d.department, a.aisle_id, d.department_id\\n\", \"        FROM products p\\n\", \"        LEFT JOIN aisles a ON (p.aisle_id=a.aisle_id)\\n\", \"        LEFT JOIN departments d ON (p.department_id=d.department_id)\\n\", \"        WHERE a.aisle_id > 100 OR d.department_id<10\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 30)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Retrieve the most recent data** (compare datetime)\\n\", \"\\n\", \"Imagine we had a table that contained quarterly sales\\n\", \"\\n\", \"| dt | store_id | sales |\\n\", \"| ---- | ---- | ---- |\\n\", \"| 2020-03-31 | 1 | 100 |\\n\", \"| 2020-06-30 | 1 | 200 |\\n\", \"| 2020-09-30 | 1 | 300 |\\n\", \"| 2020-12-31 | 1 | 400 |\\n\", \"| 2020-03-31 | 2 | 1000 |\\n\", \"| 2020-06-30 | 2 | 2000 |\\n\", \"| 2020-09-30 | 2 | 3000 |\\n\", \"| 2020-12-31 | 2 | 4000 |\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"If we wanted to select only the observations from quarter 1, we could write\\n\", \"\\n\", \"```sql\\n\", \"SELECT *\\n\", \"FROM sales\\n\", \"WHERE dt<'2020-04-01'\\n\", \"```\\n\", \"\\n\", \"| dt | store_id | sales |\\n\", \"| ---- | ---- | ---- |\\n\", \"| 2020-03-31 | 1 | 100 |\\n\", \"| 2020-03-31 | 2 | 1000 |\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"If we wanted to select observations from Q3 and Q4, we could write\\n\", \"\\n\", \"```sql\\n\", \"SELECT *\\n\", \"FROM sales\\n\", \"WHERE dt>'2020-06-31'\\n\", \"```\\n\", \"\\n\", \"| dt | store_id | sales |\\n\", \"| ---- | ---- | ---- |\\n\", \"| 2020-09-30 | 1 | 300 |\\n\", \"| 2020-12-31 | 1 | 400 |\\n\", \"| 2020-09-30 | 2 | 3000 |\\n\", \"| 2020-12-31 | 2 | 4000 |\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### GROUP BY\\n\", \"\\n\", \"The `GROUP BY` argument allows us to aggregate certain groups of values (much like the pandas `groupby` method).\\n\", \"\\n\", \"When you perform a `GROUP BY`, any column that is not an element of the \\\"group\\\" must have a reduction function applied to it\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Group by single column**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT order_dow, COUNT(user_id) AS norder\\n\", \"        FROM orders o\\n\", \"        GROUP BY order_dow\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 15)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Group by multiple columns**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT user_id, order_dow, COUNT(order_id) AS norder\\n\", \"        FROM orders o\\n\", \"        GROUP BY user_id, order_dow\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 15)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Aggregate multiple columns**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT user_id, order_dow,\\n\", \"               COUNT(order_id) AS norder,\\n\", \"               AVG(days_since_prior_order) AS avg_days_since_order\\n\", \"        FROM orders o\\n\", \"        GROUP BY user_id, order_dow\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 15)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### ORDER BY\\n\", \"\\n\", \"`ORDER BY` allows us to sort the output of a query\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Order by single column**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT order_id, user_id, order_number, days_since_prior_order\\n\", \"        FROM orders o\\n\", \"        ORDER BY user_id\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 15)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Order by multiple columns**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT order_id, user_id, order_number, days_since_prior_order\\n\", \"        FROM orders o\\n\", \"        ORDER BY user_id, order_number\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 15)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Order by ascending/descending**\\n\", \"\\n\", \"The keywords for specifying the order of ordering are `ASC` (for ascending) and `DESC` (for descending)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT order_id, user_id, order_number, days_since_prior_order\\n\", \"        FROM orders o\\n\", \"        WHERE days_since_prior_order < 30\\n\", \"        ORDER BY days_since_prior_order DESC, user_id ASC\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"run_query(eng, query, 15)\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### LIMIT\\n\", \"\\n\", \"`LIMIT` is a SQL clause that specifies the (maximum) number of rows that should be returned.\\n\", \"\\n\", \"It performs the same role as the pandas dataframe `head` method -- It allows you to select the $n$ largest/smallest values or simply get a preview of your data\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Retrieve first n rows**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"%%time\\n\", \"\\n\", \"query_l10 = \\\"\\\"\\\"\\n\", \"        SELECT *\\n\", \"        FROM orders o\\n\", \"        LIMIT 10\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"_ = eng.execute(query_l10).fetchall()\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"%%time\\n\", \"\\n\", \"query_all = \\\"\\\"\\\"\\n\", \"        SELECT *\\n\", \"        FROM orders o\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"_ = eng.execute(query_all).fetchall()\\n\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"### Reading with pandas\\n\", \"\\n\", \"We have directly used SQLAlchemy's engine to read in data up until this point, but we can also read from the engine using pandas!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"        SELECT order_id, user_id, order_number, days_since_prior_order\\n\", \"        FROM orders o\\n\", \"        ORDER BY days_since_prior_order DESC, user_id ASC\\n\", \"        \\\"\\\"\\\"\\n\", \"\\n\", \"pd.read_sql(query, eng)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Redoing our reorder example in SQL using a `WITH` clause\\n\", \"\\n\", \"`WITH` clauses allow us to define a \\\"temporary table\\\" that can be used in a subsequent query\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"query = \\\"\\\"\\\"\\n\", \"    WITH agg_po AS (\\n\", \"        SELECT po.product_id,\\n\", \"               COUNT(po.add_to_cart_order) AS norder,\\n\", \"               SUM(po.reordered) AS nreorder\\n\", \"        FROM products_ordered po\\n\", \"        LEFT JOIN orders o ON po.order_id=o.order_id\\n\", \"        WHERE o.days_since_prior_order IS NOT NULL\\n\", \"        GROUP BY po.product_id\\n\", \"    )\\n\", \"    SELECT apo.product_id, apo.norder, apo.nreorder,\\n\", \"           (apo.nreorder*1.0 / apo.norder) AS frac_reorder,\\n\", \"           p.product_name, p.aisle_id, p.department_id\\n\", \"    FROM agg_po as apo\\n\", \"    LEFT JOIN products p ON apo.product_id=p.product_id\\n\", \"    WHERE apo.nreorder > 10\\n\", \"    ORDER BY frac_reorder DESC\\n\", \"\\\"\\\"\\\"\\n\", \"\\n\", \"df = pd.read_sql(query, eng)\"], \"outputs\": [], \"metadata\": {\"jupyter\": {\"outputs_hidden\": false}, \"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"interpreter\": {\"hash\": \"06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"02_sql.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.788Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 10,
    "position": 8,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 11,
    "description": "SQL Introduction",
    "title": "Lecture 09",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 117,
    "position": 9,
    "content_id": 155,
    "lecture_id": 11,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 155,
    "type": "video",
    "description": "Understand the structure of a grocery shopping dataset from Instacart",
    "title": "Instacart Data Description",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=_XnHqgh8934\", \"youtubeVideoId\": \"_XnHqgh8934\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 12,
    "position": 9,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 13,
    "description": "Markov Chains",
    "title": "Lecture 10",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 125,
    "position": 11,
    "content_id": 166,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 166,
    "type": "video",
    "description": "Introduction to Markov Chains (4/4)",
    "title": "Markov Chains 4",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=m4Ij4Dd9myc\", \"youtubeVideoId\": \"m4Ij4Dd9myc\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 12,
    "position": 9,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 13,
    "description": "Markov Chains",
    "title": "Lecture 10",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 122,
    "position": 9,
    "content_id": 163,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 163,
    "type": "video",
    "description": "Introduction to Markov Chains (1/4)",
    "title": "Markov Chains 1",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=b_sBJzwmKFQ\", \"youtubeVideoId\": \"b_sBJzwmKFQ\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 12,
    "position": 9,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 13,
    "description": "Markov Chains",
    "title": "Lecture 10",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 123,
    "position": 9,
    "content_id": 164,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 164,
    "type": "video",
    "description": "Introduction to Markov Chains (2/4)",
    "title": "Markov Chains 2",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=RLRFg-IAFyo\", \"youtubeVideoId\": \"RLRFg-IAFyo\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 12,
    "position": 9,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 13,
    "description": "Markov Chains",
    "title": "Lecture 10",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 126,
    "position": 11,
    "content_id": 180,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 180,
    "type": "notebook",
    "description": "Model the game of Chutes and Ladders using a Markov Chain",
    "title": "chutes-ladders.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"import numpy as np\\n\", \"import quantecon as qe\\n\", \"\\n\", \"np.set_printoptions(precision=3, suppress=True)\\n\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"![http://jakevdp.github.io/images/ChutesAndLadders-board.gif](http://jakevdp.github.io/images/ChutesAndLadders-board.gif)\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Mapping of start : end spaces of chutes & ladders\\n\", \"chutes_ladders = {\\n\", \"    1:38, 4:14, 9:31, 16:6, 21:42, 28:84, 36:44,\\n\", \"    47:26, 49:11, 51:67, 56:53, 62:19, 64:60,\\n\", \"    71:91, 80:100, 87:24, 93:73, 95:75, 98:78\\n\", \"}\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"def make_P(N, chutes_ladders):\\n\", \"    # WANT: create a transition matrix P for a game of chutes and ladders\\n\", \"    # conditions:\\n\", \"    #   - all elements are >= 0\\n\", \"    #   - each row sums to 1\\n\", \"    #   - row `i` represents the probability of beginning your turn on square i and ending on any other square.\\n\", \"    P = np.zeros((N+1, N+1))\\n\", \"    \\n\", \"    # start with basic case, ignoring all chutes and ladders\\n\", \"    for i in range(1, N+1):\\n\", \"        P[i-1, i:(i+6)] = 1/6\\n\", \"    \\n\", \"    # once you are here, you win!\\n\", \"    P[N, N] = 1\\n\", \"\\n\", \"    # House rules: you don't need to land on 100, just reach it.\\n\", \"    P[95:100,100] += np.linspace(1/6, 5/6, 5)\\n\", \"\\n\", \"    for (i1, i2) in chutes_ladders.items():\\n\", \"        # iw keeps track of when we would have landed on i1\\n\", \"        iw = np.where(P[:, i1] > 0)\\n\", \"        P[:, i1] = 0  # can't land here...\\n\", \"        P[iw, i2] += 1/6  # move to i2 instead\\n\", \"        \\n\", \"    return P\\n\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"P = make_P(100, chutes_ladders)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"P[0, 1:7]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"P[0, [14, 38]]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"P.sum(axis=1)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"mc = qe.MarkovChain(P)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Questions\\n\", \"\\n\", \"- Stationary Distribution\\n\", \"- Distribution after N rounds\\n\", \"- Expected number of rounds\\n\", \"- Percent of games finished in N rounds\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"mc.stationary_distributions  # the game always ends!\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"simulations = mc.simulate(200, init=0, num_reps=5000)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"simulations[0, :]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"(simulations[0, :] < 100).sum()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"import matplotlib.pyplot as plt\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"stopping_times = (simulations < 100).sum(axis=1)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"fig, ax = plt.subplots(figsize=(10, 8))\\n\", \"\\n\", \"ax.hist(stopping_times, bins=20, density=True, cumulative=True)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"simulations\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"s0 = np.zeros(101)\\n\", \"s0[0] = 1\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"s0 @ np.linalg.matrix_power(P, 100)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# distribution after 1 round\\n\", \"s1 = s0@P\\n\", \"s1\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# distribution after 2 rounds\\n\", \"s2 = s1 @ P  # == s0 @ P @ P\\n\", \"s2\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# distribution after 100 rounds\\n\", \"s0 @ np.linalg.matrix_power(P, 100)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"pct_finished_at_round = (simulations == 100).mean(axis=0)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"pct_finished_at_round[-10]\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"import matplotlib.pyplot as plt\\n\", \"%matplotlib inline\\n\", \"\\n\", \"fig, ax = plt.subplots()\\n\", \"ax.plot(pct_finished_at_round)\\n\", \"ax.set_xlabel(\\\"round number\\\")\\n\", \"ax.set_ylabel(\\\"cumulative percent of games finished\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"interpreter\": {\"hash\": \"06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 2}",
    "md_content": null,
    "properties": "{\"fileName\": \"chutes-ladders.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.788Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 12,
    "position": 9,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 13,
    "description": "Markov Chains",
    "title": "Lecture 10",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 124,
    "position": 10,
    "content_id": 165,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 165,
    "type": "video",
    "description": "Introduction to Markov Chains (3/4)",
    "title": "Markov Chains 3",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=-MoBx_5rFM0\", \"youtubeVideoId\": \"-MoBx_5rFM0\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 12,
    "position": 9,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 13,
    "description": "Markov Chains",
    "title": "Lecture 10",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 121,
    "position": 9,
    "content_id": 181,
    "lecture_id": 13,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 181,
    "type": "notebook",
    "description": "QuantEcon lecture on Markov Chains",
    "title": "finite_markov.ipynb",
    "nb_content": "{\"cells\": [{\"id\": \"a909d150\", \"source\": [\"\\n\", \"<a id='mc'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"f58ac32c\", \"source\": [\"# Finite Markov Chains\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"b6fc3f6f\", \"source\": [\"## Contents\\n\", \"\\n\", \"- [Finite Markov Chains](#Finite-Markov-Chains)  \\n\", \"  - [Overview](#Overview)  \\n\", \"  - [Definitions](#Definitions)  \\n\", \"  - [Simulation](#Simulation)  \\n\", \"  - [Marginal Distributions](#Marginal-Distributions)  \\n\", \"  - [Irreducibility and Aperiodicity](#Irreducibility-and-Aperiodicity)  \\n\", \"  - [Stationary Distributions](#Stationary-Distributions)  \\n\", \"  - [Ergodicity](#Ergodicity)  \\n\", \"  - [Computing Expectations](#Computing-Expectations)  \\n\", \"  - [Exercises](#Exercises)  \"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"a26c6f2c\", \"source\": [\"In addition to what’s in Anaconda, this lecture will need the following libraries:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"60f011ac\", \"source\": [\"!pip install quantecon\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"f3bebf3f\", \"source\": [\"## Overview\\n\", \"\\n\", \"Markov chains are one of the most useful classes of stochastic processes, being\\n\", \"\\n\", \"- simple, flexible and supported by many elegant theoretical results  \\n\", \"- valuable for building intuition about random dynamic models  \\n\", \"- central to quantitative modeling in their own right  \\n\", \"\\n\", \"\\n\", \"You will find them in many of the workhorse models of economics and finance.\\n\", \"\\n\", \"In this lecture, we review some of the theory of Markov chains.\\n\", \"\\n\", \"We will also introduce some of the high-quality routines for working with Markov chains available in [QuantEcon.py](https://quantecon.org/quantecon-py/).\\n\", \"\\n\", \"Prerequisite knowledge is basic probability and linear algebra.\\n\", \"\\n\", \"Let’s start with some standard imports:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"ca20edd0\", \"source\": [\"%matplotlib inline\\n\", \"import matplotlib.pyplot as plt\\n\", \"plt.rcParams[\\\"figure.figsize\\\"] = (11, 5)  #set default figure size\\n\", \"import quantecon as qe\\n\", \"import numpy as np\\n\", \"from mpl_toolkits.mplot3d import Axes3D\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"f3f3eeda\", \"source\": [\"## Definitions\\n\", \"\\n\", \"The following concepts are fundamental.\\n\", \"\\n\", \"\\n\", \"<a id='finite-dp-stoch-mat'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"c024dbbb\", \"source\": [\"### Stochastic Matrices\\n\", \"\\n\", \"\\n\", \"<a id='index-2'></a>\\n\", \"A **stochastic matrix** (or **Markov matrix**)  is an $ n \\\\times n $ square matrix $ P $\\n\", \"such that\\n\", \"\\n\", \"1. each element of $ P $ is nonnegative, and  \\n\", \"1. each row of $ P $ sums to one  \\n\", \"\\n\", \"\\n\", \"Each row of $ P $ can be regarded as a probability mass function over $ n $ possible outcomes.\\n\", \"\\n\", \"It is too not difficult to check <sup><a href=#pm id=pm-link>[1]</a></sup> that if $ P $ is a stochastic matrix, then so is the $ k $-th power $ P^k $ for all $ k \\\\in \\\\mathbb N $.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"930a8861\", \"source\": [\"### Markov Chains\\n\", \"\\n\", \"\\n\", \"<a id='index-4'></a>\\n\", \"There is a close connection between stochastic matrices and Markov chains.\\n\", \"\\n\", \"To begin, let $ S $ be a finite set with $ n $ elements $ \\\\{x_1, \\\\ldots, x_n\\\\} $.\\n\", \"\\n\", \"The set $ S $ is called the **state space** and $ x_1, \\\\ldots, x_n $ are the **state values**.\\n\", \"\\n\", \"A **Markov chain** $ \\\\{X_t\\\\} $ on $ S $ is a sequence of random variables on $ S $ that have the **Markov property**.\\n\", \"\\n\", \"This means that, for any date $ t $ and any state $ y \\\\in S $,\\n\", \"\\n\", \"\\n\", \"<a id='equation-fin-markov-mp'></a>\\n\", \"$$\\n\", \"\\\\mathbb P \\\\{ X_{t+1} = y  \\\\,|\\\\, X_t \\\\}\\n\", \"= \\\\mathbb P \\\\{ X_{t+1}  = y \\\\,|\\\\, X_t, X_{t-1}, \\\\ldots \\\\} \\\\tag{26.1}\\n\", \"$$\\n\", \"\\n\", \"In other words, knowing the current state is enough to know probabilities for future states.\\n\", \"\\n\", \"In particular, the dynamics of a Markov chain are fully determined by the set of values\\n\", \"\\n\", \"\\n\", \"<a id='equation-mpp'></a>\\n\", \"$$\\n\", \"P(x, y) := \\\\mathbb P \\\\{ X_{t+1} = y \\\\,|\\\\, X_t = x \\\\}\\n\", \"\\\\qquad (x, y \\\\in S) \\\\tag{26.2}\\n\", \"$$\\n\", \"\\n\", \"By construction,\\n\", \"\\n\", \"- $ P(x, y) $ is the probability of going from $ x $ to $ y $ in one unit of time (one step)  \\n\", \"- $ P(x, \\\\cdot) $ is the conditional distribution of $ X_{t+1} $ given $ X_t = x $  \\n\", \"\\n\", \"\\n\", \"We can view $ P $ as a stochastic matrix where\\n\", \"\\n\", \"$$\\n\", \"P_{ij} = P(x_i, x_j)\\n\", \"\\\\qquad 1 \\\\leq i, j \\\\leq n\\n\", \"$$\\n\", \"\\n\", \"Going the other way, if we take a stochastic matrix $ P $, we can generate a Markov\\n\", \"chain $ \\\\{X_t\\\\} $ as follows:\\n\", \"\\n\", \"- draw $ X_0 $ from a marginal distribution $ \\\\psi $  \\n\", \"- for each $ t = 0, 1, \\\\ldots $, draw $ X_{t+1} $ from $ P(X_t,\\\\cdot) $  \\n\", \"\\n\", \"\\n\", \"By construction, the resulting process satisfies [(26.2)](#equation-mpp).\\n\", \"\\n\", \"\\n\", \"<a id='mc-eg1'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"bbe9ee5b\", \"source\": [\"### Example 1\\n\", \"\\n\", \"Consider a worker who, at any given time $ t $, is either unemployed (state 0) or employed (state 1).\\n\", \"\\n\", \"Suppose that, over a one month period,\\n\", \"\\n\", \"1. An unemployed worker finds a job with probability $ \\\\alpha \\\\in (0, 1) $.  \\n\", \"1. An employed worker loses her job and becomes unemployed with probability $ \\\\beta \\\\in (0, 1) $.  \\n\", \"\\n\", \"\\n\", \"In terms of a Markov model, we have\\n\", \"\\n\", \"- $ S = \\\\{ 0, 1\\\\} $  \\n\", \"- $ P(0, 1) = \\\\alpha $ and $ P(1, 0) = \\\\beta $  \\n\", \"\\n\", \"\\n\", \"We can write out the transition probabilities in matrix form as\\n\", \"\\n\", \"\\n\", \"<a id='equation-p-unempemp'></a>\\n\", \"$$\\n\", \"P\\n\", \"= \\\\left(\\n\", \"\\\\begin{array}{cc}\\n\", \"    1 - \\\\alpha & \\\\alpha \\\\\\\\\\n\", \"    \\\\beta & 1 - \\\\beta\\n\", \"\\\\end{array}\\n\", \"  \\\\right) \\\\tag{26.3}\\n\", \"$$\\n\", \"\\n\", \"Once we have the values $ \\\\alpha $ and $ \\\\beta $, we can address a range of questions, such as\\n\", \"\\n\", \"- What is the average duration of unemployment?  \\n\", \"- Over the long-run, what fraction of time does a worker find herself unemployed?  \\n\", \"- Conditional on employment, what is the probability of becoming unemployed at least once over the next 12 months?  \\n\", \"\\n\", \"\\n\", \"We’ll cover such applications below.\\n\", \"\\n\", \"\\n\", \"<a id='mc-eg2'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"7b170678\", \"source\": [\"### Example 2\\n\", \"\\n\", \"From  US unemployment data, Hamilton [[Ham05](https://python.quantecon.org/zreferences.html#id165)] estimated the stochastic matrix\\n\", \"\\n\", \"$$\\n\", \"P =\\n\", \"\\\\left(\\n\", \"  \\\\begin{array}{ccc}\\n\", \"     0.971 & 0.029 & 0 \\\\\\\\\\n\", \"     0.145 & 0.778 & 0.077 \\\\\\\\\\n\", \"     0 & 0.508 & 0.492\\n\", \"  \\\\end{array}\\n\", \"\\\\right)\\n\", \"$$\\n\", \"\\n\", \"where\\n\", \"\\n\", \"- the frequency is monthly  \\n\", \"- the first state represents “normal growth”  \\n\", \"- the second state represents “mild recession”  \\n\", \"- the third state represents “severe recession”  \\n\", \"\\n\", \"\\n\", \"For example, the matrix tells us that when the state is normal growth, the state will again be normal growth next month with probability 0.97.\\n\", \"\\n\", \"In general, large values on the main diagonal indicate persistence in the process $ \\\\{ X_t \\\\} $.\\n\", \"\\n\", \"This Markov process can also be represented as a directed graph, with edges labeled by transition probabilities\\n\", \"\\n\", \"![https://python.quantecon.org/_static/lecture_specific/finite_markov/hamilton_graph.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/hamilton_graph.png)\\n\", \"\\n\", \"  \\n\", \"Here “ng” is normal growth, “mr” is mild recession, etc.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"6a29b284\", \"source\": [\"## Simulation\\n\", \"\\n\", \"\\n\", \"<a id='index-5'></a>\\n\", \"One natural way to answer questions about Markov chains is to simulate them.\\n\", \"\\n\", \"(To approximate the probability of event $ E $, we can simulate many times and count the fraction of times that $ E $ occurs).\\n\", \"\\n\", \"Nice functionality for simulating Markov chains exists in [QuantEcon.py](http://quantecon.org/quantecon-py).\\n\", \"\\n\", \"- Efficient, bundled with lots of other useful routines for handling Markov chains.  \\n\", \"\\n\", \"\\n\", \"However, it’s also a good exercise to roll our own routines — let’s do that first and then come back to the methods in [QuantEcon.py](http://quantecon.org/quantecon-py).\\n\", \"\\n\", \"In these exercises, we’ll take the state space to be $ S = 0,\\\\ldots, n-1 $.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"24c8f6db\", \"source\": [\"### Rolling Our Own\\n\", \"\\n\", \"To simulate a Markov chain, we need its stochastic matrix $ P $ and a marginal probability distribution $ \\\\psi $  from which to  draw a realization of $ X_0 $.\\n\", \"\\n\", \"The Markov chain is then constructed as discussed above.  To repeat:\\n\", \"\\n\", \"1. At time $ t=0 $, draw a realization of  $ X_0 $  from $ \\\\psi $.  \\n\", \"1. At each subsequent time $ t $, draw a realization of the new state $ X_{t+1} $ from $ P(X_t, \\\\cdot) $.  \\n\", \"\\n\", \"\\n\", \"To implement this simulation procedure, we need a method for generating draws from a discrete distribution.\\n\", \"\\n\", \"For this task, we’ll use `random.draw` from [QuantEcon](http://quantecon.org/quantecon-py), which works as follows:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"9fc3b5e3\", \"source\": [\"ψ = (0.3, 0.7)           # probabilities over {0, 1}\\n\", \"cdf = np.cumsum(ψ)       # convert into cummulative distribution\\n\", \"qe.random.draw(cdf, 5)   # generate 5 independent draws from ψ\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"60bb3414\", \"source\": [\"We’ll write our code as a function that accepts the following three arguments\\n\", \"\\n\", \"- A stochastic matrix `P`  \\n\", \"- An initial state `init`  \\n\", \"- A positive integer `sample_size` representing the length of the time series the function should return  \"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"9cfd7f12\", \"source\": [\"def mc_sample_path(P, ψ_0=None, sample_size=1_000):\\n\", \"\\n\", \"    # set up\\n\", \"    P = np.asarray(P)\\n\", \"    X = np.empty(sample_size, dtype=int)\\n\", \"\\n\", \"    # Convert each row of P into a cdf\\n\", \"    n = len(P)\\n\", \"    P_dist = [np.cumsum(P[i, :]) for i in range(n)]\\n\", \"\\n\", \"    # draw initial state, defaulting to 0\\n\", \"    if ψ_0 is not None:\\n\", \"        X_0 = qe.random.draw(np.cumsum(ψ_0))\\n\", \"    else:\\n\", \"        X_0 = 0\\n\", \"\\n\", \"    # simulate\\n\", \"    X[0] = X_0\\n\", \"    for t in range(sample_size - 1):\\n\", \"        X[t+1] = qe.random.draw(P_dist[X[t]])\\n\", \"\\n\", \"    return X\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"75ef4022\", \"source\": [\"Let’s see how it works using the small matrix\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"0d6fe074\", \"source\": [\"P = [[0.4, 0.6],\\n\", \"     [0.2, 0.8]]\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"54015e2c\", \"source\": [\"As we’ll see later, for a long series drawn from `P`, the fraction of the sample that takes value 0 will be about 0.25.\\n\", \"\\n\", \"Moreover, this is true, regardless of the initial distribution from which\\n\", \"$ X_0 $ is drawn.\\n\", \"\\n\", \"The following code illustrates this\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"31969b68\", \"source\": [\"X = mc_sample_path(P, ψ_0=[0.1, 0.9], sample_size=100_000)\\n\", \"np.mean(X == 0)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"0d83b82e\", \"source\": [\"You can try changing the initial distribution to confirm that the output is\\n\", \"always close to 0.25, at least for the `P` matrix above.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"139f4e8c\", \"source\": [\"### Using QuantEcon’s Routines\\n\", \"\\n\", \"As discussed above, [QuantEcon.py](http://quantecon.org/quantecon-py) has routines for handling Markov chains, including simulation.\\n\", \"\\n\", \"Here’s an illustration using the same P as the preceding example\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"f0935cf9\", \"source\": [\"from quantecon import MarkovChain\\n\", \"\\n\", \"mc = qe.MarkovChain(P)\\n\", \"X = mc.simulate(ts_length=1_000_000)\\n\", \"np.mean(X == 0)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"7a8379b7\", \"source\": [\"The [QuantEcon.py](http://quantecon.org/quantecon-py) routine is [JIT compiled](https://python-programming.quantecon.org/numba.html#numba-link) and much faster.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"34aa1f93\", \"source\": [\"%time mc_sample_path(P, sample_size=1_000_000) # Our homemade code version\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"f17c97ee\", \"source\": [\"%time mc.simulate(ts_length=1_000_000) # qe code version\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"02e0c40b\", \"source\": [\"#### Adding State Values and Initial Conditions\\n\", \"\\n\", \"If we wish to, we can provide a specification of state values to `MarkovChain`.\\n\", \"\\n\", \"These state values can be integers, floats, or even strings.\\n\", \"\\n\", \"The following code illustrates\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"035b52c2\", \"source\": [\"mc = qe.MarkovChain(P, state_values=('unemployed', 'employed'))\\n\", \"mc.simulate(ts_length=4, init='employed')\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"04d83d8a\", \"source\": [\"mc.simulate(ts_length=4, init='unemployed')\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"1af902c8\", \"source\": [\"mc.simulate(ts_length=4)  # Start at randomly chosen initial state\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"3ea4a06e\", \"source\": [\"If we want to see indices rather than state values as outputs as  we can use\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"a90838da\", \"source\": [\"mc.simulate_indices(ts_length=4)\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"17b9e9af\", \"source\": [\"\\n\", \"<a id='mc-md'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"a53f1585\", \"source\": [\"## Marginal Distributions\\n\", \"\\n\", \"\\n\", \"<a id='index-7'></a>\\n\", \"Suppose that\\n\", \"\\n\", \"1. $ \\\\{X_t\\\\} $ is a Markov chain with stochastic matrix $ P $  \\n\", \"1. the marginal distribution of $ X_t $ is known to be $ \\\\psi_t $  \\n\", \"\\n\", \"\\n\", \"What then is the marginal distribution of $ X_{t+1} $, or, more generally, of $ X_{t+m} $?\\n\", \"\\n\", \"To answer this, we let $ \\\\psi_t $ be the marginal distribution of $ X_t $ for $ t = 0, 1, 2, \\\\ldots $.\\n\", \"\\n\", \"Our first aim is to find $ \\\\psi_{t + 1} $ given $ \\\\psi_t $ and $ P $.\\n\", \"\\n\", \"To begin, pick any $ y  \\\\in S $.\\n\", \"\\n\", \"Using the [law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability), we can decompose the probability that $ X_{t+1} = y $ as follows:\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb P \\\\{X_{t+1} = y \\\\}\\n\", \"   = \\\\sum_{x \\\\in S} \\\\mathbb P \\\\{ X_{t+1} = y \\\\, | \\\\, X_t = x \\\\}\\n\", \"               \\\\cdot \\\\mathbb P \\\\{ X_t = x \\\\}\\n\", \"$$\\n\", \"\\n\", \"In words, to get the probability of being at $ y $ tomorrow, we account for\\n\", \"all  ways this can happen and sum their probabilities.\\n\", \"\\n\", \"Rewriting this statement in terms of  marginal and conditional probabilities gives\\n\", \"\\n\", \"$$\\n\", \"\\\\psi_{t+1}(y) = \\\\sum_{x \\\\in S} P(x,y) \\\\psi_t(x)\\n\", \"$$\\n\", \"\\n\", \"There are $ n $ such equations, one for each $ y \\\\in S $.\\n\", \"\\n\", \"If we think of $ \\\\psi_{t+1} $ and $ \\\\psi_t $ as *row vectors*, these $ n $ equations are summarized by the matrix expression\\n\", \"\\n\", \"\\n\", \"<a id='equation-fin-mc-fr'></a>\\n\", \"$$\\n\", \"\\\\psi_{t+1} = \\\\psi_t P \\\\tag{26.4}\\n\", \"$$\\n\", \"\\n\", \"Thus, to move a marginal distribution forward one unit of time, we postmultiply by $ P $.\\n\", \"\\n\", \"By postmultiplying $ m $ times, we move a marginal distribution forward $ m $ steps into the future.\\n\", \"\\n\", \"Hence, iterating on [(26.4)](#equation-fin-mc-fr), the expression $ \\\\psi_{t+m} = \\\\psi_t P^m $ is also valid — here $ P^m $ is the $ m $-th power of $ P $.\\n\", \"\\n\", \"As a special case, we see that if $ \\\\psi_0 $ is the initial distribution from\\n\", \"which $ X_0 $ is drawn, then $ \\\\psi_0 P^m $ is the distribution of\\n\", \"$ X_m $.\\n\", \"\\n\", \"This is very important, so let’s repeat it\\n\", \"\\n\", \"\\n\", \"<a id='equation-mdfmc'></a>\\n\", \"$$\\n\", \"X_0 \\\\sim \\\\psi_0 \\\\quad \\\\implies \\\\quad X_m \\\\sim \\\\psi_0 P^m \\\\tag{26.5}\\n\", \"$$\\n\", \"\\n\", \"and, more generally,\\n\", \"\\n\", \"\\n\", \"<a id='equation-mdfmc2'></a>\\n\", \"$$\\n\", \"X_t \\\\sim \\\\psi_t \\\\quad \\\\implies \\\\quad X_{t+m} \\\\sim \\\\psi_t P^m \\\\tag{26.6}\\n\", \"$$\\n\", \"\\n\", \"\\n\", \"<a id='finite-mc-mstp'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"e6ed4e5c\", \"source\": [\"### Multiple Step Transition Probabilities\\n\", \"\\n\", \"We know that the probability of transitioning from $ x $ to $ y $ in\\n\", \"one step is $ P(x,y) $.\\n\", \"\\n\", \"It turns out that the probability of transitioning from $ x $ to $ y $ in\\n\", \"$ m $ steps is $ P^m(x,y) $, the $ (x,y) $-th element of the\\n\", \"$ m $-th power of $ P $.\\n\", \"\\n\", \"To see why, consider again [(26.6)](#equation-mdfmc2), but now with a $ \\\\psi_t $ that puts all probability on state $ x $ so that the transition probabilities are\\n\", \"\\n\", \"- 1 in the $ x $-th position and zero elsewhere  \\n\", \"\\n\", \"\\n\", \"Inserting this into [(26.6)](#equation-mdfmc2), we see that, conditional on $ X_t = x $, the distribution of $ X_{t+m} $ is the $ x $-th row of $ P^m $.\\n\", \"\\n\", \"In particular\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb P \\\\{X_{t+m} = y \\\\,|\\\\, X_t = x \\\\} = P^m(x, y) = (x, y) \\\\text{-th element of } P^m\\n\", \"$$\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"bd7f1cb8\", \"source\": [\"### Example: Probability of Recession\\n\", \"\\n\", \"\\n\", \"<a id='index-8'></a>\\n\", \"Recall the stochastic matrix $ P $ for recession and growth [considered above](#mc-eg2).\\n\", \"\\n\", \"Suppose that the current state is unknown — perhaps statistics are available only  at the *end* of the current month.\\n\", \"\\n\", \"We guess that the probability that the economy is in state $ x $ is $ \\\\psi(x) $.\\n\", \"\\n\", \"The probability of being in recession (either mild or severe) in 6 months time is given by the inner product\\n\", \"\\n\", \"$$\\n\", \"\\\\psi P^6\\n\", \"\\\\cdot\\n\", \"\\\\left(\\n\", \"  \\\\begin{array}{c}\\n\", \"     0 \\\\\\\\\\n\", \"     1 \\\\\\\\\\n\", \"     1\\n\", \"  \\\\end{array}\\n\", \"\\\\right)\\n\", \"$$\\n\", \"\\n\", \"\\n\", \"<a id='mc-eg1-1'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"5a026d69\", \"source\": [\"### Example 2: Cross-Sectional Distributions\\n\", \"\\n\", \"\\n\", \"<a id='index-9'></a>\\n\", \"The marginal distributions we have been studying can be viewed either as\\n\", \"probabilities or as cross-sectional frequencies that a Law of Large Numbers leads us to anticipate for  large samples.\\n\", \"\\n\", \"To illustrate, recall our model of employment/unemployment dynamics for a given worker [discussed above](#mc-eg1).\\n\", \"\\n\", \"Consider a large population of workers, each of whose lifetime experience is described by the specified dynamics, with each worker’s\\n\", \"outcomes being realizations of processes that are statistically independent of all other workers’ processes.\\n\", \"\\n\", \"Let $ \\\\psi $ be the current *cross-sectional* distribution over $ \\\\{ 0, 1 \\\\} $.\\n\", \"\\n\", \"The cross-sectional distribution records fractions of workers employed and unemployed at a given moment.\\n\", \"\\n\", \"- For example, $ \\\\psi(0) $ is the unemployment rate.  \\n\", \"\\n\", \"\\n\", \"What will the cross-sectional distribution be in 10 periods hence?\\n\", \"\\n\", \"The answer is $ \\\\psi P^{10} $, where $ P $ is the stochastic matrix in\\n\", \"[(26.3)](#equation-p-unempemp).\\n\", \"\\n\", \"This is because each worker’s state evolves according to $ P $, so\\n\", \"$ \\\\psi P^{10} $ is a marginal distibution  for a single randomly selected\\n\", \"worker.\\n\", \"\\n\", \"But when the sample is large, outcomes and probabilities are roughly equal (by an application of the Law\\n\", \"of Large Numbers).\\n\", \"\\n\", \"So for a very large (tending to infinite) population,\\n\", \"$ \\\\psi P^{10} $ also represents  fractions of workers in\\n\", \"each state.\\n\", \"\\n\", \"This is exactly the cross-sectional distribution.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"87699a14\", \"source\": [\"## Irreducibility and Aperiodicity\\n\", \"\\n\", \"\\n\", \"<a id='index-11'></a>\\n\", \"Irreducibility and aperiodicity are central concepts of modern Markov chain theory.\\n\", \"\\n\", \"Let’s see what they’re about.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"164ded5d\", \"source\": [\"### Irreducibility\\n\", \"\\n\", \"Let $ P $ be a fixed stochastic matrix.\\n\", \"\\n\", \"Two states $ x $ and $ y $ are said to **communicate** with each other if\\n\", \"there exist positive integers $ j $ and $ k $ such that\\n\", \"\\n\", \"$$\\n\", \"P^j(x, y) > 0\\n\", \"\\\\quad \\\\text{and} \\\\quad\\n\", \"P^k(y, x) > 0\\n\", \"$$\\n\", \"\\n\", \"In view of our discussion [above](#finite-mc-mstp), this means precisely\\n\", \"that\\n\", \"\\n\", \"- state $ x $ can eventually be reached  from state $ y $, and  \\n\", \"- state $ y $ can eventually  be reached from state $ x $  \\n\", \"\\n\", \"\\n\", \"The stochastic matrix $ P $ is called **irreducible** if all states\\n\", \"communicate; that is, if $ x $ and $ y $ communicate for all\\n\", \"$ (x, y) $ in $ S \\\\times S $.\\n\", \"\\n\", \"For example, consider the following transition probabilities for wealth of a fictitious set of\\n\", \"households\\n\", \"\\n\", \"![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility1.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility1.png)\\n\", \"\\n\", \"  \\n\", \"We can translate this into a stochastic matrix, putting zeros where\\n\", \"there’s no edge between nodes\\n\", \"\\n\", \"$$\\n\", \"P :=\\n\", \"\\\\left(\\n\", \"  \\\\begin{array}{ccc}\\n\", \"     0.9 & 0.1 & 0 \\\\\\\\\\n\", \"     0.4 & 0.4 & 0.2 \\\\\\\\\\n\", \"     0.1 & 0.1 & 0.8\\n\", \"  \\\\end{array}\\n\", \"\\\\right)\\n\", \"$$\\n\", \"\\n\", \"It’s clear from the graph that this stochastic matrix is irreducible: we can  eventually\\n\", \"reach any state from any other state.\\n\", \"\\n\", \"We can also test this using [QuantEcon.py](http://quantecon.org/quantecon-py)’s MarkovChain class\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"cb3edddb\", \"source\": [\"P = [[0.9, 0.1, 0.0],\\n\", \"     [0.4, 0.4, 0.2],\\n\", \"     [0.1, 0.1, 0.8]]\\n\", \"\\n\", \"mc = qe.MarkovChain(P, ('poor', 'middle', 'rich'))\\n\", \"mc.is_irreducible\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"8aa687e5\", \"source\": [\"Here’s a more pessimistic scenario in which  poor people remain poor forever\\n\", \"\\n\", \"![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility2.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility2.png)\\n\", \"\\n\", \"  \\n\", \"This stochastic matrix is not irreducible, since, for example, rich is not accessible from poor.\\n\", \"\\n\", \"Let’s confirm this\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"d73d17b7\", \"source\": [\"P = [[1.0, 0.0, 0.0],\\n\", \"     [0.1, 0.8, 0.1],\\n\", \"     [0.0, 0.2, 0.8]]\\n\", \"\\n\", \"mc = qe.MarkovChain(P, ('poor', 'middle', 'rich'))\\n\", \"mc.is_irreducible\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"185f843a\", \"source\": [\"We can also determine the “communication classes”\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"acc613ce\", \"source\": [\"mc.communication_classes\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"71c1675c\", \"source\": [\"It might be clear to you already that irreducibility is going to be important in terms of long run outcomes.\\n\", \"\\n\", \"For example, poverty is a life sentence in the second graph but not the first.\\n\", \"\\n\", \"We’ll come back to this a bit later.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"1562991e\", \"source\": [\"### Aperiodicity\\n\", \"\\n\", \"Loosely speaking, a Markov chain is called **periodic** if it cycles in a predictable way, and **aperiodic** otherwise.\\n\", \"\\n\", \"Here’s a trivial example with three states\\n\", \"\\n\", \"![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity1.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity1.png)\\n\", \"\\n\", \"  \\n\", \"The chain cycles with period 3:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"bfa3bc8e\", \"source\": [\"P = [[0, 1, 0],\\n\", \"     [0, 0, 1],\\n\", \"     [1, 0, 0]]\\n\", \"\\n\", \"mc = qe.MarkovChain(P)\\n\", \"mc.period\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"ecd1607c\", \"source\": [\"More formally, the **period** of a state $ x $ is the largest common divisor\\n\", \"of a set of integers\\n\", \"\\n\", \"$$\\n\", \"D(x) := \\\\{j \\\\geq 1 : P^j(x, x) > 0\\\\}\\n\", \"$$\\n\", \"\\n\", \"In the last example, $ D(x) = \\\\{3, 6, 9, \\\\ldots\\\\} $ for every state $ x $, so the period is 3.\\n\", \"\\n\", \"A stochastic matrix is called **aperiodic** if the period of every state is 1, and **periodic** otherwise.\\n\", \"\\n\", \"For example, the stochastic matrix associated with the transition probabilities below is periodic because, for example, state $ a $ has period 2\\n\", \"\\n\", \"![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity2.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity2.png)\\n\", \"\\n\", \"  \\n\", \"We can confirm that the stochastic matrix is periodic with the following code\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"22ad9645\", \"source\": [\"P = [[0.0, 1.0, 0.0, 0.0],\\n\", \"     [0.5, 0.0, 0.5, 0.0],\\n\", \"     [0.0, 0.5, 0.0, 0.5],\\n\", \"     [0.0, 0.0, 1.0, 0.0]]\\n\", \"\\n\", \"mc = qe.MarkovChain(P)\\n\", \"mc.period\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"e81e4de2\", \"source\": [\"mc.is_aperiodic\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"e9ac122f\", \"source\": [\"## Stationary Distributions\\n\", \"\\n\", \"\\n\", \"<a id='index-13'></a>\\n\", \"As seen in [(26.4)](#equation-fin-mc-fr), we can shift a marginal distribution forward one unit of time via postmultiplication by $ P $.\\n\", \"\\n\", \"Some distributions are invariant under this updating process — for example,\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"4c5833f2\", \"source\": [\"P = np.array([[0.4, 0.6],\\n\", \"              [0.2, 0.8]])\\n\", \"ψ = (0.25, 0.75)\\n\", \"ψ @ P\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"e4c91445\", \"source\": [\"Such distributions are called **stationary** or **invariant**.\\n\", \"\\n\", \"\\n\", \"<a id='mc-stat-dd'></a>\\n\", \"Formally, a marginal distribution $ \\\\psi^* $ on $ S $ is called **stationary** for $ P $ if $ \\\\psi^* = \\\\psi^* P $.\\n\", \"\\n\", \"(This is the same notion of stationarity that we learned about in the\\n\", \"[lecture on AR(1) processes](https://python.quantecon.org/ar1_processes.html) applied to a different setting.)\\n\", \"\\n\", \"From this equality, we immediately get $ \\\\psi^* = \\\\psi^* P^t $ for all $ t $.\\n\", \"\\n\", \"This tells us an important fact: If the distribution of $ X_0 $ is a stationary distribution, then $ X_t $ will have this same distribution for all $ t $.\\n\", \"\\n\", \"Hence stationary distributions have a natural interpretation as **stochastic steady states** — we’ll discuss this more soon.\\n\", \"\\n\", \"Mathematically, a stationary distribution is a fixed point of $ P $ when $ P $ is thought of as the map $ \\\\psi \\\\mapsto \\\\psi P $ from (row) vectors to (row) vectors.\\n\", \"\\n\", \"**Theorem.** Every stochastic matrix $ P $ has at least one stationary distribution.\\n\", \"\\n\", \"(We are assuming here that the state space $ S $ is finite; if not more assumptions are required)\\n\", \"\\n\", \"For proof of this result, you can apply [Brouwer’s fixed point theorem](https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem), or see [EDTC](https://johnstachurski.net/edtc.html), theorem 4.3.5.\\n\", \"\\n\", \"There can be many stationary distributions corresponding to a given stochastic matrix $ P $.\\n\", \"\\n\", \"- For example, if $ P $ is the identity matrix, then all marginal distributions are stationary.  \\n\", \"\\n\", \"\\n\", \"To get uniqueness an invariant distribution, the transition matrix $ P $ must have the property that no nontrivial subsets of\\n\", \"the state space are **infinitely persistent**.\\n\", \"\\n\", \"A subset of the state space is infinitely persistent if other parts of the\\n\", \"state space cannot be accessed from it.\\n\", \"\\n\", \"Thus, infinite persistence of a non-trivial subset is the opposite of irreducibility.\\n\", \"\\n\", \"This gives some intuition for the following fundamental theorem.\\n\", \"\\n\", \"\\n\", \"<a id='mc-conv-thm'></a>\\n\", \"**Theorem.** If $ P $ is both aperiodic and irreducible, then\\n\", \"\\n\", \"1. $ P $ has exactly one stationary distribution $ \\\\psi^* $.  \\n\", \"1. For any initial marginal distribution $ \\\\psi_0 $, we have $ \\\\| \\\\psi_0 P^t - \\\\psi^* \\\\| \\\\to 0 $ as $ t \\\\to \\\\infty $.  \\n\", \"\\n\", \"\\n\", \"For a proof, see, for example, theorem 5.2 of [[Haggstrom02](https://python.quantecon.org/zreferences.html#id133)].\\n\", \"\\n\", \"(Note that part 1 of the theorem only requires  irreducibility, whereas part 2\\n\", \"requires both irreducibility and aperiodicity)\\n\", \"\\n\", \"A stochastic matrix that satisfies the conditions of the theorem is sometimes called **uniformly ergodic**.\\n\", \"\\n\", \"A sufficient condition for aperiodicity and irreducibility is that every element of $ P $ is strictly positive.\\n\", \"\\n\", \"- Try to convince yourself of this.  \"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"55b35a0b\", \"source\": [\"### Example\\n\", \"\\n\", \"Recall our model of the employment/unemployment dynamics of a particular worker [discussed above](#mc-eg1).\\n\", \"\\n\", \"Assuming $ \\\\alpha \\\\in (0,1) $ and $ \\\\beta \\\\in (0,1) $, the uniform ergodicity condition is satisfied.\\n\", \"\\n\", \"Let $ \\\\psi^* = (p, 1-p) $ be the stationary distribution, so that $ p $ corresponds to unemployment (state 0).\\n\", \"\\n\", \"Using $ \\\\psi^* = \\\\psi^* P $ and a bit of algebra yields\\n\", \"\\n\", \"$$\\n\", \"p = \\\\frac{\\\\beta}{\\\\alpha + \\\\beta}\\n\", \"$$\\n\", \"\\n\", \"This is, in some sense, a steady state probability of unemployment — more about the  interpretation of this below.\\n\", \"\\n\", \"Not surprisingly it tends to zero as $ \\\\beta \\\\to 0 $, and to one as $ \\\\alpha \\\\to 0 $.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"33815ad4\", \"source\": [\"### Calculating Stationary Distributions\\n\", \"\\n\", \"\\n\", \"<a id='index-14'></a>\\n\", \"As discussed above, a particular Markov matrix $ P $ can have many stationary distributions.\\n\", \"\\n\", \"That is, there can be many row vectors $ \\\\psi $ such that $ \\\\psi = \\\\psi P $.\\n\", \"\\n\", \"In fact if $ P $ has two distinct stationary distributions $ \\\\psi_1,\\n\", \"\\\\psi_2 $ then it has infinitely many, since in this case, as you can verify,  for any $ \\\\lambda \\\\in [0, 1] $\\n\", \"\\n\", \"$$\\n\", \"\\\\psi_3 := \\\\lambda \\\\psi_1 + (1 - \\\\lambda) \\\\psi_2\\n\", \"$$\\n\", \"\\n\", \"is a stationary distribution for $ P $.\\n\", \"\\n\", \"If we restrict attention to the case in which only one stationary distribution exists, one way to  finding it is to solve the system\\n\", \"\\n\", \"\\n\", \"<a id='equation-eq-eqpsifixed'></a>\\n\", \"$$\\n\", \"\\\\psi (I_n - P) = 0 \\\\tag{26.7}\\n\", \"$$\\n\", \"\\n\", \"for $ \\\\psi $, where $ I_n $ is the $ n \\\\times n $ identity.\\n\", \"\\n\", \"But the zero vector solves system [(26.7)](#equation-eq-eqpsifixed),  so we must proceed cautiously.\\n\", \"\\n\", \"We want to impose the restriction that $ \\\\psi $ is  a probability distribution.\\n\", \"\\n\", \"There are various ways to do this.\\n\", \"\\n\", \"One option is to regard solving system [(26.7)](#equation-eq-eqpsifixed)  as an eigenvector problem: a vector\\n\", \"$ \\\\psi $ such that $ \\\\psi = \\\\psi P $ is a left eigenvector associated\\n\", \"with the unit eigenvalue $ \\\\lambda = 1 $.\\n\", \"\\n\", \"A stable and sophisticated algorithm specialized for stochastic matrices is implemented in [QuantEcon.py](http://quantecon.org/quantecon-py).\\n\", \"\\n\", \"This is the one we recommend:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"1229f313\", \"source\": [\"P = [[0.4, 0.6],\\n\", \"     [0.2, 0.8]]\\n\", \"\\n\", \"mc = qe.MarkovChain(P)\\n\", \"mc.stationary_distributions  # Show all stationary distributions\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"40dc20e2\", \"source\": [\"### Convergence to Stationarity\\n\", \"\\n\", \"\\n\", \"<a id='index-15'></a>\\n\", \"Part 2 of the Markov chain convergence theorem [stated above](#mc-conv-thm) tells us that the marginal distribution of $ X_t $ converges to the stationary distribution regardless of where we begin.\\n\", \"\\n\", \"This adds considerable authority to our interpretation of $ \\\\psi^* $ as a stochastic steady state.\\n\", \"\\n\", \"The convergence in the theorem is illustrated in the next figure\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"3046b048\", \"source\": [\"P = ((0.971, 0.029, 0.000),\\n\", \"     (0.145, 0.778, 0.077),\\n\", \"     (0.000, 0.508, 0.492))\\n\", \"P = np.array(P)\\n\", \"\\n\", \"ψ = (0.0, 0.2, 0.8)        # Initial condition\\n\", \"\\n\", \"fig = plt.figure(figsize=(8, 6))\\n\", \"ax = fig.add_subplot(111, projection='3d')\\n\", \"\\n\", \"ax.set(xlim=(0, 1), ylim=(0, 1), zlim=(0, 1),\\n\", \"       xticks=(0.25, 0.5, 0.75),\\n\", \"       yticks=(0.25, 0.5, 0.75),\\n\", \"       zticks=(0.25, 0.5, 0.75))\\n\", \"\\n\", \"x_vals, y_vals, z_vals = [], [], []\\n\", \"for t in range(20):\\n\", \"    x_vals.append(ψ[0])\\n\", \"    y_vals.append(ψ[1])\\n\", \"    z_vals.append(ψ[2])\\n\", \"    ψ = ψ @ P\\n\", \"\\n\", \"ax.scatter(x_vals, y_vals, z_vals, c='r', s=60)\\n\", \"ax.view_init(30, 210)\\n\", \"\\n\", \"mc = qe.MarkovChain(P)\\n\", \"ψ_star = mc.stationary_distributions[0]\\n\", \"ax.scatter(ψ_star[0], ψ_star[1], ψ_star[2], c='k', s=60)\\n\", \"\\n\", \"plt.show()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"7b443b2c\", \"source\": [\"Here\\n\", \"\\n\", \"- $ P $ is the stochastic matrix for recession and growth [considered above](#mc-eg2).  \\n\", \"- The highest red dot is an arbitrarily chosen initial marginal probability distribution  $ \\\\psi $, represented as a vector in $ \\\\mathbb R^3 $.  \\n\", \"- The other red dots are the marginal distributions $ \\\\psi P^t $ for $ t = 1, 2, \\\\ldots $.  \\n\", \"- The black dot is $ \\\\psi^* $.  \\n\", \"\\n\", \"\\n\", \"You might like to try experimenting with different initial conditions.\\n\", \"\\n\", \"\\n\", \"<a id='ergodicity'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"647039ce\", \"source\": [\"## Ergodicity\\n\", \"\\n\", \"\\n\", \"<a id='index-17'></a>\\n\", \"Under irreducibility, yet another important result obtains: for all $ x \\\\in S $,\\n\", \"\\n\", \"\\n\", \"<a id='equation-llnfmc0'></a>\\n\", \"$$\\n\", \"\\\\frac{1}{m} \\\\sum_{t = 1}^m \\\\mathbf{1}\\\\{X_t = x\\\\}  \\\\to \\\\psi^*(x)\\n\", \"    \\\\quad \\\\text{as } m \\\\to \\\\infty \\\\tag{26.8}\\n\", \"$$\\n\", \"\\n\", \"Here\\n\", \"\\n\", \"- $ \\\\mathbf{1}\\\\{X_t = x\\\\} = 1 $ if $ X_t = x $ and zero otherwise  \\n\", \"- convergence is with probability one  \\n\", \"- the result does not depend on the marginal distribution  of $ X_0 $  \\n\", \"\\n\", \"\\n\", \"The result tells us that the fraction of time the chain spends at state $ x $ converges to $ \\\\psi^*(x) $ as time goes to infinity.\\n\", \"\\n\", \"\\n\", \"<a id='new-interp-sd'></a>\\n\", \"This gives us another way to interpret the stationary distribution — provided that the convergence result in [(26.8)](#equation-llnfmc0) is valid.\\n\", \"\\n\", \"The convergence asserted in [(26.8)](#equation-llnfmc0) is a special case of a law of large numbers result for Markov chains — see [EDTC](http://johnstachurski.net/edtc.html), section 4.3.4 for some additional information.\\n\", \"\\n\", \"\\n\", \"<a id='mc-eg1-2'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"6ed8d1a7\", \"source\": [\"### Example\\n\", \"\\n\", \"Recall our cross-sectional interpretation of the employment/unemployment model [discussed above](#mc-eg1-1).\\n\", \"\\n\", \"Assume that $ \\\\alpha \\\\in (0,1) $ and $ \\\\beta \\\\in (0,1) $, so that irreducibility and aperiodicity both hold.\\n\", \"\\n\", \"We saw that the stationary distribution is $ (p, 1-p) $, where\\n\", \"\\n\", \"$$\\n\", \"p = \\\\frac{\\\\beta}{\\\\alpha + \\\\beta}\\n\", \"$$\\n\", \"\\n\", \"In the cross-sectional interpretation, this is the fraction of people unemployed.\\n\", \"\\n\", \"In view of our latest (ergodicity) result, it is also the fraction of time that a single worker can expect to spend unemployed.\\n\", \"\\n\", \"Thus, in the long-run, cross-sectional averages for a population and time-series averages for a given person coincide.\\n\", \"\\n\", \"This is one aspect of the concept  of ergodicity.\\n\", \"\\n\", \"\\n\", \"<a id='finite-mc-expec'></a>\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"849a9001\", \"source\": [\"## Computing Expectations\\n\", \"\\n\", \"\\n\", \"<a id='index-18'></a>\\n\", \"We sometimes want to  compute mathematical  expectations of functions of $ X_t $ of the form\\n\", \"\\n\", \"\\n\", \"<a id='equation-mc-une'></a>\\n\", \"$$\\n\", \"\\\\mathbb E [ h(X_t) ] \\\\tag{26.9}\\n\", \"$$\\n\", \"\\n\", \"and conditional expectations such as\\n\", \"\\n\", \"\\n\", \"<a id='equation-mc-cce'></a>\\n\", \"$$\\n\", \"\\\\mathbb E [ h(X_{t + k})  \\\\mid X_t = x] \\\\tag{26.10}\\n\", \"$$\\n\", \"\\n\", \"where\\n\", \"\\n\", \"- $ \\\\{X_t\\\\} $ is a Markov chain generated by $ n \\\\times n $ stochastic matrix $ P $  \\n\", \"- $ h $ is a given function, which, in terms of matrix\\n\", \"  algebra, we’ll think of as the column vector  \\n\", \"\\n\", \"\\n\", \"$$\\n\", \"h\\n\", \"= \\\\left(\\n\", \"\\\\begin{array}{c}\\n\", \"    h(x_1) \\\\\\\\\\n\", \"    \\\\vdots \\\\\\\\\\n\", \"    h(x_n)\\n\", \"\\\\end{array}\\n\", \"  \\\\right)\\n\", \"$$\\n\", \"\\n\", \"Computing the unconditional expectation [(26.9)](#equation-mc-une) is easy.\\n\", \"\\n\", \"We just sum over the marginal  distribution  of $ X_t $ to get\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb E [ h(X_t) ]\\n\", \"= \\\\sum_{x \\\\in S} (\\\\psi P^t)(x) h(x)\\n\", \"$$\\n\", \"\\n\", \"Here $ \\\\psi $ is the distribution of $ X_0 $.\\n\", \"\\n\", \"Since $ \\\\psi $ and hence $ \\\\psi P^t $ are row vectors, we can also\\n\", \"write this as\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb E [ h(X_t) ]\\n\", \"=  \\\\psi P^t h\\n\", \"$$\\n\", \"\\n\", \"For the conditional expectation [(26.10)](#equation-mc-cce), we need to sum over\\n\", \"the conditional distribution of $ X_{t + k} $ given $ X_t = x $.\\n\", \"\\n\", \"We already know that this is $ P^k(x, \\\\cdot) $, so\\n\", \"\\n\", \"\\n\", \"<a id='equation-mc-cce2'></a>\\n\", \"$$\\n\", \"\\\\mathbb E [ h(X_{t + k})  \\\\mid X_t = x]\\n\", \"= (P^k h)(x) \\\\tag{26.11}\\n\", \"$$\\n\", \"\\n\", \"The vector $ P^k h $ stores the conditional expectation $ \\\\mathbb E [ h(X_{t + k})  \\\\mid X_t = x] $ over all $ x $.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"8ac6c31b\", \"source\": [\"### Iterated Expectations\\n\", \"\\n\", \"The **law of iterated expectations** states that\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb E \\\\left[ \\\\mathbb E [ h(X_{t + k})  \\\\mid X_t = x] \\\\right] = \\\\mathbb E [  h(X_{t + k}) ]\\n\", \"$$\\n\", \"\\n\", \"where the outer $ \\\\mathbb E $ on the left side is an unconditional distribution taken with respect to the marginal distribution  $ \\\\psi_t $ of $ X_t $\\n\", \"(again see equation [(26.6)](#equation-mdfmc2)).\\n\", \"\\n\", \"To verify the law of iterated expectations, use  equation [(26.11)](#equation-mc-cce2) to substitute $ (P^k h)(x) $ for $ E [ h(X_{t + k})  \\\\mid X_t = x] $, write\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb E \\\\left[ \\\\mathbb E [ h(X_{t + k})  \\\\mid X_t = x] \\\\right] = \\\\psi_t P^k h,\\n\", \"$$\\n\", \"\\n\", \"and note $ \\\\psi_t P^k h = \\\\psi_{t+k} h = \\\\mathbb E [  h(X_{t + k}) ] $.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"e24a63a6\", \"source\": [\"### Expectations of Geometric Sums\\n\", \"\\n\", \"Sometimes we want to compute the mathematical expectation of a geometric sum, such as\\n\", \"$ \\\\sum_t \\\\beta^t h(X_t) $.\\n\", \"\\n\", \"In view of the preceding discussion, this is\\n\", \"\\n\", \"$$\\n\", \"\\\\mathbb{E} [\\n\", \"        \\\\sum_{j=0}^\\\\infty \\\\beta^j h(X_{t+j}) \\\\mid X_t = x\\n\", \"    \\\\Bigr]\\n\", \"= [(I - \\\\beta P)^{-1} h](x)\\n\", \"$$\\n\", \"\\n\", \"where\\n\", \"\\n\", \"$$\\n\", \"(I - \\\\beta P)^{-1}  = I + \\\\beta P + \\\\beta^2 P^2 + \\\\cdots\\n\", \"$$\\n\", \"\\n\", \"Premultiplication by $ (I - \\\\beta P)^{-1} $ amounts to “applying the **resolvent operator**”.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"16b113b9\", \"source\": [\"## Exercises\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"91a07b9a\", \"source\": [\"## Exercise 26.1\\n\", \"\\n\", \"According to the discussion [above](#mc-eg1-2), if a worker’s employment dynamics obey the stochastic matrix\\n\", \"\\n\", \"$$\\n\", \"P\\n\", \"= \\\\left(\\n\", \"\\\\begin{array}{cc}\\n\", \"    1 - \\\\alpha & \\\\alpha \\\\\\\\\\n\", \"    \\\\beta & 1 - \\\\beta\\n\", \"\\\\end{array}\\n\", \"  \\\\right)\\n\", \"$$\\n\", \"\\n\", \"with $ \\\\alpha \\\\in (0,1) $ and $ \\\\beta \\\\in (0,1) $, then, in the long-run, the fraction\\n\", \"of time spent unemployed will be\\n\", \"\\n\", \"$$\\n\", \"p := \\\\frac{\\\\beta}{\\\\alpha + \\\\beta}\\n\", \"$$\\n\", \"\\n\", \"In other words, if $ \\\\{X_t\\\\} $ represents the Markov chain for\\n\", \"employment, then $ \\\\bar X_m \\\\to p $ as $ m \\\\to \\\\infty $, where\\n\", \"\\n\", \"$$\\n\", \"\\\\bar X_m := \\\\frac{1}{m} \\\\sum_{t = 1}^m \\\\mathbf{1}\\\\{X_t = 0\\\\}\\n\", \"$$\\n\", \"\\n\", \"This exercise asks you to illustrate convergence by computing\\n\", \"$ \\\\bar X_m $ for large $ m $ and checking that\\n\", \"it is close to $ p $.\\n\", \"\\n\", \"You will see that this statement is true regardless of the choice of initial\\n\", \"condition or the values of $ \\\\alpha, \\\\beta $, provided both lie in\\n\", \"$ (0, 1) $.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"419a12d1\", \"source\": [\"## Solution to[ Exercise 26.1](https://python.quantecon.org/#fm_ex1)\\n\", \"\\n\", \"We will address this exercise graphically.\\n\", \"\\n\", \"The plots show the time series of $ \\\\bar X_m - p $ for two initial\\n\", \"conditions.\\n\", \"\\n\", \"As $ m $ gets large, both series converge to zero.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"0abfaa7b\", \"source\": [\"α = β = 0.1\\n\", \"N = 10000\\n\", \"p = β / (α + β)\\n\", \"\\n\", \"P = ((1 - α,       α),               # Careful: P and p are distinct\\n\", \"     (    β,   1 - β))\\n\", \"mc = MarkovChain(P)\\n\", \"\\n\", \"fig, ax = plt.subplots(figsize=(9, 6))\\n\", \"ax.set_ylim(-0.25, 0.25)\\n\", \"ax.grid()\\n\", \"ax.hlines(0, 0, N, lw=2, alpha=0.6)   # Horizonal line at zero\\n\", \"\\n\", \"for x0, col in ((0, 'blue'), (1, 'green')):\\n\", \"    # Generate time series for worker that starts at x0\\n\", \"    X = mc.simulate(N, init=x0)\\n\", \"    # Compute fraction of time spent unemployed, for each n\\n\", \"    X_bar = (X == 0).cumsum() / (1 + np.arange(N, dtype=float))\\n\", \"    # Plot\\n\", \"    ax.fill_between(range(N), np.zeros(N), X_bar - p, color=col, alpha=0.1)\\n\", \"    ax.plot(X_bar - p, color=col, label=f'$X_0 = \\\\, {x0} $')\\n\", \"    # Overlay in black--make lines clearer\\n\", \"    ax.plot(X_bar - p, 'k-', alpha=0.6)\\n\", \"\\n\", \"ax.legend(loc='upper right')\\n\", \"plt.show()\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"4093bd17\", \"source\": [\"## Exercise 26.2\\n\", \"\\n\", \"A topic of interest for economics and many other disciplines is *ranking*.\\n\", \"\\n\", \"Let’s now consider one of the most practical and important ranking problems\\n\", \"— the rank assigned to web pages by search engines.\\n\", \"\\n\", \"(Although the problem is motivated from outside of economics, there is in fact a deep connection between search ranking systems and prices in certain competitive equilibria — see [[DLP13](https://python.quantecon.org/zreferences.html#id157)].)\\n\", \"\\n\", \"To understand the issue, consider the set of results returned by a query to a web search engine.\\n\", \"\\n\", \"For the user, it is desirable to\\n\", \"\\n\", \"1. receive a large set of accurate matches  \\n\", \"1. have the matches returned in order, where the order corresponds to some measure of “importance”  \\n\", \"\\n\", \"\\n\", \"Ranking according to a measure of importance is the problem we now consider.\\n\", \"\\n\", \"The methodology developed to solve this problem by Google founders Larry Page and Sergey Brin\\n\", \"is known as [PageRank](https://en.wikipedia.org/wiki/PageRank).\\n\", \"\\n\", \"To illustrate the idea, consider the following diagram\\n\", \"\\n\", \"![https://python.quantecon.org/_static/lecture_specific/finite_markov/web_graph.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/web_graph.png)\\n\", \"\\n\", \"  \\n\", \"Imagine that this is a miniature version of the WWW, with\\n\", \"\\n\", \"- each node representing a web page  \\n\", \"- each arrow representing the existence of a link from one page to another  \\n\", \"\\n\", \"\\n\", \"Now let’s think about which pages are likely to be important, in the sense of being valuable to a search engine user.\\n\", \"\\n\", \"One possible criterion for the importance of a page is the number of inbound links — an indication of popularity.\\n\", \"\\n\", \"By this measure, `m` and `j` are the most important pages, with 5 inbound links each.\\n\", \"\\n\", \"However, what if the pages linking to `m`, say, are not themselves important?\\n\", \"\\n\", \"Thinking this way, it seems appropriate to weight the inbound nodes by relative importance.\\n\", \"\\n\", \"The PageRank algorithm does precisely this.\\n\", \"\\n\", \"A slightly simplified presentation that captures the basic idea is as follows.\\n\", \"\\n\", \"Letting $ j $ be (the integer index of) a typical page and $ r_j $ be its ranking, we set\\n\", \"\\n\", \"$$\\n\", \"r_j = \\\\sum_{i \\\\in L_j} \\\\frac{r_i}{\\\\ell_i}\\n\", \"$$\\n\", \"\\n\", \"where\\n\", \"\\n\", \"- $ \\\\ell_i $ is the total number of outbound links from $ i $  \\n\", \"- $ L_j $ is the set of all pages $ i $ such that $ i $ has a link to $ j $  \\n\", \"\\n\", \"\\n\", \"This is a measure of the number of inbound links, weighted by their own ranking (and normalized by $ 1 / \\\\ell_i $).\\n\", \"\\n\", \"There is, however, another interpretation, and it brings us back to Markov chains.\\n\", \"\\n\", \"Let $ P $ be the matrix given by $ P(i, j) = \\\\mathbf 1\\\\{i \\\\to j\\\\} / \\\\ell_i $ where $ \\\\mathbf 1\\\\{i \\\\to j\\\\} = 1 $ if $ i $ has a link to $ j $ and zero otherwise.\\n\", \"\\n\", \"The matrix $ P $ is a stochastic matrix provided that each page has at least one link.\\n\", \"\\n\", \"With this definition of $ P $ we have\\n\", \"\\n\", \"$$\\n\", \"r_j\\n\", \"= \\\\sum_{i \\\\in L_j} \\\\frac{r_i}{\\\\ell_i}\\n\", \"= \\\\sum_{\\\\text{all } i} \\\\mathbf 1\\\\{i \\\\to j\\\\} \\\\frac{r_i}{\\\\ell_i}\\n\", \"= \\\\sum_{\\\\text{all } i} P(i, j) r_i\\n\", \"$$\\n\", \"\\n\", \"Writing $ r $ for the row vector of rankings, this becomes $ r = r P $.\\n\", \"\\n\", \"Hence $ r $ is the stationary distribution of the stochastic matrix $ P $.\\n\", \"\\n\", \"Let’s think of $ P(i, j) $ as the probability of “moving” from page $ i $ to page $ j $.\\n\", \"\\n\", \"The value $ P(i, j) $ has the interpretation\\n\", \"\\n\", \"- $ P(i, j) = 1/k $ if $ i $ has $ k $ outbound links and $ j $ is one of them  \\n\", \"- $ P(i, j) = 0 $ if $ i $ has no direct link to $ j $  \\n\", \"\\n\", \"\\n\", \"Thus, motion from page to page is that of a web surfer who moves from one page to another by randomly clicking on one of the links on that page.\\n\", \"\\n\", \"Here “random” means that each link is selected with equal probability.\\n\", \"\\n\", \"Since $ r $ is the stationary distribution of $ P $, assuming that the uniform ergodicity condition is valid, we [can interpret](#new-interp-sd) $ r_j $ as the fraction of time that a (very persistent) random surfer spends at page $ j $.\\n\", \"\\n\", \"Your exercise is to apply this ranking algorithm to the graph pictured above\\n\", \"and return the list of pages ordered by rank.\\n\", \"\\n\", \"There is a total of 14 nodes (i.e., web pages), the first named `a` and the last named `n`.\\n\", \"\\n\", \"A typical line from the file has the form\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"4f1308ca\", \"source\": [\"```text\\n\", \"d -> h;\\n\", \"```\\n\"], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"6f248ffd\", \"source\": [\"This should be interpreted as meaning that there exists a link from `d` to `h`.\\n\", \"\\n\", \"The data for this graph is shown below, and read into a file called `web_graph_data.txt` when the cell is executed.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"9753b68e\", \"source\": [\"%%file web_graph_data.txt\\n\", \"a -> d;\\n\", \"a -> f;\\n\", \"b -> j;\\n\", \"b -> k;\\n\", \"b -> m;\\n\", \"c -> c;\\n\", \"c -> g;\\n\", \"c -> j;\\n\", \"c -> m;\\n\", \"d -> f;\\n\", \"d -> h;\\n\", \"d -> k;\\n\", \"e -> d;\\n\", \"e -> h;\\n\", \"e -> l;\\n\", \"f -> a;\\n\", \"f -> b;\\n\", \"f -> j;\\n\", \"f -> l;\\n\", \"g -> b;\\n\", \"g -> j;\\n\", \"h -> d;\\n\", \"h -> g;\\n\", \"h -> l;\\n\", \"h -> m;\\n\", \"i -> g;\\n\", \"i -> h;\\n\", \"i -> n;\\n\", \"j -> e;\\n\", \"j -> i;\\n\", \"j -> k;\\n\", \"k -> n;\\n\", \"l -> m;\\n\", \"m -> g;\\n\", \"n -> c;\\n\", \"n -> j;\\n\", \"n -> m;\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"1e4771ef\", \"source\": [\"To parse this file and extract the relevant information, you can use [regular expressions](https://docs.python.org/3/library/re.html).\\n\", \"\\n\", \"The following code snippet provides a hint as to how you can go about this\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"a98f7147\", \"source\": [\"import re\\n\", \"re.findall('\\\\w', 'x +++ y ****** z')  # \\\\w matches alphanumerics\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"bc8ab3e6\", \"source\": [\"re.findall('\\\\w', 'a ^^ b &&& $$ c')\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"bf46ad51\", \"source\": [\"When you solve for the ranking, you will find that the highest ranked node is in fact `g`, while the lowest is `a`.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"6e033454\", \"source\": [\"## Solution to[ Exercise 26.2](https://python.quantecon.org/#fm_ex2)\\n\", \"\\n\", \"Here is one solution:\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"d41a631b\", \"source\": [\"\\\"\\\"\\\"\\n\", \"Return list of pages, ordered by rank\\n\", \"\\\"\\\"\\\"\\n\", \"import re\\n\", \"from operator import itemgetter\\n\", \"\\n\", \"infile = 'web_graph_data.txt'\\n\", \"alphabet = 'abcdefghijklmnopqrstuvwxyz'\\n\", \"\\n\", \"n = 14 # Total number of web pages (nodes)\\n\", \"\\n\", \"# Create a matrix Q indicating existence of links\\n\", \"#  * Q[i, j] = 1 if there is a link from i to j\\n\", \"#  * Q[i, j] = 0 otherwise\\n\", \"Q = np.zeros((n, n), dtype=int)\\n\", \"with open(infile) as f: \\n\", \"    edges = f.readlines()\\n\", \"for edge in edges:\\n\", \"    from_node, to_node = re.findall('\\\\w', edge)\\n\", \"    i, j = alphabet.index(from_node), alphabet.index(to_node)\\n\", \"    Q[i, j] = 1\\n\", \"# Create the corresponding Markov matrix P\\n\", \"P = np.empty((n, n))\\n\", \"for i in range(n):\\n\", \"    P[i, :] = Q[i, :] / Q[i, :].sum()\\n\", \"mc = MarkovChain(P)\\n\", \"# Compute the stationary distribution r\\n\", \"r = mc.stationary_distributions[0]\\n\", \"ranked_pages = {alphabet[i] : r[i] for i in range(n)}\\n\", \"# Print solution, sorted from highest to lowest rank\\n\", \"print('Rankings\\\\n ***')\\n\", \"for name, rank in sorted(ranked_pages.items(), key=itemgetter(1), reverse=1):\\n\", \"    print(f'{name}: {rank:.4}')\"], \"outputs\": [], \"metadata\": {\"hide-output\": false}, \"cell_type\": \"code\", \"execution_count\": null}, {\"id\": \"50990afe\", \"source\": [\"## Exercise 26.3\\n\", \"\\n\", \"In numerical work, it is sometimes convenient to replace a continuous model with a discrete one.\\n\", \"\\n\", \"In particular, Markov chains are routinely generated as discrete approximations to AR(1) processes of the form\\n\", \"\\n\", \"$$\\n\", \"y_{t+1} = \\\\rho y_t + u_{t+1}\\n\", \"$$\\n\", \"\\n\", \"Here $ {u_t} $ is assumed to be IID and $ N(0, \\\\sigma_u^2) $.\\n\", \"\\n\", \"The variance of the stationary probability distribution of $ \\\\{ y_t \\\\} $ is\\n\", \"\\n\", \"$$\\n\", \"\\\\sigma_y^2 := \\\\frac{\\\\sigma_u^2}{1-\\\\rho^2}\\n\", \"$$\\n\", \"\\n\", \"Tauchen’s method [[Tau86](https://python.quantecon.org/zreferences.html#id222)] is the most common method for approximating this continuous state process with a finite state Markov chain.\\n\", \"\\n\", \"A routine for this already exists in [QuantEcon.py](http://quantecon.org/quantecon-py) but let’s write our own version as an exercise.\\n\", \"\\n\", \"As a first step, we choose\\n\", \"\\n\", \"- $ n $, the number of states for the discrete approximation  \\n\", \"- $ m $, an integer that parameterizes the width of the state space  \\n\", \"\\n\", \"\\n\", \"Next, we create a state space $ \\\\{x_0, \\\\ldots, x_{n-1}\\\\} \\\\subset \\\\mathbb R $\\n\", \"and a stochastic $ n \\\\times n $ matrix $ P $ such that\\n\", \"\\n\", \"- $ x_0 = - m \\\\, \\\\sigma_y $  \\n\", \"- $ x_{n-1} = m \\\\, \\\\sigma_y $  \\n\", \"- $ x_{i+1} = x_i + s $ where $ s = (x_{n-1} - x_0) / (n - 1) $  \\n\", \"\\n\", \"\\n\", \"Let $ F $ be the cumulative distribution function of the normal distribution $ N(0, \\\\sigma_u^2) $.\\n\", \"\\n\", \"The values $ P(x_i, x_j) $ are computed to approximate the AR(1) process — omitting the derivation, the rules are as follows:\\n\", \"\\n\", \"1. If $ j = 0 $, then set  \\n\", \"  $$\\n\", \"  P(x_i, x_j) = P(x_i, x_0) = F(x_0-\\\\rho x_i + s/2)\\n\", \"  $$\\n\", \"1. If $ j = n-1 $, then set  \\n\", \"  $$\\n\", \"  P(x_i, x_j) = P(x_i, x_{n-1}) = 1 - F(x_{n-1} - \\\\rho x_i - s/2)\\n\", \"  $$\\n\", \"1. Otherwise, set  \\n\", \"  $$\\n\", \"  P(x_i, x_j) = F(x_j - \\\\rho x_i + s/2) - F(x_j - \\\\rho x_i - s/2)\\n\", \"  $$\\n\", \"\\n\", \"\\n\", \"The exercise is to write a function `approx_markov(rho, sigma_u, m=3, n=7)` that returns\\n\", \"$ \\\\{x_0, \\\\ldots, x_{n-1}\\\\} \\\\subset \\\\mathbb R $ and $ n \\\\times n $ matrix\\n\", \"$ P $ as described above.\\n\", \"\\n\", \"- Even better, write a function that returns an instance of [QuantEcon.py’s](http://quantecon.org/quantecon-py) MarkovChain class.  \"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"b8e9a87c\", \"source\": [\"## Solution to[ Exercise 26.3](https://python.quantecon.org/#fm_ex3)\\n\", \"\\n\", \"A solution from the [QuantEcon.py](http://quantecon.org/quantecon-py) library\\n\", \"can be found [here](https://github.com/QuantEcon/QuantEcon.py/blob/master/quantecon/markov/approximation.py).\\n\", \"\\n\", \"<p><a id=pm href=#pm-link><strong>[1]</strong></a> Hint: First show that if $ P $ and $ Q $ are stochastic matrices then so is their product — to check the row sums, try post multiplying by a column vector of ones.  Finally, argue that $ P^n $ is a stochastic matrix using induction.\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}], \"metadata\": {\"date\": 1680677899.74526, \"title\": \"Finite Markov Chains\", \"filename\": \"finite_markov.md\", \"kernelspec\": {\"name\": \"python3\", \"language\": \"python3\", \"display_name\": \"Python\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.9\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 5}",
    "md_content": null,
    "properties": "{\"fileName\": \"finite_markov.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.789Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 14,
    "position": 10,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 15,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 15,
    "description": "Lake Model of Unemployment",
    "title": "Lecture 11",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 128,
    "position": 9,
    "content_id": 187,
    "lecture_id": 15,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 187,
    "type": "notebook",
    "description": "Code to implement the Lake model of unemployment flows",
    "title": "v1_employment_model.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"# Employment-Unemployment Model\\n\", \"\\n\", \"<br>\\n\", \"<br>\\n\", \"\\n\", \"**Goal**:\\n\", \"\\n\", \"Estimate a Markov chain using employment data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import datetime as dt\\n\", \"import matplotlib.pyplot as plt\\n\", \"import numpy as np\\n\", \"import pandas as pd\\n\", \"\\n\", \"%matplotlib inline\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Steps to \\\"understanding the world around us\\\" (a short digression)\\n\", \"\\n\", \"\\n\", \"Our friend Jim Savage has thought about what a \\\"modern statistical workflow\\\" entails. We summarize some steps he has proposed for understanding the world around us:\\n\", \"\\n\", \"> 1. Prepare and visualize your data.\\n\", \"> 2. Create a generative model for the data...\\n\", \">   - A first model should be as simple as possible.\\n\", \"> 3. Simulate some artificial data from your model given some assumed parameters that you \\\"pick out of a hat\\\" ($\\\\theta$)\\n\", \"> 4. Use the artificial data to estimate the model parameters ($\\\\hat{\\\\theta})$\\n\", \"> 5. Check that you recovered a good approximation of the \\\"known unknowns\\\" (aka, $\\\\theta \\\\approx \\\\hat{\\\\theta}$)\\n\", \">   - Possibly repeat 3-5 with different estimators and true parameters ($\\\\theta$), to get an understanding of how well the fitting procedure works\\n\", \"> 6. Fit the model to your real data, check the fit\\n\", \"> 7. Argue about the results with your friends and colleagues\\n\", \"> 8. Go back to 2. with a slightly richer model. Repeat.\\n\", \"> 9. Think carefully about what decisions will be made from the analysis, encode a loss function, and perform statistical decision analysis... Note that in many cases, we will do step 9 before steps 1-8!\\n\", \"\\n\", \"Later this semester, we will talk formally about what it means to \\\"fit\\\" your model (and the work that it entails), but, for now, we find it sufficient to say that it's a process to ensure that the probability distribution over outcomes generated by your model lines up with the data (aka, finding the right model parameters).\\n\", \"\\n\", \"We'll do a version of steps 1-6 to help us improve our understanding of the labor data that we previously saw.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Our plan\\n\", \"\\n\", \"1. Prepare and visualize our data.\\n\", \"2. Develop a generative model of employment and unemployment\\n\", \"3. Simulate data from our generative model for given parameters\\n\", \"4. Fit our model to the simulated data\\n\", \"5. Explore different ways that we might have chosen to fit the data\\n\", \"6. Fit the model with the BLS data\\n\", \"7. Examine what our model implies for the effects of COVID on employment/unemployment\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Step 1: Prepare and visualize our data\\n\", \"\\n\", \"We have done this in earlier lectures and will not repeat the work here!\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Step 2: Create a generative model\\n\", \"\\n\", \"**A simple model of employment**\\n\", \"\\n\", \"In the vein of, \\\"the first model created should be as simple as possible\\\", we use the employment model that we studied earlier.\\n\", \"\\n\", \"Consider a single individual that transitions between employment and unemployment\\n\", \"\\n\", \"* When unemployed, they find a new job with probability $\\\\alpha$\\n\", \"* When employed, they lose their job with probability $\\\\beta$\\n\", \"\\n\", \"<br>\\n\", \"<br>\\n\", \"\\n\", \"![ModelFlowchart](model_diagram.png)\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Step 3: Simulate data from our generative model\\n\", \"\\n\", \"<br>\\n\", \"<br>\\n\", \"\\n\", \"We will break simulating data from the model into two steps:\\n\", \"\\n\", \"1. Given today's state and the transition probabilities, draw from tomorrow's state\\n\", \"2. Given an initial state and transition probabilities, simulate an entire history of employment/unemployment using the one-step transition kernel\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Simulate the one-step employment transition**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def next_state(s_t, alpha, beta):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Transitions from employment/unemployment in period t to\\n\", \"    employment/unemployment in period t+1\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    s_t : int\\n\", \"        The individual's current state... s_t = 0 maps to\\n\", \"        unemployed and s_t = 1 maps to employed\\n\", \"    alpha : float\\n\", \"        The probability that an individual goes from\\n\", \"        unemployed to employed\\n\", \"    beta : float\\n\", \"        The probability that an individual goes from\\n\", \"        employed to unemployed\\n\", \"\\n\", \"    Returns\\n\", \"    -------\\n\", \"    s_tp1 : int\\n\", \"        The individual's employment state in `t+1`\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Draw a random number\\n\", \"    u_t = np.random.rand()\\n\", \"\\n\", \"    # Let 0 be unemployed... If unemployed and draws\\n\", \"    # a value less than lambda then becomes employed\\n\", \"    if (s_t == 0) and (u_t < alpha):\\n\", \"        s_tp1 = 1\\n\", \"    # Let 1 be employed... If employed and draws a\\n\", \"    # value less than beta then becomes unemployed\\n\", \"    elif (s_t == 1) and (u_t < beta):\\n\", \"        s_tp1 = 0\\n\", \"    # Otherwise, he keeps the same state as he had\\n\", \"    # at period t\\n\", \"    else:\\n\", \"        s_tp1 = s_t\\n\", \"\\n\", \"    return s_tp1\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice how this function incorporates the Markov property that our model assumes.\\n\", \"\\n\", \"The Markov property says $\\\\text{Probability}(s_{t+1} | s_{t}) = \\\\text{Probability}(s_{t+1} | s_{t}, s_{t-1}, \\\\dots, s_0)$.\\n\", \"\\n\", \"This means that, other than the transition probabilities, we only need to know today's state and not the entire history.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Testing our function**\\n\", \"\\n\", \"It's always a good idea to write some simple test cases for functions that we create.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"next_state(0, 0.5, 0.5)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Should never become employed from unemployment\\n\", \"# if alpha is 0\\n\", \"next_state(0, 0.0, 0.5) == 0\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Should never become unemployed from employment\\n\", \"# if beta is 0\\n\", \"next_state(1, 0.5, 0.0) == 1\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Should always transition to employment from unemployment\\n\", \"# when alpha is 1\\n\", \"next_state(0, 1.0, 0.5) == 1\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# Should always transition to unemployment from employment\\n\", \"# when beta is 1\\n\", \"next_state(1, 0.5, 1.0) == 0\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Simulate entire history**\\n\", \"\\n\", \"**Note**: Later we will allow $\\\\alpha$ and $\\\\beta$ to change over time, so while we want you think of them as constant for now, we will write code that allows for them to fluctate period-by-period\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def simulate_employment_history(alpha, beta, s_0):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Simulates the history of employment/unemployment. It\\n\", \"    will simulate as many periods as elements in `alpha`\\n\", \"    and `beta`\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    alpha : np.array(float, ndim=1)\\n\", \"        The probability that an individual goes from\\n\", \"        unemployed to employed\\n\", \"    beta : np.array(float, ndim=1)\\n\", \"        The probability that an individual goes from\\n\", \"        employed to unemployed\\n\", \"    s_0 : int\\n\", \"        The initial state of unemployment/employment, which\\n\", \"        should take value of 0 (unemployed) or 1 (employed)\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Create array to hold the values of our simulation\\n\", \"    assert(len(alpha) == len(beta))\\n\", \"    T = len(alpha)\\n\", \"    s_hist = np.zeros(T+1, dtype=int)\\n\", \"\\n\", \"    s_hist[0] = s_0\\n\", \"    for t in range(T):\\n\", \"        # Step one period into the future\\n\", \"        s_0 = next_state(s_0, alpha[t], beta[t])  # Notice alpha[t] and beta[t]\\n\", \"        s_hist[t+1] = s_0\\n\", \"\\n\", \"    return s_hist\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Check output of the function**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"alpha = np.ones(50)*0.25\\n\", \"beta = np.ones(50)*0.025\\n\", \"\\n\", \"simulate_employment_history(alpha, beta, 0)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Step 4: Fit your model to our artificial data\\n\", \"\\n\", \"There are lots of procedures that one could use to infer parameters values from data. Here we'll just count relative frequencies of transitions.\\n\", \"\\n\", \"Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.\\n\", \"\\n\", \"$$P \\\\equiv \\\\begin{bmatrix} p_{11} & p_{12} & \\\\dots & p_{1N} \\\\\\\\ p_{21} & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ p_{N1} & p_{N2} & \\\\dots & p_{NN} \\\\end{bmatrix}$$\\n\", \"\\n\", \"Let $\\\\{y_0, y_1, \\\\dots, y_T\\\\}$ be a sequence of observations generated from the $N$-state Markov chain, then our \\\"fitting\\\" procedure would assign the following value to $p_{ij}$:\\n\", \"\\n\", \"$$p_{ij} = \\\\frac{\\\\sum_{t=0}^T \\\\mathbb{1}_{y_{t} == i} \\\\mathbb{1}_{y_{t+1} == j}}{\\\\sum_{t=0}^T \\\\mathbb{1}_{y_{t} == i}}$$\\n\", \"\\n\", \"**Note**: If you'd like to understand why this procedure makes sense, we recommend computing $\\\\sum_{j=1}^N p_{ij}$ for a given $i$. What value do you get? Why?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Counting frequencies**\\n\", \"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def count_frequencies_individual(history):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Computes the transition probabilities for a two-state\\n\", \"    Markov chain\\n\", \"\\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    history : np.array(int, ndim=1)\\n\", \"        An array with the state values of a two-state Markov chain\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    alpha : float\\n\", \"        The probability of transitioning from state 0 to 1\\n\", \"    beta : float\\n\", \"        The probability of transitioning from state 1 to 0\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Get length of the simulation and an index tracker\\n\", \"    T = len(history)\\n\", \"    idx = np.arange(T)\\n\", \"\\n\", \"    # Determine when the chain had values 0 and 1 -- Notice\\n\", \"    # that we can't use the last value because we don't see\\n\", \"    # where it transitions to\\n\", \"    zero_idxs = idx[(history == 0) & (idx < T-1)]\\n\", \"    one_idxs = idx[(history == 1) & (idx < T-1)]\\n\", \"\\n\", \"    # Check what percent of the t+1 values were 0/1\\n\", \"    alpha = np.sum(history[zero_idxs+1]) / len(zero_idxs)\\n\", \"    beta = np.sum(1 - history[one_idxs+1]) / len(one_idxs)\\n\", \"\\n\", \"    return alpha, beta\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Checking the fit**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def check_accuracy(T, alpha=0.25, beta=0.025):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Checks the accuracy of our fit by printing the true values\\n\", \"    and the fitted values for a given T\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    T : int\\n\", \"        The length of our simulation\\n\", \"    alpha : float\\n\", \"        The probability that an individual goes from\\n\", \"        unemployed to employed\\n\", \"    beta : float\\n\", \"        The probability that an individual goes from\\n\", \"        employed to unemployed\\n\", \"    \\\"\\\"\\\"\\n\", \"    idx = np.arange(T)\\n\", \"    alpha_np = np.ones(T)*alpha\\n\", \"    beta_np = np.ones(T)*beta\\n\", \"\\n\", \"    # Simulate a sample history\\n\", \"    emp_history = simulate_employment_history(alpha_np, beta_np, 0)\\n\", \"\\n\", \"    # Check the fit\\n\", \"    alpha_hat, beta_hat = count_frequencies_individual(emp_history)\\n\", \"    \\n\", \"    print(f\\\"True alpha was {alpha} and fitted value was {alpha_hat}\\\")\\n\", \"    print(f\\\"True beta was {beta} and fitted value was {beta_hat}\\\")\\n\", \"    \\n\", \"    return alpha, alpha_hat, beta, beta_hat\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"check_accuracy(10_000, 0.25, 0.025)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Well... If we observe 10,000 months of employment history for someone then we know that we can back out the parameters of our models...\\n\", \"\\n\", \"Unfortunately, our real world data won't have that much information.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"What about for an entire lifetime of employment transitions?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"check_accuracy(45*12, 0.25, 0.025)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"What about for just two years of observations?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"check_accuracy(2*12, 0.25, 0.025)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Step 3 and 4 (second try)\\n\", \"\\n\", \"Data for the employment history of a single individual will not give us a good chance of fitting our model accurately...\\n\", \"\\n\", \"However, the BLS isn't infering EU/UE rates from its observation of a single individual. Rather, they're using a cross-section of individuals!\\n\", \"\\n\", \"Can we use a cross-section rather than for one individual?\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Yes, but, in order for a version of our \\\"frequency counting\\\" procedure to work, we want independence across individuals, i.e.\\n\", \"\\n\", \"$$\\\\text{Probability}(s_{i, t+1}, s_{j, t+1} | s_{i, t}, s_{j, t}, \\\\alpha, \\\\beta) = \\\\text{Probability}(s_{i, t+1} | s_{i, t}, \\\\alpha, \\\\beta) \\\\text{Probability}(s_{j, t+1} | s_{j, t}, \\\\alpha, \\\\beta)$$\\n\", \"\\n\", \"When we observed only a single individual, the Markov property did a lot of the work to get independence for us.\\n\", \"\\n\", \"When might this not be the case?\\n\", \"\\n\", \"* Change in government policy results in a \\\"jobs guarantee\\\"\\n\", \"* Technological change results in the destruction of an entire industries jobs\\n\", \"* Recession causes increased firing across entire country\\n\", \"\\n\", \"(Spoiler alert: Some of these will present problems for us... which is why we'll allow for $\\\\alpha$ and $\\\\beta$ to move each period)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Simulating a cross-section**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def simulate_employment_cross_section(alpha, beta, s_0, N=500):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Simulates a cross-section of employment/unemployment using\\n\", \"    the model we've described above.\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    alpha : np.array(float, ndim=1)\\n\", \"        The probability that an individual goes from\\n\", \"        unemployed to employed\\n\", \"    beta : np.array(float, ndim=1)\\n\", \"        The probability that an individual goes from\\n\", \"        employed to unemployed\\n\", \"    s_0 : np.array(int, ndim=1)\\n\", \"        The fraction of the population that begins in each\\n\", \"        employment state\\n\", \"    N : int\\n\", \"        The number of individuals in our cross-section\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    s_hist_cs : np.array(int, ndim=2)\\n\", \"        An `N x T` matrix that contains an individual\\n\", \"        history of employment along each row\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Make sure transitions are same size and get the length\\n\", \"    # of the simulation from the length of the transition\\n\", \"    # probabilities\\n\", \"    assert(len(alpha) == len(beta))\\n\", \"    T = len(alpha)\\n\", \"\\n\", \"    # Check the fractions add to one and figure out how many\\n\", \"    # zeros we should have\\n\", \"    assert(np.abs(np.sum(s_0) - 1.0) < 1e-8)\\n\", \"    Nz = np.floor(s_0[0]*N).astype(int)\\n\", \"\\n\", \"    # Allocate space to store the simulations\\n\", \"    s_hist_cs = np.zeros((N, T+1), dtype=int)\\n\", \"    s_hist_cs[Nz:, 0] = 1\\n\", \"    \\n\", \"    for i in range(N):\\n\", \"        s_hist_cs[i, :] = simulate_employment_history(\\n\", \"            alpha, beta, s_hist_cs[i, 0]\\n\", \"        )\\n\", \"    \\n\", \"    return s_hist_cs\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"alpha = np.ones(1)*0.25\\n\", \"beta = np.ones(1)*0.025\\n\", \"\\n\", \"simulate_employment_cross_section(alpha, beta, np.array([0.35, 0.65]), 10)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Store the simulation in pandas**\\n\", \"\\n\", \"Real world data will typically be stored in a DataFrame, so let's store our artificial data in a DataFrame as well\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def pandas_employment_cross_section(eu_ue_df, s_0, N=500):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Simulate a cross-section of employment experiences\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    eu_ue_df : pd.DataFrame\\n\", \"        A DataFrame with columns `dt`, `alpha`, and `beta`\\n\", \"        that have the monthly eu/ue transition rates\\n\", \"    s_0 : np.array(float, ndim=1)\\n\", \"        The fraction of the population that begins in each\\n\", \"        employment state\\n\", \"    N : int\\n\", \"        The numbers of individuals in our cross-section\\n\", \"\\n\", \"    Returns\\n\", \"    -------\\n\", \"    df : pd.DataFrame\\n\", \"        A DataFrame with the dates and an employment outcome\\n\", \"        associated with each date of `eu_ue_df`\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Make sure that `ue_ue_df` is sorted by date\\n\", \"    eu_ue_df = eu_ue_df.sort_values(\\\"dt\\\")\\n\", \"    alpha = eu_ue_df[\\\"alpha\\\"].to_numpy()\\n\", \"    beta = eu_ue_df[\\\"beta\\\"].to_numpy()\\n\", \"\\n\", \"    # Simulate cross-section\\n\", \"    employment_history = simulate_employment_cross_section(\\n\", \"        alpha, beta, s_0, N\\n\", \"    )\\n\", \"\\n\", \"    df = pd.DataFrame(employment_history[:, :-1].T)\\n\", \"    df = pd.concat([eu_ue_df[\\\"dt\\\"], df], axis=1)\\n\", \"    df = pd.melt(\\n\", \"        df, id_vars=[\\\"dt\\\"],\\n\", \"        var_name=\\\"pid\\\", value_name=\\\"employment\\\"\\n\", \"    )\\n\", \"\\n\", \"    return df\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"T = 24\\n\", \"eu_ue_df = pd.DataFrame(\\n\", \"    {\\n\", \"        \\\"dt\\\": pd.date_range(\\\"2018-01-01\\\", periods=T, freq=\\\"MS\\\"), \\n\", \"        \\\"alpha\\\": np.ones(T)*0.25,\\n\", \"        \\\"beta\\\": np.ones(T)*0.025\\n\", \"    }\\n\", \")\\n\", \"\\n\", \"df = pandas_employment_cross_section(eu_ue_df, np.array([0.25, 0.75]), N=5_000)\\n\", \"df.head()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Simulating the CPS**\\n\", \"\\n\", \"Just to \\\"keep it interesting\\\", let's tie our hands in a similar way to how the BLS has their hands tied.\\n\", \"\\n\", \"We will simulate an individual's full employment history, but will only keep the subset that corresponds to when they would have been interviewed by the CPS\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def cps_interviews(df, start_year, start_month):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Takes an individual simulated employment/unemployment\\n\", \"    history and \\\"interviews\\\" the individual as if they were\\n\", \"    in the CPS\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    df : pd.DataFrame\\n\", \"        A DataFrame with at least the columns `pid`, `dt`,\\n\", \"        and `employment`\\n\", \"    start_year : int\\n\", \"        The year in which their interviewing begins\\n\", \"    start_month : int\\n\", \"        The month in which their interviewing begins\\n\", \"\\n\", \"    Returns\\n\", \"    -------\\n\", \"    cps : pd.DataFrame\\n\", \"        A DataFrame with the same columns as `df` but only\\n\", \"        with observations that correspond to the CPS\\n\", \"        interview schedule for someone who starts\\n\", \"        interviewing in f`{start_year}/{start_month}`\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Get dates that are associated with being interviewed in\\n\", \"    # the CPS\\n\", \"    start_date_y1 = dt.datetime(start_year, start_month, 1)\\n\", \"    dates_y1 = pd.date_range(start_date_y1, periods=4, freq=\\\"MS\\\")\\n\", \"    start_date_y2 = dt.datetime(start_year+1, start_month, 1)\\n\", \"    dates_y2 = pd.date_range(start_date_y2, periods=4, freq=\\\"MS\\\")\\n\", \"    dates = dates_y1.append(dates_y2)\\n\", \"\\n\", \"    # Filter data that's not in the dates\\n\", \"    cps = df.loc[df[\\\"dt\\\"].isin(dates), :]\\n\", \"\\n\", \"    return cps\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"interview = lambda x: cps_interviews(\\n\", \"    x,\\n\", \"    np.random.choice(x[\\\"dt\\\"].dt.year.unique()),\\n\", \"    np.random.randint(1, 13)\\n\", \")\\n\", \"\\n\", \"cps_data = (\\n\", \"    df.groupby(\\\"pid\\\")\\n\", \"      .apply(\\n\", \"          lambda x: interview(x)\\n\", \"      )\\n\", \"      .reset_index(drop=True)\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**How many people are we observing per month?**\\n\", \"\\n\", \"If we think about the pattern used for the CPS interviews, we can form an idea of how many people might be interviewed each month.\\n\", \"\\n\", \"Consider if we started interviewing $m$ new individuals per month. How many would we be interviewing in any given month?\\n\", \"\\n\", \"Well. We'd at least be interviewing the $m$ new individuals. We would also be interviewing all of the individuals that had started their interviews in the previous 3 months. Additionally, we would be interviewing all of the individuals who had begun their interviews during those four months of the previous year.\\n\", \"\\n\", \"We can see this below -- Note that our \\\"survey\\\" begins in January 2018, so at first we only have $m$ individuals being interviewed, but, as the survey progresses, we move towards $8 m$ individuals being interviewed each month.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"cps_data[\\\"dt\\\"].value_counts().sort_index()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Fitting to a cross-section**\\n\", \"\\n\", \"Well... Now our data look exactly like what the BLS uses to estimate the EU and UE transition rates from the raw data.\\n\", \"\\n\", \"In order to fit the data, we are going to continue using the \\\"frequency of transition\\\" concept that we previously proposed, but, we must account for the shape of the data we receive now.\\n\", \"\\n\", \"Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.\\n\", \"\\n\", \"$$P \\\\equiv \\\\begin{bmatrix} p_{11} & p_{12} & \\\\dots & p_{1N} \\\\\\\\ p_{21} & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ p_{N1} & p_{N2} & \\\\dots & p_{NN} \\\\end{bmatrix}$$\\n\", \"\\n\", \"Let $\\\\{ \\\\{y_{i, 0}, y_{i, 1}, \\\\dots, y_{i, T_i}\\\\} \\\\; \\\\forall i \\\\in \\\\{0, 1, \\\\dots, I\\\\}\\\\}$ be a $I$ sequences of observations generated from the $N$-state Markov chain, then our new \\\"fitting\\\" procedure would assign the following value to $p_{ij}$:\\n\", \"\\n\", \"$$p_{ij} = \\\\frac{\\\\sum_{m=0}^I \\\\sum_{t=0}^T \\\\mathbb{1}_{y_{m, t} == i} \\\\mathbb{1}_{y_{m, t+1} == j}}{\\\\sum_{m=0}^I \\\\sum_{t=0}^T \\\\mathbb{1}_{y_{m, t} == i}}$$\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"**Cross-sectional counting frequencies**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def cps_count_frequencies(df):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Estimates the transition probability from employment\\n\", \"    and unemployment histories of a CPS sample of\\n\", \"    individuals\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    df : pd.DataFrame\\n\", \"        A sample of individuals from the CPS survey. Must\\n\", \"        have columns `dt`, `pid`, and `employment`.\\n\", \"\\n\", \"    Returns\\n\", \"    -------\\n\", \"    alpha : float\\n\", \"        The probability of transitioning from unemployment\\n\", \"        to employment\\n\", \"    beta : float\\n\", \"        The probability of transitioning from employment\\n\", \"        to unemployment\\n\", \"    \\\"\\\"\\\"\\n\", \"    # Set the index to be dt/pid\\n\", \"    data_t = df.set_index([\\\"dt\\\", \\\"pid\\\"])\\n\", \"\\n\", \"    # Now find the \\\"t+1\\\" months and \\\"pid\\\"s\\n\", \"    tp1 = data_t.index.get_level_values(\\\"dt\\\").shift(periods=1, freq=\\\"MS\\\")\\n\", \"    pid = data_t.index.get_level_values(\\\"pid\\\")\\n\", \"    idx = pd.MultiIndex.from_arrays([tp1, pid], names=[\\\"dt\\\", \\\"pid\\\"])\\n\", \"\\n\", \"    # Now \\\"index\\\" into the data and reset index\\n\", \"    data_tp1 = (\\n\", \"        data_t.reindex(idx)\\n\", \"            .rename(columns={\\\"employment\\\": \\\"employment_tp1\\\"})\\n\", \"    )\\n\", \"    out = pd.concat(\\n\", \"        [\\n\", \"            data_t.reset_index().loc[:, [\\\"dt\\\", \\\"pid\\\", \\\"employment\\\"]],\\n\", \"            data_tp1.reset_index()[\\\"employment_tp1\\\"]\\n\", \"        ], axis=1, sort=True\\n\", \"    ).dropna(subset=[\\\"employment_tp1\\\"])\\n\", \"    out[\\\"employment_tp1\\\"] = out[\\\"employment_tp1\\\"].astype(int)\\n\", \"\\n\", \"    # Count how frequently we go from 0 to 1\\n\", \"    out_zeros = out.query(\\\"employment == 0\\\")\\n\", \"    alpha = out_zeros[\\\"employment_tp1\\\"].mean()\\n\", \"    \\n\", \"    # Count how frequently we go from 1 to 0\\n\", \"    out_ones = out.query(\\\"employment == 1\\\")\\n\", \"    beta = (1 - out_ones[\\\"employment_tp1\\\"]).mean()\\n\", \"\\n\", \"    return alpha, beta\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Checking accuracy**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def check_accuracy_cs(N, T, alpha=0.25, beta=0.025):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Checks the accuracy of our fit by printing the true values\\n\", \"    and the fitted values for a given T\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    N : int\\n\", \"        The total number of people we ever interview\\n\", \"    T : int\\n\", \"        The length of our simulation\\n\", \"    alpha : float\\n\", \"        The probability that an individual goes from\\n\", \"        unemployed to employed\\n\", \"    beta : float\\n\", \"        The probability that an individual goes from\\n\", \"        employed to unemployed\\n\", \"    \\\"\\\"\\\"\\n\", \"    alpha_beta_df = pd.DataFrame(\\n\", \"        {\\n\", \"            \\\"dt\\\": pd.date_range(\\\"2018-01-01\\\", periods=T, freq=\\\"MS\\\"), \\n\", \"            \\\"alpha\\\": np.ones(T)*alpha,\\n\", \"            \\\"beta\\\": np.ones(T)*beta\\n\", \"        }\\n\", \"    )\\n\", \"\\n\", \"    # Simulate the full cross-section\\n\", \"    frac_unemployed = beta / (alpha + beta)\\n\", \"    frac_employed = alpha / (alpha + beta)\\n\", \"    df = pandas_employment_cross_section(\\n\", \"        alpha_beta_df, np.array([frac_unemployed, frac_employed]), N\\n\", \"    )\\n\", \"\\n\", \"    # Interview individuals according to the cps interviews\\n\", \"    interview = lambda x: cps_interviews(\\n\", \"        x,\\n\", \"        np.random.choice(df[\\\"dt\\\"].dt.year.unique()),\\n\", \"        np.random.randint(1, 13)\\n\", \"    )\\n\", \"    cps_data = (\\n\", \"        df.groupby(\\\"pid\\\")\\n\", \"          .apply(\\n\", \"              lambda x: interview(x)\\n\", \"          )\\n\", \"          .reset_index(drop=True)\\n\", \"    )\\n\", \"\\n\", \"    # Check the fit\\n\", \"    alpha_hat, beta_hat = cps_count_frequencies(cps_data)\\n\", \"    \\n\", \"    print(f\\\"True alpha was {alpha} and fitted value was {alpha_hat}\\\")\\n\", \"    print(f\\\"True beta was {beta} and fitted value was {beta_hat}\\\")\\n\", \"    \\n\", \"    return alpha, alpha_hat, beta, beta_hat\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"check_accuracy_cs(1_000, 24, 0.25, 0.025)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"check_accuracy_cs(500, 24, 0.25, 0.025)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"check_accuracy_cs(100, 24, 0.25, 0.025)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Step 7: Fit the model with actual CPS data\\n\", \"\\n\", \"We've downloaded (and cleaned!) a subset of real CPS data for the years 2018 and 2019.\\n\", \"\\n\", \"Let's see what our constant parameter model does with this data.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# Load real CPS data that we've cleaned for you.\\n\", \"cps_data = pd.read_parquet(\\\"cps_data.parquet\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**What does this data contain?**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"cps_data.head()\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Finding an employment history**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"cps_count_sum = cps_data.groupby(\\\"pid\\\").agg(\\n\", \"    {\\\"dt\\\": \\\"count\\\", \\\"employment\\\": \\\"sum\\\"}\\n\", \").sort_values(\\\"dt\\\")\\n\", \"\\n\", \"cps_count_sum.tail()\\n\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"cps_data.query(\\\"pid == 20180602828701\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Find an individual who experiences unemployment?**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"cps_count_sum.query(\\\"(dt == 8) & (employment < 8)\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"cps_data.query(\\\"pid == 20180307173302\\\")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"**Computing $\\\\alpha$ and $\\\\beta$**\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"alpha_cps, beta_cps = cps_count_frequencies(cps_data)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"alpha_cps\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"beta_cps\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"beta_cps / (alpha_cps + beta_cps)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"-\"}}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"@webio\": {\"lastCommId\": null, \"lastKernelId\": null}, \"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"interpreter\": {\"hash\": \"06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"v1_employment_model.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.789Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 14,
    "position": 10,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 15,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 15,
    "description": "Lake Model of Unemployment",
    "title": "Lecture 11",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 127,
    "position": 9,
    "content_id": 162,
    "lecture_id": 15,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 162,
    "type": "video",
    "description": "Learn how to model unemployment flows using a Markov Chain with the Lake Model",
    "title": "Lake Model of Unemployment",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=09uA4LsZKms\", \"youtubeVideoId\": \"09uA4LsZKms\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 11,
    "position": 11,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 12,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 12,
    "description": "Lake Model Meets Data",
    "title": "Lecture 12",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 131,
    "position": 9,
    "content_id": 161,
    "lecture_id": 12,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 161,
    "type": "video",
    "description": "More examples of using the Lake Model to analyze labor market flows",
    "title": "Lake Model Meets Data 2",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=HYOEYY-0vLI\", \"youtubeVideoId\": \"HYOEYY-0vLI\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 11,
    "position": 11,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 12,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 12,
    "description": "Lake Model Meets Data",
    "title": "Lecture 12",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 129,
    "position": 9,
    "content_id": 182,
    "lecture_id": 12,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 182,
    "type": "notebook",
    "description": "Use the lake model code and BLS data to estimate model parameters",
    "title": "modeling_decisions.ipynb",
    "nb_content": "{\"cells\": [{\"source\": [\"## The Lake Model meets US data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Recall the setting and some standard language that helps us keep sight of how various\\n\", \"pieces of our analysis fit together.\\n\", \"\\n\", \"* A **statistical model** is a joint probability density $f(Y; \\\\theta)$ for a sequence $Y$ of random variables indexed by a list $\\\\theta$ of parameters.\\n\", \"\\n\", \"* The **direct problem** is to draw a random sample $y$ from the statistical model\\n\", \"$f(Y; \\\\theta)$ for an assumed value of the parameter vector $\\\\theta$.\\n\", \"\\n\", \"* The **inverse problem** is to take a set of data $\\\\tilde y$ assumed to be drawn from\\n\", \"$f(Y;\\\\theta)$ and to use them to make inferences about the unknown parameter vector $\\\\theta$.\\n\", \"\\n\", \"Thus, the **direct problem** is to construct a **simulation** of model $f(Y;\\\\theta)$\\n\", \"for a given $\\\\theta$.  \\n\", \"\\n\", \"* Other names for  the simulated data $ y$ are *artificial data* or *fake data*\\n\", \"\\n\", \"The **inverse problem** is also called the **parameter estimation problem**.\\n\", \"\\n\", \"The **direct problem** takes parameters as inputs and artificial or fake data as outputs\\n\", \"\\n\", \"The **inverse problem** takes real world data (i.e., \\\"the observations\\\") as inputs and statements about parameters as outputs.\\n\", \"\\n\", \"Both problems assume the same statistical model $f(Y; \\\\theta)$.\\n\", \"\\n\", \"\\n\", \"Tools that allow us to solve the **direct problem** often turn out to be helpful in \\n\", \"solving the **inverse problem**.\\n\", \"\\n\", \"\\n\", \"**Application to lake model**\\n\", \"\\n\", \"Please keep this framework in mind as we use various versions of the  \\\"lake model\\\"\\n\", \"to try to understand employment and unemployment rates over time.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import pandas as pd\\n\", \"import numpy as np\\n\", \"import random\\n\", \"import matplotlib.pyplot as plt\\n\", \"plt.style.use(\\\"fivethirtyeight\\\")\\n\", \"\\n\", \"# for reproducibility\\n\", \"np.random.seed(42)\\n\", \"random.seed(42)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Recent unemployment rates in the US\\n\", \"\\n\", \"The BLS data show a large shift in the employment/unemployment numbers in the US in early 2020\\n\", \"\\n\", \"Let's remind ourselves what these data look like:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_bls = pd.read_csv(\\\"bls_recessions_data.csv\\\")\\n\", \"df_bls[\\\"labor_supply\\\"] = df_bls.eval(\\\"employed + unemployed\\\")\\n\", \"df_bls[\\\"employed_pct\\\"] = df_bls.eval(\\\"employed / labor_supply\\\")\\n\", \"df_bls[\\\"unemployed_pct\\\"] = df_bls.eval(\\\"unemployed / labor_supply\\\")\\n\", \"df_bls.head()\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"def plot_recession_data(df):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Plot the employment and unemployment percents contained in df\\n\", \"    \\n\", \"    The x value is `months_from` with interpretation of the months\\n\", \"    from the trough of a recession. A negative number indicates a \\n\", \"    date before the trough\\n\", \"    \\n\", \"    There will be subplots for employment and unemployment percentages\\n\", \"    \\n\", \"    There will be one line per subplot for each unique value of the \\n\", \"    `recession` column\\n\", \"    \\n\", \"    Paramters\\n\", \"    ---------\\n\", \"    df: pd.DataFrame\\n\", \"        A pandas DataFrame containing the labor market data to be plotted\\n\", \"        \\n\", \"    Returns\\n\", \"    -------\\n\", \"    fig: matplotlib.pyplot.Figure\\n\", \"        A matplotlib Figure with the chart\\n\", \"    \\\"\\\"\\\"\\n\", \"    # create figure with two subplots, one for emp, one for unemp\\n\", \"    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\\n\", \"    \\n\", \"    # list of columns to plot\\n\", \"    columns = [\\\"employed_pct\\\", \\\"unemployed_pct\\\"]\\n\", \"    for ax, col in zip(axs, columns):        \\n\", \"        # loop over data and plot column on axes for each recession\\n\", \"        for recession, data in df.groupby(\\\"recession\\\"):\\n\", \"            data.plot(x=\\\"months_from\\\", y=col, ax=ax, label=recession)\\n\", \"        # beautify axes\\n\", \"        ax.set_title(col)\\n\", \"        ax.spines[\\\"right\\\"].set_visible(False)\\n\", \"        ax.spines[\\\"top\\\"].set_visible(False)\\n\", \"        ax.set_xlabel(\\\"Months from trough\\\")\\n\", \"    fig.tight_layout()\\n\", \"    return fig\\n\", \"\\n\", \"plot_recession_data(df_bls);\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We've been using a lake model with two parameters:\\n\", \"\\n\", \"- $\\\\alpha$: probability of unemployed worker finding a job. Called the *job finding rate*\\n\", \"- $\\\\beta$: probability of employed worker losing a job. Called the *job separation rate*\\n\", \"\\n\", \"And one state variable:\\n\", \"\\n\", \"- $s$: a  $2 \\\\times 1$ vector of  percentages of unemployed and employed workers, respectively.\\n\", \"\\n\", \"With constant values  of $\\\\alpha$ and $\\\\beta$, our model cannot generate sudden large changes in the unemployment rate.\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's look at an example using code from `v1_employment_model.py`\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"import v1_employment_model as emp\\n\", \"import numpy as np\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We will first create a new Python class that we'll use throughout this lesson\\n\", \"\\n\", \"The class will help us:\\n\", \"\\n\", \"- Keep track of model parameters\\n\", \"- Simulate a panel of workers\\n\", \"- Plot simulated results alongside data from BLS\\n\", \"\\n\", \"The class looks long, but most of the lines are docstrings and comments\\n\", \"\\n\", \"We'll review it one step at a time and it will be clear\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"class LakeModelSimulator:\\n\", \"    def __init__(self, N:int = 5000, s_0: np.ndarray=np.array([0.0237, 0.9763])):\\n\", \"        \\\"\\\"\\\"\\n\", \"        Helper class that can simulate individuals working in an economy\\n\", \"        where job loss and finding is governed by the Lake model\\n\", \"        \\n\", \"        Parameters\\n\", \"        ----------\\n\", \"        \\n\", \"        N: int\\n\", \"            The number of individuals in the simulated panels\\n\", \"        s_0: np.ndarray(float, ndim=1, size=2)\\n\", \"            The initial distribution of [unemployed, employed] workers\\n\", \"            in the economy. These should be two numbers between [0, 1]\\n\", \"            that sum to one. The default value is the long run value of the\\n\", \"            employment to unemployment ratio computed using data from the\\n\", \"            CPS\\n\", \"        \\\"\\\"\\\"\\n\", \"        self.N = N\\n\", \"        self._s_0 = s_0\\n\", \"    \\n\", \"    # define s_0 as Python property so we can ensure it cannot be \\n\", \"    # set to an inappropriate value\\n\", \"    \\n\", \"    # define a `get_s_0` method. lm.s_0\\n\", \"    def get_s_0(self) -> np.ndarray:\\n\", \"        return self._s_0\\n\", \"        \\n\", \"    # and a set_s_0 method\\n\", \"    def set_s_0(self, new_s_0: np.ndarray):\\n\", \"        \\\"\\\"\\\"\\n\", \"        Change the value of s_0\\n\", \"        \\n\", \"        Parameters\\n\", \"        ----------\\n\", \"        new_s_0: np.ndarray\\n\", \"            The new value of s_0 that should be used in future simulations\\n\", \"        \\\"\\\"\\\"\\n\", \"        assert len(new_s_0) == 2\\n\", \"        assert abs(sum(new_s_0) - 1.0) < 1e-10\\n\", \"        self._s_0 = np.asarray(new_s_0)\\n\", \"    \\n\", \"    # combine the two to create the official s_0\\n\", \"    s_0 = property(get_s_0, set_s_0)\\n\", \"    \\n\", \"    def combine_with_bls_data(self, df_sim: pd.DataFrame, df_bls: pd.DataFrame=df_bls) -> pd.DataFrame:\\n\", \"        \\\"\\\"\\\"\\n\", \"        Combine the simulated data from `df_sim` with the BLS data in\\n\", \"        `df_bls` and plot the percent of workers employed and unemployed\\n\", \"        \\n\", \"        Parameters\\n\", \"        ----------\\n\", \"        df_sim: pd.DataFrame\\n\", \"            The simulated DataFrame constructed by this class\\n\", \"        \\n\", \"        df_bls: pd.DataFrame\\n\", \"            The DataFrame containing official data from the BLS\\n\", \"            \\n\", \"        Returns\\n\", \"        -------\\n\", \"        df: pandas.DataFrame\\n\", \"            A DataFrame containing the combined data\\n\", \"            \\n\", \"        Notes\\n\", \"        -----\\n\", \"        Both DataFrames should have at least the following columns:\\n\", \"        [\\\"months_from\\\", \\\"employed_pct\\\", \\\"unemployed_pct\\\", \\\"recession\\\"]\\n\", \"        \\n\", \"        Also, the `months_from` column in `df_sim` will be modified as\\n\", \"        `df_sim[\\\"months_from\\\"] -= df_bls[\\\"months_from\\\"].min()` to align\\n\", \"        dates\\n\", \"        \\\"\\\"\\\"\\n\", \"        # adjust the months_from on the simulated data by -35 to match\\n\", \"        # the bls data\\n\", \"        df_sim_plot = df_sim.copy()\\n\", \"        df_sim_plot[\\\"months_from\\\"] -= 35\\n\", \"        df = pd.concat([df_sim_plot, df_bls], ignore_index=True)\\n\", \"        return df\\n\", \"    \\n\", \"    def simulate_panel(\\n\", \"            self,\\n\", \"            alpha: np.ndarray, \\n\", \"            beta: np.ndarray,\\n\", \"        ) -> pd.DataFrame:\\n\", \"        \\\"\\\"\\\"\\n\", \"        Simulate a panel of employees whose transitions in and out\\n\", \"        of employment are goverened by the lake model with parameters\\n\", \"        alpha and beta\\n\", \"\\n\", \"        Parameters\\n\", \"        ----------\\n\", \"        alpha: np.array(float, ndim=1)\\n\", \"            The probability that an individual goes from\\n\", \"            unemployed to employed\\n\", \"        beta: np.array(float, ndim=1)\\n\", \"            The probability that an individual goes from\\n\", \"            employed to unemployed\\n\", \"\\n\", \"        Returns\\n\", \"        -------\\n\", \"        df: pd.DataFrame\\n\", \"            A pandas DataFrame in the same form as the `df_bls` DataFrame\\n\", \"            from above. In particular, it has columns \\n\", \"            [\\\"months_from\\\", \\\"employed_pct\\\", \\\"unemployed_pct\\\", \\\"recession\\\"]\\n\", \"        \\\"\\\"\\\"\\n\", \"        # workers is N x T array of (0, 1)\\n\", \"        workers = emp.simulate_employment_cross_section(\\n\", \"            alpha, beta, self.s_0, N=self.N\\n\", \"        )\\n\", \"        T = workers.shape[1]\\n\", \"\\n\", \"        # sum over all workers to get count of \\n\", \"        # how many of the 500 workers were employed in each period\\n\", \"        employed = workers.sum(axis=0)\\n\", \"\\n\", \"        # put into dataframe with columns expected by the plot function\\n\", \"        return (\\n\", \"            pd.DataFrame(dict(\\n\", \"                months_from=np.arange(T),\\n\", \"                employed_pct=employed/self.N,\\n\", \"                unemployed_pct=1 - employed/self.N,\\n\", \"            )).assign(recession=\\\"model\\\")\\n\", \"        )\\n\", \"    \\n\", \"    def simulate_panel_fixed_alpha_beta(\\n\", \"            self,\\n\", \"            alpha_0: float, \\n\", \"            beta_0: float, \\n\", \"            T:int = 72,\\n\", \"        ) -> pd.DataFrame:\\n\", \"        \\\"\\\"\\\"\\n\", \"        Helper function to simulate Lake model when alpha and beta\\n\", \"        are constant\\n\", \"\\n\", \"        Parameters\\n\", \"        ----------\\n\", \"        alpha: float\\n\", \"            The probability that an individual goes from\\n\", \"            unemployed to employed\\n\", \"        beta: float\\n\", \"            The probability that an individual goes from\\n\", \"            employed to unemployed\\n\", \"        T: int\\n\", \"            The number of periods for which to simulate. by default this \\n\", \"            is 60 (to match length of 'gr' subset of `df_bls`)\\n\", \"\\n\", \"        Returns\\n\", \"        -------\\n\", \"        df: pd.DataFrame\\n\", \"            A pandas DataFrame in the same form as the `df_bls` DataFrame\\n\", \"            from above. In particular, it has columns \\n\", \"            [\\\"months_from\\\", \\\"employed_pct\\\", \\\"unemployed_pct\\\", \\\"recession\\\"]\\n\", \"\\n\", \"        See Also\\n\", \"        --------\\n\", \"\\n\", \"        See also the method `simulate_employment_panel_df`\\n\", \"        \\\"\\\"\\\"\\n\", \"        alpha_vec = np.ones(T) * alpha_0\\n\", \"        beta_vec = np.ones(T) * beta_0\\n\", \"        return self.simulate_panel(alpha_vec, beta_vec)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now let's try out this class with reasonable \\\"normal time\\\" parameters for $\\\\alpha$ and $\\\\beta$\\n\", \"\\n\", \"We'll call these normal-time parameters $\\\\bar{\\\\alpha} = 0.37$ and $\\\\bar{\\\\beta} = 0.01$ and refer to them as the steady state parameters\\n\", \"\\n\", \"These values were estimated using CPS data in an earlier lecture\\n\", \"\\n\", \"Also note that we'll start with $s_0 = \\\\bar{s} = [0.0237, 0.9763]$, which are implied steady state percentages of unemployed and employed workers (also estimated previously using CPS data)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"alpha_bar = 0.37\\n\", \"beta_bar = 0.01\\n\", \"s_bar = np.array([0.0237, 0.9763])\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"lm = LakeModelSimulator(s_0=np.array([0.2, 0.8]), N=150000)\\n\", \"df_sim = lm.combine_with_bls_data(\\n\", \"    lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar)\\n\", \") \\n\", \"plot_recession_data(df_sim);\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Visually, we can see that the steady state version of this model does not match the features of either recession, which are:\\n\", \"\\n\", \"- The COVID recession has an extremely sharp movement in the employment distribution\\n\", \"- The great recession featured a prolonged departure from pre-crisis levels\\n\", \"\\n\", \"Our goal in this lecture is to discover deviations from $\\\\alpha_t = \\\\bar{\\\\alpha}, \\\\beta_t = \\\\bar{\\\\beta}\\\\; \\\\forall t$ that will allow the Lake model to do a better job of describing  times of stress in the US labor market\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Loss functions\\n\", \"\\n\", \"Before looking at different assumptions about $\\\\{\\\\alpha_t, \\\\beta_t \\\\}$, we will first posit a **loss function**\\n\", \"\\n\", \"A loss function is a deterministic function that measures the difference between a model's output and its target\\n\", \"\\n\", \"The loss function is a key component to any statistical optimization routine and is heavily used  machine learning\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We will use a very common loss function called the mean-squared-error loss function, or MSE\\n\", \"\\n\", \"Given a sequence of targets and model outputs $\\\\{y_i, \\\\hat{y}_i\\\\}_{i=1}^N$ the MSE is defined as\\n\", \"\\n\", \"$$MSE(y, \\\\hat{y}) = \\\\frac{1}{N} \\\\sum_{i=1}^N (y_i - \\\\hat{y}_i)^2$$\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"For our current exercise, the model output will be the time series of percentage of workers that are employed\\n\", \"\\n\", \"The target will be the correspoinding time-series from the BLS\\n\", \"\\n\", \"We will compute the loss function indpendently for the great recession and COVID recession times\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def compute_mse(df):\\n\", \"    \\\"\\\"\\\"\\n\", \"    Given a DataFrame with columns [\\\"months_from\\\", \\\"recession\\\", and \\\"employed_pct\\\"],\\n\", \"    compute the MSE between model output and both great recession and COVID era\\n\", \"    unemployment data\\n\", \"    \\n\", \"    Parameters\\n\", \"    ----------\\n\", \"    df: pandas.DataFrame\\n\", \"        A DataFrame containing the unemployment BLS data and model output\\n\", \"    \\n\", \"    Returns\\n\", \"    -------\\n\", \"    mse: pandas.Series\\n\", \"        A pandas Series with `mse_gr` and `mse_covid` representing the MSE between\\n\", \"        model output and great recession data and COVID era data, repsectively\\n\", \"    \\\"\\\"\\\"\\n\", \"    # set index so pandas will align data for us. Also multiply by 100 to move into\\n\", \"    # percentage units. This rescaling will make the MSE larger and easier to compare\\n\", \"    # across models\\n\", \"    _df = df.pivot(index=\\\"months_from\\\", columns=\\\"recession\\\", values=\\\"employed_pct\\\") * 100\\n\", \"    return pd.Series(dict(\\n\", \"        mse_gr=_df.eval(\\\"(model - gr)**2\\\").mean(),\\n\", \"        mse_covid=_df.eval(\\\"(model - covid)**2\\\").mean()\\n\", \"    ))\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"compute_mse(df_sim)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Using the loss function will allow us to move from qualitative to quantitative statements about the goodness of fit\\n\", \"\\n\", \"For example, instead of saying \\n\", \"\\n\", \"> Model 2 appears to do better than model 1 at matching the spike in COVID era unemployment\\n\", \"\\n\", \"we could say \\n\", \"\\n\", \"> The `mse_covid` for model 2 is 5.1 relative to 11.5 for model 1\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Model 1: one time shock to $s_t$\\n\", \"\\n\", \"Consider the hypothesis that the COVID-19 spike in unemployment was only a shift of the employment/unemployment distribution ($s_0$), but didn't actually cause changes to the job creation ($\\\\alpha$) or separation ($\\\\beta$) rates\\n\", \"\\n\", \"In term of our model parameters, this means\\n\", \"\\n\", \"- $\\\\alpha_t = \\\\bar{\\\\alpha}, \\\\beta_t = \\\\bar{\\\\beta}\\\\; \\\\forall t$\\n\", \"- $s_0 = \\\\bar{s}$, $s_t = \\\\text{\\\"shocked\\\" } s$, where $t$ is date of shock\\n\", \"- $s_{\\\\tau}$ with $\\\\tau > t$ is dictated by $\\\\bar{\\\\alpha}$ and $\\\\bar{\\\\beta}$\\n\", \"\\n\", \"> Note: for $\\\\tau >> t$ we will have $s_{\\\\tau} = \\\\bar{s}$ again. This is what is meant by **steady state**, i.e.,  a place of rest according to the internal dynamics of the model. \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We have all the tools we need  to  inspect this hypothesis visually!\\n\", \"\\n\", \"We'll follow these steps:\\n\", \"\\n\", \"\\n\", \"1. Set $s_0 = \\\\bar{s}$\\n\", \"1. Set $\\\\alpha_t = \\\\bar{\\\\alpha}, \\\\beta_t = \\\\bar{\\\\beta}\\\\; \\\\forall t$\\n\", \"1. Simulate for first 35 periods (for $t = -35, \\\\dots -1$)\\n\", \"1. At $t = 0$ set $s_t = [0.14, 0.86]$ which is approximately the peak level of unemployment in spring 2020 due to COVID-19\\n\", \"1. Simulate for $t = 0, \\\\dots, 36$ starting from that point\\n\", \"1. Combine the 2 halfs of the simulation and plot\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def simulate_single_shock(\\n\", \"        lm: LakeModelSimulator, \\n\", \"        s_t: np.ndarray=np.array([0.14, 0.86])\\n\", \"    ):\\n\", \"    # Step 1 -- make sure s_0 is s_bar\\n\", \"    lm.s_0 = s_bar\\n\", \"    \\n\", \"    # Step 2 and 3 -- simulate for first 35 periods\\n\", \"    df_before = lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar, T=34)\\n\", \"    \\n\", \"    # Step 4 -- \\\"shock\\\" s_t. Note we do this by changing s_0\\n\", \"    # we'll adjust for the difference between timing `0` and `t` after\\n\", \"    # simulation\\n\", \"    lm.s_0 = s_t\\n\", \"    \\n\", \"    # Step 5 -- simulate for remainder of 35 periods\\n\", \"    df_after = lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar, T=35)\\n\", \"    df_after[\\\"months_from\\\"] += 35\\n\", \"    \\n\", \"    # Step 6 -- combine simulation 1/2s\\n\", \"    out = pd.concat([df_before, df_after], ignore_index=True)  \\n\", \"    return lm.combine_with_bls_data(out)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df_shock = simulate_single_shock(lm)\\n\", \"fig = plot_recession_data(df_shock);\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Some things to notice:\\n\", \"\\n\", \"- Up until the period after the shock, the model and COVID lines line up very closely\\n\", \"- The model seems to recover even faster than the data suggests the COVID recovery is happening\\n\", \"- The model recovers *much* faster than the great recession recovery, which says more about the difference between teh two recessions than anything else\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The loss function for this model is\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"compute_mse(df_shock)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"As we might have expected, `mse_covid < mse_gr` because we forced the model to match the most extreme value of the COVID era recession\"], \"metadata\": {}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Model 2: Single shock to $s_t$ and temporary shifts in $\\\\alpha$, $\\\\beta$\\n\", \"\\n\", \"We just learned that our model is recovering even faster than actual COVID-era data\\n\", \"\\n\", \"Perhaps the assumption that $\\\\alpha$ and $\\\\beta$ reamined constant is too strong...\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's now relax that assumption, but in a disciplined way\\n\", \"\\n\", \"We will assume that until the $t=0$ shock, $\\\\alpha_t$ and $\\\\beta_t$ are at their steady-state levels\\n\", \"\\n\", \"Then, we assume that $s_t$ immediately jumps to the spiked COVID levels\\n\", \"\\n\", \"Finally, we assume that  starting at zero and continuing temporarily for $N$ (a finite number of) periods, $\\\\alpha$ and $\\\\beta$ move to new values $\\\\hat{\\\\alpha}$ and $\\\\hat{\\\\beta}$\\n\", \"\\n\", \"After  $N$ periods, $\\\\alpha$ and $\\\\beta$ return to $\\\\bar{\\\\alpha}$ and $\\\\bar{\\\\beta}$\\n\", \"\\n\", \"Mathematically, these assumptions can be expressed as\\n\", \"\\n\", \"\\\\begin{align*}\\n\", \"\\\\alpha_t &= \\\\begin{cases}  \\\\hat{\\\\alpha} & 0 \\\\le t \\\\le N \\\\\\\\ \\\\bar{\\\\alpha} & \\\\; \\\\text{ else } \\\\end{cases} \\\\\\\\\\n\", \"\\\\beta_t &= \\\\begin{cases} \\\\hat{\\\\beta} & 0 \\\\le t \\\\le N \\\\\\\\ \\\\bar{\\\\beta} & \\\\; \\\\text{ else } \\\\end{cases} \\\\\\\\\\n\", \"\\\\end{align*}\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def simulate_alpha_beta_shock(\\n\", \"        lm: LakeModelSimulator,\\n\", \"        alpha_hat: float, \\n\", \"        beta_hat: float, \\n\", \"        N: int,\\n\", \"        s_t: np.ndarray=np.array([0.14, 0.86])\\n\", \"    ):\\n\", \"    # pre t=0 remains the same...\\n\", \"    lm.s_0 = s_bar\\n\", \"    df_before = lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar, T=34)\\n\", \"    \\n\", \"    # still shock at t=0\\n\", \"    lm.s_0 = s_t\\n\", \"    \\n\", \"    # prepare alpha_vec and beta_vec for t >=0 simulation\\n\", \"    alpha_vec = np.ones(35) * alpha_bar\\n\", \"    alpha_vec[:N] = alpha_hat\\n\", \"    \\n\", \"    beta_vec = np.ones(35) * beta_bar\\n\", \"    beta_vec[:N] = beta_hat\\n\", \"    \\n\", \"    # simulate using vector of alpha and beta\\n\", \"    df_after = lm.simulate_panel(alpha_vec, beta_vec)\\n\", \"    df_after[\\\"months_from\\\"] += 35\\n\", \"    \\n\", \"    # combine simulation 1/2 and return\\n\", \"    out = pd.concat([df_before, df_after], ignore_index=True)\\n\", \"    return lm.combine_with_bls_data(out)\\n\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Now that we have this routine, let's test a version of the model where the job finding rate ($\\\\alpha$) falls from 0.37 to 0.28  and the job separation rate ($\\\\beta$) rises from 0.01 to 0.05\\n\", \"\\n\", \"We'll suppose that this lasts for 3 periods\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_alpha_beta = simulate_alpha_beta_shock(lm, 0.28, 0.05, 3)\\n\", \"plot_recession_data(df_alpha_beta);\\n\", \"compute_mse(df_alpha_beta)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Our initial test values of $\\\\hat{\\\\alpha}$, $\\\\hat{\\\\beta}$, and $N$ did improve the `mse_covid` from about 4.0 to about 2.6\\n\", \"\\n\", \"We chose these values informally (only using our intuition about what might have happened at the peak of the COVID era spike)\\n\", \"\\n\", \"There are other values of these parameters that could have been better (lower `mse_covid`)\\n\", \"\\n\", \"Let's assume the role of the \\\"visual econometrician\\\" and approximate superior values visually\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"from ipywidgets import interact, FloatSlider\\n\", \"\\n\", \"def explore_alpha_beta_shock(alpha_hat: float, beta_hat:float , N: int):\\n\", \"    df_alpha_beta = simulate_alpha_beta_shock(lm, alpha_hat, beta_hat, N)\\n\", \"    plot_recession_data(df_alpha_beta);\\n\", \"    return compute_mse(df_alpha_beta)\\n\", \"\\n\", \"interact(\\n\", \"    explore_alpha_beta_shock, \\n\", \"    alpha_hat=FloatSlider(value=0.28, min=0.2, max=0.5, step=0.025),\\n\", \"    beta_hat=FloatSlider(value=0.05, min=0.01, max=0.2, step=0.01),\\n\", \"    N=3\\n\", \")\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"In our experimenting, we found the following values to trace the \\\"COVID recovery\\\" quite well\\n\", \"\\n\", \"- $\\\\hat{\\\\alpha} = 0.25$\\n\", \"- $\\\\hat{\\\\beta} = 0.02$\\n\", \"- $N = 4$\\n\", \"\\n\", \"The `mse_covid` fell to about 1.7\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Model 3: shock to $\\\\alpha_{t-1}$ and/or $\\\\beta_{t-1}$\\n\", \"\\n\", \"In our experiments thus far, we have moved $s_0$ by hand\\n\", \"\\n\", \"We might want to answer the question, \\\"What change in $\\\\alpha$ and $\\\\beta$ at period t-1 could have generated the drop in employment seen at time 0?\\\"\\n\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### $\\\\alpha_{-1}$ or $\\\\beta_{-1}$?\\n\", \"\\n\", \"In principle, either a decrease in the job finding rate or an increase in the job separation rate could lead to a decrease in the percentage of employed workers\\n\", \"\\n\", \"Let's look specifically at the BLS data to for COVID era `months_from` -1 and 0 to identify the magnitude of the change in employment status\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_bls[[\\\"months_from\\\", \\\"recession\\\", \\\"employed_pct\\\"]].query(\\\"months_from in [-1, 0] and recession=='covid'\\\")\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"#### Attempt 1: change $\\\\alpha_{-1}$\\n\", \"\\n\", \"Recall the steady-state job finding rate $\\\\bar{\\\\alpha}=0.37$ and job separation rate $\\\\bar{\\\\beta} = 0.01$\\n\", \"\\n\", \"Suppose we believe that the change the employment data was driven entirely by a drop in the finding rate\\n\", \"\\n\", \"Let's set $\\\\alpha_{-1}$ to 0 and see what impact that has on $s_0$:\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# number of employed workers at \\\"m\\\"inus 1\\n\", \"emp_m = 0.955\\n\", \"\\n\", \"# move forward this percentage one period -- setting alpha to 0\\n\", \"emp_0_alpha = emp_m * (1-beta_bar) + 0 * (1-emp_m)\\n\", \"\\n\", \"print(\\\"emp_0 when beta = beta_bar and alpha = 0:\\\", emp_0_alpha)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This is far greater than the 0.855 from the BLS data\\n\", \"\\n\", \"This shows that within the context of our model, the hypothesis that the drop in employment could not have come entirely from the job finding rate\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"#### Attempt 2: Change $\\\\beta_{-1}$\\n\", \"\\n\", \"Let's now consider a change in the job separation rate $\\\\beta_{-1}$\\n\", \"\\n\", \"Suppose we set this to 1 and keep $\\\\alpha_{-1} = \\\\bar{\\\\alpha}$\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"# move forward this percentage one period -- setting alpha to 0\\n\", \"emp_0_beta = emp_m * (1-1) + alpha_bar * (1-emp_m)\\n\", \"\\n\", \"print(\\\"emp_0 when beta_{-1} = 0 and alpha_{-1} = alpha_bar:\\\", emp_0_beta)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This extreme value in the job separation rate puts time 0 employment down at 1.7%\\n\", \"\\n\", \"This validates that it is *possible* (again within the context of our model) for a change in the job separation rate to be consistent with the BLS data\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's now answer the question: \\\"Keeping $\\\\alpha_{-1} = \\\\bar{\\\\alpha}$, what value of $\\\\beta_{-1}$ could generate a shift from \\n\", \"0.954 of workers employed to 0.8555 workers employed?\\\"\\n\", \"\\n\", \"We'll call this value $\\\\tilde{\\\\beta}$ \"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"We can re-arrange the law of motion for the fraction of employed workers to solve for $\\\\tilde{\\\\beta}$:\\n\", \"\\n\", \"$$\\\\tilde{\\\\beta} = 1 - \\\\frac{e_0 - \\\\bar{\\\\alpha}(1-e_{-1})}{e_{-1}},$$\\n\", \"\\n\", \"Where $e_t$ represents fraction of workers employed at time $t$\\n\", \"\\n\", \"Let's compute this\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"emp_0 = 0.8556\\n\", \"beta_tilde = 1 - (emp_0 - alpha_bar*(1 - emp_m)) / emp_m\\n\", \"print(\\\"beta_tilde is: \\\", beta_tilde)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This suggests that in order for our model to generate the drop in employment observed in the COVID era, the job separation rate would need to move from the steady state value of 0.01 to 0.12\\n\", \"\\n\", \"Thus, 12% of workers lost their jobs\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"fragment\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Let's now simulate our model where we assume a one time shock to $\\\\beta_{-1}$ and then an immediate return to $\\\\bar{\\\\beta}$\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def simulate_one_time_beta_shock(\\n\", \"        lm: LakeModelSimulator, \\n\", \"        beta_tilde: float\\n\", \"    ):\\n\", \"    # create vectors of alpha and beta\\n\", \"    lm.s_0 = s_bar\\n\", \"    alpha_vec = np.ones(71) * alpha_bar\\n\", \"    beta_vec = np.ones(71) * beta_bar\\n\", \"    \\n\", \"    # move only beta_{t-1} to beta_tilde\\n\", \"    beta_vec[34] = beta_tilde\\n\", \"    \\n\", \"    # simulate\\n\", \"    out = lm.simulate_panel(alpha_vec, beta_vec)\\n\", \"    return lm.combine_with_bls_data(out)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"df_beta_shock = simulate_one_time_beta_shock(lm, beta_tilde)\\n\", \"plot_recession_data(df_beta_shock)\\n\", \"compute_mse(df_beta_shock)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"This looks very similar to the original model in which supposed a one time shock to $s_0$...why?\\n\", \"\\n\", \"We found the value of $\\\\beta_{-1}$ that caused $s_0$ to equal the shocked value \\n\", \"\\n\", \"Then, we kept $\\\\beta_t$ ($t \\\\not= -1$) and $\\\\alpha_t$ equal to their steady-state values\\n\", \"\\n\", \"We allowed the model to *generate* the change in $s_0$ instead of imposing it\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"If the model outputs are the same, why could this be a desirable outcome?\\n\", \"\\n\", \"Becuase the change in $s_0$ is coming from within the model, we can now do experiments like:\\n\", \"\\n\", \"- Suppose the government could have saved 1/3 of the lost jobs in time 0. What impact would that have had on $s_t$, $t >=0$\\n\", \"\\n\", \"In this sense we are able to use the model as a vehicle for doing counterfactual exercises\\n\", \"\\n\", \"This idea captures the general spirit of computational social science: we build models that can capture a feature(s) of actual data we'd like to better understand so that we can analyze the impact of decisions on these variables of interest\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"## Model 4: Shock $\\\\beta_{-1}$ and $\\\\alpha_{t}$, $\\\\beta_{t}$\\n\", \"\\n\", \"In model 3 we were able to replicate the spike in COVID era unemployment\\n\", \"\\n\", \"In model 2 (where we moved $\\\\alpha_t$ and $\\\\beta_{t}\\\\; t = 0, \\\\dots N$) we were able to replicate the first few periods of the recovery\\n\", \"\\n\", \"Let's combine these two to have a single model that can match the COVID era labor market dynamics\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def shock_beta_m_and_alpha_beta(\\n\", \"        lm: LakeModelSimulator,\\n\", \"        beta_tilde: float,\\n\", \"        alpha_hat: float, \\n\", \"        beta_hat: float, \\n\", \"        N: int,\\n\", \"    ):\\n\", \"    # create vectors of alpha and beta\\n\", \"    lm.s_0 = s_bar\\n\", \"    alpha_vec = np.ones(71) * alpha_bar\\n\", \"    beta_vec = np.ones(71) * beta_bar\\n\", \"    \\n\", \"    # move beta_{t-1} to beta_tilde\\n\", \"    beta_vec[34] = beta_tilde\\n\", \"    \\n\", \"    # move alpha_0 to alpha_N to alpha_hat\\n\", \"    alpha_vec[35:(35+N)] = alpha_hat\\n\", \"    \\n\", \"    # same for beta\\n\", \"    beta_vec[35:(35+N)] = beta_hat\\n\", \"    \\n\", \"    out = lm.simulate_panel(alpha_vec, beta_vec)\\n\", \"    return lm.combine_with_bls_data(out)\\n\", \"    \"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"We'll use the values found above: \\n\", \"\\n\", \"\\\\begin{align*}\\n\", \"\\\\tilde{\\\\beta} &= 0.12 \\\\\\\\\\n\", \"\\\\hat{\\\\beta} &= 0.02 \\\\\\\\ \\n\", \"\\\\hat{\\\\alpha} &= 0.25 \\\\\\\\\\n\", \"N &= 4\\n\", \"\\\\end{align*}\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"df_betam_and_alpha_beta = shock_beta_m_and_alpha_beta(lm, beta_tilde, 0.25, 0.02, 4)\\n\", \"plot_recession_data(df_betam_and_alpha_beta)\\n\", \"compute_mse(df_betam_and_alpha_beta)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"## Model 5: Time varying $\\\\alpha_t$ and $\\\\beta_t$\\n\", \"\\n\", \"Our analysis has focused on models where $\\\\alpha$ and $\\\\beta$ are at their steady state values, except for a few periods\\n\", \"\\n\", \"This may be reasonable for the very sharp COVID era activity, but less reasonable for the great recession era\\n\", \"\\n\", \"The great recession featured a prolonged, gradual increase in unemployment and then a slow decline\\n\", \"\\n\", \"In order for our model to achieve something like this, we would need to allow $\\\\alpha_t$ and $\\\\beta_t$ to be different from their steady state values for multiple periods\\n\", \"\\n\", \"Let's now turn to the question of \\\"What values of $\\\\alpha_t$ and $\\\\beta_t$, for $t \\\\in [-35, 35]$ could generate great recession era dynamics?\\\"\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"The `df_bls` DataFrame has columns `ee`, `eu`, `ue`, and `uu`\\n\", \"\\n\", \"These represent rates of individuals moving between employment and unemployment status (based on first letter of each word)\\n\", \"\\n\", \"The `ue` and `eu` map very closely into our $\\\\alpha$ and $\\\\beta$\\n\", \"\\n\", \"We'll treat them as a time series of $\\\\alpha_t$ and $\\\\beta_t$, respectively as we simulate our Lake model\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"def simulate_great_recession(\\n\", \"        lm: LakeModelSimulator,\\n\", \"        df_bls: pd.DataFrame,\\n\", \"        s_init: np.ndarray,\\n\", \"    ):\\n\", \"    df_gr = df_bls.query(\\\"recession == 'gr'\\\").sort_values(\\\"months_from\\\")\\n\", \"    alpha_vec = df_gr[\\\"ue\\\"].to_numpy()\\n\", \"    beta_vec = df_gr[\\\"eu\\\"].to_numpy()\\n\", \"    lm.s_0 = s_init\\n\", \"    out = lm.simulate_panel(alpha_vec, beta_vec)\\n\", \"    return lm.combine_with_bls_data(out)\"], \"outputs\": [], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"# set initial state for great recession times\\n\", \"s_init_gr = [0.05, 0.95]\\n\", \"df_model_gr = simulate_great_recession(lm, df_bls, s_init_gr)\\n\", \"plot_recession_data(df_model_gr)\\n\", \"compute_mse(df_model_gr)\"], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}, {\"source\": [\"Notice now that our model can track the labor market dynamics in teh great recession era quite well; `mse_gr` is down to 0.51\\n\", \"\\n\", \"However the `mse_covid` has risen to 9.05\\n\", \"\\n\", \"This illustrates a common curse (and blessing!) to all modeling exercsies: modeling decisions comes with tradeoffs\\n\", \"\\n\", \"Our two state markov chain view of labor market fluctuations cannot match both the great recession era and the COVID era\\n\", \"\\n\", \"The blessing in disguise is\\n\", \"\\n\", \"- We must be delibrate in specifying our modeling goals: do we want to match great recession era dynamics or COVID era dynamics?\\n\", \"- We can afford to use simpler models:  we want the simplest model that allows us to achieve the modeling goal\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"### Looking ahead\\n\", \"\\n\", \"Why go through the trouble of building models?\\n\", \"\\n\", \"As social scientists, we rarely have the luxury of setting up controlled experiments\\n\", \"\\n\", \"The nature of our subject of study -- society and people -- does not allow for easy, controlled testing of hypothesis\\n\", \"\\n\", \"So, in order to employ the scientific method, we must create for ourselves a laboratory\\n\", \"\\n\", \"We use laboratories constructed of mathematical equation and statistical structure\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"slide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"A model allows us to consider \\\"what if\\\" scenarios, often called counterfactuals\\n\", \"\\n\", \"In these scenarios we can evaluate the tradeoffs of potential policy decisions\\n\", \"\\n\", \"For example, our Lake model could be the labor market piece of a larger economic framework\\n\", \"\\n\", \"This larger framework could include \\n\", \"\\n\", \"- Household decisions like saving vs. spending\\n\", \"- A government that must raise taxes and enact policies for labor market relief\\n\", \"- International trading partners that can increase diversity of goods and smooth out labor shocks\\n\", \"\\n\", \"Building up a rich framework by connecting well-understood pieces allows us to make a change in one component (e.g. government spending to subsidise labor market) and see the corresponding impact on other parts of the economy (e.g. a lower separation rate and increased household consumption/welfare)\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}, {\"source\": [\"Throughout this course we will put our data, programming, math, modeling tools to work to build models and perform experiments like this\"], \"metadata\": {\"slideshow\": {\"slide_type\": \"subslide\"}}, \"cell_type\": \"markdown\"}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"celltoolbar\": \"Slideshow\", \"interpreter\": {\"hash\": \"06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 4}",
    "md_content": null,
    "properties": "{\"fileName\": \"modeling_decisions.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.789Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 11,
    "position": 11,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 12,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 12,
    "description": "Lake Model Meets Data",
    "title": "Lecture 12",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 130,
    "position": 9,
    "content_id": 160,
    "lecture_id": 12,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 160,
    "type": "video",
    "description": "Empirically estimate the Lake Model using data from the BLS",
    "title": "Lake Model Meets Data",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=mc0StBqQkLY\", \"youtubeVideoId\": \"mc0StBqQkLY\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 18,
    "position": 12,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 19,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 19,
    "description": "Sharing your work with others",
    "title": "Lecture 13",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 132,
    "position": 9,
    "content_id": 176,
    "lecture_id": 19,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 176,
    "type": "video",
    "description": "Learn how to share your work with others using various tools from the PyData ecosystem",
    "title": "Sharing Your Work",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=oc2a1pni8P0\", \"youtubeVideoId\": \"oc2a1pni8P0\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 18,
    "position": 12,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 19,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 19,
    "description": "Sharing your work with others",
    "title": "Lecture 13",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 133,
    "position": 9,
    "content_id": 183,
    "lecture_id": 19,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 183,
    "type": "notebook",
    "description": "Practice using the tools from PyData ecosystem to share your results",
    "title": "sharing-results.ipynb",
    "nb_content": "{\"cells\": [{\"id\": \"8a1f2b88\", \"source\": [\"# Sharing results\\n\", \"\\n\", \"## Intro\\n\", \"\\n\", \"-   As a researchers and data analysts, we are knowledge workers\\n\", \"-   A knowledge worker is a worker whose main capital is knowledge\\n\", \"-   A primary responsibility of knowledge workers (especially those who\\n\", \"    work with data) is to use information to assist decision making\\n\", \"    processes\\n\", \"-   This includes both performing analysis and communicating results\\n\", \"\\n\", \"### Communicating results\\n\", \"\\n\", \"-   Being able to effectively communicate and share the output of our\\n\", \"    work is essential\\n\", \"-   Without effective communication our work will have minimal impact\\n\", \"-   To communicate effectively we need to decide:\\n\", \"    -   Audience\\n\", \"    -   Content\\n\", \"    -   Medium/technology\\n\", \"\\n\", \"### Example: this class\\n\", \"\\n\", \"-   We have made a decision to distribute the materials for this course\\n\", \"    as Jupyter notebooks\\n\", \"-   Why?\\n\", \"    -   Audience: fellow data workers\\n\", \"    -   Content: text, code, results\\n\", \"    -   Medium/technology: Jupyter notebook via cocalc\\n\", \"\\n\", \"## Sharing Jupyter notebooks\\n\", \"\\n\", \"-   There are many options for sharing Jupyter notebooks\\n\", \"-   We will cover a few of them here and offer a suggestion for audience\\n\", \"    and content that would make each decision suitable\\n\", \"-   We will also walk through an example of how to share a notebook with\\n\", \"    each technology\\n\", \"\\n\", \"### Github\\n\", \"\\n\", \"-   GitHub is a web service that provides remote git repositories and\\n\", \"    collaboration/sharing features\\n\", \"-   GitHub is home to millions of open source projects\\n\", \"-   When viewed from a web browser, files on GitHub are rendered nicely\\n\", \"-   This includes syntax highlighting for code, images, pdfs, and\\n\", \"    static+rendered Jupyter notebooks\\n\", \"\\n\", \"1.  Why Share Jupyter Notebooks on GitHub\\n\", \"\\n\", \"    There are two main reasons to share on GitHub\\n\", \"\\n\", \"    1.  Collaborate with others:\\n\", \"        -   Audience: Colleagues, co-authors\\n\", \"        -   Content: Research code, documentation, examples, or\\n\", \"            experiments\\n\", \"    2.  Share publicly\\n\", \"        -   Audience: Public\\n\", \"        -   Content: reports, appendixes\\n\", \"\\n\", \"2.  Example: Sharing on GitHub\\n\", \"\\n\", \"    -   Let's do an example of sharing on GitHub\\n\", \"    -   We'll share the notebook `v01_statistics_2.ipynb`\\n\", \"\\n\", \"### QuantEcon notes\\n\", \"\\n\", \"-   [QuantEcon Notes](https://notes.quantecon.org/) is a free online\\n\", \"    service from QuantEcon\\n\", \"-   The purpose of Notes is to be a library of high-quality notebooks on\\n\", \"    topics related to economics\\n\", \"-   Notebooks on Notes can execute javascript, have embedded javascript\\n\", \"    plots (e.g. altair or plotly)\\n\", \"-   Other Notes users can give a thumbs up or down to a notebooks\\n\", \"-   Notes supports commenting on notebooks to facilitate discussion\\n\", \"\\n\", \"1.  Why Share on QuantEcon Notes\\n\", \"\\n\", \"    -   Audience: public, research community\\n\", \"    -   Content: Academic content with relation to computation or\\n\", \"        economics\\n\", \"\\n\", \"2.  Example: Sharing on Notes\\n\", \"\\n\", \"    -   Again, we'll share the contents of `v01_statistics_2.ipynb`,\\n\", \"        this time on QuantEcon Notes\\n\", \"\\n\", \"### nbconvert\\n\", \"\\n\", \"-   `nbconvert` is a tool from the jupyter team to convert/export\\n\", \"    notebooks to other files types\\n\", \"-   `nbconvert` is a `pip` installable python package that must be\\n\", \"    installed and run on your local computer\\n\", \"-   `nbconvert` can convert to many file types, like pdf, html,\\n\", \"    slideshows, latex, and more\\n\", \"\\n\", \"1.  Why Share with `nbconvert`\\n\", \"\\n\", \"    -   Audience: public or private – need static, single-file version\\n\", \"        of your work\\n\", \"    -   Content: books, reports, documents created in Jupyter\\n\", \"\\n\", \"2.  Example: Sharing with `nbconvert`\\n\", \"\\n\", \"    -   We'll now use `nbconvert` to share `v01_statistics_2.ipynb` as…\\n\", \"\\n\", \"        -   A Python script\\n\", \"        -   An html page\\n\", \"        -   A pdf\\n\", \"        -   HTML slides\\n\", \"\\n\", \"    -   In each case we'll use a command that looks like this:\\n\", \"\\n\", \"        ``` bash\\n\", \"        jupyter nbconvert v01_statistics_2.ipynb --to XXX\\n\", \"        ```\\n\", \"\\n\", \"        where `XXX` represents the format we would like to generate\\n\", \"\\n\", \"## Interactive Sharing\\n\", \"\\n\", \"-   As Python programmers, we have many options for sharing our analysis\\n\", \"-   Some of these options are static, like posting code or Jupyter\\n\", \"    notebooks to GitHub, QuantEcon Notes, or some other web service\\n\", \"-   Others are interactive, where the users interacting with our content\\n\", \"    have the opportunity to provide input, which will trigger our code\\n\", \"    to run and return new outputs\\n\", \"\\n\", \"### Methods of Interactive Sharing\\n\", \"\\n\", \"-   There are many options for interactive sharing of analysis:\\n\", \"    -   Publishing Python package to [PyPi](https://pypi.org/) (so it\\n\", \"        can be `pip` installable)\\n\", \"        -   Audience: other programmers\\n\", \"        -   Content: source ocde\\n\", \"    -   Web APIs: create a server that can listen for requests and\\n\", \"        return responses\\n\", \"        -   Audience: other programmers, data consumers\\n\", \"        -   Content: data or images produced by code\\n\", \"    -   Interactive applications: our code as a website for users to\\n\", \"        interact with\\n\", \"        -   Audience: consumers of our analysis\\n\", \"        -   Content: data or images produced by our code\\n\", \"\\n\", \"### Voila\\n\", \"\\n\", \"-   Voila is a python package for exposing a Jupyter notebook as a web\\n\", \"    service\\n\", \"-   Voila renders notebook content as a website `and` connects to a\\n\", \"    Python process for handling user input\\n\", \"-   Tightly integrated into Jupyter ecosystem, includes ability to use\\n\", \"    jupyter widgets from webpage\\n\", \"\\n\", \"1.  Why Share with Viola?\\n\", \"\\n\", \"    -   Audience: public, co-authors, colleagues\\n\", \"    -   Content: interactive Jupyter notebooks\\n\", \"\\n\", \"2.  Example of Sharing with Viola\\n\", \"\\n\", \"    -   Let's now turn `v01_statistics_2.ipynb` into an interactive web\\n\", \"        app using Voila\\n\", \"\\n\", \"    -   To do this we'll use the following command:\\n\", \"\\n\", \"        ``` bash\\n\", \"        voila v01_statistics_2.ipynb\\n\", \"        ```\\n\", \"\\n\", \"### Streamlit\\n\", \"\\n\", \"-   Streamlit is a Python library that translates Python scripts into\\n\", \"    interactive websites\\n\", \"-   Requires zero html, javascript, or css to create beautiful,\\n\", \"    interactive dashboards\\n\", \"\\n\", \"1.  Why Share With Streamlit?\\n\", \"\\n\", \"    -   Audience: public\\n\", \"    -   Content: interactive Python scripts\\n\", \"\\n\", \"2.  Example of Sharing with Streamlit\\n\", \"\\n\", \"    -   We'll turn some of the codes from `v01_statistics_2.ipynb` into\\n\", \"        a Python script and then create a Streamlit application\\n\", \"\\n\", \"## Summary\\n\", \"\\n\", \"-   Communication is essential for having an impact as a knowledge\\n\", \"    worker\\n\", \"-   Effective communication requires an understanding of audience and\\n\", \"    content, and the ability to choose the correct medium and technology\\n\", \"    for distribution\\n\", \"-   There are many static ways to share Jupyter notebooks: GitHub,\\n\", \"    QuantEcon notes, `nbconvert`\\n\", \"-   There are also many interactive ways to share our Python-based\\n\", \"    analysis: publish to PyPi, create an API, make an interactive\\n\", \"    application (voila/streamlit)\"], \"metadata\": {}, \"cell_type\": \"markdown\"}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 5}",
    "md_content": null,
    "properties": "{\"fileName\": \"sharing-results.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.789Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 8,
    "position": 13,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 9,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 9,
    "description": "Introduction to Web Scraping",
    "title": "Lecture 14",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 207,
    "position": 10,
    "content_id": 256,
    "lecture_id": 9,
    "inserted_at": "2023-09-27 10:10:58",
    "updated_at": "2023-09-27 10:10:58",
    "id": 256,
    "type": "quiz",
    "description": "test",
    "title": "test",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"id\": \"7a3d1b02-d52f-4aea-a13d-b5c318abbb74\", \"url\": null, \"link\": null, \"fileName\": null, \"n_uploads\": 0, \"lastModified\": null, \"vimeoVideoId\": null, \"youtubeVideoId\": null, \"upload_extensions\": [\".ipynb\"]}",
    "quiz_id": 35,
    "author_id": null,
    "inserted_at": "2023-09-27 10:10:58",
    "updated_at": "2023-09-27 10:10:58"
  },
  {
    "id": 8,
    "position": 13,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 9,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 9,
    "description": "Introduction to Web Scraping",
    "title": "Lecture 14",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 135,
    "position": 9,
    "content_id": 195,
    "lecture_id": 9,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 195,
    "type": "notebook",
    "description": "Practice scraping data from the web",
    "title": "web-scraping.ipynb",
    "nb_content": "{\"cells\": [{\"id\": \"b3b5d3db\", \"source\": [\"# Intro\\n\", \"\\n\", \"-   The internet is full of data\\n\", \"-   Much of our lives and livelihoods lives on the internet\\n\", \"-   The programming and theoretical skills we've developed can allow us\\n\", \"    to do meaningful analysis of internet data\\n\", \"\\n\", \"… If we can get it into Python!\\n\", \"\\n\", \"## Accessing Website Data\\n\", \"\\n\", \"-   Some data we see on websites data is easier to access:\\n\", \"    -   Provided as a downloadable file we can `pd.read_csv`\\n\", \"    -   Accessible via an API\\n\", \"    -   Contained in a clean html table for `pd.read_html`\\n\", \"-   … but some of it isn't\\n\", \"-   Today we'll learn how to access some of the data that is publicly\\n\", \"    visible, but not easy to download\\n\", \"\\n\", \"## HTML: Language of the web\\n\", \"\\n\", \"-   Web pages are written in a markup language called HTML\\n\", \"\\n\", \"-   HTML stands for \\\"hyper text markup language\\\"\\n\", \"\\n\", \"-   All HTML documents are composed of a tree of nested elements\\n\", \"\\n\", \"-   For example, this bullet point list would be written like this in\\n\", \"    HTML:\\n\", \"\\n\", \"    ``` html\\n\", \"    <ul>\\n\", \"      <li>Web pages are..</li>\\n\", \"      <li>HTML stands for...</li>\\n\", \"      <li>All HTML documents...</li>\\n\", \"      <li>For example, this...</li>\\n\", \"    </ul>\\n\", \"    ```\\n\", \"\\n\", \"### Components of HTML\\n\", \"\\n\", \"-   Below is an image that annotates the core parts of an HTML document\\n\", \"\\n\", \"![](attachment:./html_parts.png)\\n\", \"\\n\", \"-   Tag: name or type of an element\\n\", \"-   CSS classes: used to style and change appearance (we'll use it to\\n\", \"    identify specific elements!)\\n\", \"-   id: Unique identifier for element **on whole webpage**\\n\", \"-   value: `class` is one property, syntax is `property\\\\=value`\\n\", \"-   text: the actual text contained in the element\\n\", \"\\n\", \"### Structure of Webpage\\n\", \"\\n\", \"-   Most webpages follow a very common structure:\\n\", \"\\n\", \"    ``` html\\n\", \"    <!DOCTYPE HTML>\\n\", \"    <html>\\n\", \"    <head>\\n\", \"      <meta>...</meta>\\n\", \"      <title>...</title>\\n\", \"      <link>...</link>\\n\", \"    </head>\\n\", \"    <body>\\n\", \"      <h1>Title</h1>\\n\", \"      .... MANY MORE ELEMENTS HERE ...\\n\", \"    </body>\\n\", \"    </html>\\n\", \"    ```\\n\", \"\\n\", \"-   Almost all the data we will want to scrape is contained inside the\\n\", \"    `<body>` element\\n\", \"\\n\", \"### See it in Action!\\n\", \"\\n\", \"-   Let's see this in action\\n\", \"-   We'll navigate to\\n\", \"    [<http://quotes.toscrape.com/random>](http://quotes.toscrape.com/random)\\n\", \"    and use our web browser to look at the HTML\\n\", \"-   Things to look for:\\n\", \"    -   The outline from previous slide\\n\", \"    -   The use of `class`\\n\", \"    -   The hierarchy of the page\\n\", \"\\n\", \"### Multiple Quotes?\\n\", \"\\n\", \"-   Now that we are warmed up, let's look at a page with multiple quotes\\n\", \"-   Navigate to\\n\", \"    [<http://quotes.toscrape.com/>](http://quotes.toscrape.com/) and\\n\", \"    look at the source\\n\", \"-   We'll watch for the same main concepts/components\\n\", \"\\n\", \"### How to \\\"Scrape\\\"?\\n\", \"\\n\", \"-   Now the main question: \\\"How could we scrape this data?\\\"\\n\", \"-   The key to web scraping is to be able to **identify** patterns\\n\", \"-   My main strategy for doing this is to follow these steps:\\n\", \"    1.  View the webpage and identify visually the data I'd like to\\n\", \"        scrape\\n\", \"    2.  Open browser tools and \\\"inspect\\\" the element containing my data\\n\", \"    3.  Look at that element's tag, classes, id to see how I could tell\\n\", \"        a computer to identify it\\n\", \"    4.  Look outwards to other elements to scrape\\n\", \"        -   Same type of data, e.g. a price (find pattern in structure\\n\", \"            that matches original element)\\n\", \"        -   Different type of data, e.g. an average review: start\\n\", \"            process again\\n\", \"\\n\", \"# Scrapy\\n\", \"\\n\", \"-   There are many Python libraries for scraping websites\\n\", \"-   Perhaps the most widely used of these is called `scrapy`\\n\", \"-   We will use `scrapy` to extract the quote information we saw on the\\n\", \"    example websites\\n\", \"-   First step would be to install scrapy if you haven't yet:\\n\", \"    `pip install scrapy`\\n\", \"\\n\", \"## Running Scrapy\\n\", \"\\n\", \"-   Scrapy can be run in the `scrapy shell` as we just saw\\n\", \"-   However, one benefit from learning how to scrape websites is to be\\n\", \"    able to have scrapers run as programs that don't need manual\\n\", \"    interactions\\n\", \"-   Scrapy was built for this use case\\n\", \"\\n\", \"## Scrapy Project\\n\", \"\\n\", \"-   Scrapy provides a scaffold we can use to organize our web scrapers\\n\", \"-   This is called a scrapy project\\n\", \"-   We can create one by running `scrapy startproject NAME` where `NAME`\\n\", \"    is the name of our project\\n\", \"-   Let's try it!\\n\", \"\\n\", \"## Spiders\\n\", \"\\n\", \"-   We need to teach scrapy how to extract data from the web pages we\\n\", \"    have it visit\\n\", \"-   To do this we create a Spider\\n\", \"-   A spider is a class we define that has at least the following\\n\", \"    features:\\n\", \"    -   A list of websites to scrape\\n\", \"    -   A Python function for how to exract data from a single webpage\\n\", \"-   We'll create our first spider now\\n\", \"\\n\", \"## Working with Spiders\\n\", \"\\n\", \"-   We can run a spider using `scrapy crawl NAME -o OUTFILE.EXT`, where\\n\", \"    -   `NAME` is the name of the spider\\n\", \"    -   `OUTFILE.EXT` are the name and extension for storing the data\\n\", \"-   We can also have the spider continue on to another page\\n\", \"    -   To do this we need to find the next url to visit and tell scrapy\\n\", \"        to scrape it\\n\", \"    -   To scrape the next url we use the `response.follow(URL)` method\"], \"metadata\": {}, \"cell_type\": \"markdown\", \"attachments\": {}}, {\"id\": \"5ae1788d\", \"source\": [], \"outputs\": [], \"metadata\": {}, \"cell_type\": \"code\", \"execution_count\": null}], \"metadata\": {\"kernelspec\": {\"name\": \"python3\", \"language\": \"python\", \"display_name\": \"Python 3 (ipykernel)\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.9.7\", \"mimetype\": \"text/x-python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython3\", \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3}, \"nbconvert_exporter\": \"python\"}}, \"nbformat\": 4, \"nbformat_minor\": 5}",
    "md_content": null,
    "properties": "{\"fileName\": \"web-scraping.ipynb\", \"lastModified\": \"2023-07-19T04:02:08.789Z\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  },
  {
    "id": 8,
    "position": 13,
    "available_at": "2023-09-08 12:54:25",
    "due_date": null,
    "course_id": 5,
    "lecture_id": 9,
    "inserted_at": "2023-09-08 12:54:25",
    "updated_at": "2023-09-08 12:54:25",
    "id": 9,
    "description": "Introduction to Web Scraping",
    "title": "Lecture 14",
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02",
    "id": 134,
    "position": 9,
    "content_id": 179,
    "lecture_id": 9,
    "inserted_at": "2023-09-08 13:24:17",
    "updated_at": "2023-09-08 13:24:17",
    "id": 179,
    "type": "video",
    "description": "Learn the basics of web scraping",
    "title": "Web Scraping Introduction",
    "nb_content": null,
    "md_content": null,
    "properties": "{\"url\": \"https://www.youtube.com/watch?v=aYKF-qczV44\", \"youtubeVideoId\": \"aYKF-qczV44\"}",
    "quiz_id": null,
    "author_id": 1,
    "inserted_at": "2023-08-18 17:30:02",
    "updated_at": "2023-08-18 17:30:02"
  }
]