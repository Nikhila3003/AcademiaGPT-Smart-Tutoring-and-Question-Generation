# Employment-Unemployment Model

<br>
<br>

**Goal**:

Estimate a Markov chain using employment data

```python
import datetime as dt
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

%matplotlib inline
```

## Steps to "understanding the world around us" (a short digression)


Our friend Jim Savage has thought about what a "modern statistical workflow" entails. We summarize some steps he has proposed for understanding the world around us:

> 1. Prepare and visualize your data.
> 2. Create a generative model for the data...
>   - A first model should be as simple as possible.
> 3. Simulate some artificial data from your model given some assumed parameters that you "pick out of a hat" ($\theta$)
> 4. Use the artificial data to estimate the model parameters ($\hat{\theta})$
> 5. Check that you recovered a good approximation of the "known unknowns" (aka, $\theta \approx \hat{\theta}$)
>   - Possibly repeat 3-5 with different estimators and true parameters ($\theta$), to get an understanding of how well the fitting procedure works
> 6. Fit the model to your real data, check the fit
> 7. Argue about the results with your friends and colleagues
> 8. Go back to 2. with a slightly richer model. Repeat.
> 9. Think carefully about what decisions will be made from the analysis, encode a loss function, and perform statistical decision analysis... Note that in many cases, we will do step 9 before steps 1-8!

Later this semester, we will talk formally about what it means to "fit" your model (and the work that it entails), but, for now, we find it sufficient to say that it's a process to ensure that the probability distribution over outcomes generated by your model lines up with the data (aka, finding the right model parameters).

We'll do a version of steps 1-6 to help us improve our understanding of the labor data that we previously saw.

### Our plan

1. Prepare and visualize our data.
2. Develop a generative model of employment and unemployment
3. Simulate data from our generative model for given parameters
4. Fit our model to the simulated data
5. Explore different ways that we might have chosen to fit the data
6. Fit the model with the BLS data
7. Examine what our model implies for the effects of COVID on employment/unemployment

## Step 1: Prepare and visualize our data

We have done this in earlier lectures and will not repeat the work here!

## Step 2: Create a generative model

**A simple model of employment**

In the vein of, "the first model created should be as simple as possible", we use the employment model that we studied earlier.

Consider a single individual that transitions between employment and unemployment

* When unemployed, they find a new job with probability $\alpha$
* When employed, they lose their job with probability $\beta$

<br>
<br>

![ModelFlowchart](model_diagram.png)


## Step 3: Simulate data from our generative model

<br>
<br>

We will break simulating data from the model into two steps:

1. Given today's state and the transition probabilities, draw from tomorrow's state
2. Given an initial state and transition probabilities, simulate an entire history of employment/unemployment using the one-step transition kernel


**Simulate the one-step employment transition**

```python
def next_state(s_t, alpha, beta):
    """
    Transitions from employment/unemployment in period t to
    employment/unemployment in period t+1
    
    Parameters
    ----------
    s_t : int
        The individual's current state... s_t = 0 maps to
        unemployed and s_t = 1 maps to employed
    alpha : float
        The probability that an individual goes from
        unemployed to employed
    beta : float
        The probability that an individual goes from
        employed to unemployed

    Returns
    -------
    s_tp1 : int
        The individual's employment state in `t+1`
    """
    # Draw a random number
    u_t = np.random.rand()

    # Let 0 be unemployed... If unemployed and draws
    # a value less than lambda then becomes employed
    if (s_t == 0) and (u_t < alpha):
        s_tp1 = 1
    # Let 1 be employed... If employed and draws a
    # value less than beta then becomes unemployed
    elif (s_t == 1) and (u_t < beta):
        s_tp1 = 0
    # Otherwise, he keeps the same state as he had
    # at period t
    else:
        s_tp1 = s_t

    return s_tp1

```

Notice how this function incorporates the Markov property that our model assumes.

The Markov property says $\text{Probability}(s_{t+1} | s_{t}) = \text{Probability}(s_{t+1} | s_{t}, s_{t-1}, \dots, s_0)$.

This means that, other than the transition probabilities, we only need to know today's state and not the entire history.

**Testing our function**

It's always a good idea to write some simple test cases for functions that we create.

```python
next_state(0, 0.5, 0.5)
```

```python
# Should never become employed from unemployment
# if alpha is 0
next_state(0, 0.0, 0.5) == 0
```

```python
# Should never become unemployed from employment
# if beta is 0
next_state(1, 0.5, 0.0) == 1
```

```python
# Should always transition to employment from unemployment
# when alpha is 1
next_state(0, 1.0, 0.5) == 1
```

```python
# Should always transition to unemployment from employment
# when beta is 1
next_state(1, 0.5, 1.0) == 0
```

**Simulate entire history**

**Note**: Later we will allow $\alpha$ and $\beta$ to change over time, so while we want you think of them as constant for now, we will write code that allows for them to fluctate period-by-period

```python
def simulate_employment_history(alpha, beta, s_0):
    """
    Simulates the history of employment/unemployment. It
    will simulate as many periods as elements in `alpha`
    and `beta`
    
    Parameters
    ----------
    alpha : np.array(float, ndim=1)
        The probability that an individual goes from
        unemployed to employed
    beta : np.array(float, ndim=1)
        The probability that an individual goes from
        employed to unemployed
    s_0 : int
        The initial state of unemployment/employment, which
        should take value of 0 (unemployed) or 1 (employed)
    """
    # Create array to hold the values of our simulation
    assert(len(alpha) == len(beta))
    T = len(alpha)
    s_hist = np.zeros(T+1, dtype=int)

    s_hist[0] = s_0
    for t in range(T):
        # Step one period into the future
        s_0 = next_state(s_0, alpha[t], beta[t])  # Notice alpha[t] and beta[t]
        s_hist[t+1] = s_0

    return s_hist
```

**Check output of the function**

```python
alpha = np.ones(50)*0.25
beta = np.ones(50)*0.025

simulate_employment_history(alpha, beta, 0)
```

## Step 4: Fit your model to our artificial data

There are lots of procedures that one could use to infer parameters values from data. Here we'll just count relative frequencies of transitions.

Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.

$$P \equiv \begin{bmatrix} p_{11} & p_{12} & \dots & p_{1N} \\ p_{21} & \vdots & \ddots & \vdots \\ p_{N1} & p_{N2} & \dots & p_{NN} \end{bmatrix}$$

Let $\{y_0, y_1, \dots, y_T\}$ be a sequence of observations generated from the $N$-state Markov chain, then our "fitting" procedure would assign the following value to $p_{ij}$:

$$p_{ij} = \frac{\sum_{t=0}^T \mathbb{1}_{y_{t} == i} \mathbb{1}_{y_{t+1} == j}}{\sum_{t=0}^T \mathbb{1}_{y_{t} == i}}$$

**Note**: If you'd like to understand why this procedure makes sense, we recommend computing $\sum_{j=1}^N p_{ij}$ for a given $i$. What value do you get? Why?

**Counting frequencies**



```python
def count_frequencies_individual(history):
    """
    Computes the transition probabilities for a two-state
    Markov chain

    Parameters
    ----------
    history : np.array(int, ndim=1)
        An array with the state values of a two-state Markov chain
    
    Returns
    -------
    alpha : float
        The probability of transitioning from state 0 to 1
    beta : float
        The probability of transitioning from state 1 to 0
    """
    # Get length of the simulation and an index tracker
    T = len(history)
    idx = np.arange(T)

    # Determine when the chain had values 0 and 1 -- Notice
    # that we can't use the last value because we don't see
    # where it transitions to
    zero_idxs = idx[(history == 0) & (idx < T-1)]
    one_idxs = idx[(history == 1) & (idx < T-1)]

    # Check what percent of the t+1 values were 0/1
    alpha = np.sum(history[zero_idxs+1]) / len(zero_idxs)
    beta = np.sum(1 - history[one_idxs+1]) / len(one_idxs)

    return alpha, beta
```

**Checking the fit**

```python
def check_accuracy(T, alpha=0.25, beta=0.025):
    """
    Checks the accuracy of our fit by printing the true values
    and the fitted values for a given T
    
    Parameters
    ----------
    T : int
        The length of our simulation
    alpha : float
        The probability that an individual goes from
        unemployed to employed
    beta : float
        The probability that an individual goes from
        employed to unemployed
    """
    idx = np.arange(T)
    alpha_np = np.ones(T)*alpha
    beta_np = np.ones(T)*beta

    # Simulate a sample history
    emp_history = simulate_employment_history(alpha_np, beta_np, 0)

    # Check the fit
    alpha_hat, beta_hat = count_frequencies_individual(emp_history)
    
    print(f"True alpha was {alpha} and fitted value was {alpha_hat}")
    print(f"True beta was {beta} and fitted value was {beta_hat}")
    
    return alpha, alpha_hat, beta, beta_hat
```

```python
check_accuracy(10_000, 0.25, 0.025)
```

Well... If we observe 10,000 months of employment history for someone then we know that we can back out the parameters of our models...

Unfortunately, our real world data won't have that much information.

What about for an entire lifetime of employment transitions?

```python
check_accuracy(45*12, 0.25, 0.025)
```

What about for just two years of observations?

```python
check_accuracy(2*12, 0.25, 0.025)
```

## Step 3 and 4 (second try)

Data for the employment history of a single individual will not give us a good chance of fitting our model accurately...

However, the BLS isn't infering EU/UE rates from its observation of a single individual. Rather, they're using a cross-section of individuals!

Can we use a cross-section rather than for one individual?

Yes, but, in order for a version of our "frequency counting" procedure to work, we want independence across individuals, i.e.

$$\text{Probability}(s_{i, t+1}, s_{j, t+1} | s_{i, t}, s_{j, t}, \alpha, \beta) = \text{Probability}(s_{i, t+1} | s_{i, t}, \alpha, \beta) \text{Probability}(s_{j, t+1} | s_{j, t}, \alpha, \beta)$$

When we observed only a single individual, the Markov property did a lot of the work to get independence for us.

When might this not be the case?

* Change in government policy results in a "jobs guarantee"
* Technological change results in the destruction of an entire industries jobs
* Recession causes increased firing across entire country

(Spoiler alert: Some of these will present problems for us... which is why we'll allow for $\alpha$ and $\beta$ to move each period)

**Simulating a cross-section**

```python
def simulate_employment_cross_section(alpha, beta, s_0, N=500):
    """
    Simulates a cross-section of employment/unemployment using
    the model we've described above.
    
    Parameters
    ----------
    alpha : np.array(float, ndim=1)
        The probability that an individual goes from
        unemployed to employed
    beta : np.array(float, ndim=1)
        The probability that an individual goes from
        employed to unemployed
    s_0 : np.array(int, ndim=1)
        The fraction of the population that begins in each
        employment state
    N : int
        The number of individuals in our cross-section
    
    Returns
    -------
    s_hist_cs : np.array(int, ndim=2)
        An `N x T` matrix that contains an individual
        history of employment along each row
    """
    # Make sure transitions are same size and get the length
    # of the simulation from the length of the transition
    # probabilities
    assert(len(alpha) == len(beta))
    T = len(alpha)

    # Check the fractions add to one and figure out how many
    # zeros we should have
    assert(np.abs(np.sum(s_0) - 1.0) < 1e-8)
    Nz = np.floor(s_0[0]*N).astype(int)

    # Allocate space to store the simulations
    s_hist_cs = np.zeros((N, T+1), dtype=int)
    s_hist_cs[Nz:, 0] = 1
    
    for i in range(N):
        s_hist_cs[i, :] = simulate_employment_history(
            alpha, beta, s_hist_cs[i, 0]
        )
    
    return s_hist_cs

```

```python
alpha = np.ones(1)*0.25
beta = np.ones(1)*0.025

simulate_employment_cross_section(alpha, beta, np.array([0.35, 0.65]), 10)
```

**Store the simulation in pandas**

Real world data will typically be stored in a DataFrame, so let's store our artificial data in a DataFrame as well

```python
def pandas_employment_cross_section(eu_ue_df, s_0, N=500):
    """
    Simulate a cross-section of employment experiences
    
    Parameters
    ----------
    eu_ue_df : pd.DataFrame
        A DataFrame with columns `dt`, `alpha`, and `beta`
        that have the monthly eu/ue transition rates
    s_0 : np.array(float, ndim=1)
        The fraction of the population that begins in each
        employment state
    N : int
        The numbers of individuals in our cross-section

    Returns
    -------
    df : pd.DataFrame
        A DataFrame with the dates and an employment outcome
        associated with each date of `eu_ue_df`
    """
    # Make sure that `ue_ue_df` is sorted by date
    eu_ue_df = eu_ue_df.sort_values("dt")
    alpha = eu_ue_df["alpha"].to_numpy()
    beta = eu_ue_df["beta"].to_numpy()

    # Simulate cross-section
    employment_history = simulate_employment_cross_section(
        alpha, beta, s_0, N
    )

    df = pd.DataFrame(employment_history[:, :-1].T)
    df = pd.concat([eu_ue_df["dt"], df], axis=1)
    df = pd.melt(
        df, id_vars=["dt"],
        var_name="pid", value_name="employment"
    )

    return df
```

```python
T = 24
eu_ue_df = pd.DataFrame(
    {
        "dt": pd.date_range("2018-01-01", periods=T, freq="MS"), 
        "alpha": np.ones(T)*0.25,
        "beta": np.ones(T)*0.025
    }
)

df = pandas_employment_cross_section(eu_ue_df, np.array([0.25, 0.75]), N=5_000)
df.head()

```

**Simulating the CPS**

Just to "keep it interesting", let's tie our hands in a similar way to how the BLS has their hands tied.

We will simulate an individual's full employment history, but will only keep the subset that corresponds to when they would have been interviewed by the CPS

```python
def cps_interviews(df, start_year, start_month):
    """
    Takes an individual simulated employment/unemployment
    history and "interviews" the individual as if they were
    in the CPS
    
    Parameters
    ----------
    df : pd.DataFrame
        A DataFrame with at least the columns `pid`, `dt`,
        and `employment`
    start_year : int
        The year in which their interviewing begins
    start_month : int
        The month in which their interviewing begins

    Returns
    -------
    cps : pd.DataFrame
        A DataFrame with the same columns as `df` but only
        with observations that correspond to the CPS
        interview schedule for someone who starts
        interviewing in f`{start_year}/{start_month}`
    """
    # Get dates that are associated with being interviewed in
    # the CPS
    start_date_y1 = dt.datetime(start_year, start_month, 1)
    dates_y1 = pd.date_range(start_date_y1, periods=4, freq="MS")
    start_date_y2 = dt.datetime(start_year+1, start_month, 1)
    dates_y2 = pd.date_range(start_date_y2, periods=4, freq="MS")
    dates = dates_y1.append(dates_y2)

    # Filter data that's not in the dates
    cps = df.loc[df["dt"].isin(dates), :]

    return cps
```

```python
interview = lambda x: cps_interviews(
    x,
    np.random.choice(x["dt"].dt.year.unique()),
    np.random.randint(1, 13)
)

cps_data = (
    df.groupby("pid")
      .apply(
          lambda x: interview(x)
      )
      .reset_index(drop=True)
)
```

**How many people are we observing per month?**

If we think about the pattern used for the CPS interviews, we can form an idea of how many people might be interviewed each month.

Consider if we started interviewing $m$ new individuals per month. How many would we be interviewing in any given month?

Well. We'd at least be interviewing the $m$ new individuals. We would also be interviewing all of the individuals that had started their interviews in the previous 3 months. Additionally, we would be interviewing all of the individuals who had begun their interviews during those four months of the previous year.

We can see this below -- Note that our "survey" begins in January 2018, so at first we only have $m$ individuals being interviewed, but, as the survey progresses, we move towards $8 m$ individuals being interviewed each month.

```python
cps_data["dt"].value_counts().sort_index()

```

**Fitting to a cross-section**

Well... Now our data look exactly like what the BLS uses to estimate the EU and UE transition rates from the raw data.

In order to fit the data, we are going to continue using the "frequency of transition" concept that we previously proposed, but, we must account for the shape of the data we receive now.

Let's think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.

$$P \equiv \begin{bmatrix} p_{11} & p_{12} & \dots & p_{1N} \\ p_{21} & \vdots & \ddots & \vdots \\ p_{N1} & p_{N2} & \dots & p_{NN} \end{bmatrix}$$

Let $\{ \{y_{i, 0}, y_{i, 1}, \dots, y_{i, T_i}\} \; \forall i \in \{0, 1, \dots, I\}\}$ be a $I$ sequences of observations generated from the $N$-state Markov chain, then our new "fitting" procedure would assign the following value to $p_{ij}$:

$$p_{ij} = \frac{\sum_{m=0}^I \sum_{t=0}^T \mathbb{1}_{y_{m, t} == i} \mathbb{1}_{y_{m, t+1} == j}}{\sum_{m=0}^I \sum_{t=0}^T \mathbb{1}_{y_{m, t} == i}}$$


**Cross-sectional counting frequencies**

```python
def cps_count_frequencies(df):
    """
    Estimates the transition probability from employment
    and unemployment histories of a CPS sample of
    individuals
    
    Parameters
    ----------
    df : pd.DataFrame
        A sample of individuals from the CPS survey. Must
        have columns `dt`, `pid`, and `employment`.

    Returns
    -------
    alpha : float
        The probability of transitioning from unemployment
        to employment
    beta : float
        The probability of transitioning from employment
        to unemployment
    """
    # Set the index to be dt/pid
    data_t = df.set_index(["dt", "pid"])

    # Now find the "t+1" months and "pid"s
    tp1 = data_t.index.get_level_values("dt").shift(periods=1, freq="MS")
    pid = data_t.index.get_level_values("pid")
    idx = pd.MultiIndex.from_arrays([tp1, pid], names=["dt", "pid"])

    # Now "index" into the data and reset index
    data_tp1 = (
        data_t.reindex(idx)
            .rename(columns={"employment": "employment_tp1"})
    )
    out = pd.concat(
        [
            data_t.reset_index().loc[:, ["dt", "pid", "employment"]],
            data_tp1.reset_index()["employment_tp1"]
        ], axis=1, sort=True
    ).dropna(subset=["employment_tp1"])
    out["employment_tp1"] = out["employment_tp1"].astype(int)

    # Count how frequently we go from 0 to 1
    out_zeros = out.query("employment == 0")
    alpha = out_zeros["employment_tp1"].mean()
    
    # Count how frequently we go from 1 to 0
    out_ones = out.query("employment == 1")
    beta = (1 - out_ones["employment_tp1"]).mean()

    return alpha, beta
```

**Checking accuracy**

```python
def check_accuracy_cs(N, T, alpha=0.25, beta=0.025):
    """
    Checks the accuracy of our fit by printing the true values
    and the fitted values for a given T
    
    Parameters
    ----------
    N : int
        The total number of people we ever interview
    T : int
        The length of our simulation
    alpha : float
        The probability that an individual goes from
        unemployed to employed
    beta : float
        The probability that an individual goes from
        employed to unemployed
    """
    alpha_beta_df = pd.DataFrame(
        {
            "dt": pd.date_range("2018-01-01", periods=T, freq="MS"), 
            "alpha": np.ones(T)*alpha,
            "beta": np.ones(T)*beta
        }
    )

    # Simulate the full cross-section
    frac_unemployed = beta / (alpha + beta)
    frac_employed = alpha / (alpha + beta)
    df = pandas_employment_cross_section(
        alpha_beta_df, np.array([frac_unemployed, frac_employed]), N
    )

    # Interview individuals according to the cps interviews
    interview = lambda x: cps_interviews(
        x,
        np.random.choice(df["dt"].dt.year.unique()),
        np.random.randint(1, 13)
    )
    cps_data = (
        df.groupby("pid")
          .apply(
              lambda x: interview(x)
          )
          .reset_index(drop=True)
    )

    # Check the fit
    alpha_hat, beta_hat = cps_count_frequencies(cps_data)
    
    print(f"True alpha was {alpha} and fitted value was {alpha_hat}")
    print(f"True beta was {beta} and fitted value was {beta_hat}")
    
    return alpha, alpha_hat, beta, beta_hat
```

```python
check_accuracy_cs(1_000, 24, 0.25, 0.025)
```

```python
check_accuracy_cs(500, 24, 0.25, 0.025)
```

```python
check_accuracy_cs(100, 24, 0.25, 0.025)
```

## Step 7: Fit the model with actual CPS data

We've downloaded (and cleaned!) a subset of real CPS data for the years 2018 and 2019.

Let's see what our constant parameter model does with this data.

```python
# Load real CPS data that we've cleaned for you.
cps_data = pd.read_parquet("cps_data.parquet")
```

**What does this data contain?**

```python
cps_data.head()
```

**Finding an employment history**

```python
cps_count_sum = cps_data.groupby("pid").agg(
    {"dt": "count", "employment": "sum"}
).sort_values("dt")

cps_count_sum.tail()

```

```python
cps_data.query("pid == 20180602828701")
```

**Find an individual who experiences unemployment?**

```python
cps_count_sum.query("(dt == 8) & (employment < 8)")
```

```python
cps_data.query("pid == 20180307173302")
```

**Computing $\alpha$ and $\beta$**

```python
alpha_cps, beta_cps = cps_count_frequencies(cps_data)
```

```python
alpha_cps
```

```python
beta_cps
```

```python
beta_cps / (alpha_cps + beta_cps)
```

