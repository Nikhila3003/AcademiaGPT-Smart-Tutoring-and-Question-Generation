page_content='<br>\n<br>  \n**Goal**:  \nEstimate a Markov chain using employment data  \n```python\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd  \n%matplotlib inline\n```' metadata={'Header 1': 'Employment-Unemployment Model'}page_content='Our friend Jim Savage has thought about what a "modern statistical workflow" entails. We summarize some steps he has proposed for understanding the world around us:  \n> 1. Prepare and visualize your data.\n> 2. Create a generative model for the data...\n>   - A first model should be as simple as possible.\n> 3. Simulate some artificial data from your model given some assumed parameters that you "pick out of a hat" ($\\theta$)\n> 4. Use the artificial data to estimate the model parameters ($\\hat{\\theta})$\n> 5. Check that you recovered a good approximation of the "known unknowns" (aka, $\\theta \\approx \\hat{\\theta}$)\n>   - Possibly repeat 3-5 with different estimators and true parameters ($\\theta$), to get an understanding of how well the fitting procedure works\n> 6. Fit the model to your real data, check the fit\n> 7. Argue about the results with your friends and colleagues\n> 8. Go back to 2. with a slightly richer model. Repeat.\n> 9. Think carefully about what decisions will be made from the analysis, encode a loss function, and perform statistical decision analysis... Note that in many cases, we will do step 9 before steps 1-8!  \nLater this semester, we will talk formally about what it means to "fit" your model (and the work that it entails), but, for now, we find it sufficient to say that it\'s a process to ensure that the probability distribution over outcomes generated by your model lines up with the data (aka, finding the right model parameters).  \nWe\'ll do a version of steps 1-6 to help us improve our understanding of the labor data that we previously saw.' metadata={'Header 1': 'Employment-Unemployment Model', 'Header 2': 'Steps to "understanding the world around us" (a short digression)'}page_content='1. Prepare and visualize our data.\n2. Develop a generative model of employment and unemployment\n3. Simulate data from our generative model for given parameters\n4. Fit our model to the simulated data\n5. Explore different ways that we might have chosen to fit the data\n6. Fit the model with the BLS data\n7. Examine what our model implies for the effects of COVID on employment/unemployment' metadata={'Header 1': 'Employment-Unemployment Model', 'Header 2': 'Steps to "understanding the world around us" (a short digression)', 'Header 3': 'Our plan'}page_content='We have done this in earlier lectures and will not repeat the work here!' metadata={'Header 1': 'Employment-Unemployment Model', 'Header 2': 'Step 1: Prepare and visualize our data'}page_content='**A simple model of employment**  \nIn the vein of, "the first model created should be as simple as possible", we use the employment model that we studied earlier.  \nConsider a single individual that transitions between employment and unemployment  \n* When unemployed, they find a new job with probability $\\alpha$\n* When employed, they lose their job with probability $\\beta$  \n<br>\n<br>  \n![ModelFlowchart](model_diagram.png)' metadata={'Header 1': 'Employment-Unemployment Model', 'Header 2': 'Step 2: Create a generative model'}page_content='<br>\n<br>  \nWe will break simulating data from the model into two steps:  \n1. Given today\'s state and the transition probabilities, draw from tomorrow\'s state\n2. Given an initial state and transition probabilities, simulate an entire history of employment/unemployment using the one-step transition kernel  \n**Simulate the one-step employment transition**  \n```python\ndef next_state(s_t, alpha, beta):\n"""\nTransitions from employment/unemployment in period t to\nemployment/unemployment in period t+1  \nParameters\n----------\ns_t : int\nThe individual\'s current state... s_t = 0 maps to\nunemployed and s_t = 1 maps to employed\nalpha : float\nThe probability that an individual goes from\nunemployed to employed\nbeta : float\nThe probability that an individual goes from\nemployed to unemployed  \nReturns\n-------\ns_tp1 : int\nThe individual\'s employment state in `t+1`\n"""' metadata={'Header 1': 'Employment-Unemployment Model', 'Header 2': 'Step 3: Simulate data from our generative model'}page_content='u_t = np.random.rand()' metadata={'Header 1': 'Draw a random number'}page_content='if (s_t == 0) and (u_t < alpha):\ns_tp1 = 1' metadata={'Header 1': 'a value less than lambda then becomes employed'}page_content='elif (s_t == 1) and (u_t < beta):\ns_tp1 = 0' metadata={'Header 1': 'value less than beta then becomes unemployed'}page_content="else:\ns_tp1 = s_t  \nreturn s_tp1  \n```  \nNotice how this function incorporates the Markov property that our model assumes.  \nThe Markov property says $\\text{Probability}(s_{t+1} | s_{t}) = \\text{Probability}(s_{t+1} | s_{t}, s_{t-1}, \\dots, s_0)$.  \nThis means that, other than the transition probabilities, we only need to know today's state and not the entire history.  \n**Testing our function**  \nIt's always a good idea to write some simple test cases for functions that we create.  \n```python\nnext_state(0, 0.5, 0.5)\n```  \n```python" metadata={'Header 1': 'at period t'}page_content='next_state(0, 0.0, 0.5) == 0\n```  \n```python' metadata={'Header 1': 'if alpha is 0'}page_content='next_state(1, 0.5, 0.0) == 1\n```  \n```python' metadata={'Header 1': 'if beta is 0'}page_content='next_state(0, 1.0, 0.5) == 1\n```  \n```python' metadata={'Header 1': 'when alpha is 1'}page_content='next_state(1, 0.5, 1.0) == 0\n```  \n**Simulate entire history**  \n**Note**: Later we will allow $\\alpha$ and $\\beta$ to change over time, so while we want you think of them as constant for now, we will write code that allows for them to fluctate period-by-period  \n```python\ndef simulate_employment_history(alpha, beta, s_0):\n"""\nSimulates the history of employment/unemployment. It\nwill simulate as many periods as elements in `alpha`\nand `beta`  \nParameters\n----------\nalpha : np.array(float, ndim=1)\nThe probability that an individual goes from\nunemployed to employed\nbeta : np.array(float, ndim=1)\nThe probability that an individual goes from\nemployed to unemployed\ns_0 : int\nThe initial state of unemployment/employment, which\nshould take value of 0 (unemployed) or 1 (employed)\n"""' metadata={'Header 1': 'when beta is 1'}page_content='assert(len(alpha) == len(beta))\nT = len(alpha)\ns_hist = np.zeros(T+1, dtype=int)  \ns_hist[0] = s_0\nfor t in range(T):' metadata={'Header 1': 'Create array to hold the values of our simulation'}page_content='s_0 = next_state(s_0, alpha[t], beta[t])  # Notice alpha[t] and beta[t]\ns_hist[t+1] = s_0  \nreturn s_hist\n```  \n**Check output of the function**  \n```python\nalpha = np.ones(50)*0.25\nbeta = np.ones(50)*0.025  \nsimulate_employment_history(alpha, beta, 0)\n```' metadata={'Header 1': 'Step one period into the future'}page_content='There are lots of procedures that one could use to infer parameters values from data. Here we\'ll just count relative frequencies of transitions.  \nLet\'s think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.  \n$$P \\equiv \\begin{bmatrix} p_{11} & p_{12} & \\dots & p_{1N} \\\\ p_{21} & \\vdots & \\ddots & \\vdots \\\\ p_{N1} & p_{N2} & \\dots & p_{NN} \\end{bmatrix}$$  \nLet $\\{y_0, y_1, \\dots, y_T\\}$ be a sequence of observations generated from the $N$-state Markov chain, then our "fitting" procedure would assign the following value to $p_{ij}$:  \n$$p_{ij} = \\frac{\\sum_{t=0}^T \\mathbb{1}_{y_{t} == i} \\mathbb{1}_{y_{t+1} == j}}{\\sum_{t=0}^T \\mathbb{1}_{y_{t} == i}}$$  \n**Note**: If you\'d like to understand why this procedure makes sense, we recommend computing $\\sum_{j=1}^N p_{ij}$ for a given $i$. What value do you get? Why?  \n**Counting frequencies**  \n```python\ndef count_frequencies_individual(history):\n"""\nComputes the transition probabilities for a two-state\nMarkov chain  \nParameters\n----------\nhistory : np.array(int, ndim=1)\nAn array with the state values of a two-state Markov chain  \nReturns\n-------\nalpha : float\nThe probability of transitioning from state 0 to 1\nbeta : float\nThe probability of transitioning from state 1 to 0\n"""' metadata={'Header 1': 'Step one period into the future', 'Header 2': 'Step 4: Fit your model to our artificial data'}page_content='T = len(history)\nidx = np.arange(T)' metadata={'Header 1': 'Get length of the simulation and an index tracker'}page_content='zero_idxs = idx[(history == 0) & (idx < T-1)]\none_idxs = idx[(history == 1) & (idx < T-1)]' metadata={'Header 1': 'where it transitions to'}page_content='alpha = np.sum(history[zero_idxs+1]) / len(zero_idxs)\nbeta = np.sum(1 - history[one_idxs+1]) / len(one_idxs)  \nreturn alpha, beta\n```  \n**Checking the fit**  \n```python\ndef check_accuracy(T, alpha=0.25, beta=0.025):\n"""\nChecks the accuracy of our fit by printing the true values\nand the fitted values for a given T  \nParameters\n----------\nT : int\nThe length of our simulation\nalpha : float\nThe probability that an individual goes from\nunemployed to employed\nbeta : float\nThe probability that an individual goes from\nemployed to unemployed\n"""\nidx = np.arange(T)\nalpha_np = np.ones(T)*alpha\nbeta_np = np.ones(T)*beta' metadata={'Header 1': 'Check what percent of the t+1 values were 0/1'}page_content='emp_history = simulate_employment_history(alpha_np, beta_np, 0)' metadata={'Header 1': 'Simulate a sample history'}page_content='alpha_hat, beta_hat = count_frequencies_individual(emp_history)  \nprint(f"True alpha was {alpha} and fitted value was {alpha_hat}")\nprint(f"True beta was {beta} and fitted value was {beta_hat}")  \nreturn alpha, alpha_hat, beta, beta_hat\n```  \n```python\ncheck_accuracy(10_000, 0.25, 0.025)\n```  \nWell... If we observe 10,000 months of employment history for someone then we know that we can back out the parameters of our models...  \nUnfortunately, our real world data won\'t have that much information.  \nWhat about for an entire lifetime of employment transitions?  \n```python\ncheck_accuracy(45*12, 0.25, 0.025)\n```  \nWhat about for just two years of observations?  \n```python\ncheck_accuracy(2*12, 0.25, 0.025)\n```' metadata={'Header 1': 'Check the fit'}page_content='Data for the employment history of a single individual will not give us a good chance of fitting our model accurately...  \nHowever, the BLS isn\'t infering EU/UE rates from its observation of a single individual. Rather, they\'re using a cross-section of individuals!  \nCan we use a cross-section rather than for one individual?  \nYes, but, in order for a version of our "frequency counting" procedure to work, we want independence across individuals, i.e.  \n$$\\text{Probability}(s_{i, t+1}, s_{j, t+1} | s_{i, t}, s_{j, t}, \\alpha, \\beta) = \\text{Probability}(s_{i, t+1} | s_{i, t}, \\alpha, \\beta) \\text{Probability}(s_{j, t+1} | s_{j, t}, \\alpha, \\beta)$$  \nWhen we observed only a single individual, the Markov property did a lot of the work to get independence for us.  \nWhen might this not be the case?  \n* Change in government policy results in a "jobs guarantee"\n* Technological change results in the destruction of an entire industries jobs\n* Recession causes increased firing across entire country  \n(Spoiler alert: Some of these will present problems for us... which is why we\'ll allow for $\\alpha$ and $\\beta$ to move each period)  \n**Simulating a cross-section**  \n```python\ndef simulate_employment_cross_section(alpha, beta, s_0, N=500):\n"""\nSimulates a cross-section of employment/unemployment using\nthe model we\'ve described above.  \nParameters\n----------\nalpha : np.array(float, ndim=1)\nThe probability that an individual goes from\nunemployed to employed\nbeta : np.array(float, ndim=1)\nThe probability that an individual goes from\nemployed to unemployed\ns_0 : np.array(int, ndim=1)\nThe fraction of the population that begins in each\nemployment state\nN : int\nThe number of individuals in our cross-section  \nReturns\n-------\ns_hist_cs : np.array(int, ndim=2)\nAn `N x T` matrix that contains an individual\nhistory of employment along each row\n"""' metadata={'Header 1': 'Check the fit', 'Header 2': 'Step 3 and 4 (second try)'}page_content='assert(len(alpha) == len(beta))\nT = len(alpha)' metadata={'Header 1': 'probabilities'}page_content='assert(np.abs(np.sum(s_0) - 1.0) < 1e-8)\nNz = np.floor(s_0[0]*N).astype(int)' metadata={'Header 1': 'zeros we should have'}page_content='s_hist_cs = np.zeros((N, T+1), dtype=int)\ns_hist_cs[Nz:, 0] = 1  \nfor i in range(N):\ns_hist_cs[i, :] = simulate_employment_history(\nalpha, beta, s_hist_cs[i, 0]\n)  \nreturn s_hist_cs  \n```  \n```python\nalpha = np.ones(1)*0.25\nbeta = np.ones(1)*0.025  \nsimulate_employment_cross_section(alpha, beta, np.array([0.35, 0.65]), 10)\n```  \n**Store the simulation in pandas**  \nReal world data will typically be stored in a DataFrame, so let\'s store our artificial data in a DataFrame as well  \n```python\ndef pandas_employment_cross_section(eu_ue_df, s_0, N=500):\n"""\nSimulate a cross-section of employment experiences  \nParameters\n----------\neu_ue_df : pd.DataFrame\nA DataFrame with columns `dt`, `alpha`, and `beta`\nthat have the monthly eu/ue transition rates\ns_0 : np.array(float, ndim=1)\nThe fraction of the population that begins in each\nemployment state\nN : int\nThe numbers of individuals in our cross-section  \nReturns\n-------\ndf : pd.DataFrame\nA DataFrame with the dates and an employment outcome\nassociated with each date of `eu_ue_df`\n"""' metadata={'Header 1': 'Allocate space to store the simulations'}page_content='eu_ue_df = eu_ue_df.sort_values("dt")\nalpha = eu_ue_df["alpha"].to_numpy()\nbeta = eu_ue_df["beta"].to_numpy()' metadata={'Header 1': 'Make sure that `ue_ue_df` is sorted by date'}page_content='employment_history = simulate_employment_cross_section(\nalpha, beta, s_0, N\n)  \ndf = pd.DataFrame(employment_history[:, :-1].T)\ndf = pd.concat([eu_ue_df["dt"], df], axis=1)\ndf = pd.melt(\ndf, id_vars=["dt"],\nvar_name="pid", value_name="employment"\n)  \nreturn df\n```  \n```python\nT = 24\neu_ue_df = pd.DataFrame(\n{\n"dt": pd.date_range("2018-01-01", periods=T, freq="MS"),\n"alpha": np.ones(T)*0.25,\n"beta": np.ones(T)*0.025\n}\n)  \ndf = pandas_employment_cross_section(eu_ue_df, np.array([0.25, 0.75]), N=5_000)\ndf.head()  \n```  \n**Simulating the CPS**  \nJust to "keep it interesting", let\'s tie our hands in a similar way to how the BLS has their hands tied.  \nWe will simulate an individual\'s full employment history, but will only keep the subset that corresponds to when they would have been interviewed by the CPS  \n```python\ndef cps_interviews(df, start_year, start_month):\n"""\nTakes an individual simulated employment/unemployment\nhistory and "interviews" the individual as if they were\nin the CPS  \nParameters\n----------\ndf : pd.DataFrame\nA DataFrame with at least the columns `pid`, `dt`,\nand `employment`\nstart_year : int\nThe year in which their interviewing begins\nstart_month : int\nThe month in which their interviewing begins  \nReturns\n-------\ncps : pd.DataFrame\nA DataFrame with the same columns as `df` but only\nwith observations that correspond to the CPS\ninterview schedule for someone who starts\ninterviewing in f`{start_year}/{start_month}`\n"""' metadata={'Header 1': 'Simulate cross-section'}page_content='start_date_y1 = dt.datetime(start_year, start_month, 1)\ndates_y1 = pd.date_range(start_date_y1, periods=4, freq="MS")\nstart_date_y2 = dt.datetime(start_year+1, start_month, 1)\ndates_y2 = pd.date_range(start_date_y2, periods=4, freq="MS")\ndates = dates_y1.append(dates_y2)' metadata={'Header 1': 'the CPS'}page_content='cps = df.loc[df["dt"].isin(dates), :]  \nreturn cps\n```  \n```python\ninterview = lambda x: cps_interviews(\nx,\nnp.random.choice(x["dt"].dt.year.unique()),\nnp.random.randint(1, 13)\n)  \ncps_data = (\ndf.groupby("pid")\n.apply(\nlambda x: interview(x)\n)\n.reset_index(drop=True)\n)\n```  \n**How many people are we observing per month?**  \nIf we think about the pattern used for the CPS interviews, we can form an idea of how many people might be interviewed each month.  \nConsider if we started interviewing $m$ new individuals per month. How many would we be interviewing in any given month?  \nWell. We\'d at least be interviewing the $m$ new individuals. We would also be interviewing all of the individuals that had started their interviews in the previous 3 months. Additionally, we would be interviewing all of the individuals who had begun their interviews during those four months of the previous year.  \nWe can see this below -- Note that our "survey" begins in January 2018, so at first we only have $m$ individuals being interviewed, but, as the survey progresses, we move towards $8 m$ individuals being interviewed each month.  \n```python\ncps_data["dt"].value_counts().sort_index()  \n```  \n**Fitting to a cross-section**  \nWell... Now our data look exactly like what the BLS uses to estimate the EU and UE transition rates from the raw data.  \nIn order to fit the data, we are going to continue using the "frequency of transition" concept that we previously proposed, but, we must account for the shape of the data we receive now.  \nLet\'s think about the general case. Consider an $N$-state Markov chain. The parameters of the Markov chain are the elements of the transition matrix, $P$.  \n$$P \\equiv \\begin{bmatrix} p_{11} & p_{12} & \\dots & p_{1N} \\\\ p_{21} & \\vdots & \\ddots & \\vdots \\\\ p_{N1} & p_{N2} & \\dots & p_{NN} \\end{bmatrix}$$  \nLet $\\{ \\{y_{i, 0}, y_{i, 1}, \\dots, y_{i, T_i}\\} \\; \\forall i \\in \\{0, 1, \\dots, I\\}\\}$ be a $I$ sequences of observations generated from the $N$-state Markov chain, then our new "fitting" procedure would assign the following value to $p_{ij}$:  \n$$p_{ij} = \\frac{\\sum_{m=0}^I \\sum_{t=0}^T \\mathbb{1}_{y_{m, t} == i} \\mathbb{1}_{y_{m, t+1} == j}}{\\sum_{m=0}^I \\sum_{t=0}^T \\mathbb{1}_{y_{m, t} == i}}$$  \n**Cross-sectional counting frequencies**  \n```python\ndef cps_count_frequencies(df):\n"""\nEstimates the transition probability from employment\nand unemployment histories of a CPS sample of\nindividuals  \nParameters\n----------\ndf : pd.DataFrame\nA sample of individuals from the CPS survey. Must\nhave columns `dt`, `pid`, and `employment`.  \nReturns\n-------\nalpha : float\nThe probability of transitioning from unemployment\nto employment\nbeta : float\nThe probability of transitioning from employment\nto unemployment\n"""' metadata={'Header 1': "Filter data that's not in the dates"}page_content='data_t = df.set_index(["dt", "pid"])' metadata={'Header 1': 'Set the index to be dt/pid'}page_content='tp1 = data_t.index.get_level_values("dt").shift(periods=1, freq="MS")\npid = data_t.index.get_level_values("pid")\nidx = pd.MultiIndex.from_arrays([tp1, pid], names=["dt", "pid"])' metadata={'Header 1': 'Now find the "t+1" months and "pid"s'}page_content='data_tp1 = (\ndata_t.reindex(idx)\n.rename(columns={"employment": "employment_tp1"})\n)\nout = pd.concat(\n[\ndata_t.reset_index().loc[:, ["dt", "pid", "employment"]],\ndata_tp1.reset_index()["employment_tp1"]\n], axis=1, sort=True\n).dropna(subset=["employment_tp1"])\nout["employment_tp1"] = out["employment_tp1"].astype(int)' metadata={'Header 1': 'Now "index" into the data and reset index'}page_content='out_zeros = out.query("employment == 0")\nalpha = out_zeros["employment_tp1"].mean()' metadata={'Header 1': 'Count how frequently we go from 0 to 1'}page_content='out_ones = out.query("employment == 1")\nbeta = (1 - out_ones["employment_tp1"]).mean()  \nreturn alpha, beta\n```  \n**Checking accuracy**  \n```python\ndef check_accuracy_cs(N, T, alpha=0.25, beta=0.025):\n"""\nChecks the accuracy of our fit by printing the true values\nand the fitted values for a given T  \nParameters\n----------\nN : int\nThe total number of people we ever interview\nT : int\nThe length of our simulation\nalpha : float\nThe probability that an individual goes from\nunemployed to employed\nbeta : float\nThe probability that an individual goes from\nemployed to unemployed\n"""\nalpha_beta_df = pd.DataFrame(\n{\n"dt": pd.date_range("2018-01-01", periods=T, freq="MS"),\n"alpha": np.ones(T)*alpha,\n"beta": np.ones(T)*beta\n}\n)' metadata={'Header 1': 'Count how frequently we go from 1 to 0'}page_content='frac_unemployed = beta / (alpha + beta)\nfrac_employed = alpha / (alpha + beta)\ndf = pandas_employment_cross_section(\nalpha_beta_df, np.array([frac_unemployed, frac_employed]), N\n)' metadata={'Header 1': 'Simulate the full cross-section'}page_content='interview = lambda x: cps_interviews(\nx,\nnp.random.choice(df["dt"].dt.year.unique()),\nnp.random.randint(1, 13)\n)\ncps_data = (\ndf.groupby("pid")\n.apply(\nlambda x: interview(x)\n)\n.reset_index(drop=True)\n)' metadata={'Header 1': 'Interview individuals according to the cps interviews'}page_content='alpha_hat, beta_hat = cps_count_frequencies(cps_data)  \nprint(f"True alpha was {alpha} and fitted value was {alpha_hat}")\nprint(f"True beta was {beta} and fitted value was {beta_hat}")  \nreturn alpha, alpha_hat, beta, beta_hat\n```  \n```python\ncheck_accuracy_cs(1_000, 24, 0.25, 0.025)\n```  \n```python\ncheck_accuracy_cs(500, 24, 0.25, 0.025)\n```  \n```python\ncheck_accuracy_cs(100, 24, 0.25, 0.025)\n```' metadata={'Header 1': 'Check the fit'}page_content="We've downloaded (and cleaned!) a subset of real CPS data for the years 2018 and 2019.  \nLet's see what our constant parameter model does with this data.  \n```python" metadata={'Header 1': 'Check the fit', 'Header 2': 'Step 7: Fit the model with actual CPS data'}page_content='cps_data = pd.read_parquet("cps_data.parquet")\n```  \n**What does this data contain?**  \n```python\ncps_data.head()\n```  \n**Finding an employment history**  \n```python\ncps_count_sum = cps_data.groupby("pid").agg(\n{"dt": "count", "employment": "sum"}\n).sort_values("dt")  \ncps_count_sum.tail()  \n```  \n```python\ncps_data.query("pid == 20180602828701")\n```  \n**Find an individual who experiences unemployment?**  \n```python\ncps_count_sum.query("(dt == 8) & (employment < 8)")\n```  \n```python\ncps_data.query("pid == 20180307173302")\n```  \n**Computing $\\alpha$ and $\\beta$**  \n```python\nalpha_cps, beta_cps = cps_count_frequencies(cps_data)\n```  \n```python\nalpha_cps\n```  \n```python\nbeta_cps\n```  \n```python\nbeta_cps / (alpha_cps + beta_cps)\n```' metadata={'Header 1': "Load real CPS data that we've cleaned for you."}