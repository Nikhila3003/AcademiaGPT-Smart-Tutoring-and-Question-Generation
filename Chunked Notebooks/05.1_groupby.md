page_content='**Prerequisites**  \n- [Functions](../python_fundamentals/functions.ipynb)\n- pandas introduction [1](intro.ipynb) and [2](basics.ipynb)\n- [Reshape](reshape.ipynb)  \n**Outcomes**  \n- Understand the split-apply-combine strategy for aggregate\ncomputations on groups of data\n- Be able use basic aggregation methods on `df.groupby` to compute\nwithin group statistics\n- Understand how to group by multiple keys at once  \n**Data**  \n- Details for all delayed US domestic flights in December 2016,\nobtained from the [Bureau of Transportation\nStatistics](https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time)  \n```python\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt  \n%matplotlib inline\n```' metadata={'Header 1': 'GroupBy'}page_content='One powerful paradigm for analyzing data is the “Split-Apply-Combine”\nstrategy  \nThis strategy has three steps:  \n1. `Split`: split the data into groups based on values in one or more columns.\n1. `Apply`: apply a function or routine to each group separately.\n1. `Combine`: combine the output of the apply step into a DataFrame,\nusing the group identifiers as the index  \nWe will cover the core concepts here  \nWe **strongly** encourage you\nto also study the [official\ndocumentation](https://pandas.pydata.org/pandas-docs/stable/groupby.html)  \nTo describe the concepts, we will need some data  \nWe\'ll start with artificial data and then use a real-world dataset  \n```python\nC = np.arange(1, 7, dtype=float)\nC[[3, 5]] = np.nan\ndf = pd.DataFrame({\n"A" : [1, 1, 1, 2, 2, 2],\n"B" : [1, 1, 2, 2, 1, 1],\n"C": C,\n})\ndf\n```' metadata={'Header 1': 'GroupBy', 'Header 2': 'Split-Apply-Combine'}page_content='To perform the **Split** step, we call the `groupby` method on our\nDataFrame  \nFirst argument to `groupby` is how we want to form group  \nThe most basic form of splitting is to use a single column  \n```python\ngbA = df.groupby("A")\n```  \n`gbA` has type `DataFrameGroupBy`:  \n```python\ntype(gbA)\n```  \nWe usually refer to this type as `GroupBy` for short  \nWe use `gb.get_group(group_name)` for the group with value `group_name`:  \n```python\ngbA.get_group(1)\n```  \n```python\ngbA.get_group(2)\n```  \nNote that we used the *values* in the `A` column to access groups  \n<a id=\'exercise-0\'></a>\n**Exercise 1**  \nWe can *apply* some of our favorite aggregation functions directly on the\n`GroupBy` object  \nLook closely at the output of the cells below  \nHow did pandas compute the sum of `gbA`? What happened to the `NaN`\nentries in column `C`?  \nWrite your thoughts  \nHint: try `gbA.count()` or `gbA.mean()` if you can’t decide what\nhappened to the `NaN`  \n```python\ndf\n```  \n```python\ngbA.sum()\n```  \n<a id=\'exercise-1\'></a>\n**Exercise 2**  \nUse introspection (tab completion) to see what other aggregations are\ndefined for GroupBy objects.  \nPick three and evaluate them in the cells below.  \nDoes the output of each of these commands have the same features as the\noutput of `gbA.sum()` from above? If not, what is different?  \n```python' metadata={'Header 1': 'GroupBy', 'Header 2': 'Split-Apply-Combine', 'Header 3': 'First Example'}page_content='```  \n```python' metadata={'Header 1': 'method 1'}page_content='```  \n```python' metadata={'Header 1': 'method 2'}page_content='```  \nWe can also group by multiple columns  \nHow?  pass a list of strings to `groupby`  \nDataFrame will be split into collections of rows with unique combinations of requested columns  \n```python\ngbAB = df.groupby(["A", "B"])\ntype(gbAB)\n```  \n```python' metadata={'Header 1': 'method 3'}page_content='gbAB.get_group((1, 1))\n```  \nNotice that we still have a GroupBy object, so we can apply our favorite\naggregations.  \n```python\ngbAB.count()\n```  \nNotice that the output is a DataFrame with two levels on the index\nand a single column `C`. (Quiz: how do we know it is a DataFrame with\none column and not a Series?)  \nThis highlights a principle of how pandas handles the *Combine* part of\nthe strategy:  \n> The index of the combined DataFrame will be the group identifiers,\nwith one index level per group key' metadata={'Header 1': 'all rows below have A = 1 AND B = 1'}page_content='So far, we have been applying built-in aggregations to our GroupBy object  \nWe can also apply custom aggregations to each group of a GroupBy in two\nsteps:  \n1. Write our custom aggregation as a Python function\n1. Passing our function as an argument to the `.agg` method of a GroupBy  \nLet\'s try it!  \n```python\ndef num_missing(df):\n"Return the number of missing items in each column of df"\nreturn df.isnull().sum()\n```  \nWe can call this function on our original DataFrame to get the number of\nmissing items in each column  \n```python\nnum_missing(df)\n```  \nWe can also apply it to a GroupBy object to get the number of missing\nitems in each column *for each group*  \n```python\ngbA.agg(num_missing)\n```  \nThe function we write should either  \n- Consume `DataFrame` and return `Series`\n- Consume `Series`  and return `scalar`  \nPandas calls the function for each group  \nFor DataFrames, the function is called separately for each column' metadata={'Header 1': 'all rows below have A = 1 AND B = 1', 'Header 3': 'Custom Aggregate Functions'}page_content='As we saw in the [basics lecture](basics.ipynb), we can apply transforms to DataFrames  \nWe can do the same with GroupBy objects using the `.apply` method  \nLet’s see an example  \n```python\ndf\n```  \n```python\ndef smallest_by_b(df):\nreturn df.nsmallest(2, "B")\n```  \n```python\ngbA.apply(smallest_by_b)\n```  \n**NOTE**: The return value above has a two-level index  \n1. The value of `A`\n2. The index from the original DataFrame  \nThe second layer carried the original DataFrames\'s index because `smallest_by_b` kept the original index in its return value  \nIf `smallest_by_b` returned a different index, that would have shown up in `gbA.apply(smallest_by_b)`  \n<a id=\'exercise-2\'></a>\n**Exercise 3**  \nThis exercise has a few steps:  \n1. Write a function that, given a DataFrame, computes each entry’s deviation from the mean of its column\n2. Apply the function to `gbA`\n3. With your neighbor describe what the index and and columns are? Where are the group keys (the `A` column)?\n4. Determine the correct way to add these results back into `df` as new columns (Hint: remember the [merge](merge.ipynb) lecture)  \n```python' metadata={'Header 1': 'all rows below have A = 1 AND B = 1', 'Header 3': 'Transforms: The `apply` Method'}page_content='def deviation_from_mean(x):\n"""\nCompute the deviation of each value of x from its mean  \nParameters\n----------\nx: pd.Series, pd.DataFrame\nThe Series or DataFrame for which to do the computation  \nReturns\n-------\nx_hat: type(x)\nThe transformed version of x\n"""\nreturn x - x.mean()' metadata={'Header 1': 'write function here'}page_content='deviations = gbA.apply(deviation_from_mean)\ndeviations\n```  \n```python' metadata={'Header 1': 'apply function here'}page_content='df.merge(\ndeviations,\nleft_index=True,\nright_index=True,\nsuffixes=("", "_deviation")\n)\n```' metadata={'Header 1': 'add output of function as new columns to df here...'}page_content='Columns don\'t always contain desired groups  \nSome examples are:  \n- Grouping by a column and a level of the index\n- Grouping time series data at a particular frequency  \npandas lets you do this through the `pd.Grouper` type  \nTo see it in action, let’s make a copy of `df` with `A` moved to the\nindex and a `Date` column added  \n```python\ndf2 = df.copy()\ndf2["Date"] = pd.date_range(\nstart=pd.datetime.today().strftime("%m/%d/%Y"),\nfreq="BQ",\nperiods=df.shape[0]\n)\ndf2 = df2.set_index("A")\ndf2\n```  \nWe can group by year  \n```python\ndf2.groupby(pd.Grouper(key="Date", freq="A")).count()\n```  \nWe can group by the `A` level of the index  \n```python\ndf2.groupby(pd.Grouper(level="A")).count()\n```  \nWe can combine these to group by both  \n```python\ndf2.groupby([pd.Grouper(key="Date", freq="A"), pd.Grouper(level="A")]).count()\n```  \nAnd we can combine `pd.Grouper` with a string, where the string\ndenotes a column name  \n```python\ndf2.groupby([pd.Grouper(key="Date", freq="A"), "B"]).count()\n```' metadata={'Header 1': 'add output of function as new columns to df here...', 'Header 3': '`pd.Grouper`'}page_content='Let\'s practice on some real data!  \nWe\'ll revisit the airline dataset from the [merge](merge.ipynb) lecture  \n```python\nurl = "https://datascience.quantecon.org/assets/data/airline_performance_dec16.csv.zip"\nair_dec = pd.read_csv(url, parse_dates = [\'Date\'])\n```  \nFirst, we compute the average delay in arrival time for all carriers\neach week  \n```python\nweekly_delays = (\nair_dec\n.groupby([pd.Grouper(key="Date", freq="W"), "Carrier"])\n["ArrDelay"]               # extract one column\n.mean()                    # take average\n.unstack(level="Carrier")  # Flip carrier up as column names\n)\nweekly_delays\n```  \nLet’s also plot this data  \n```python' metadata={'Header 1': 'add output of function as new columns to df here...', 'Header 2': 'Case Study: Airline Delays'}page_content='axs = weekly_delays.plot.bar(\nfigsize=(10, 8), subplots=True, legend=False, sharex=True,\nsharey=True, layout=(4, 3), grid=False\n)' metadata={'Header 1': 'plot'}page_content='axs[0,0].get_figure().tight_layout()\nfor ax in axs[-1, :]:\nax.set_xticklabels(weekly_delays.index.strftime("%a, %b. %d\'"))\n```  \nMost delayed week ended on Sunday December (except for Frontier, who did *worse* on week of 25th)  \nLet’s see why...  \nThe `air_dec` DataFrame has information on the minutes of delay\nattributed to 5 different categories:  \n```python\ndelay_cols = [\n\'CarrierDelay\',\n\'WeatherDelay\',\n\'NASDelay\',\n\'SecurityDelay\',\n\'LateAircraftDelay\'\n]\n```  \nLet’s take a quick look at each of those delay categories for the week ending December 18, 2016  \n```python\npre_christmas = air_dec.loc[\n(air_dec["Date"] >= "2016-12-12") & (air_dec["Date"] <= "2016-12-18")\n]' metadata={'Header 1': 'tweak spacing between subplots and xaxis   labels'}page_content='def positive(df):\nreturn (df > 0).sum()  \ndelay_totals = pre_christmas.groupby("Carrier")[delay_cols].agg(["sum", "mean", positive])\ndelay_totals\n```  \n**Want**: plot total, average, and number of each type of delay by\ncarrier  \nTo do this, we need to have a DataFrame with:  \n- Delay type in index (so it is on horizontal-axis)\n- Aggregation method on *outer* most level of columns (so we can do\n`data["mean"]` to get averages)\n- Carrier name on inner level of columns  \nMany sequences of the reshaping commands can accomplish this  \nWe show one example below  \n```python\nreshaped_delays = (\ndelay_totals\n.stack()             # move aggregation method into index (with Carrier)\n.T                   # put delay type in index and Carrier+agg in column\n.swaplevel(axis=1)   # make agg method outer level of column label\n.sort_index(axis=1)  # sort column labels so it prints nicely\n)\nreshaped_delays\n```  \n```python\nfor agg in ["mean", "sum", "positive"]:\naxs = reshaped_delays[agg].plot(\nkind="bar", subplots=True, layout=(4, 3), figsize=(10, 8), legend=False,\nsharex=True, sharey=True\n)\nfig = axs[0, 0].get_figure()\nfig.suptitle(agg)' metadata={'Header 1': 'custom agg function'}page_content="```  \n<a id='exercise-3'></a>\n**Exercise 4**  \nThink about what is shown in the the plots above  \nAnswer questions like:  \n- Which type of delay was the most common?\n- Which one caused the largest average delay?\n- Does that vary by airline?  \nWrite your thoughts  \n```python" metadata={'Header 1': 'fig.tight_layout();'}page_content='```  \nLet’s summarize what we did:  \n- Computed average flight delay for each airline for each week\n- Noticed that one week had more delays for all airlines\n- Studied the flights in that week to determine the *cause* of the\ndelays in that week  \nSuppose now that we want to repeat that analysis, but at a daily\nfrequency instead of weekly  \nWe could copy/paste the code from above and change the `W` to a `D`,\nbut there’s a better way…  \nLet’s convert the steps above into two functions:  \n1. Produce the set of bar charts for average delays at each frequency\n1. Produce the second set of charts for the total, average, and number of occurrences of each type of delay  \n```python\ndef mean_delay_plot(df, freq, figsize=(10, 8)):\n"""\nMake a bar chart of average flight delays for each carrier at\na given frequency.\n"""\nmean_delays = (\ndf\n.groupby([pd.Grouper(key="Date", freq=freq), "Carrier"])\n["ArrDelay"]               # extract one column\n.mean()                    # take average\n.unstack(level="Carrier")  # Flip carrier up as column names\n)' metadata={'Header 1': 'your code here if needed'}page_content='axs = mean_delays.plot.bar(\nfigsize=figsize, subplots=True, legend=False, sharex=True,\nsharey=True, layout=(4, 3), grid=False\n)' metadata={'Header 1': 'plot'}page_content='axs[0, 0].get_figure().tight_layout()\nfor ax in axs[-1, :]:\nax.set_xticklabels(mean_delays.index.strftime("%a, %b. %d\'"))' metadata={'Header 1': 'tweak spacing between subplots and x-axis labels'}page_content='return axs\n```  \n```python\ndef delay_type_plot(df, start, end):\n"""\nMake bar charts for total minutes, average minutes, and number of\noccurrences for each delay type, for all flights that were scheduled\nbetween `start` date and `end` date\n"""\nsub_df = df.loc[\n(df["Date"] >= start) & (df["Date"] <= end)\n]  \ndef positive(df):\nreturn (df > 0).sum()  \naggs = sub_df.groupby("Carrier")[delay_cols].agg(["sum", "mean", positive])  \nreshaped = aggs.stack().T.swaplevel(axis=1).sort_index(axis=1)  \nfor agg in ["mean", "sum", "positive"]:\naxs = reshaped[agg].plot(\nkind="bar", subplots=True, layout=(4, 3), figsize=(10, 8), legend=False,\nsharex=True, sharey=True\n)\nfig = axs[0, 0].get_figure()\nfig.suptitle(agg)' metadata={'Header 1': 'return the axes in case we want to further tweak the plot outside the function'}page_content="```  \n<a id='exercise-4'></a>\n**Exercise 5**  \nVerify that we wrote the functions properly by setting the arguments to\nappropriate values to replicate the plots from above.  \n```python" metadata={'Header 1': 'fig.tight_layout();'}page_content='mean_delay_plot(air_dec, "W")\n```  \n```python' metadata={'Header 1': 'call mean_delay_plot here'}page_content='delay_type_plot(air_dec, "2016-12-12", "2016-12-18")\n```  \nNow let’s look at that plot at a daily frequency  \n```python' metadata={'Header 1': 'call delay_type_plot here'}page_content='mean_delay_plot(air_dec, "D", figsize=(16, 8));\n```  \nAs we expected given our analysis above, the longest average delays\nseemed to happen in the third week  \nIn particular, it looks like December 17th and 18th had — on average —\nhigher delays than other days in December  \nLet’s use the `delay_type_plot` function to determine the cause of the\ndelays on those two days  \nBecause our analysis is captured in a single function, we can look at\nthe days together and separately without much effort  \n```python' metadata={'Header 1': 'figure needs to be a bit wider to see all the dates'}page_content='delay_type_plot(air_dec, "12-17-16", "12-18-16")\n```  \n```python' metadata={'Header 1': 'both days'}page_content='delay_type_plot(air_dec, "12-17-16", "12-17-16")\n```  \n```python' metadata={'Header 1': 'only the 17th'}page_content='delay_type_plot(air_dec, "12-18-16", "12-18-16")\n```  \n- The purpose of this exercise was to drive home the ability to *automate* tasks\n- We wrote a pair of `functions` that allow us to easily repeat the exact same analysis on different subsets of the data, or different datasets entirely (e.g. we could do the same analysis on November 2016 data, with two lines of code)\n- These principles can be applied in many settings\n- Keep that in mind as we work through the rest of the materials' metadata={'Header 1': 'only the 18th'}page_content='The `qeds` library includes routines to simulate data sets in the\nformat of common sources  \nOne of these sources is [Shopify](https://www.shopify.com/) — an\ne-commerce platform used by many retail companies for online sales  \nThe code below will simulate a fairly large data set that has the\nproperties of a order-detail report from Shopify  \nWe’ll first look at the data, and then describe the exercise  \n```python' metadata={'Header 1': 'only the 18th', 'Header 2': 'Exercise: Cohort Analysis using Shopify Data'}page_content='random.seed(42)\nnp.random.seed(42)  \nurl = "https://datascience.quantecon.org/assets/data/shopify_orders.csv.zip"\norders = pd.read_csv(url)\norders.info()  \norders.head()\n```  \n**Definition:** A customer’s cohort is the month in which a customer placed\ntheir first order  \nThe customer type column indicates whether order was placed by a new or returning customer  \nWe now describe the *want* for the exercise, which we ask you to complete  \n**Want**: Compute the monthly total number of orders, total sales, and\ntotal quantity separated by customer cohort and customer type  \nRead that carefully one more time…' metadata={'Header 1': 'Set the "randomness" seeds'}page_content='Using the reshape and `groupby` tools you have learned, apply the want\noperator described above  \nSee below for advice on how to proceed  \nWhen you are finished, you should have something that looks like this:  \n<img src="https://datascience.quantecon.org/_images/groupby_cohort_analysis_exercise_output.png" alt="groupby\\_cohort\\_analysis\\_exercise\\_output.png" style="">  \nTwo notes on the table above:  \n1. Your actual output will be much bigger. This is just to give you an idea of what it might look like\n1. The numbers you produce should actually be the same as what are included in this table… Index into your answer and compare what you have with this table to verify your progress  \nNow, how to do it?  \nThere is more than one way to code this, but here are some suggested\nsteps.  \n1. Convert the `Day` column to have a `datetime` `dtype` instead of object (Hint: use the `pd.to_datetime` function)\n1. Add a new column that specifies the date associated with each\ncustomer’s `"First-time"` order\n- Hint 1: You can do this with a combination of `groupby` and\n`join`\n- Hint 2: `customer_type` is always one of `Returning` and\n`First-time`\n- Hint 3: Some customers don’t have a\n`customer_type == "First-time"` entry. You will need to set the\nvalue for these users to some date that precedes the dates in the\nsample. After adding valid data back into `orders` DataFrame,\nyou can identify which customers don’t have a `"First-Time"`\nentry by checking for missing data in the new column.\n1. You’ll need to group by 3 things\n1. You can apply one of the built-in aggregation functions to the GroupBy\n1. After doing the aggregation, you’ll need to use your reshaping skills to\nmove things to the right place in rows and columns  \nGood luck!  \n```python  \n```' metadata={'Header 1': 'Set the "randomness" seeds', 'Header 3': 'Extended Exercise'}