page_content="-   The internet is full of data\n-   Much of our lives and livelihoods lives on the internet\n-   The programming and theoretical skills we've developed can allow us\nto do meaningful analysis of internet data  \n… If we can get it into Python!" metadata={'Header 1': 'Intro'}page_content="-   Some data we see on websites data is easier to access:\n-   Provided as a downloadable file we can `pd.read_csv`\n-   Accessible via an API\n-   Contained in a clean html table for `pd.read_html`\n-   … but some of it isn't\n-   Today we'll learn how to access some of the data that is publicly\nvisible, but not easy to download" metadata={'Header 1': 'Intro', 'Header 2': 'Accessing Website Data'}page_content='-   Web pages are written in a markup language called HTML  \n-   HTML stands for "hyper text markup language"  \n-   All HTML documents are composed of a tree of nested elements  \n-   For example, this bullet point list would be written like this in\nHTML:  \n``` html\n<ul>\n<li>Web pages are..</li>\n<li>HTML stands for...</li>\n<li>All HTML documents...</li>\n<li>For example, this...</li>\n</ul>\n```' metadata={'Header 1': 'Intro', 'Header 2': 'HTML: Language of the web'}page_content="-   Below is an image that annotates the core parts of an HTML document  \n![](attachment:./html_parts.png)  \n-   Tag: name or type of an element\n-   CSS classes: used to style and change appearance (we'll use it to\nidentify specific elements!)\n-   id: Unique identifier for element **on whole webpage**\n-   value: `class` is one property, syntax is `property\\=value`\n-   text: the actual text contained in the element" metadata={'Header 1': 'Intro', 'Header 2': 'HTML: Language of the web', 'Header 3': 'Components of HTML'}page_content='-   Most webpages follow a very common structure:  \n``` html\n<!DOCTYPE HTML>\n<html>\n<head>\n<meta>...</meta>\n<title>...</title>\n<link>...</link>\n</head>\n<body>\n<h1>Title</h1>\n.... MANY MORE ELEMENTS HERE ...\n</body>\n</html>\n```  \n-   Almost all the data we will want to scrape is contained inside the\n`<body>` element' metadata={'Header 1': 'Intro', 'Header 2': 'HTML: Language of the web', 'Header 3': 'Structure of Webpage'}page_content="-   Let's see this in action\n-   We'll navigate to\n[<http://quotes.toscrape.com/random>](http://quotes.toscrape.com/random)\nand use our web browser to look at the HTML\n-   Things to look for:\n-   The outline from previous slide\n-   The use of `class`\n-   The hierarchy of the page" metadata={'Header 1': 'Intro', 'Header 2': 'HTML: Language of the web', 'Header 3': 'See it in Action!'}page_content="-   Now that we are warmed up, let's look at a page with multiple quotes\n-   Navigate to\n[<http://quotes.toscrape.com/>](http://quotes.toscrape.com/) and\nlook at the source\n-   We'll watch for the same main concepts/components" metadata={'Header 1': 'Intro', 'Header 2': 'HTML: Language of the web', 'Header 3': 'Multiple Quotes?'}page_content='-   Now the main question: "How could we scrape this data?"\n-   The key to web scraping is to be able to **identify** patterns\n-   My main strategy for doing this is to follow these steps:\n1.  View the webpage and identify visually the data I\'d like to\nscrape\n2.  Open browser tools and "inspect" the element containing my data\n3.  Look at that element\'s tag, classes, id to see how I could tell\na computer to identify it\n4.  Look outwards to other elements to scrape\n-   Same type of data, e.g. a price (find pattern in structure\nthat matches original element)\n-   Different type of data, e.g. an average review: start\nprocess again' metadata={'Header 1': 'Intro', 'Header 2': 'HTML: Language of the web', 'Header 3': 'How to "Scrape"?'}page_content="-   There are many Python libraries for scraping websites\n-   Perhaps the most widely used of these is called `scrapy`\n-   We will use `scrapy` to extract the quote information we saw on the\nexample websites\n-   First step would be to install scrapy if you haven't yet:\n`pip install scrapy`" metadata={'Header 1': 'Scrapy'}page_content="-   Scrapy can be run in the `scrapy shell` as we just saw\n-   However, one benefit from learning how to scrape websites is to be\nable to have scrapers run as programs that don't need manual\ninteractions\n-   Scrapy was built for this use case" metadata={'Header 1': 'Scrapy', 'Header 2': 'Running Scrapy'}page_content="-   Scrapy provides a scaffold we can use to organize our web scrapers\n-   This is called a scrapy project\n-   We can create one by running `scrapy startproject NAME` where `NAME`\nis the name of our project\n-   Let's try it!" metadata={'Header 1': 'Scrapy', 'Header 2': 'Scrapy Project'}page_content="-   We need to teach scrapy how to extract data from the web pages we\nhave it visit\n-   To do this we create a Spider\n-   A spider is a class we define that has at least the following\nfeatures:\n-   A list of websites to scrape\n-   A Python function for how to exract data from a single webpage\n-   We'll create our first spider now" metadata={'Header 1': 'Scrapy', 'Header 2': 'Spiders'}page_content='-   We can run a spider using `scrapy crawl NAME -o OUTFILE.EXT`, where\n-   `NAME` is the name of the spider\n-   `OUTFILE.EXT` are the name and extension for storing the data\n-   We can also have the spider continue on to another page\n-   To do this we need to find the next url to visit and tell scrapy\nto scrape it\n-   To scrape the next url we use the `response.follow(URL)` method  \n```python  \n```' metadata={'Header 1': 'Scrapy', 'Header 2': 'Working with Spiders'}