page_content='**Prerequisites**  \n- [pandas Intro](./v01_pandas_intro.ipynb)  \n**Outcomes**  \n- Be familiar with `datetime`\n- Use built-in aggregation functions and be able to create your own and\napply them using `agg`\n- Use built-in Series transformation functions and be able to create your\nown and apply them using `apply`\n- Use built-in scalar transformation functions and be able to create your\nown and apply them using `applymap`\n- Be able to select subsets of the DataFrame using boolean selection\n- Know what the “want operator” is and how to apply it  \n**Data**  \n- US state unemployment data from Bureau of Labor Statistics' metadata={'Header 1': 'Basic Functionality'}page_content='- [Basic Functionality](#Basic-Functionality)\n- [State Unemployment Data](#State-Unemployment-Data)\n- [Dates in pandas](#Dates-in-pandas)\n- [DataFrame Aggregations](#DataFrame-Aggregations)\n- [Transforms](#Transforms)\n- [Boolean Selection](#Boolean-Selection)\n- [Exercises](#Exercises)' metadata={'Header 1': 'Basic Functionality', 'Header 2': 'Outline'}page_content='In this lecture, we will use unemployment data by state at a monthly\nfrequency.  \n```python\nimport pandas as pd  \n%matplotlib inline' metadata={'Header 1': 'Basic Functionality', 'Header 2': 'State Unemployment Data'}page_content='import qeds\nqeds.themes.mpl_style();  \npd.__version__\n```  \nFirst, we will download the data directly from a url and read it into a pandas DataFrame.  \n```python' metadata={'Header 1': 'activate plot theme'}page_content='url = "https://datascience.quantecon.org/assets/data/state_unemployment.csv"\nunemp_raw = pd.read_csv(url, parse_dates=["Date"])\n```  \nThe pandas `read_csv` will determine most datatypes of the underlying columns.  The\nexception here is that we need to give pandas a hint so it can load up the `Date` column as a Python datetime type: the `parse_dates=["Date"]`.  \nWe can see the basic structure of the downloaded data by getting the first 5 rows, which directly matches\nthe underlying CSV file.  \n```python\nunemp_raw.head()\n```  \nNote that a row has a date, state, labor force size, and unemployment rate.  \nFor our analysis, we want to look at the unemployment rate across different states over time, which\nrequires a transformation of the data similar to an Excel pivot-table.  \n```python' metadata={'Header 1': 'activate plot theme', 'Header 2': 'Load up the data -- this will take a couple seconds'}page_content='unemp_all = (\nunemp_raw\n.reset_index()\n.pivot_table(index="Date", columns="state", values="UnemploymentRate")\n)\nunemp_all.head()\n```  \nFinally, we can filter it to look at a subset of the columns (i.e. “state” in this case).  \n```python\nstates = [\n"Arizona", "California", "Florida", "Illinois",\n"Michigan", "New York", "Texas"\n]\nunemp = unemp_all[states]\nunemp.head()\n```  \nWhen plotting, a DataFrame knows the column and index names.  \n```python\nunemp.plot(figsize=(8, 6))\n```' metadata={'Header 1': "Don't worry about the details here quite yet"}page_content='You might have noticed that our index now has a nice format for the\ndates (`YYYY-MM-DD`) rather than just a year.  \nThis is because the `dtype` of the index is a variant of `datetime`.  \n```python\nunemp.index\n```  \nWe can index into a DataFrame with a `DatetimeIndex` using string\nrepresentations of dates.  \nFor example  \n```python' metadata={'Header 1': "Don't worry about the details here quite yet", 'Header 2': 'Dates in pandas'}page_content='unemp.loc["2000-01-01", :]\n```  \n```python' metadata={'Header 1': 'Data corresponding to a single date'}page_content='unemp.loc["01/01/2000":"06/01/2000", :]\n```  \nWe will learn more about what pandas can do with dates and times in an\nupcoming lecture on time series data.' metadata={'Header 1': 'Data for all days between New Years Day and June first in the year 2000'}page_content='Let’s talk about *aggregations*.  \nLoosely speaking, an aggregation is an operation that combines multiple\nvalues into a single value.  \nFor example, computing the mean of three numbers (for example\n`[0, 1, 2]`) returns a single number (1).  \nWe will use aggregations extensively as we analyze and manipulate our data.  \nThankfully, pandas makes this easy!' metadata={'Header 1': 'Data for all days between New Years Day and June first in the year 2000', 'Header 2': 'DataFrame Aggregations'}page_content='pandas already has some of the most frequently used aggregations.  \nFor example:  \n- Mean  (`mean`)\n- Variance (`var`)\n- Standard deviation (`std`)\n- Minimum (`min`)\n- Median (`median`)\n- Maximum (`max`)\n- etc…  \n>**Note**\n>\n>When looking for common operations, using “tab completion” goes a long way.  \n```python\nunemp.mean()\n```  \nAs seen above, the aggregation’s default is to aggregate each column.  \nHowever, by using the `axis` keyword argument, you can do aggregations by\nrow as well.  \n```python\nunemp.var(axis=1).head()\n```' metadata={'Header 1': 'Data for all days between New Years Day and June first in the year 2000', 'Header 2': 'DataFrame Aggregations', 'Header 3': 'Built-in Aggregations'}page_content='The built-in aggregations will get us pretty far in our analysis, but\nsometimes we need more flexibility.  \nWe can have pandas perform custom aggregations by following these two\nsteps:  \n1. Write a Python function that takes a `Series` as an input and\noutputs a single value.\n1. Call the `agg` method with our new function as an argument.  \nFor example, below, we will classify states as “low unemployment” or\n“high unemployment” based on whether their mean unemployment level is\nabove or below 6.5.  \n```python' metadata={'Header 1': 'Data for all days between New Years Day and June first in the year 2000', 'Header 2': 'DataFrame Aggregations', 'Header 3': 'Writing Your Own Aggregation'}page_content='def high_or_low(s):\n"""\nThis function takes a pandas Series object and returns high\nif the mean is above 6.5 and low if the mean is below 6.5\n"""\nif s.mean() < 6.5:\nout = "Low"\nelse:\nout = "High"  \nreturn out\n```  \n```python  \nunemp.agg(high_or_low)\n```  \n```python' metadata={'Header 1': ''}page_content='unemp.agg(high_or_low, axis=1).head()\n```  \nNotice that `agg` can also accept multiple functions at once.  \n```python\nunemp.agg([min, max, high_or_low])\n```  \n**Exercise 2**  \nDo the following exercises in separate code cells below:  \n- At each date, what is the minimum unemployment rate across all states\nin our sample?\n- What was the median unemployment rate in each state?\n- What was the maximum unemployment rate across the states in our\nsample? What state did it happen in? In what month/year was this\nachieved?\n- Hint 1: What Python type (not `dtype`) is returned by the\naggregation?\n- Hint 2: Read documentation for the method `idxmax`\n- Classify each state as high or low volatility based on whether the\nvariance of their unemployment is above or below 4.  \n```python\nunemp.min(axis=1)\n```  \n```python\nunemp.median()\n```  \n```python\nprint(unemp.max().max())\nprint(unemp.max().idxmax())\nprint(unemp.max(axis=1).idxmax())\n```  \n```python\ndef ur_volatility_classify(s):\n"""\nClassifies the volatility of a series. A series is deemed\nhigh volatility if the variance is higher than 4  \nParameters\n----------\ns : Series\nThe series that we are classifying  \nReturns\n-------\n_ : str\nEither "high" or "low"\n"""\nvar = s.var()  \nreturn "high" if var > 4 else "low"  \nunemp.apply(ur_volatility_classify)\n```' metadata={'Header 1': 'How does this differ from unemp.agg(high_or_low)?'}page_content='Many analytical operations do not necessarily involve an aggregation.  \nThe output of a function applied to a Series might need to be a new\nSeries.  \nSome examples:  \n- Compute the percentage change in unemployment from month to month.\n- Calculate the cumulative sum of elements in each column.' metadata={'Header 1': 'How does this differ from unemp.agg(high_or_low)?', 'Header 2': 'Transforms'}page_content='pandas comes with many transform functions including:  \n- Cumulative sum/max/min/product (`cum(sum|min|max|prod)`)\n- Difference  (`diff`)\n- Elementwise addition/subtraction/multiplication/division (`+`, `-`, `*`, `/`)\n- Percent change (`pct_change`)\n- Number of occurrences of each distinct value (`value_counts`)\n- Absolute value (`abs`)  \nAgain, tab completion is helpful when trying to find these functions.  \n```python\nunemp.head()\n```  \n```python\nunemp.pct_change().head()\n```  \n```python\nunemp.diff().head()\n```  \nTransforms can be split into to several main categories:  \n1. *Series transforms*: functions that take in one Series and produce another Series. The index of the input and output does not need to be the same.\n1. *Scalar transforms*: functions that take a single value and produce a single value. An example is the `abs` method, or adding a constant to each value of a Series.' metadata={'Header 1': 'How does this differ from unemp.agg(high_or_low)?', 'Header 2': 'Transforms', 'Header 3': 'Built-in Transforms'}page_content='pandas also simplifies applying custom Series transforms to a Series or the\ncolumns of a DataFrame. The steps are:  \n1. Write a Python function that takes a Series and outputs a new Series.\n1. Pass our new function as an argument to the `apply` method (alternatively, the `transform` method).  \nAs an example, we will standardize our unemployment data to have mean 0\nand standard deviation 1.  \nAfter doing this, we can use an aggregation to determine at which date the\nunemployment rate is most different from “normal times” for each state.  \n```python' metadata={'Header 1': 'How does this differ from unemp.agg(high_or_low)?', 'Header 2': 'Transforms', 'Header 3': 'Custom Series Transforms'}page_content='def standardize_data(x):\n"""\nChanges the data in a Series to become mean 0 with standard deviation 1\n"""\nmu = x.mean()\nstd = x.std()  \nreturn (x - mu)/std\n```  \n```python  \nstd_unemp = unemp.apply(standardize_data)\nstd_unemp.head()\n```  \n```python' metadata={'Header 1': ''}page_content='abs_std_unemp = std_unemp.abs()  \nabs_std_unemp.head()\n```  \n```python' metadata={'Header 1': 'Takes the absolute value of all elements of a function'}page_content='def idxmax(x):' metadata={'Header 1': 'find the date when unemployment was "most different from normal" for each State'}page_content='return x.idxmax()  \nabs_std_unemp.agg(idxmax)\n```' metadata={'Header 1': 'idxmax of Series will return index of maximal value'}page_content='As you may have predicted, we can also apply custom scalar transforms to our\npandas data.  \nTo do this, we use the following pattern:  \n1. Define a Python function that takes in a scalar and produces a scalar.\n1. Pass this function as an argument to the `applymap` Series or DataFrame method.  \n**Example**  \nImagine that we want to determine whether unemployment was high (> 6.5),\nmedium (4.5 < x <= 6.5), or low (<= 4.5) for each state and each month.  \n1. Write a Python function that takes a single number as an input and\noutputs a single string noting if that number is high, medium, or low.\n2. Pass your function to `applymap` (quiz: why `applymap` and not\n`agg` or `apply`?) and save the result in a new DataFrame called\n`unemp_bins`.  \n```python\ndef unemployment_classifier(ur):\n"""\nClassifies the unemployment rate as high, medium, or low\nbased on the value  \nParameters\n----------\nur : scalar(float)\nThe unemployment rate  \nReturns\n-------\nout : str\nThe classification "high", "medium", or "low"\n"""\nif ur > 6.5:\nreturn "high"\nelif ur > 4.5:\nreturn "medium"\nelse:\nreturn "low"\n```  \n```python\nunemp_bins = unemp.applymap(unemployment_classifier)\nunemp_bins.head()\n```  \n3. (Challenging) This exercise has multiple parts:\n1. Use another transform on `unemp_bins` to count how many\ntimes each state had each of the three classifications.\n- Hint 1: Will this value counting function be a Series or scalar\ntransform?\n- Hint 2: Try googling "pandas count unique value" or something\nsimilar to find the right transform.\n2. Construct a horizontal bar chart of the number of occurrences of\neach level with one bar per state and classification (21 total\nbars).  \n```python\nunemp_class_count = unemp_bins.apply(\npd.Series.value_counts\n)  \nunemp_class_count.head()\n```  \n```python\nunemp_class_count.T.loc[:, ["low", "medium", "high"]].plot(kind="bar")\n```  \n4. (Challenging) Repeat the previous step, but count how many states had\neach classification in each month. Which month had the most states\nwith high unemployment? What about medium and low?  \n```python\nunemp_monthly_count = unemp_bins.apply(\npd.Series.value_counts, axis=1\n).fillna(0.0)  \nunemp_monthly_count.head()\n```  \n```python\nunemp_monthly_count["high"].idxmax()\n```  \n```python\nunemp_monthly_count["medium"].idxmax()\n```  \n```python\nunemp_monthly_count["low"].idxmax()\n```' metadata={'Header 1': 'idxmax of Series will return index of maximal value', 'Header 3': 'Custom Scalar Transforms'}page_content="We have seen how we can select subsets of data by referring to the index\nor column names.  \nHowever, we often want to select based on conditions met by\nthe data itself.  \nSome examples are:  \n- Restrict analysis to all individuals older than 18.\n- Look at data that corresponds to particular time periods.\n- Analyze only data that corresponds to a recession.\n- Obtain data for a specific product or customer ID.  \nWe will be able to do this by using a Series or list of boolean values\nto index into a Series or DataFrame.  \nLet’s look at some examples.  \n```python\nunemp_small = unemp.head()  # Create smaller data so we can see what's happening\nunemp_small\n```  \n```python" metadata={'Header 1': 'idxmax of Series will return index of maximal value', 'Header 2': 'Boolean Selection'}page_content='unemp_small.loc[[True, True, True, False, False], :]\n```  \n```python\npd.DataFrame({\'Arizona\': [4.1, 4.1, 4.0, 4.0, 4.0],\n\'California\': [5.0, 5.0, 5.0, 5.1, 5.1],\n\'Florida\': [3.7, 3.7, 3.7, 3.7, 3.7],\n\'Illinois\': [4.2, 4.2, 4.3, 4.3, 4.3],\n\'Michigan\': [3.3, 3.2, 3.2, 3.3, 3.5],\n\'New York\': [4.7, 4.7, 4.6, 4.6, 4.6],\n\'Texas\': [4.6, 4.6, 4.5, 4.4, 4.3]})\n```  \n```python\ndf = unemp_small\ndf.loc[(df["California"] < 5.1) & (df["Texas"].isin([4.6, 4.3]))]\n```  \n```python\nunemp_small.to_dict(orient="list")\n```  \n```python' metadata={'Header 1': 'list of booleans selects rows'}page_content='unemp_small.loc[[True, False, True, False, True], :]\n```  \n```python' metadata={'Header 1': 'here we use it to select all columns'}page_content='unemp_small.loc[[True, True, True, False, False], [True, False, False, False, False, True, True]]\n```' metadata={'Header 1': 'can use booleans to select both rows and columns'}page_content='We can use [conditional statements](../python_fundamentals/control_flow.ipynb) to\nconstruct Series of booleans from our data.  \n```python\nunemp_small["Texas"] < 4.5\n```  \nOnce we have our Series of bools, we can use it to extract subsets of\nrows from our DataFrame.  \n```python\nunemp_small.loc[unemp_small["Texas"] < 4.5, :]\n```  \n```python\nunemp_small["New York"] > unemp_small["Texas"]\n```  \n```python\nbig_NY = unemp_small["New York"] > unemp_small["Texas"]\nunemp_small.loc[big_NY]\n```' metadata={'Header 1': 'can use booleans to select both rows and columns', 'Header 3': 'Creating Boolean DataFrames/Series'}page_content='In the boolean section of the [basics lecture](../python_fundamentals/basics.ipynb), we saw\nthat we can use the words `and` and `or` to combine multiple booleans into\na single bool.  \nRecall:  \n- `True and False -> False`\n- `True and True -> True`\n- `False and False -> False`\n- `True or False -> True`\n- `True or True -> True`\n- `False or False -> False`  \nWe can do something similar in pandas, but instead of\n`bool1 and bool2` we write:  \n```python\n(bool_series1) & (bool_series2)\n```  \nLikewise, instead of `bool1 or bool2` we write:  \n```python\n(bool_series1) | (bool_series2)\n```  \n```python\nsmall_NYTX = (unemp_small["Texas"] < 4.7) & (unemp_small["New York"] < 4.7)\nsmall_NYTX\n```  \n```python\nunemp_small[small_NYTX]\n```' metadata={'Header 1': 'can use booleans to select both rows and columns', 'Header 3': 'Creating Boolean DataFrames/Series', 'Header 4': 'Multiple Conditions'}page_content='Sometimes, we will want to check whether a data point takes on one of a\nseveral fixed values.  \nWe could do this by writing `(df["x"] == val_1) | (df["x"] == val_2)`\n(like we did above), but there is a better way: the `.isin` method  \n```python\nunemp_small["Michigan"].isin([3.3, 3.2])\n```  \n```python' metadata={'Header 1': 'can use booleans to select both rows and columns', 'Header 3': 'Creating Boolean DataFrames/Series', 'Header 4': '`isin`'}page_content='unemp_small.loc[unemp_small["Michigan"].isin([3.3, 3.2])]\n```' metadata={'Header 1': 'now select full rows where this Series is True'}page_content='Recall from the boolean section of the [basics lecture](../python_fundamentals/basics.ipynb)\nthat the Python functions `any` and `all` are aggregation functions that\ntake a collection of booleans and return a single boolean.  \n`any` returns True whenever at least one of the inputs are True while\n`all` is True only when all the inputs are `True`.  \nSeries and DataFrames with `dtype` bool have `.any` and `.all`\nmethods that apply this logic to pandas objects.  \nLet’s use these methods to count how many months all the states in our\nsample had high unemployment.  \nAs we work through this example, consider the [“want\noperator”](http://albertjmenkveld.com/2014/07/07/endogeneous-price-dispersion/), a helpful\nconcept we learned from [Tom\nSargent](http://www.tomsargent.com) for clearly stating the goal of our analysis and\ndetermining the steps necessary to reach the goal.  \nWe always begin by writing `Want:` followed by what we want to\naccomplish.  \nIn this case, we would write:  \n> Want: Count the number of months in which all states in our sample\nhad unemployment above 6.5%  \nAfter identifying the **want**, we work *backwards* to identify the\nsteps necessary to accomplish our goal.  \nSo, starting from the result, we have:  \n1. Sum the number of `True` values in a Series indicating dates for\nwhich all states had high unemployment.\n1. Build the Series used in the last step by using the `.all` method\non a DataFrame containing booleans indicating whether each state had\nhigh unemployment at each date.\n1. Build the DataFrame used in the previous step using a `>`\ncomparison.  \nNow that we have a clear plan, let’s follow through and *apply* the want\noperator:  \n```python' metadata={'Header 1': 'now select full rows where this Series is True', 'Header 4': '`.any` and `.all`'}page_content='high = unemp > 6.5\nhigh.head()\n```  \n```python' metadata={'Header 1': 'Step 3: construct the DataFrame of bools'}page_content='all_high = high.all(axis=1)\nall_high.head()\n```  \n```python' metadata={'Header 1': 'Step 2: use the .all method on axis=1 to get the dates where all states have a True'}page_content='msg = "Out of {} months, {} had high unemployment across all states"\nprint(msg.format(len(all_high), all_high.sum()))\n```' metadata={'Header 1': '(note that True == 1 and False == 0 in Python, so .sum will count Trues)'}