{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3903ba51-a3cf-472d-97c8-449c03e7fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9644ab-ccbe-4e3c-88d4-c60846180859",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '2.1.1 pandas intro.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b41d57-b3ae-48a1-a100-e83574e028fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(video_path, 'rb') as video_file:\n",
    "    video_data = video_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0eadb0-fbfd-4d55-a56c-e6773fea6ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a9f293-5b41-4d95-ac07-308085746639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55517554"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caabe73a-2f6a-4abe-a71c-036b3d1f40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f25bd04-ce39-4e5a-a41e-956a74adf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111d634c-81f1-4f33-9ec2-c75edb86f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b4fc16-74b0-4906-ae61-04f8f7a642ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85684b99-e9e2-48ca-8332-b00a71ec817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mp.VideoFileClip(\"./2.1.1 pandas intro.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc307604-9b50-4c43-8713-d9c95e446ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<moviepy.video.io.VideoFileClip.VideoFileClip at 0x7f7fcb63d190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38556e21-5eac-4d41-a38f-8e2382f1b7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./2.1.1 pandas intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "m.audio.write_audiofile(\"./2.1.1 pandas intro.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62aea331-4cd3-4560-b150-c98c99ae3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:13<00:00, 11.2MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515c7884-a630-43bb-a209-18b9ad784a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(\"./2.1.1 pandas intro.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0f9c374-baa2-4634-8c17-9bef1f942bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m   \u001b[0msrt_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m        WriteSRT\n",
       "\u001b[0;31mString form:\u001b[0m <whisper.utils.WriteSRT object at 0x7f7fcb49fb10>\n",
       "\u001b[0;31mFile:\u001b[0m        ~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/whisper/utils.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?srt_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e73e364a-b86f-4649-ab83-9cef87db3630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'segments', 'language'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7b934ad-d714-4abd-8d33-3a2b71c86fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.utils import get_writer\n",
    "\n",
    "output_directory = \"./\"\n",
    "audio = \"./2.1.1 pandas intro.mp3\"\n",
    "options = {\n",
    "    \"max_line_width\": 100, \n",
    "    \"max_line_count\": 10000000000000000, \n",
    "    \"highlight_words\": False\n",
    "}\n",
    "\n",
    "# Save as an SRT file\n",
    "srt_writer = get_writer(\"srt\", output_directory)\n",
    "srt_writer(result, audio, options)\n",
    "\n",
    "# Save as a VTT file\n",
    "vtt_writer = get_writer(\"vtt\", output_directory)\n",
    "vtt_writer(result, audio, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af22023-f5cf-4b6b-8551-c455c3c37730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'seek': 0,\n",
       "  'start': 0.0,\n",
       "  'end': 5.0,\n",
       "  'text': \" Hi, this is Chase Coleman. Today we're going to be talking about the Python library pandas.\",\n",
       "  'tokens': [50364,\n",
       "   2421,\n",
       "   11,\n",
       "   341,\n",
       "   307,\n",
       "   21384,\n",
       "   49930,\n",
       "   13,\n",
       "   2692,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   312,\n",
       "   1417,\n",
       "   466,\n",
       "   264,\n",
       "   15329,\n",
       "   6405,\n",
       "   4565,\n",
       "   296,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.14858338038126628,\n",
       "  'compression_ratio': 1.6916666666666667,\n",
       "  'no_speech_prob': 0.06699376553297043},\n",
       " {'id': 1,\n",
       "  'seek': 0,\n",
       "  'start': 5.0,\n",
       "  'end': 12.0,\n",
       "  'text': ' Prior today you should have completed the Python fundamentals training. This was something we offered over the summer.',\n",
       "  'tokens': [50614,\n",
       "   24032,\n",
       "   965,\n",
       "   291,\n",
       "   820,\n",
       "   362,\n",
       "   7365,\n",
       "   264,\n",
       "   15329,\n",
       "   29505,\n",
       "   3097,\n",
       "   13,\n",
       "   639,\n",
       "   390,\n",
       "   746,\n",
       "   321,\n",
       "   8059,\n",
       "   670,\n",
       "   264,\n",
       "   4266,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.14858338038126628,\n",
       "  'compression_ratio': 1.6916666666666667,\n",
       "  'no_speech_prob': 0.06699376553297043},\n",
       " {'id': 2,\n",
       "  'seek': 0,\n",
       "  'start': 12.0,\n",
       "  'end': 19.0,\n",
       "  'text': ' At the end of today you should understand some of the core pandas objects such as a series and a data frame.',\n",
       "  'tokens': [50964,\n",
       "   1711,\n",
       "   264,\n",
       "   917,\n",
       "   295,\n",
       "   965,\n",
       "   291,\n",
       "   820,\n",
       "   1223,\n",
       "   512,\n",
       "   295,\n",
       "   264,\n",
       "   4965,\n",
       "   4565,\n",
       "   296,\n",
       "   6565,\n",
       "   1270,\n",
       "   382,\n",
       "   257,\n",
       "   2638,\n",
       "   293,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.14858338038126628,\n",
       "  'compression_ratio': 1.6916666666666667,\n",
       "  'no_speech_prob': 0.06699376553297043},\n",
       " {'id': 3,\n",
       "  'seek': 0,\n",
       "  'start': 19.0,\n",
       "  'end': 24.0,\n",
       "  'text': ' You should understand how to index into particular elements of series and data frames.',\n",
       "  'tokens': [51314,\n",
       "   509,\n",
       "   820,\n",
       "   1223,\n",
       "   577,\n",
       "   281,\n",
       "   8186,\n",
       "   666,\n",
       "   1729,\n",
       "   4959,\n",
       "   295,\n",
       "   2638,\n",
       "   293,\n",
       "   1412,\n",
       "   12083,\n",
       "   13,\n",
       "   51564],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.14858338038126628,\n",
       "  'compression_ratio': 1.6916666666666667,\n",
       "  'no_speech_prob': 0.06699376553297043},\n",
       " {'id': 4,\n",
       "  'seek': 2400,\n",
       "  'start': 24.0,\n",
       "  'end': 30.0,\n",
       "  'text': ' And you should understand some of the basic types that can be stored inside of a series or a data frame.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   291,\n",
       "   820,\n",
       "   1223,\n",
       "   512,\n",
       "   295,\n",
       "   264,\n",
       "   3875,\n",
       "   3467,\n",
       "   300,\n",
       "   393,\n",
       "   312,\n",
       "   12187,\n",
       "   1854,\n",
       "   295,\n",
       "   257,\n",
       "   2638,\n",
       "   420,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   50664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09225326425888959,\n",
       "  'compression_ratio': 1.8506224066390042,\n",
       "  'no_speech_prob': 0.0013921794015914202},\n",
       " {'id': 5,\n",
       "  'seek': 2400,\n",
       "  'start': 30.0,\n",
       "  'end': 35.0,\n",
       "  'text': \" Additionally, as we go you'll learn how to make some basic visualizations.\",\n",
       "  'tokens': [50664,\n",
       "   19927,\n",
       "   11,\n",
       "   382,\n",
       "   321,\n",
       "   352,\n",
       "   291,\n",
       "   603,\n",
       "   1466,\n",
       "   577,\n",
       "   281,\n",
       "   652,\n",
       "   512,\n",
       "   3875,\n",
       "   5056,\n",
       "   14455,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09225326425888959,\n",
       "  'compression_ratio': 1.8506224066390042,\n",
       "  'no_speech_prob': 0.0013921794015914202},\n",
       " {'id': 6,\n",
       "  'seek': 2400,\n",
       "  'start': 35.0,\n",
       "  'end': 43.0,\n",
       "  'text': ' The outline for today is to talk a little bit about the pandas library to introduce what a series is and then introduce what a data frame is.',\n",
       "  'tokens': [50914,\n",
       "   440,\n",
       "   16387,\n",
       "   337,\n",
       "   965,\n",
       "   307,\n",
       "   281,\n",
       "   751,\n",
       "   257,\n",
       "   707,\n",
       "   857,\n",
       "   466,\n",
       "   264,\n",
       "   4565,\n",
       "   296,\n",
       "   6405,\n",
       "   281,\n",
       "   5366,\n",
       "   437,\n",
       "   257,\n",
       "   2638,\n",
       "   307,\n",
       "   293,\n",
       "   550,\n",
       "   5366,\n",
       "   437,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   307,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09225326425888959,\n",
       "  'compression_ratio': 1.8506224066390042,\n",
       "  'no_speech_prob': 0.0013921794015914202},\n",
       " {'id': 7,\n",
       "  'seek': 2400,\n",
       "  'start': 43.0,\n",
       "  'end': 51.0,\n",
       "  'text': ' To talk about some of the different data types available in pandas and then to talk about modifying some of the data frames.',\n",
       "  'tokens': [51314,\n",
       "   1407,\n",
       "   751,\n",
       "   466,\n",
       "   512,\n",
       "   295,\n",
       "   264,\n",
       "   819,\n",
       "   1412,\n",
       "   3467,\n",
       "   2435,\n",
       "   294,\n",
       "   4565,\n",
       "   296,\n",
       "   293,\n",
       "   550,\n",
       "   281,\n",
       "   751,\n",
       "   466,\n",
       "   42626,\n",
       "   512,\n",
       "   295,\n",
       "   264,\n",
       "   1412,\n",
       "   12083,\n",
       "   13,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09225326425888959,\n",
       "  'compression_ratio': 1.8506224066390042,\n",
       "  'no_speech_prob': 0.0013921794015914202},\n",
       " {'id': 8,\n",
       "  'seek': 5400,\n",
       "  'start': 54.0,\n",
       "  'end': 61.0,\n",
       "  'text': \" Great. So let's go ahead and get started.\",\n",
       "  'tokens': [50364,\n",
       "   3769,\n",
       "   13,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   352,\n",
       "   2286,\n",
       "   293,\n",
       "   483,\n",
       "   1409,\n",
       "   13,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10478768167616445,\n",
       "  'compression_ratio': 1.574468085106383,\n",
       "  'no_speech_prob': 0.03404330089688301},\n",
       " {'id': 9,\n",
       "  'seek': 5400,\n",
       "  'start': 61.0,\n",
       "  'end': 70.0,\n",
       "  'text': \" So the pandas package will be imported and typically you'll give it the alias PD. So we'll write import pandas as PD.\",\n",
       "  'tokens': [50714,\n",
       "   407,\n",
       "   264,\n",
       "   4565,\n",
       "   296,\n",
       "   7372,\n",
       "   486,\n",
       "   312,\n",
       "   25524,\n",
       "   293,\n",
       "   5850,\n",
       "   291,\n",
       "   603,\n",
       "   976,\n",
       "   309,\n",
       "   264,\n",
       "   419,\n",
       "   4609,\n",
       "   10464,\n",
       "   13,\n",
       "   407,\n",
       "   321,\n",
       "   603,\n",
       "   2464,\n",
       "   974,\n",
       "   4565,\n",
       "   296,\n",
       "   382,\n",
       "   10464,\n",
       "   13,\n",
       "   51164],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10478768167616445,\n",
       "  'compression_ratio': 1.574468085106383,\n",
       "  'no_speech_prob': 0.03404330089688301},\n",
       " {'id': 10,\n",
       "  'seek': 5400,\n",
       "  'start': 70.0,\n",
       "  'end': 79.0,\n",
       "  'text': \" Don't worry about what this line does for now, but basically what it does is it's going to allow us to put plots inside of our notebook.\",\n",
       "  'tokens': [51164,\n",
       "   1468,\n",
       "   380,\n",
       "   3292,\n",
       "   466,\n",
       "   437,\n",
       "   341,\n",
       "   1622,\n",
       "   775,\n",
       "   337,\n",
       "   586,\n",
       "   11,\n",
       "   457,\n",
       "   1936,\n",
       "   437,\n",
       "   309,\n",
       "   775,\n",
       "   307,\n",
       "   309,\n",
       "   311,\n",
       "   516,\n",
       "   281,\n",
       "   2089,\n",
       "   505,\n",
       "   281,\n",
       "   829,\n",
       "   28609,\n",
       "   1854,\n",
       "   295,\n",
       "   527,\n",
       "   21060,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10478768167616445,\n",
       "  'compression_ratio': 1.574468085106383,\n",
       "  'no_speech_prob': 0.03404330089688301},\n",
       " {'id': 11,\n",
       "  'seek': 7900,\n",
       "  'start': 79.0,\n",
       "  'end': 90.0,\n",
       "  'text': \" And then finally we're going to activate the want econ data science plotting theme.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   550,\n",
       "   2721,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   13615,\n",
       "   264,\n",
       "   528,\n",
       "   23692,\n",
       "   1412,\n",
       "   3497,\n",
       "   41178,\n",
       "   6314,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13161516189575195,\n",
       "  'compression_ratio': 1.3798882681564246,\n",
       "  'no_speech_prob': 0.06343890726566315},\n",
       " {'id': 12,\n",
       "  'seek': 7900,\n",
       "  'start': 90.0,\n",
       "  'end': 94.0,\n",
       "  'text': \" Now let's go ahead and figure out what version of pandas everyone has.\",\n",
       "  'tokens': [50914,\n",
       "   823,\n",
       "   718,\n",
       "   311,\n",
       "   352,\n",
       "   2286,\n",
       "   293,\n",
       "   2573,\n",
       "   484,\n",
       "   437,\n",
       "   3037,\n",
       "   295,\n",
       "   4565,\n",
       "   296,\n",
       "   1518,\n",
       "   575,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13161516189575195,\n",
       "  'compression_ratio': 1.3798882681564246,\n",
       "  'no_speech_prob': 0.06343890726566315},\n",
       " {'id': 13,\n",
       "  'seek': 7900,\n",
       "  'start': 94.0,\n",
       "  'end': 98.0,\n",
       "  'text': ' So on my computer I have version 1.1.1.',\n",
       "  'tokens': [51114,\n",
       "   407,\n",
       "   322,\n",
       "   452,\n",
       "   3820,\n",
       "   286,\n",
       "   362,\n",
       "   3037,\n",
       "   502,\n",
       "   13,\n",
       "   16,\n",
       "   13,\n",
       "   16,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13161516189575195,\n",
       "  'compression_ratio': 1.3798882681564246,\n",
       "  'no_speech_prob': 0.06343890726566315},\n",
       " {'id': 14,\n",
       "  'seek': 7900,\n",
       "  'start': 98.0,\n",
       "  'end': 108.0,\n",
       "  'text': \" But anything that's higher than 0.25 should be okay.\",\n",
       "  'tokens': [51314,\n",
       "   583,\n",
       "   1340,\n",
       "   300,\n",
       "   311,\n",
       "   2946,\n",
       "   813,\n",
       "   1958,\n",
       "   13,\n",
       "   6074,\n",
       "   820,\n",
       "   312,\n",
       "   1392,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13161516189575195,\n",
       "  'compression_ratio': 1.3798882681564246,\n",
       "  'no_speech_prob': 0.06343890726566315},\n",
       " {'id': 15,\n",
       "  'seek': 10800,\n",
       "  'start': 108.0,\n",
       "  'end': 115.0,\n",
       "  'text': \" Great. So the first data type we're going to introduce is called a pandas series.\",\n",
       "  'tokens': [50364,\n",
       "   3769,\n",
       "   13,\n",
       "   407,\n",
       "   264,\n",
       "   700,\n",
       "   1412,\n",
       "   2010,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   5366,\n",
       "   307,\n",
       "   1219,\n",
       "   257,\n",
       "   4565,\n",
       "   296,\n",
       "   2638,\n",
       "   13,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09554120741392437,\n",
       "  'compression_ratio': 1.5408163265306123,\n",
       "  'no_speech_prob': 0.0065357559360563755},\n",
       " {'id': 16,\n",
       "  'seek': 10800,\n",
       "  'start': 115.0,\n",
       "  'end': 124.0,\n",
       "  'text': ' And a series represents a single column of data and it will be associated with row labels for each observation.',\n",
       "  'tokens': [50714,\n",
       "   400,\n",
       "   257,\n",
       "   2638,\n",
       "   8855,\n",
       "   257,\n",
       "   2167,\n",
       "   7738,\n",
       "   295,\n",
       "   1412,\n",
       "   293,\n",
       "   309,\n",
       "   486,\n",
       "   312,\n",
       "   6615,\n",
       "   365,\n",
       "   5386,\n",
       "   16949,\n",
       "   337,\n",
       "   1184,\n",
       "   14816,\n",
       "   13,\n",
       "   51164],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09554120741392437,\n",
       "  'compression_ratio': 1.5408163265306123,\n",
       "  'no_speech_prob': 0.0065357559360563755},\n",
       " {'id': 17,\n",
       "  'seek': 10800,\n",
       "  'start': 124.0,\n",
       "  'end': 133.0,\n",
       "  'text': ' And pandas is going to refer to these row labels, which are right here in this case 0, 1 and 2 as the index.',\n",
       "  'tokens': [51164,\n",
       "   400,\n",
       "   4565,\n",
       "   296,\n",
       "   307,\n",
       "   516,\n",
       "   281,\n",
       "   2864,\n",
       "   281,\n",
       "   613,\n",
       "   5386,\n",
       "   16949,\n",
       "   11,\n",
       "   597,\n",
       "   366,\n",
       "   558,\n",
       "   510,\n",
       "   294,\n",
       "   341,\n",
       "   1389,\n",
       "   1958,\n",
       "   11,\n",
       "   502,\n",
       "   293,\n",
       "   568,\n",
       "   382,\n",
       "   264,\n",
       "   8186,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09554120741392437,\n",
       "  'compression_ratio': 1.5408163265306123,\n",
       "  'no_speech_prob': 0.0065357559360563755},\n",
       " {'id': 18,\n",
       "  'seek': 13300,\n",
       "  'start': 133.0,\n",
       "  'end': 138.0,\n",
       "  'text': ' And we might have named, we might have a name associated with this series.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   321,\n",
       "   1062,\n",
       "   362,\n",
       "   4926,\n",
       "   11,\n",
       "   321,\n",
       "   1062,\n",
       "   362,\n",
       "   257,\n",
       "   1315,\n",
       "   6615,\n",
       "   365,\n",
       "   341,\n",
       "   2638,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10923742147592398,\n",
       "  'compression_ratio': 1.6413793103448275,\n",
       "  'no_speech_prob': 0.0670553669333458},\n",
       " {'id': 19,\n",
       "  'seek': 13300,\n",
       "  'start': 138.0,\n",
       "  'end': 146.0,\n",
       "  'text': ' In this case maybe we just call it s. And so the series named s has an index of 0, 1, 2.',\n",
       "  'tokens': [50614,\n",
       "   682,\n",
       "   341,\n",
       "   1389,\n",
       "   1310,\n",
       "   321,\n",
       "   445,\n",
       "   818,\n",
       "   309,\n",
       "   262,\n",
       "   13,\n",
       "   400,\n",
       "   370,\n",
       "   264,\n",
       "   2638,\n",
       "   4926,\n",
       "   262,\n",
       "   575,\n",
       "   364,\n",
       "   8186,\n",
       "   295,\n",
       "   1958,\n",
       "   11,\n",
       "   502,\n",
       "   11,\n",
       "   568,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10923742147592398,\n",
       "  'compression_ratio': 1.6413793103448275,\n",
       "  'no_speech_prob': 0.0670553669333458},\n",
       " {'id': 20,\n",
       "  'seek': 13300,\n",
       "  'start': 146.0,\n",
       "  'end': 154.0,\n",
       "  'text': ' And the values that are stored inside of the series are called the values.',\n",
       "  'tokens': [51014,\n",
       "   400,\n",
       "   264,\n",
       "   4190,\n",
       "   300,\n",
       "   366,\n",
       "   12187,\n",
       "   1854,\n",
       "   295,\n",
       "   264,\n",
       "   2638,\n",
       "   366,\n",
       "   1219,\n",
       "   264,\n",
       "   4190,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10923742147592398,\n",
       "  'compression_ratio': 1.6413793103448275,\n",
       "  'no_speech_prob': 0.0670553669333458},\n",
       " {'id': 21,\n",
       "  'seek': 15400,\n",
       "  'start': 154.0,\n",
       "  'end': 164.0,\n",
       "  'text': \" So on the next slide we're going to create a series which includes the US unemployment for every other year starting in 1995.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   322,\n",
       "   264,\n",
       "   958,\n",
       "   4137,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   1884,\n",
       "   257,\n",
       "   2638,\n",
       "   597,\n",
       "   5974,\n",
       "   264,\n",
       "   2546,\n",
       "   17438,\n",
       "   337,\n",
       "   633,\n",
       "   661,\n",
       "   1064,\n",
       "   2891,\n",
       "   294,\n",
       "   22601,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11555642830698114,\n",
       "  'compression_ratio': 1.5051020408163265,\n",
       "  'no_speech_prob': 0.01521363016217947},\n",
       " {'id': 22,\n",
       "  'seek': 15400,\n",
       "  'start': 164.0,\n",
       "  'end': 175.0,\n",
       "  'text': ' So here we have the values. And here we have each of the years notice we started 1995 and we go until 2017.',\n",
       "  'tokens': [50864,\n",
       "   407,\n",
       "   510,\n",
       "   321,\n",
       "   362,\n",
       "   264,\n",
       "   4190,\n",
       "   13,\n",
       "   400,\n",
       "   510,\n",
       "   321,\n",
       "   362,\n",
       "   1184,\n",
       "   295,\n",
       "   264,\n",
       "   924,\n",
       "   3449,\n",
       "   321,\n",
       "   1409,\n",
       "   22601,\n",
       "   293,\n",
       "   321,\n",
       "   352,\n",
       "   1826,\n",
       "   6591,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11555642830698114,\n",
       "  'compression_ratio': 1.5051020408163265,\n",
       "  'no_speech_prob': 0.01521363016217947},\n",
       " {'id': 23,\n",
       "  'seek': 15400,\n",
       "  'start': 175.0,\n",
       "  'end': 182.0,\n",
       "  'text': ' And we take a step by two. So this will have 1995, 1997, etc.',\n",
       "  'tokens': [51414,\n",
       "   400,\n",
       "   321,\n",
       "   747,\n",
       "   257,\n",
       "   1823,\n",
       "   538,\n",
       "   732,\n",
       "   13,\n",
       "   407,\n",
       "   341,\n",
       "   486,\n",
       "   362,\n",
       "   22601,\n",
       "   11,\n",
       "   22383,\n",
       "   11,\n",
       "   5183,\n",
       "   13,\n",
       "   51764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11555642830698114,\n",
       "  'compression_ratio': 1.5051020408163265,\n",
       "  'no_speech_prob': 0.01521363016217947},\n",
       " {'id': 24,\n",
       "  'seek': 18200,\n",
       "  'start': 182.0,\n",
       "  'end': 192.0,\n",
       "  'text': \" And then we're going to create a series by putting the values as the data argument, the years as the index argument,\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   550,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   1884,\n",
       "   257,\n",
       "   2638,\n",
       "   538,\n",
       "   3372,\n",
       "   264,\n",
       "   4190,\n",
       "   382,\n",
       "   264,\n",
       "   1412,\n",
       "   6770,\n",
       "   11,\n",
       "   264,\n",
       "   924,\n",
       "   382,\n",
       "   264,\n",
       "   8186,\n",
       "   6770,\n",
       "   11,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0833697576780577,\n",
       "  'compression_ratio': 1.7383720930232558,\n",
       "  'no_speech_prob': 0.006708218716084957},\n",
       " {'id': 25,\n",
       "  'seek': 18200,\n",
       "  'start': 192.0,\n",
       "  'end': 197.0,\n",
       "  'text': \" and then we're going to give this series the name unemployment.\",\n",
       "  'tokens': [50864,\n",
       "   293,\n",
       "   550,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   976,\n",
       "   341,\n",
       "   2638,\n",
       "   264,\n",
       "   1315,\n",
       "   17438,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0833697576780577,\n",
       "  'compression_ratio': 1.7383720930232558,\n",
       "  'no_speech_prob': 0.006708218716084957},\n",
       " {'id': 26,\n",
       "  'seek': 18200,\n",
       "  'start': 197.0,\n",
       "  'end': 208.0,\n",
       "  'text': \" And let's see what we have. So we can see like we just talked in the previous slide that we have the index right here.\",\n",
       "  'tokens': [51114,\n",
       "   400,\n",
       "   718,\n",
       "   311,\n",
       "   536,\n",
       "   437,\n",
       "   321,\n",
       "   362,\n",
       "   13,\n",
       "   407,\n",
       "   321,\n",
       "   393,\n",
       "   536,\n",
       "   411,\n",
       "   321,\n",
       "   445,\n",
       "   2825,\n",
       "   294,\n",
       "   264,\n",
       "   3894,\n",
       "   4137,\n",
       "   300,\n",
       "   321,\n",
       "   362,\n",
       "   264,\n",
       "   8186,\n",
       "   558,\n",
       "   510,\n",
       "   13,\n",
       "   51664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0833697576780577,\n",
       "  'compression_ratio': 1.7383720930232558,\n",
       "  'no_speech_prob': 0.006708218716084957},\n",
       " {'id': 27,\n",
       "  'seek': 20800,\n",
       "  'start': 208.0,\n",
       "  'end': 219.0,\n",
       "  'text': ' And it was this list of years that we talked about. And then over here we have each of the values that are associated with that index.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   309,\n",
       "   390,\n",
       "   341,\n",
       "   1329,\n",
       "   295,\n",
       "   924,\n",
       "   300,\n",
       "   321,\n",
       "   2825,\n",
       "   466,\n",
       "   13,\n",
       "   400,\n",
       "   550,\n",
       "   670,\n",
       "   510,\n",
       "   321,\n",
       "   362,\n",
       "   1184,\n",
       "   295,\n",
       "   264,\n",
       "   4190,\n",
       "   300,\n",
       "   366,\n",
       "   6615,\n",
       "   365,\n",
       "   300,\n",
       "   8186,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06200030509461748,\n",
       "  'compression_ratio': 1.4959349593495934,\n",
       "  'no_speech_prob': 0.009616431780159473},\n",
       " {'id': 28,\n",
       "  'seek': 20800,\n",
       "  'start': 219.0,\n",
       "  'end': 228.0,\n",
       "  'text': ' And we have the name of our series at the bottom.',\n",
       "  'tokens': [50914,\n",
       "   400,\n",
       "   321,\n",
       "   362,\n",
       "   264,\n",
       "   1315,\n",
       "   295,\n",
       "   527,\n",
       "   2638,\n",
       "   412,\n",
       "   264,\n",
       "   2767,\n",
       "   13,\n",
       "   51364],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06200030509461748,\n",
       "  'compression_ratio': 1.4959349593495934,\n",
       "  'no_speech_prob': 0.009616431780159473},\n",
       " {'id': 29,\n",
       "  'seek': 22800,\n",
       "  'start': 228.0,\n",
       "  'end': 236.0,\n",
       "  'text': ' So we can look at the index or the values. Notice the values are stored inside of a Numpy array.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   321,\n",
       "   393,\n",
       "   574,\n",
       "   412,\n",
       "   264,\n",
       "   8186,\n",
       "   420,\n",
       "   264,\n",
       "   4190,\n",
       "   13,\n",
       "   13428,\n",
       "   264,\n",
       "   4190,\n",
       "   366,\n",
       "   12187,\n",
       "   1854,\n",
       "   295,\n",
       "   257,\n",
       "   426,\n",
       "   36142,\n",
       "   10225,\n",
       "   13,\n",
       "   50764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.16032503419003244,\n",
       "  'compression_ratio': 1.5,\n",
       "  'no_speech_prob': 0.028051773086190224},\n",
       " {'id': 30,\n",
       "  'seek': 22800,\n",
       "  'start': 236.0,\n",
       "  'end': 243.0,\n",
       "  'text': ' And the index is stored inside of a pandas index object.',\n",
       "  'tokens': [50764,\n",
       "   400,\n",
       "   264,\n",
       "   8186,\n",
       "   307,\n",
       "   12187,\n",
       "   1854,\n",
       "   295,\n",
       "   257,\n",
       "   4565,\n",
       "   296,\n",
       "   8186,\n",
       "   2657,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.16032503419003244,\n",
       "  'compression_ratio': 1.5,\n",
       "  'no_speech_prob': 0.028051773086190224},\n",
       " {'id': 31,\n",
       "  'seek': 22800,\n",
       "  'start': 243.0,\n",
       "  'end': 249.0,\n",
       "  'text': ' So now that we have a series, what can we do with it?',\n",
       "  'tokens': [51114,\n",
       "   407,\n",
       "   586,\n",
       "   300,\n",
       "   321,\n",
       "   362,\n",
       "   257,\n",
       "   2638,\n",
       "   11,\n",
       "   437,\n",
       "   393,\n",
       "   321,\n",
       "   360,\n",
       "   365,\n",
       "   309,\n",
       "   30,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.16032503419003244,\n",
       "  'compression_ratio': 1.5,\n",
       "  'no_speech_prob': 0.028051773086190224},\n",
       " {'id': 32,\n",
       "  'seek': 24900,\n",
       "  'start': 249.0,\n",
       "  'end': 257.0,\n",
       "  'text': ' So typically our data will have kind of thousands or hundreds of thousands of rows or maybe even millions or billions of rows.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   5850,\n",
       "   527,\n",
       "   1412,\n",
       "   486,\n",
       "   362,\n",
       "   733,\n",
       "   295,\n",
       "   5383,\n",
       "   420,\n",
       "   6779,\n",
       "   295,\n",
       "   5383,\n",
       "   295,\n",
       "   13241,\n",
       "   420,\n",
       "   1310,\n",
       "   754,\n",
       "   6803,\n",
       "   420,\n",
       "   17375,\n",
       "   295,\n",
       "   13241,\n",
       "   13,\n",
       "   50764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.048327652847065646,\n",
       "  'compression_ratio': 1.6373626373626373,\n",
       "  'no_speech_prob': 0.08800175786018372},\n",
       " {'id': 33,\n",
       "  'seek': 24900,\n",
       "  'start': 257.0,\n",
       "  'end': 262.0,\n",
       "  'text': \" So we obviously don't want to display all of it at once.\",\n",
       "  'tokens': [50764,\n",
       "   407,\n",
       "   321,\n",
       "   2745,\n",
       "   500,\n",
       "   380,\n",
       "   528,\n",
       "   281,\n",
       "   4674,\n",
       "   439,\n",
       "   295,\n",
       "   309,\n",
       "   412,\n",
       "   1564,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.048327652847065646,\n",
       "  'compression_ratio': 1.6373626373626373,\n",
       "  'no_speech_prob': 0.08800175786018372},\n",
       " {'id': 34,\n",
       "  'seek': 24900,\n",
       "  'start': 262.0,\n",
       "  'end': 271.0,\n",
       "  'text': \" And there's two methods for series that will just show us the first few observations or the last few observations.\",\n",
       "  'tokens': [51014,\n",
       "   400,\n",
       "   456,\n",
       "   311,\n",
       "   732,\n",
       "   7150,\n",
       "   337,\n",
       "   2638,\n",
       "   300,\n",
       "   486,\n",
       "   445,\n",
       "   855,\n",
       "   505,\n",
       "   264,\n",
       "   700,\n",
       "   1326,\n",
       "   18163,\n",
       "   420,\n",
       "   264,\n",
       "   1036,\n",
       "   1326,\n",
       "   18163,\n",
       "   13,\n",
       "   51464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.048327652847065646,\n",
       "  'compression_ratio': 1.6373626373626373,\n",
       "  'no_speech_prob': 0.08800175786018372},\n",
       " {'id': 35,\n",
       "  'seek': 27100,\n",
       "  'start': 271.0,\n",
       "  'end': 280.0,\n",
       "  'text': ' So notice an employment dot head will show us the first five and unemployment dot tail will show us the last five.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   3449,\n",
       "   364,\n",
       "   11949,\n",
       "   5893,\n",
       "   1378,\n",
       "   486,\n",
       "   855,\n",
       "   505,\n",
       "   264,\n",
       "   700,\n",
       "   1732,\n",
       "   293,\n",
       "   17438,\n",
       "   5893,\n",
       "   6838,\n",
       "   486,\n",
       "   855,\n",
       "   505,\n",
       "   264,\n",
       "   1036,\n",
       "   1732,\n",
       "   13,\n",
       "   50814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09141361713409424,\n",
       "  'compression_ratio': 1.7132867132867133,\n",
       "  'no_speech_prob': 0.06007964909076691},\n",
       " {'id': 36,\n",
       "  'seek': 27100,\n",
       "  'start': 280.0,\n",
       "  'end': 294.0,\n",
       "  'text': ' We could also give an argument to either of these of just an integer and it will show us the last N observations from this series.',\n",
       "  'tokens': [50814,\n",
       "   492,\n",
       "   727,\n",
       "   611,\n",
       "   976,\n",
       "   364,\n",
       "   6770,\n",
       "   281,\n",
       "   2139,\n",
       "   295,\n",
       "   613,\n",
       "   295,\n",
       "   445,\n",
       "   364,\n",
       "   24922,\n",
       "   293,\n",
       "   309,\n",
       "   486,\n",
       "   855,\n",
       "   505,\n",
       "   264,\n",
       "   1036,\n",
       "   426,\n",
       "   18163,\n",
       "   490,\n",
       "   341,\n",
       "   2638,\n",
       "   13,\n",
       "   51514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09141361713409424,\n",
       "  'compression_ratio': 1.7132867132867133,\n",
       "  'no_speech_prob': 0.06007964909076691},\n",
       " {'id': 37,\n",
       "  'seek': 29400,\n",
       "  'start': 294.0,\n",
       "  'end': 304.0,\n",
       "  'text': \" So let's go ahead and do some basic plotting. And if we plot a series using the plot method, all it will do is plot the values.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   352,\n",
       "   2286,\n",
       "   293,\n",
       "   360,\n",
       "   512,\n",
       "   3875,\n",
       "   41178,\n",
       "   13,\n",
       "   400,\n",
       "   498,\n",
       "   321,\n",
       "   7542,\n",
       "   257,\n",
       "   2638,\n",
       "   1228,\n",
       "   264,\n",
       "   7542,\n",
       "   3170,\n",
       "   11,\n",
       "   439,\n",
       "   309,\n",
       "   486,\n",
       "   360,\n",
       "   307,\n",
       "   7542,\n",
       "   264,\n",
       "   4190,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09936301991090936,\n",
       "  'compression_ratio': 1.4825174825174825,\n",
       "  'no_speech_prob': 0.19039560854434967},\n",
       " {'id': 38,\n",
       "  'seek': 29400,\n",
       "  'start': 304.0,\n",
       "  'end': 314.0,\n",
       "  'text': ' So if we go ahead and look at the values we had, notice 5.6 is associated with 1995.',\n",
       "  'tokens': [50864,\n",
       "   407,\n",
       "   498,\n",
       "   321,\n",
       "   352,\n",
       "   2286,\n",
       "   293,\n",
       "   574,\n",
       "   412,\n",
       "   264,\n",
       "   4190,\n",
       "   321,\n",
       "   632,\n",
       "   11,\n",
       "   3449,\n",
       "   1025,\n",
       "   13,\n",
       "   21,\n",
       "   307,\n",
       "   6615,\n",
       "   365,\n",
       "   22601,\n",
       "   13,\n",
       "   51364],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09936301991090936,\n",
       "  'compression_ratio': 1.4825174825174825,\n",
       "  'no_speech_prob': 0.19039560854434967},\n",
       " {'id': 39,\n",
       "  'seek': 31400,\n",
       "  'start': 314.0,\n",
       "  'end': 321.0,\n",
       "  'text': ' 5.3 is associated with 1997, 4.3 in 1999, etc. etc.',\n",
       "  'tokens': [50364,\n",
       "   1025,\n",
       "   13,\n",
       "   18,\n",
       "   307,\n",
       "   6615,\n",
       "   365,\n",
       "   22383,\n",
       "   11,\n",
       "   1017,\n",
       "   13,\n",
       "   18,\n",
       "   294,\n",
       "   19952,\n",
       "   11,\n",
       "   5183,\n",
       "   13,\n",
       "   5183,\n",
       "   13,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13162301870492787,\n",
       "  'compression_ratio': 1.4967741935483871,\n",
       "  'no_speech_prob': 0.11972978711128235},\n",
       " {'id': 40,\n",
       "  'seek': 31400,\n",
       "  'start': 321.0,\n",
       "  'end': 335.0,\n",
       "  'text': \" And it's created this nice little line plot for us. And as we mentioned earlier, we needed this percent map plot lib in line in order to get the plot to show up in the right place.\",\n",
       "  'tokens': [50714,\n",
       "   400,\n",
       "   309,\n",
       "   311,\n",
       "   2942,\n",
       "   341,\n",
       "   1481,\n",
       "   707,\n",
       "   1622,\n",
       "   7542,\n",
       "   337,\n",
       "   505,\n",
       "   13,\n",
       "   400,\n",
       "   382,\n",
       "   321,\n",
       "   2835,\n",
       "   3071,\n",
       "   11,\n",
       "   321,\n",
       "   2978,\n",
       "   341,\n",
       "   3043,\n",
       "   4471,\n",
       "   7542,\n",
       "   22854,\n",
       "   294,\n",
       "   1622,\n",
       "   294,\n",
       "   1668,\n",
       "   281,\n",
       "   483,\n",
       "   264,\n",
       "   7542,\n",
       "   281,\n",
       "   855,\n",
       "   493,\n",
       "   294,\n",
       "   264,\n",
       "   558,\n",
       "   1081,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13162301870492787,\n",
       "  'compression_ratio': 1.4967741935483871,\n",
       "  'no_speech_prob': 0.11972978711128235},\n",
       " {'id': 41,\n",
       "  'seek': 33500,\n",
       "  'start': 335.0,\n",
       "  'end': 346.0,\n",
       "  'text': \" So in this data set, it's not such an interesting observation. But in other data sets, you might be interested in knowing what unique values are in a series.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   294,\n",
       "   341,\n",
       "   1412,\n",
       "   992,\n",
       "   11,\n",
       "   309,\n",
       "   311,\n",
       "   406,\n",
       "   1270,\n",
       "   364,\n",
       "   1880,\n",
       "   14816,\n",
       "   13,\n",
       "   583,\n",
       "   294,\n",
       "   661,\n",
       "   1412,\n",
       "   6352,\n",
       "   11,\n",
       "   291,\n",
       "   1062,\n",
       "   312,\n",
       "   3102,\n",
       "   294,\n",
       "   5276,\n",
       "   437,\n",
       "   3845,\n",
       "   4190,\n",
       "   366,\n",
       "   294,\n",
       "   257,\n",
       "   2638,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0796968678393996,\n",
       "  'compression_ratio': 1.5960591133004927,\n",
       "  'no_speech_prob': 0.24348661303520203},\n",
       " {'id': 42,\n",
       "  'seek': 33500,\n",
       "  'start': 346.0,\n",
       "  'end': 362.0,\n",
       "  'text': ' So for example, if you had a series that had the ages of the individuals in your data set, that might take the values 18, 19, 20, 21, dot, dot, until the maximum age.',\n",
       "  'tokens': [50914,\n",
       "   407,\n",
       "   337,\n",
       "   1365,\n",
       "   11,\n",
       "   498,\n",
       "   291,\n",
       "   632,\n",
       "   257,\n",
       "   2638,\n",
       "   300,\n",
       "   632,\n",
       "   264,\n",
       "   12357,\n",
       "   295,\n",
       "   264,\n",
       "   5346,\n",
       "   294,\n",
       "   428,\n",
       "   1412,\n",
       "   992,\n",
       "   11,\n",
       "   300,\n",
       "   1062,\n",
       "   747,\n",
       "   264,\n",
       "   4190,\n",
       "   2443,\n",
       "   11,\n",
       "   1294,\n",
       "   11,\n",
       "   945,\n",
       "   11,\n",
       "   5080,\n",
       "   11,\n",
       "   5893,\n",
       "   11,\n",
       "   5893,\n",
       "   11,\n",
       "   1826,\n",
       "   264,\n",
       "   6674,\n",
       "   3205,\n",
       "   13,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0796968678393996,\n",
       "  'compression_ratio': 1.5960591133004927,\n",
       "  'no_speech_prob': 0.24348661303520203},\n",
       " {'id': 43,\n",
       "  'seek': 36200,\n",
       "  'start': 362.0,\n",
       "  'end': 370.0,\n",
       "  'text': \" So again, here it's not interesting because each value is different, but it's something you might do in the future.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   797,\n",
       "   11,\n",
       "   510,\n",
       "   309,\n",
       "   311,\n",
       "   406,\n",
       "   1880,\n",
       "   570,\n",
       "   1184,\n",
       "   2158,\n",
       "   307,\n",
       "   819,\n",
       "   11,\n",
       "   457,\n",
       "   309,\n",
       "   311,\n",
       "   746,\n",
       "   291,\n",
       "   1062,\n",
       "   360,\n",
       "   294,\n",
       "   264,\n",
       "   2027,\n",
       "   13,\n",
       "   50764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06601326978659328,\n",
       "  'compression_ratio': 1.5953488372093023,\n",
       "  'no_speech_prob': 0.008158712647855282},\n",
       " {'id': 44,\n",
       "  'seek': 36200,\n",
       "  'start': 370.0,\n",
       "  'end': 385.0,\n",
       "  'text': \" One of the things that you'll frequently want to do is to select particular elements from a series. And we can use this using the dot lock method or modifier, where index items is going to be some item from inside of the index.\",\n",
       "  'tokens': [50764,\n",
       "   1485,\n",
       "   295,\n",
       "   264,\n",
       "   721,\n",
       "   300,\n",
       "   291,\n",
       "   603,\n",
       "   10374,\n",
       "   528,\n",
       "   281,\n",
       "   360,\n",
       "   307,\n",
       "   281,\n",
       "   3048,\n",
       "   1729,\n",
       "   4959,\n",
       "   490,\n",
       "   257,\n",
       "   2638,\n",
       "   13,\n",
       "   400,\n",
       "   321,\n",
       "   393,\n",
       "   764,\n",
       "   341,\n",
       "   1228,\n",
       "   264,\n",
       "   5893,\n",
       "   4017,\n",
       "   3170,\n",
       "   420,\n",
       "   38011,\n",
       "   11,\n",
       "   689,\n",
       "   8186,\n",
       "   4754,\n",
       "   307,\n",
       "   516,\n",
       "   281,\n",
       "   312,\n",
       "   512,\n",
       "   3174,\n",
       "   490,\n",
       "   1854,\n",
       "   295,\n",
       "   264,\n",
       "   8186,\n",
       "   13,\n",
       "   51514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06601326978659328,\n",
       "  'compression_ratio': 1.5953488372093023,\n",
       "  'no_speech_prob': 0.008158712647855282},\n",
       " {'id': 45,\n",
       "  'seek': 38500,\n",
       "  'start': 385.0,\n",
       "  'end': 400.0,\n",
       "  'text': ' So if we look here, we can see we have our unemployment series. And if we wanted to get the value associated with 1995, all we would do is write dot lock 1995.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   498,\n",
       "   321,\n",
       "   574,\n",
       "   510,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   536,\n",
       "   321,\n",
       "   362,\n",
       "   527,\n",
       "   17438,\n",
       "   2638,\n",
       "   13,\n",
       "   400,\n",
       "   498,\n",
       "   321,\n",
       "   1415,\n",
       "   281,\n",
       "   483,\n",
       "   264,\n",
       "   2158,\n",
       "   6615,\n",
       "   365,\n",
       "   22601,\n",
       "   11,\n",
       "   439,\n",
       "   321,\n",
       "   576,\n",
       "   360,\n",
       "   307,\n",
       "   2464,\n",
       "   5893,\n",
       "   4017,\n",
       "   22601,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09180575463829971,\n",
       "  'compression_ratio': 1.272,\n",
       "  'no_speech_prob': 0.5593780875205994},\n",
       " {'id': 46,\n",
       "  'seek': 40000,\n",
       "  'start': 401.0,\n",
       "  'end': 415.0,\n",
       "  'text': ' And if we wanted to get multiple values, maybe we wanted the values from 1995, 2005, and 2015, we can just give it a list of values and notice extracted the correct values.',\n",
       "  'tokens': [50414,\n",
       "   400,\n",
       "   498,\n",
       "   321,\n",
       "   1415,\n",
       "   281,\n",
       "   483,\n",
       "   3866,\n",
       "   4190,\n",
       "   11,\n",
       "   1310,\n",
       "   321,\n",
       "   1415,\n",
       "   264,\n",
       "   4190,\n",
       "   490,\n",
       "   22601,\n",
       "   11,\n",
       "   14394,\n",
       "   11,\n",
       "   293,\n",
       "   7546,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   445,\n",
       "   976,\n",
       "   309,\n",
       "   257,\n",
       "   1329,\n",
       "   295,\n",
       "   4190,\n",
       "   293,\n",
       "   3449,\n",
       "   34086,\n",
       "   264,\n",
       "   3006,\n",
       "   4190,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07775122097560337,\n",
       "  'compression_ratio': 1.4948979591836735,\n",
       "  'no_speech_prob': 0.09054785966873169},\n",
       " {'id': 47,\n",
       "  'seek': 40000,\n",
       "  'start': 415.0,\n",
       "  'end': 425.0,\n",
       "  'text': \" So let's take a minute and pause. And we're going to have each of you do some experimentation with these series methods.\",\n",
       "  'tokens': [51114,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   747,\n",
       "   257,\n",
       "   3456,\n",
       "   293,\n",
       "   10465,\n",
       "   13,\n",
       "   400,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   362,\n",
       "   1184,\n",
       "   295,\n",
       "   291,\n",
       "   360,\n",
       "   512,\n",
       "   37142,\n",
       "   365,\n",
       "   613,\n",
       "   2638,\n",
       "   7150,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07775122097560337,\n",
       "  'compression_ratio': 1.4948979591836735,\n",
       "  'no_speech_prob': 0.09054785966873169},\n",
       " {'id': 48,\n",
       "  'seek': 42500,\n",
       "  'start': 425.0,\n",
       "  'end': 431.0,\n",
       "  'text': \" So the first thing is we'd like you to display only the first two elements of the series using the head method.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   264,\n",
       "   700,\n",
       "   551,\n",
       "   307,\n",
       "   321,\n",
       "   1116,\n",
       "   411,\n",
       "   291,\n",
       "   281,\n",
       "   4674,\n",
       "   787,\n",
       "   264,\n",
       "   700,\n",
       "   732,\n",
       "   4959,\n",
       "   295,\n",
       "   264,\n",
       "   2638,\n",
       "   1228,\n",
       "   264,\n",
       "   1378,\n",
       "   3170,\n",
       "   13,\n",
       "   50664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09350797404413638,\n",
       "  'compression_ratio': 1.7582938388625593,\n",
       "  'no_speech_prob': 0.018418049439787865},\n",
       " {'id': 49,\n",
       "  'seek': 42500,\n",
       "  'start': 431.0,\n",
       "  'end': 436.0,\n",
       "  'text': \" We'd like you to use the plot method to make a bar plot.\",\n",
       "  'tokens': [50664,\n",
       "   492,\n",
       "   1116,\n",
       "   411,\n",
       "   291,\n",
       "   281,\n",
       "   764,\n",
       "   264,\n",
       "   7542,\n",
       "   3170,\n",
       "   281,\n",
       "   652,\n",
       "   257,\n",
       "   2159,\n",
       "   7542,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09350797404413638,\n",
       "  'compression_ratio': 1.7582938388625593,\n",
       "  'no_speech_prob': 0.018418049439787865},\n",
       " {'id': 50,\n",
       "  'seek': 42500,\n",
       "  'start': 436.0,\n",
       "  'end': 443.0,\n",
       "  'text': \" We'll use the dot lock to select the lowest and highest unemployment rate shown in the series.\",\n",
       "  'tokens': [50914,\n",
       "   492,\n",
       "   603,\n",
       "   764,\n",
       "   264,\n",
       "   5893,\n",
       "   4017,\n",
       "   281,\n",
       "   3048,\n",
       "   264,\n",
       "   12437,\n",
       "   293,\n",
       "   6343,\n",
       "   17438,\n",
       "   3314,\n",
       "   4898,\n",
       "   294,\n",
       "   264,\n",
       "   2638,\n",
       "   13,\n",
       "   51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09350797404413638,\n",
       "  'compression_ratio': 1.7582938388625593,\n",
       "  'no_speech_prob': 0.018418049439787865},\n",
       " {'id': 51,\n",
       "  'seek': 42500,\n",
       "  'start': 443.0,\n",
       "  'end': 450.0,\n",
       "  'text': ' And run the code unemployment dot d type below. What does it give you? What do you think it corresponds to?',\n",
       "  'tokens': [51264,\n",
       "   400,\n",
       "   1190,\n",
       "   264,\n",
       "   3089,\n",
       "   17438,\n",
       "   5893,\n",
       "   274,\n",
       "   2010,\n",
       "   2507,\n",
       "   13,\n",
       "   708,\n",
       "   775,\n",
       "   309,\n",
       "   976,\n",
       "   291,\n",
       "   30,\n",
       "   708,\n",
       "   360,\n",
       "   291,\n",
       "   519,\n",
       "   309,\n",
       "   23249,\n",
       "   281,\n",
       "   30,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09350797404413638,\n",
       "  'compression_ratio': 1.7582938388625593,\n",
       "  'no_speech_prob': 0.018418049439787865},\n",
       " {'id': 52,\n",
       "  'seek': 45000,\n",
       "  'start': 450.0,\n",
       "  'end': 455.0,\n",
       "  'text': \" And we'll go ahead and take about five minutes to do this.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   321,\n",
       "   603,\n",
       "   352,\n",
       "   2286,\n",
       "   293,\n",
       "   747,\n",
       "   466,\n",
       "   1732,\n",
       "   2077,\n",
       "   281,\n",
       "   360,\n",
       "   341,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10774540901184082,\n",
       "  'compression_ratio': 1.4603174603174602,\n",
       "  'no_speech_prob': 0.06929142028093338},\n",
       " {'id': 53,\n",
       "  'seek': 45000,\n",
       "  'start': 455.0,\n",
       "  'end': 468.0,\n",
       "  'text': ' Okay, welcome back. So as we saw a few minutes ago, the way to select the first two values from a series using the head method is to do unemployment dot head with the integer two.',\n",
       "  'tokens': [50614,\n",
       "   1033,\n",
       "   11,\n",
       "   2928,\n",
       "   646,\n",
       "   13,\n",
       "   407,\n",
       "   382,\n",
       "   321,\n",
       "   1866,\n",
       "   257,\n",
       "   1326,\n",
       "   2077,\n",
       "   2057,\n",
       "   11,\n",
       "   264,\n",
       "   636,\n",
       "   281,\n",
       "   3048,\n",
       "   264,\n",
       "   700,\n",
       "   732,\n",
       "   4190,\n",
       "   490,\n",
       "   257,\n",
       "   2638,\n",
       "   1228,\n",
       "   264,\n",
       "   1378,\n",
       "   3170,\n",
       "   307,\n",
       "   281,\n",
       "   360,\n",
       "   17438,\n",
       "   5893,\n",
       "   1378,\n",
       "   365,\n",
       "   264,\n",
       "   24922,\n",
       "   732,\n",
       "   13,\n",
       "   51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10774540901184082,\n",
       "  'compression_ratio': 1.4603174603174602,\n",
       "  'no_speech_prob': 0.06929142028093338},\n",
       " {'id': 54,\n",
       "  'seek': 45000,\n",
       "  'start': 468.0,\n",
       "  'end': 473.0,\n",
       "  'text': ' And notice it selected 1995 and 1997.',\n",
       "  'tokens': [51264, 400, 3449, 309, 8209, 22601, 293, 22383, 13, 51514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10774540901184082,\n",
       "  'compression_ratio': 1.4603174603174602,\n",
       "  'no_speech_prob': 0.06929142028093338},\n",
       " {'id': 55,\n",
       "  'seek': 47300,\n",
       "  'start': 473.0,\n",
       "  'end': 488.0,\n",
       "  'text': ' To do the plotting, we wanted you to investigate the documentation and recall from our lectures this summer that you can do this by taking the unemployment dot plot method and putting a question mark after and running the cell.',\n",
       "  'tokens': [50364,\n",
       "   1407,\n",
       "   360,\n",
       "   264,\n",
       "   41178,\n",
       "   11,\n",
       "   321,\n",
       "   1415,\n",
       "   291,\n",
       "   281,\n",
       "   15013,\n",
       "   264,\n",
       "   14333,\n",
       "   293,\n",
       "   9901,\n",
       "   490,\n",
       "   527,\n",
       "   16564,\n",
       "   341,\n",
       "   4266,\n",
       "   300,\n",
       "   291,\n",
       "   393,\n",
       "   360,\n",
       "   341,\n",
       "   538,\n",
       "   1940,\n",
       "   264,\n",
       "   17438,\n",
       "   5893,\n",
       "   7542,\n",
       "   3170,\n",
       "   293,\n",
       "   3372,\n",
       "   257,\n",
       "   1168,\n",
       "   1491,\n",
       "   934,\n",
       "   293,\n",
       "   2614,\n",
       "   264,\n",
       "   2815,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10046295589870877,\n",
       "  'compression_ratio': 1.6991525423728813,\n",
       "  'no_speech_prob': 0.10508568584918976},\n",
       " {'id': 56,\n",
       "  'seek': 47300,\n",
       "  'start': 488.0,\n",
       "  'end': 491.0,\n",
       "  'text': ' And so that brings up the documentation.',\n",
       "  'tokens': [51114, 400, 370, 300, 5607, 493, 264, 14333, 13, 51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10046295589870877,\n",
       "  'compression_ratio': 1.6991525423728813,\n",
       "  'no_speech_prob': 0.10508568584918976},\n",
       " {'id': 57,\n",
       "  'seek': 47300,\n",
       "  'start': 491.0,\n",
       "  'end': 501.0,\n",
       "  'text': \" And it tells you that the inputs are the series or data frame, the X and Y labels, and then what kind of plot you'd like to produce.\",\n",
       "  'tokens': [51264,\n",
       "   400,\n",
       "   309,\n",
       "   5112,\n",
       "   291,\n",
       "   300,\n",
       "   264,\n",
       "   15743,\n",
       "   366,\n",
       "   264,\n",
       "   2638,\n",
       "   420,\n",
       "   1412,\n",
       "   3920,\n",
       "   11,\n",
       "   264,\n",
       "   1783,\n",
       "   293,\n",
       "   398,\n",
       "   16949,\n",
       "   11,\n",
       "   293,\n",
       "   550,\n",
       "   437,\n",
       "   733,\n",
       "   295,\n",
       "   7542,\n",
       "   291,\n",
       "   1116,\n",
       "   411,\n",
       "   281,\n",
       "   5258,\n",
       "   13,\n",
       "   51764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10046295589870877,\n",
       "  'compression_ratio': 1.6991525423728813,\n",
       "  'no_speech_prob': 0.10508568584918976},\n",
       " {'id': 58,\n",
       "  'seek': 50100,\n",
       "  'start': 501.0,\n",
       "  'end': 511.0,\n",
       "  'text': ' And notice one of the options is bar. So to make the bar plot, all we have to do is unemployment dot plot kind equals bar.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   3449,\n",
       "   472,\n",
       "   295,\n",
       "   264,\n",
       "   3956,\n",
       "   307,\n",
       "   2159,\n",
       "   13,\n",
       "   407,\n",
       "   281,\n",
       "   652,\n",
       "   264,\n",
       "   2159,\n",
       "   7542,\n",
       "   11,\n",
       "   439,\n",
       "   321,\n",
       "   362,\n",
       "   281,\n",
       "   360,\n",
       "   307,\n",
       "   17438,\n",
       "   5893,\n",
       "   7542,\n",
       "   733,\n",
       "   6915,\n",
       "   2159,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10913292329702805,\n",
       "  'compression_ratio': 1.551948051948052,\n",
       "  'no_speech_prob': 0.028349051252007484},\n",
       " {'id': 59,\n",
       "  'seek': 50100,\n",
       "  'start': 511.0,\n",
       "  'end': 519.0,\n",
       "  'text': ' And by default, it will set the index as the X values and the values as the Y values.',\n",
       "  'tokens': [50864,\n",
       "   400,\n",
       "   538,\n",
       "   7576,\n",
       "   11,\n",
       "   309,\n",
       "   486,\n",
       "   992,\n",
       "   264,\n",
       "   8186,\n",
       "   382,\n",
       "   264,\n",
       "   1783,\n",
       "   4190,\n",
       "   293,\n",
       "   264,\n",
       "   4190,\n",
       "   382,\n",
       "   264,\n",
       "   398,\n",
       "   4190,\n",
       "   13,\n",
       "   51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10913292329702805,\n",
       "  'compression_ratio': 1.551948051948052,\n",
       "  'no_speech_prob': 0.028349051252007484},\n",
       " {'id': 60,\n",
       "  'seek': 50100,\n",
       "  'start': 519.0,\n",
       "  'end': 524.0,\n",
       "  'text': \" And here we've got a bar plot.\",\n",
       "  'tokens': [51264, 400, 510, 321, 600, 658, 257, 2159, 7542, 13, 51514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10913292329702805,\n",
       "  'compression_ratio': 1.551948051948052,\n",
       "  'no_speech_prob': 0.028349051252007484},\n",
       " {'id': 61,\n",
       "  'seek': 52400,\n",
       "  'start': 524.0,\n",
       "  'end': 536.0,\n",
       "  'text': ' To select the minimum and maximum values of unemployment, you might have just looked by hand and notice 2001 and was the lowest unemployment in 2011 was the highest.',\n",
       "  'tokens': [50364,\n",
       "   1407,\n",
       "   3048,\n",
       "   264,\n",
       "   7285,\n",
       "   293,\n",
       "   6674,\n",
       "   4190,\n",
       "   295,\n",
       "   17438,\n",
       "   11,\n",
       "   291,\n",
       "   1062,\n",
       "   362,\n",
       "   445,\n",
       "   2956,\n",
       "   538,\n",
       "   1011,\n",
       "   293,\n",
       "   3449,\n",
       "   16382,\n",
       "   293,\n",
       "   390,\n",
       "   264,\n",
       "   12437,\n",
       "   17438,\n",
       "   294,\n",
       "   10154,\n",
       "   390,\n",
       "   264,\n",
       "   6343,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1286280361222632,\n",
       "  'compression_ratio': 1.5943396226415094,\n",
       "  'no_speech_prob': 0.04778808355331421},\n",
       " {'id': 62,\n",
       "  'seek': 52400,\n",
       "  'start': 536.0,\n",
       "  'end': 543.0,\n",
       "  'text': ' But you also could have used the IDX min and IDX max arguments.',\n",
       "  'tokens': [50964,\n",
       "   583,\n",
       "   291,\n",
       "   611,\n",
       "   727,\n",
       "   362,\n",
       "   1143,\n",
       "   264,\n",
       "   7348,\n",
       "   55,\n",
       "   923,\n",
       "   293,\n",
       "   7348,\n",
       "   55,\n",
       "   11469,\n",
       "   12869,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1286280361222632,\n",
       "  'compression_ratio': 1.5943396226415094,\n",
       "  'no_speech_prob': 0.04778808355331421},\n",
       " {'id': 63,\n",
       "  'seek': 52400,\n",
       "  'start': 543.0,\n",
       "  'end': 553.0,\n",
       "  'text': ' And what do these do is if you look at IDX min question mark, it returns the row label of the minimum value.',\n",
       "  'tokens': [51314,\n",
       "   400,\n",
       "   437,\n",
       "   360,\n",
       "   613,\n",
       "   360,\n",
       "   307,\n",
       "   498,\n",
       "   291,\n",
       "   574,\n",
       "   412,\n",
       "   7348,\n",
       "   55,\n",
       "   923,\n",
       "   1168,\n",
       "   1491,\n",
       "   11,\n",
       "   309,\n",
       "   11247,\n",
       "   264,\n",
       "   5386,\n",
       "   7645,\n",
       "   295,\n",
       "   264,\n",
       "   7285,\n",
       "   2158,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1286280361222632,\n",
       "  'compression_ratio': 1.5943396226415094,\n",
       "  'no_speech_prob': 0.04778808355331421},\n",
       " {'id': 64,\n",
       "  'seek': 55300,\n",
       "  'start': 553.0,\n",
       "  'end': 558.0,\n",
       "  'text': ' If multiple values equal the minimum, the first row label with that value is returned.',\n",
       "  'tokens': [50364,\n",
       "   759,\n",
       "   3866,\n",
       "   4190,\n",
       "   2681,\n",
       "   264,\n",
       "   7285,\n",
       "   11,\n",
       "   264,\n",
       "   700,\n",
       "   5386,\n",
       "   7645,\n",
       "   365,\n",
       "   300,\n",
       "   2158,\n",
       "   307,\n",
       "   8752,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07500202005559747,\n",
       "  'compression_ratio': 1.4768211920529801,\n",
       "  'no_speech_prob': 0.0006003035232424736},\n",
       " {'id': 65,\n",
       "  'seek': 55300,\n",
       "  'start': 558.0,\n",
       "  'end': 565.0,\n",
       "  'text': ' So this will find us the minimum one and the maximum one could be done just like it.',\n",
       "  'tokens': [50614,\n",
       "   407,\n",
       "   341,\n",
       "   486,\n",
       "   915,\n",
       "   505,\n",
       "   264,\n",
       "   7285,\n",
       "   472,\n",
       "   293,\n",
       "   264,\n",
       "   6674,\n",
       "   472,\n",
       "   727,\n",
       "   312,\n",
       "   1096,\n",
       "   445,\n",
       "   411,\n",
       "   309,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07500202005559747,\n",
       "  'compression_ratio': 1.4768211920529801,\n",
       "  'no_speech_prob': 0.0006003035232424736},\n",
       " {'id': 66,\n",
       "  'seek': 55300,\n",
       "  'start': 565.0,\n",
       "  'end': 571.0,\n",
       "  'text': ' And then unemployment dot d type tells us float 64.',\n",
       "  'tokens': [50964,\n",
       "   400,\n",
       "   550,\n",
       "   17438,\n",
       "   5893,\n",
       "   274,\n",
       "   2010,\n",
       "   5112,\n",
       "   505,\n",
       "   15706,\n",
       "   12145,\n",
       "   13,\n",
       "   51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07500202005559747,\n",
       "  'compression_ratio': 1.4768211920529801,\n",
       "  'no_speech_prob': 0.0006003035232424736},\n",
       " {'id': 67,\n",
       "  'seek': 57100,\n",
       "  'start': 571.0,\n",
       "  'end': 589.0,\n",
       "  'text': ' So if we look at our unemployment series, just notice that the values themselves are float 64. So unemployment dot d type is just telling us what kind of values are being stored inside of our series.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   498,\n",
       "   321,\n",
       "   574,\n",
       "   412,\n",
       "   527,\n",
       "   17438,\n",
       "   2638,\n",
       "   11,\n",
       "   445,\n",
       "   3449,\n",
       "   300,\n",
       "   264,\n",
       "   4190,\n",
       "   2969,\n",
       "   366,\n",
       "   15706,\n",
       "   12145,\n",
       "   13,\n",
       "   407,\n",
       "   17438,\n",
       "   5893,\n",
       "   274,\n",
       "   2010,\n",
       "   307,\n",
       "   445,\n",
       "   3585,\n",
       "   505,\n",
       "   437,\n",
       "   733,\n",
       "   295,\n",
       "   4190,\n",
       "   366,\n",
       "   885,\n",
       "   12187,\n",
       "   1854,\n",
       "   295,\n",
       "   527,\n",
       "   2638,\n",
       "   13,\n",
       "   51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08142403875078474,\n",
       "  'compression_ratio': 1.6,\n",
       "  'no_speech_prob': 0.009703414514660835},\n",
       " {'id': 68,\n",
       "  'seek': 57100,\n",
       "  'start': 589.0,\n",
       "  'end': 596.0,\n",
       "  'text': \" Great. So now we've talked about what a series is and we'll now talk about what a data frame is.\",\n",
       "  'tokens': [51264,\n",
       "   3769,\n",
       "   13,\n",
       "   407,\n",
       "   586,\n",
       "   321,\n",
       "   600,\n",
       "   2825,\n",
       "   466,\n",
       "   437,\n",
       "   257,\n",
       "   2638,\n",
       "   307,\n",
       "   293,\n",
       "   321,\n",
       "   603,\n",
       "   586,\n",
       "   751,\n",
       "   466,\n",
       "   437,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   307,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08142403875078474,\n",
       "  'compression_ratio': 1.6,\n",
       "  'no_speech_prob': 0.009703414514660835},\n",
       " {'id': 69,\n",
       "  'seek': 59600,\n",
       "  'start': 596.0,\n",
       "  'end': 603.0,\n",
       "  'text': ' So a data frame is just going to be how pandas will store multiple columns of data.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   307,\n",
       "   445,\n",
       "   516,\n",
       "   281,\n",
       "   312,\n",
       "   577,\n",
       "   4565,\n",
       "   296,\n",
       "   486,\n",
       "   3531,\n",
       "   3866,\n",
       "   13766,\n",
       "   295,\n",
       "   1412,\n",
       "   13,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11450027969648253,\n",
       "  'compression_ratio': 1.4489795918367347,\n",
       "  'no_speech_prob': 0.08219211548566818},\n",
       " {'id': 70,\n",
       "  'seek': 59600,\n",
       "  'start': 603.0,\n",
       "  'end': 610.0,\n",
       "  'text': ' You could think about data frames is simply just multiple series stacked side by side.',\n",
       "  'tokens': [50714,\n",
       "   509,\n",
       "   727,\n",
       "   519,\n",
       "   466,\n",
       "   1412,\n",
       "   12083,\n",
       "   307,\n",
       "   2935,\n",
       "   445,\n",
       "   3866,\n",
       "   2638,\n",
       "   28867,\n",
       "   1252,\n",
       "   538,\n",
       "   1252,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11450027969648253,\n",
       "  'compression_ratio': 1.4489795918367347,\n",
       "  'no_speech_prob': 0.08219211548566818},\n",
       " {'id': 71,\n",
       "  'seek': 59600,\n",
       "  'start': 610.0,\n",
       "  'end': 614.0,\n",
       "  'text': \" You'll notice that we still have an index.\",\n",
       "  'tokens': [51064, 509, 603, 3449, 300, 321, 920, 362, 364, 8186, 13, 51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11450027969648253,\n",
       "  'compression_ratio': 1.4489795918367347,\n",
       "  'no_speech_prob': 0.08219211548566818},\n",
       " {'id': 72,\n",
       "  'seek': 61400,\n",
       "  'start': 615.0,\n",
       "  'end': 626.0,\n",
       "  'text': \" And now the index is zero. We'll just call this column zero one and two.\",\n",
       "  'tokens': [50414,\n",
       "   400,\n",
       "   586,\n",
       "   264,\n",
       "   8186,\n",
       "   307,\n",
       "   4018,\n",
       "   13,\n",
       "   492,\n",
       "   603,\n",
       "   445,\n",
       "   818,\n",
       "   341,\n",
       "   7738,\n",
       "   4018,\n",
       "   472,\n",
       "   293,\n",
       "   732,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.17487278580665588,\n",
       "  'compression_ratio': 1.9918032786885247,\n",
       "  'no_speech_prob': 0.3853035867214203},\n",
       " {'id': 73,\n",
       "  'seek': 61400,\n",
       "  'start': 626.0,\n",
       "  'end': 642.0,\n",
       "  'text': ' So index zero is associated with the a in column zero, the a in column one and the a in column two index one is associated with the b in column zero, the b in column one.',\n",
       "  'tokens': [50964,\n",
       "   407,\n",
       "   8186,\n",
       "   4018,\n",
       "   307,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   257,\n",
       "   294,\n",
       "   7738,\n",
       "   4018,\n",
       "   11,\n",
       "   264,\n",
       "   257,\n",
       "   294,\n",
       "   7738,\n",
       "   472,\n",
       "   293,\n",
       "   264,\n",
       "   257,\n",
       "   294,\n",
       "   7738,\n",
       "   732,\n",
       "   8186,\n",
       "   472,\n",
       "   307,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   272,\n",
       "   294,\n",
       "   7738,\n",
       "   4018,\n",
       "   11,\n",
       "   264,\n",
       "   272,\n",
       "   294,\n",
       "   7738,\n",
       "   472,\n",
       "   13,\n",
       "   51764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.17487278580665588,\n",
       "  'compression_ratio': 1.9918032786885247,\n",
       "  'no_speech_prob': 0.3853035867214203},\n",
       " {'id': 74,\n",
       "  'seek': 64200,\n",
       "  'start': 642.0,\n",
       "  'end': 653.0,\n",
       "  'text': ' And the b in column two and the index two is associated with the c in column zero, et cetera, et cetera.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   264,\n",
       "   272,\n",
       "   294,\n",
       "   7738,\n",
       "   732,\n",
       "   293,\n",
       "   264,\n",
       "   8186,\n",
       "   732,\n",
       "   307,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   269,\n",
       "   294,\n",
       "   7738,\n",
       "   4018,\n",
       "   11,\n",
       "   1030,\n",
       "   11458,\n",
       "   11,\n",
       "   1030,\n",
       "   11458,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10759287327528,\n",
       "  'compression_ratio': 1.5027932960893855,\n",
       "  'no_speech_prob': 0.0022049820981919765},\n",
       " {'id': 75,\n",
       "  'seek': 64200,\n",
       "  'start': 653.0,\n",
       "  'end': 667.0,\n",
       "  'text': \" So on the next slide, we're going to create a data frame that contains the unemployment rate every other year for each region in the United States starting in 1995.\",\n",
       "  'tokens': [50914,\n",
       "   407,\n",
       "   322,\n",
       "   264,\n",
       "   958,\n",
       "   4137,\n",
       "   11,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   1884,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   300,\n",
       "   8306,\n",
       "   264,\n",
       "   17438,\n",
       "   3314,\n",
       "   633,\n",
       "   661,\n",
       "   1064,\n",
       "   337,\n",
       "   1184,\n",
       "   4458,\n",
       "   294,\n",
       "   264,\n",
       "   2824,\n",
       "   3040,\n",
       "   2891,\n",
       "   294,\n",
       "   22601,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10759287327528,\n",
       "  'compression_ratio': 1.5027932960893855,\n",
       "  'no_speech_prob': 0.0022049820981919765},\n",
       " {'id': 76,\n",
       "  'seek': 66700,\n",
       "  'start': 668.0,\n",
       "  'end': 696.0,\n",
       "  'text': \" And so what you'll notice is now rather than giving us giving the data frame function a single list, we're going to give it a dictionary and we're going to say north east is the key and that maps to the list 5.9 5.6 dot dot dot south is the key, which maps to the value 5.3 5.2 dot dot dot.\",\n",
       "  'tokens': [50414,\n",
       "   400,\n",
       "   370,\n",
       "   437,\n",
       "   291,\n",
       "   603,\n",
       "   3449,\n",
       "   307,\n",
       "   586,\n",
       "   2831,\n",
       "   813,\n",
       "   2902,\n",
       "   505,\n",
       "   2902,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   2445,\n",
       "   257,\n",
       "   2167,\n",
       "   1329,\n",
       "   11,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   976,\n",
       "   309,\n",
       "   257,\n",
       "   25890,\n",
       "   293,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   584,\n",
       "   6830,\n",
       "   10648,\n",
       "   307,\n",
       "   264,\n",
       "   2141,\n",
       "   293,\n",
       "   300,\n",
       "   11317,\n",
       "   281,\n",
       "   264,\n",
       "   1329,\n",
       "   1025,\n",
       "   13,\n",
       "   24,\n",
       "   1025,\n",
       "   13,\n",
       "   21,\n",
       "   5893,\n",
       "   5893,\n",
       "   5893,\n",
       "   7377,\n",
       "   307,\n",
       "   264,\n",
       "   2141,\n",
       "   11,\n",
       "   597,\n",
       "   11317,\n",
       "   281,\n",
       "   264,\n",
       "   2158,\n",
       "   1025,\n",
       "   13,\n",
       "   18,\n",
       "   1025,\n",
       "   13,\n",
       "   17,\n",
       "   5893,\n",
       "   5893,\n",
       "   5893,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.15508462205717835,\n",
       "  'compression_ratio': 1.686046511627907,\n",
       "  'no_speech_prob': 0.06024167686700821},\n",
       " {'id': 77,\n",
       "  'seek': 69600,\n",
       "  'start': 696.0,\n",
       "  'end': 701.0,\n",
       "  'text': ' And then national is the key that maps to et cetera.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   550,\n",
       "   4048,\n",
       "   307,\n",
       "   264,\n",
       "   2141,\n",
       "   300,\n",
       "   11317,\n",
       "   281,\n",
       "   1030,\n",
       "   11458,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08229462478471838,\n",
       "  'compression_ratio': 1.680952380952381,\n",
       "  'no_speech_prob': 0.003683751681819558},\n",
       " {'id': 78,\n",
       "  'seek': 69600,\n",
       "  'start': 701.0,\n",
       "  'end': 708.0,\n",
       "  'text': \" And we're going to turn that into a data frame and we'll use the same index as we did before, which was years.\",\n",
       "  'tokens': [50614,\n",
       "   400,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   1261,\n",
       "   300,\n",
       "   666,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   293,\n",
       "   321,\n",
       "   603,\n",
       "   764,\n",
       "   264,\n",
       "   912,\n",
       "   8186,\n",
       "   382,\n",
       "   321,\n",
       "   630,\n",
       "   949,\n",
       "   11,\n",
       "   597,\n",
       "   390,\n",
       "   924,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08229462478471838,\n",
       "  'compression_ratio': 1.680952380952381,\n",
       "  'no_speech_prob': 0.003683751681819558},\n",
       " {'id': 79,\n",
       "  'seek': 69600,\n",
       "  'start': 708.0,\n",
       "  'end': 710.0,\n",
       "  'text': \" And then let's just see what that looks like.\",\n",
       "  'tokens': [50964,\n",
       "   400,\n",
       "   550,\n",
       "   718,\n",
       "   311,\n",
       "   445,\n",
       "   536,\n",
       "   437,\n",
       "   300,\n",
       "   1542,\n",
       "   411,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08229462478471838,\n",
       "  'compression_ratio': 1.680952380952381,\n",
       "  'no_speech_prob': 0.003683751681819558},\n",
       " {'id': 80,\n",
       "  'seek': 69600,\n",
       "  'start': 710.0,\n",
       "  'end': 721.0,\n",
       "  'text': \" So notice what it's done is it's taken the list that was associated with each key in the dictionary and it's turned that into a column of data.\",\n",
       "  'tokens': [51064,\n",
       "   407,\n",
       "   3449,\n",
       "   437,\n",
       "   309,\n",
       "   311,\n",
       "   1096,\n",
       "   307,\n",
       "   309,\n",
       "   311,\n",
       "   2726,\n",
       "   264,\n",
       "   1329,\n",
       "   300,\n",
       "   390,\n",
       "   6615,\n",
       "   365,\n",
       "   1184,\n",
       "   2141,\n",
       "   294,\n",
       "   264,\n",
       "   25890,\n",
       "   293,\n",
       "   309,\n",
       "   311,\n",
       "   3574,\n",
       "   300,\n",
       "   666,\n",
       "   257,\n",
       "   7738,\n",
       "   295,\n",
       "   1412,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08229462478471838,\n",
       "  'compression_ratio': 1.680952380952381,\n",
       "  'no_speech_prob': 0.003683751681819558},\n",
       " {'id': 81,\n",
       "  'seek': 72100,\n",
       "  'start': 722.0,\n",
       "  'end': 730.0,\n",
       "  'text': ' So the first value in northeast was 5.9 in 5.6 4.4, et cetera.',\n",
       "  'tokens': [50414,\n",
       "   407,\n",
       "   264,\n",
       "   700,\n",
       "   2158,\n",
       "   294,\n",
       "   40984,\n",
       "   390,\n",
       "   1025,\n",
       "   13,\n",
       "   24,\n",
       "   294,\n",
       "   1025,\n",
       "   13,\n",
       "   21,\n",
       "   1017,\n",
       "   13,\n",
       "   19,\n",
       "   11,\n",
       "   1030,\n",
       "   11458,\n",
       "   13,\n",
       "   50814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1276310166316246,\n",
       "  'compression_ratio': 1.5161290322580645,\n",
       "  'no_speech_prob': 0.03889414295554161},\n",
       " {'id': 82,\n",
       "  'seek': 72100,\n",
       "  'start': 730.0,\n",
       "  'end': 734.0,\n",
       "  'text': ' And notice now these are associated with the index 1995.',\n",
       "  'tokens': [50814,\n",
       "   400,\n",
       "   3449,\n",
       "   586,\n",
       "   613,\n",
       "   366,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   8186,\n",
       "   22601,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1276310166316246,\n",
       "  'compression_ratio': 1.5161290322580645,\n",
       "  'no_speech_prob': 0.03889414295554161},\n",
       " {'id': 83,\n",
       "  'seek': 72100,\n",
       "  'start': 734.0,\n",
       "  'end': 742.0,\n",
       "  'text': ' So in 1995, the northeast region had an unemployment rate of 5.9 while the Midwest had an unemployment rate of 4.5.',\n",
       "  'tokens': [51014,\n",
       "   407,\n",
       "   294,\n",
       "   22601,\n",
       "   11,\n",
       "   264,\n",
       "   40984,\n",
       "   4458,\n",
       "   632,\n",
       "   364,\n",
       "   17438,\n",
       "   3314,\n",
       "   295,\n",
       "   1025,\n",
       "   13,\n",
       "   24,\n",
       "   1339,\n",
       "   264,\n",
       "   33483,\n",
       "   632,\n",
       "   364,\n",
       "   17438,\n",
       "   3314,\n",
       "   295,\n",
       "   1017,\n",
       "   13,\n",
       "   20,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1276310166316246,\n",
       "  'compression_ratio': 1.5161290322580645,\n",
       "  'no_speech_prob': 0.03889414295554161},\n",
       " {'id': 84,\n",
       "  'seek': 74200,\n",
       "  'start': 742.0,\n",
       "  'end': 753.0,\n",
       "  'text': ' The south had an unemployment rate of 5.3 and the west had an unemployment rate of 6.6 and the national unemployment was 5.6.',\n",
       "  'tokens': [50364,\n",
       "   440,\n",
       "   7377,\n",
       "   632,\n",
       "   364,\n",
       "   17438,\n",
       "   3314,\n",
       "   295,\n",
       "   1025,\n",
       "   13,\n",
       "   18,\n",
       "   293,\n",
       "   264,\n",
       "   7009,\n",
       "   632,\n",
       "   364,\n",
       "   17438,\n",
       "   3314,\n",
       "   295,\n",
       "   1386,\n",
       "   13,\n",
       "   21,\n",
       "   293,\n",
       "   264,\n",
       "   4048,\n",
       "   17438,\n",
       "   390,\n",
       "   1025,\n",
       "   13,\n",
       "   21,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09081813733871669,\n",
       "  'compression_ratio': 1.8580246913580247,\n",
       "  'no_speech_prob': 0.019418228417634964},\n",
       " {'id': 85,\n",
       "  'seek': 74200,\n",
       "  'start': 753.0,\n",
       "  'end': 762.0,\n",
       "  'text': ' So again, we can extract the index from our unemployment and this is the same index that we saw earlier.',\n",
       "  'tokens': [50914,\n",
       "   407,\n",
       "   797,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   8947,\n",
       "   264,\n",
       "   8186,\n",
       "   490,\n",
       "   527,\n",
       "   17438,\n",
       "   293,\n",
       "   341,\n",
       "   307,\n",
       "   264,\n",
       "   912,\n",
       "   8186,\n",
       "   300,\n",
       "   321,\n",
       "   1866,\n",
       "   3071,\n",
       "   13,\n",
       "   51364],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09081813733871669,\n",
       "  'compression_ratio': 1.8580246913580247,\n",
       "  'no_speech_prob': 0.019418228417634964},\n",
       " {'id': 86,\n",
       "  'seek': 74200,\n",
       "  'start': 762.0,\n",
       "  'end': 768.0,\n",
       "  'text': ' And we can extract the values from the unemployment region data frame.',\n",
       "  'tokens': [51364,\n",
       "   400,\n",
       "   321,\n",
       "   393,\n",
       "   8947,\n",
       "   264,\n",
       "   4190,\n",
       "   490,\n",
       "   264,\n",
       "   17438,\n",
       "   4458,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09081813733871669,\n",
       "  'compression_ratio': 1.8580246913580247,\n",
       "  'no_speech_prob': 0.019418228417634964},\n",
       " {'id': 87,\n",
       "  'seek': 76800,\n",
       "  'start': 768.0,\n",
       "  'end': 773.0,\n",
       "  'text': \" And notice all this is, is it's a two-dimensional array, it's a matrix.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   3449,\n",
       "   439,\n",
       "   341,\n",
       "   307,\n",
       "   11,\n",
       "   307,\n",
       "   309,\n",
       "   311,\n",
       "   257,\n",
       "   732,\n",
       "   12,\n",
       "   18759,\n",
       "   10225,\n",
       "   11,\n",
       "   309,\n",
       "   311,\n",
       "   257,\n",
       "   8141,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0985215775510098,\n",
       "  'compression_ratio': 1.542857142857143,\n",
       "  'no_speech_prob': 0.004811042919754982},\n",
       " {'id': 88,\n",
       "  'seek': 76800,\n",
       "  'start': 773.0,\n",
       "  'end': 788.0,\n",
       "  'text': ' And these values 5.9 5.6 4.4 are associated with the values that correspond to northeast because it was the first column 5.9 5.6 4.4.',\n",
       "  'tokens': [50614,\n",
       "   400,\n",
       "   613,\n",
       "   4190,\n",
       "   1025,\n",
       "   13,\n",
       "   24,\n",
       "   1025,\n",
       "   13,\n",
       "   21,\n",
       "   1017,\n",
       "   13,\n",
       "   19,\n",
       "   366,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   4190,\n",
       "   300,\n",
       "   6805,\n",
       "   281,\n",
       "   40984,\n",
       "   570,\n",
       "   309,\n",
       "   390,\n",
       "   264,\n",
       "   700,\n",
       "   7738,\n",
       "   1025,\n",
       "   13,\n",
       "   24,\n",
       "   1025,\n",
       "   13,\n",
       "   21,\n",
       "   1017,\n",
       "   13,\n",
       "   19,\n",
       "   13,\n",
       "   51364],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0985215775510098,\n",
       "  'compression_ratio': 1.542857142857143,\n",
       "  'no_speech_prob': 0.004811042919754982},\n",
       " {'id': 89,\n",
       "  'seek': 76800,\n",
       "  'start': 788.0,\n",
       "  'end': 791.0,\n",
       "  'text': ' So what can we do with the data frame?',\n",
       "  'tokens': [51364, 407, 437, 393, 321, 360, 365, 264, 1412, 3920, 30, 51514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0985215775510098,\n",
       "  'compression_ratio': 1.542857142857143,\n",
       "  'no_speech_prob': 0.004811042919754982},\n",
       " {'id': 90,\n",
       "  'seek': 76800,\n",
       "  'start': 791.0,\n",
       "  'end': 796.0,\n",
       "  'text': ' The answer is pretty much everything we can do with a series and a little more.',\n",
       "  'tokens': [51514,\n",
       "   440,\n",
       "   1867,\n",
       "   307,\n",
       "   1238,\n",
       "   709,\n",
       "   1203,\n",
       "   321,\n",
       "   393,\n",
       "   360,\n",
       "   365,\n",
       "   257,\n",
       "   2638,\n",
       "   293,\n",
       "   257,\n",
       "   707,\n",
       "   544,\n",
       "   13,\n",
       "   51764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0985215775510098,\n",
       "  'compression_ratio': 1.542857142857143,\n",
       "  'no_speech_prob': 0.004811042919754982},\n",
       " {'id': 91,\n",
       "  'seek': 79600,\n",
       "  'start': 796.0,\n",
       "  'end': 805.0,\n",
       "  'text': ' So just like with the series, we can take the head and the tail of the data frame to get an idea of what our data looks like.',\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   445,\n",
       "   411,\n",
       "   365,\n",
       "   264,\n",
       "   2638,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   747,\n",
       "   264,\n",
       "   1378,\n",
       "   293,\n",
       "   264,\n",
       "   6838,\n",
       "   295,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   281,\n",
       "   483,\n",
       "   364,\n",
       "   1558,\n",
       "   295,\n",
       "   437,\n",
       "   527,\n",
       "   1412,\n",
       "   1542,\n",
       "   411,\n",
       "   13,\n",
       "   50814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.056793382673552543,\n",
       "  'compression_ratio': 1.578616352201258,\n",
       "  'no_speech_prob': 0.004448045045137405},\n",
       " {'id': 92,\n",
       "  'seek': 79600,\n",
       "  'start': 805.0,\n",
       "  'end': 817.0,\n",
       "  'text': ' Additionally, we can create a plot and notice this plot now has one line for each column in the data frame.',\n",
       "  'tokens': [50814,\n",
       "   19927,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   1884,\n",
       "   257,\n",
       "   7542,\n",
       "   293,\n",
       "   3449,\n",
       "   341,\n",
       "   7542,\n",
       "   586,\n",
       "   575,\n",
       "   472,\n",
       "   1622,\n",
       "   337,\n",
       "   1184,\n",
       "   7738,\n",
       "   294,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.056793382673552543,\n",
       "  'compression_ratio': 1.578616352201258,\n",
       "  'no_speech_prob': 0.004448045045137405},\n",
       " {'id': 93,\n",
       "  'seek': 79600,\n",
       "  'start': 817.0,\n",
       "  'end': 819.0,\n",
       "  'text': ' And we can index.',\n",
       "  'tokens': [51414, 400, 321, 393, 8186, 13, 51514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.056793382673552543,\n",
       "  'compression_ratio': 1.578616352201258,\n",
       "  'no_speech_prob': 0.004448045045137405},\n",
       " {'id': 94,\n",
       "  'seek': 81900,\n",
       "  'start': 819.0,\n",
       "  'end': 829.0,\n",
       "  'text': \" Now this is going to look a lot like the indexing for a series but it's going to be a little bit more complicated because we have to choose from both an index and a column.\",\n",
       "  'tokens': [50364,\n",
       "   823,\n",
       "   341,\n",
       "   307,\n",
       "   516,\n",
       "   281,\n",
       "   574,\n",
       "   257,\n",
       "   688,\n",
       "   411,\n",
       "   264,\n",
       "   8186,\n",
       "   278,\n",
       "   337,\n",
       "   257,\n",
       "   2638,\n",
       "   457,\n",
       "   309,\n",
       "   311,\n",
       "   516,\n",
       "   281,\n",
       "   312,\n",
       "   257,\n",
       "   707,\n",
       "   857,\n",
       "   544,\n",
       "   6179,\n",
       "   570,\n",
       "   321,\n",
       "   362,\n",
       "   281,\n",
       "   2826,\n",
       "   490,\n",
       "   1293,\n",
       "   364,\n",
       "   8186,\n",
       "   293,\n",
       "   257,\n",
       "   7738,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06491527452573671,\n",
       "  'compression_ratio': 1.6905829596412556,\n",
       "  'no_speech_prob': 0.04663187637925148},\n",
       " {'id': 95,\n",
       "  'seek': 81900,\n",
       "  'start': 829.0,\n",
       "  'end': 831.0,\n",
       "  'text': \" So let's see how we would do that.\",\n",
       "  'tokens': [50864, 407, 718, 311, 536, 577, 321, 576, 360, 300, 13, 50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06491527452573671,\n",
       "  'compression_ratio': 1.6905829596412556,\n",
       "  'no_speech_prob': 0.04663187637925148},\n",
       " {'id': 96,\n",
       "  'seek': 81900,\n",
       "  'start': 831.0,\n",
       "  'end': 847.0,\n",
       "  'text': ' So if we wanted to get the value that was associated with the unemployment rate in the northeast in 1995, we would specify 1995 as the index and northeast as the column.',\n",
       "  'tokens': [50964,\n",
       "   407,\n",
       "   498,\n",
       "   321,\n",
       "   1415,\n",
       "   281,\n",
       "   483,\n",
       "   264,\n",
       "   2158,\n",
       "   300,\n",
       "   390,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   17438,\n",
       "   3314,\n",
       "   294,\n",
       "   264,\n",
       "   40984,\n",
       "   294,\n",
       "   22601,\n",
       "   11,\n",
       "   321,\n",
       "   576,\n",
       "   16500,\n",
       "   22601,\n",
       "   382,\n",
       "   264,\n",
       "   8186,\n",
       "   293,\n",
       "   40984,\n",
       "   382,\n",
       "   264,\n",
       "   7738,\n",
       "   13,\n",
       "   51764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06491527452573671,\n",
       "  'compression_ratio': 1.6905829596412556,\n",
       "  'no_speech_prob': 0.04663187637925148},\n",
       " {'id': 97,\n",
       "  'seek': 84700,\n",
       "  'start': 847.0,\n",
       "  'end': 850.0,\n",
       "  'text': ' And that gives us the 5.9.',\n",
       "  'tokens': [50364, 400, 300, 2709, 505, 264, 1025, 13, 24, 13, 50514],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08449968115075843,\n",
       "  'compression_ratio': 1.53125,\n",
       "  'no_speech_prob': 0.011833751574158669},\n",
       " {'id': 98,\n",
       "  'seek': 84700,\n",
       "  'start': 850.0,\n",
       "  'end': 864.0,\n",
       "  'text': ' If we wanted to get the 1995 in 2005 unemployment for the south, we could give it a list of 1995 in 2005 and then the south.',\n",
       "  'tokens': [50514,\n",
       "   759,\n",
       "   321,\n",
       "   1415,\n",
       "   281,\n",
       "   483,\n",
       "   264,\n",
       "   22601,\n",
       "   294,\n",
       "   14394,\n",
       "   17438,\n",
       "   337,\n",
       "   264,\n",
       "   7377,\n",
       "   11,\n",
       "   321,\n",
       "   727,\n",
       "   976,\n",
       "   309,\n",
       "   257,\n",
       "   1329,\n",
       "   295,\n",
       "   22601,\n",
       "   294,\n",
       "   14394,\n",
       "   293,\n",
       "   550,\n",
       "   264,\n",
       "   7377,\n",
       "   13,\n",
       "   51214],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08449968115075843,\n",
       "  'compression_ratio': 1.53125,\n",
       "  'no_speech_prob': 0.011833751574158669},\n",
       " {'id': 99,\n",
       "  'seek': 84700,\n",
       "  'start': 864.0,\n",
       "  'end': 876.0,\n",
       "  'text': \" And notice in this case, we were returned just a single number but because we're now asking for multiple things, it's returned a series to us.\",\n",
       "  'tokens': [51214,\n",
       "   400,\n",
       "   3449,\n",
       "   294,\n",
       "   341,\n",
       "   1389,\n",
       "   11,\n",
       "   321,\n",
       "   645,\n",
       "   8752,\n",
       "   445,\n",
       "   257,\n",
       "   2167,\n",
       "   1230,\n",
       "   457,\n",
       "   570,\n",
       "   321,\n",
       "   434,\n",
       "   586,\n",
       "   3365,\n",
       "   337,\n",
       "   3866,\n",
       "   721,\n",
       "   11,\n",
       "   309,\n",
       "   311,\n",
       "   8752,\n",
       "   257,\n",
       "   2638,\n",
       "   281,\n",
       "   505,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.08449968115075843,\n",
       "  'compression_ratio': 1.53125,\n",
       "  'no_speech_prob': 0.011833751574158669},\n",
       " {'id': 100,\n",
       "  'seek': 87600,\n",
       "  'start': 876.0,\n",
       "  'end': 884.0,\n",
       "  'text': ' And like we can select multiple values from the index, we can also select multiple values from the columns.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   411,\n",
       "   321,\n",
       "   393,\n",
       "   3048,\n",
       "   3866,\n",
       "   4190,\n",
       "   490,\n",
       "   264,\n",
       "   8186,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   611,\n",
       "   3048,\n",
       "   3866,\n",
       "   4190,\n",
       "   490,\n",
       "   264,\n",
       "   13766,\n",
       "   13,\n",
       "   50764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10290789604187012,\n",
       "  'compression_ratio': 1.685483870967742,\n",
       "  'no_speech_prob': 0.006126911845058203},\n",
       " {'id': 101,\n",
       "  'seek': 87600,\n",
       "  'start': 884.0,\n",
       "  'end': 889.0,\n",
       "  'text': \" But notice it's also turned it into a series.\",\n",
       "  'tokens': [50764,\n",
       "   583,\n",
       "   3449,\n",
       "   309,\n",
       "   311,\n",
       "   611,\n",
       "   3574,\n",
       "   309,\n",
       "   666,\n",
       "   257,\n",
       "   2638,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10290789604187012,\n",
       "  'compression_ratio': 1.685483870967742,\n",
       "  'no_speech_prob': 0.006126911845058203},\n",
       " {'id': 102,\n",
       "  'seek': 87600,\n",
       "  'start': 889.0,\n",
       "  'end': 897.0,\n",
       "  'text': ' We can also select an entire column by using the colon.',\n",
       "  'tokens': [51014,\n",
       "   492,\n",
       "   393,\n",
       "   611,\n",
       "   3048,\n",
       "   364,\n",
       "   2302,\n",
       "   7738,\n",
       "   538,\n",
       "   1228,\n",
       "   264,\n",
       "   8255,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10290789604187012,\n",
       "  'compression_ratio': 1.685483870967742,\n",
       "  'no_speech_prob': 0.006126911845058203},\n",
       " {'id': 103,\n",
       "  'seek': 89700,\n",
       "  'start': 897.0,\n",
       "  'end': 907.0,\n",
       "  'text': ' And we also could select an entire row by doing it the other way and notice both of those return a series.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   321,\n",
       "   611,\n",
       "   727,\n",
       "   3048,\n",
       "   364,\n",
       "   2302,\n",
       "   5386,\n",
       "   538,\n",
       "   884,\n",
       "   309,\n",
       "   264,\n",
       "   661,\n",
       "   636,\n",
       "   293,\n",
       "   3449,\n",
       "   1293,\n",
       "   295,\n",
       "   729,\n",
       "   2736,\n",
       "   257,\n",
       "   2638,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0727005588765047,\n",
       "  'compression_ratio': 1.4195804195804196,\n",
       "  'no_speech_prob': 0.004220528062433004},\n",
       " {'id': 104,\n",
       "  'seek': 89700,\n",
       "  'start': 907.0,\n",
       "  'end': 922.0,\n",
       "  'text': \" You can also select multiple values from both in which case you'll be returned a new data frame.\",\n",
       "  'tokens': [50864,\n",
       "   509,\n",
       "   393,\n",
       "   611,\n",
       "   3048,\n",
       "   3866,\n",
       "   4190,\n",
       "   490,\n",
       "   1293,\n",
       "   294,\n",
       "   597,\n",
       "   1389,\n",
       "   291,\n",
       "   603,\n",
       "   312,\n",
       "   8752,\n",
       "   257,\n",
       "   777,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0727005588765047,\n",
       "  'compression_ratio': 1.4195804195804196,\n",
       "  'no_speech_prob': 0.004220528062433004},\n",
       " {'id': 105,\n",
       "  'seek': 92200,\n",
       "  'start': 922.0,\n",
       "  'end': 932.0,\n",
       "  'text': \" And if you don't use the dot lock, it will just extract the column associated with the string you pass it.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   498,\n",
       "   291,\n",
       "   500,\n",
       "   380,\n",
       "   764,\n",
       "   264,\n",
       "   5893,\n",
       "   4017,\n",
       "   11,\n",
       "   309,\n",
       "   486,\n",
       "   445,\n",
       "   8947,\n",
       "   264,\n",
       "   7738,\n",
       "   6615,\n",
       "   365,\n",
       "   264,\n",
       "   6798,\n",
       "   291,\n",
       "   1320,\n",
       "   309,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11326853434244792,\n",
       "  'compression_ratio': 1.4293193717277486,\n",
       "  'no_speech_prob': 0.06897465139627457},\n",
       " {'id': 106,\n",
       "  'seek': 92200,\n",
       "  'start': 932.0,\n",
       "  'end': 936.0,\n",
       "  'text': ' Great. So now we know how to select data inside of a data frame.',\n",
       "  'tokens': [50864,\n",
       "   3769,\n",
       "   13,\n",
       "   407,\n",
       "   586,\n",
       "   321,\n",
       "   458,\n",
       "   577,\n",
       "   281,\n",
       "   3048,\n",
       "   1412,\n",
       "   1854,\n",
       "   295,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11326853434244792,\n",
       "  'compression_ratio': 1.4293193717277486,\n",
       "  'no_speech_prob': 0.06897465139627457},\n",
       " {'id': 107,\n",
       "  'seek': 92200,\n",
       "  'start': 936.0,\n",
       "  'end': 939.0,\n",
       "  'text': ' What types of things can we do?',\n",
       "  'tokens': [51064, 708, 3467, 295, 721, 393, 321, 360, 30, 51214],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11326853434244792,\n",
       "  'compression_ratio': 1.4293193717277486,\n",
       "  'no_speech_prob': 0.06897465139627457},\n",
       " {'id': 108,\n",
       "  'seek': 92200,\n",
       "  'start': 939.0,\n",
       "  'end': 949.0,\n",
       "  'text': ' So we can divide by 100, which moves it from percent units to a rate.',\n",
       "  'tokens': [51214,\n",
       "   407,\n",
       "   321,\n",
       "   393,\n",
       "   9845,\n",
       "   538,\n",
       "   2319,\n",
       "   11,\n",
       "   597,\n",
       "   6067,\n",
       "   309,\n",
       "   490,\n",
       "   3043,\n",
       "   6815,\n",
       "   281,\n",
       "   257,\n",
       "   3314,\n",
       "   13,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11326853434244792,\n",
       "  'compression_ratio': 1.4293193717277486,\n",
       "  'no_speech_prob': 0.06897465139627457},\n",
       " {'id': 109,\n",
       "  'seek': 94900,\n",
       "  'start': 949.0,\n",
       "  'end': 954.0,\n",
       "  'text': \" And notice it's divided every element of our column by 100.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   3449,\n",
       "   309,\n",
       "   311,\n",
       "   6666,\n",
       "   633,\n",
       "   4478,\n",
       "   295,\n",
       "   527,\n",
       "   7738,\n",
       "   538,\n",
       "   2319,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05869680737692212,\n",
       "  'compression_ratio': 1.5696969696969696,\n",
       "  'no_speech_prob': 0.0713319182395935},\n",
       " {'id': 110,\n",
       "  'seek': 94900,\n",
       "  'start': 954.0,\n",
       "  'end': 958.0,\n",
       "  'text': ' We can find the maximum value.',\n",
       "  'tokens': [50614, 492, 393, 915, 264, 6674, 2158, 13, 50814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05869680737692212,\n",
       "  'compression_ratio': 1.5696969696969696,\n",
       "  'no_speech_prob': 0.0713319182395935},\n",
       " {'id': 111,\n",
       "  'seek': 94900,\n",
       "  'start': 958.0,\n",
       "  'end': 961.0,\n",
       "  'text': ' We can subtract one column from another.',\n",
       "  'tokens': [50814, 492, 393, 16390, 472, 7738, 490, 1071, 13, 50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05869680737692212,\n",
       "  'compression_ratio': 1.5696969696969696,\n",
       "  'no_speech_prob': 0.0713319182395935},\n",
       " {'id': 112,\n",
       "  'seek': 94900,\n",
       "  'start': 961.0,\n",
       "  'end': 971.0,\n",
       "  'text': \" So we've taken the unemployment in the west and subtracted the unemployment in the Midwest, which just tells us the difference.\",\n",
       "  'tokens': [50964,\n",
       "   407,\n",
       "   321,\n",
       "   600,\n",
       "   2726,\n",
       "   264,\n",
       "   17438,\n",
       "   294,\n",
       "   264,\n",
       "   7009,\n",
       "   293,\n",
       "   16390,\n",
       "   292,\n",
       "   264,\n",
       "   17438,\n",
       "   294,\n",
       "   264,\n",
       "   33483,\n",
       "   11,\n",
       "   597,\n",
       "   445,\n",
       "   5112,\n",
       "   505,\n",
       "   264,\n",
       "   2649,\n",
       "   13,\n",
       "   51464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05869680737692212,\n",
       "  'compression_ratio': 1.5696969696969696,\n",
       "  'no_speech_prob': 0.0713319182395935},\n",
       " {'id': 113,\n",
       "  'seek': 97100,\n",
       "  'start': 971.0,\n",
       "  'end': 975.0,\n",
       "  'text': ' And we can also compute the correlation between two columns.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   321,\n",
       "   393,\n",
       "   611,\n",
       "   14722,\n",
       "   264,\n",
       "   20009,\n",
       "   1296,\n",
       "   732,\n",
       "   13766,\n",
       "   13,\n",
       "   50564],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0884284477729302,\n",
       "  'compression_ratio': 1.8108108108108107,\n",
       "  'no_speech_prob': 0.6083968281745911},\n",
       " {'id': 114,\n",
       "  'seek': 97100,\n",
       "  'start': 975.0,\n",
       "  'end': 981.0,\n",
       "  'text': ' So the unemployment in the west and the unemployment in the Midwest had a 0.9 correlation.',\n",
       "  'tokens': [50564,\n",
       "   407,\n",
       "   264,\n",
       "   17438,\n",
       "   294,\n",
       "   264,\n",
       "   7009,\n",
       "   293,\n",
       "   264,\n",
       "   17438,\n",
       "   294,\n",
       "   264,\n",
       "   33483,\n",
       "   632,\n",
       "   257,\n",
       "   1958,\n",
       "   13,\n",
       "   24,\n",
       "   20009,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0884284477729302,\n",
       "  'compression_ratio': 1.8108108108108107,\n",
       "  'no_speech_prob': 0.6083968281745911},\n",
       " {'id': 115,\n",
       "  'seek': 97100,\n",
       "  'start': 981.0,\n",
       "  'end': 987.0,\n",
       "  'text': ' And if we wanted all of the correlations at once, we can simply compute the correlation matrix.',\n",
       "  'tokens': [50864,\n",
       "   400,\n",
       "   498,\n",
       "   321,\n",
       "   1415,\n",
       "   439,\n",
       "   295,\n",
       "   264,\n",
       "   13983,\n",
       "   763,\n",
       "   412,\n",
       "   1564,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   2935,\n",
       "   14722,\n",
       "   264,\n",
       "   20009,\n",
       "   8141,\n",
       "   13,\n",
       "   51164],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0884284477729302,\n",
       "  'compression_ratio': 1.8108108108108107,\n",
       "  'no_speech_prob': 0.6083968281745911},\n",
       " {'id': 116,\n",
       "  'seek': 97100,\n",
       "  'start': 987.0,\n",
       "  'end': 993.0,\n",
       "  'text': ' So notice it says, a northeast is correlated perfectly with the northeast unsurprising.',\n",
       "  'tokens': [51164,\n",
       "   407,\n",
       "   3449,\n",
       "   309,\n",
       "   1619,\n",
       "   11,\n",
       "   257,\n",
       "   40984,\n",
       "   307,\n",
       "   38574,\n",
       "   6239,\n",
       "   365,\n",
       "   264,\n",
       "   40984,\n",
       "   2693,\n",
       "   374,\n",
       "   26203,\n",
       "   13,\n",
       "   51464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0884284477729302,\n",
       "  'compression_ratio': 1.8108108108108107,\n",
       "  'no_speech_prob': 0.6083968281745911},\n",
       " {'id': 117,\n",
       "  'seek': 99300,\n",
       "  'start': 993.0,\n",
       "  'end': 1005.0,\n",
       "  'text': \" It's got a 0.87 correlation with the Midwest, a 0.96 with the south in the west, and a 0.97 with the national unemployment rate.\",\n",
       "  'tokens': [50364,\n",
       "   467,\n",
       "   311,\n",
       "   658,\n",
       "   257,\n",
       "   1958,\n",
       "   13,\n",
       "   23853,\n",
       "   20009,\n",
       "   365,\n",
       "   264,\n",
       "   33483,\n",
       "   11,\n",
       "   257,\n",
       "   1958,\n",
       "   13,\n",
       "   22962,\n",
       "   365,\n",
       "   264,\n",
       "   7377,\n",
       "   294,\n",
       "   264,\n",
       "   7009,\n",
       "   11,\n",
       "   293,\n",
       "   257,\n",
       "   1958,\n",
       "   13,\n",
       "   23247,\n",
       "   365,\n",
       "   264,\n",
       "   4048,\n",
       "   17438,\n",
       "   3314,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05551497730208032,\n",
       "  'compression_ratio': 1.551219512195122,\n",
       "  'no_speech_prob': 0.0074313003569841385},\n",
       " {'id': 118,\n",
       "  'seek': 99300,\n",
       "  'start': 1005.0,\n",
       "  'end': 1008.0,\n",
       "  'text': \" So let's stop for a minute and do another exercise.\",\n",
       "  'tokens': [50964,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   1590,\n",
       "   337,\n",
       "   257,\n",
       "   3456,\n",
       "   293,\n",
       "   360,\n",
       "   1071,\n",
       "   5380,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05551497730208032,\n",
       "  'compression_ratio': 1.551219512195122,\n",
       "  'no_speech_prob': 0.0074313003569841385},\n",
       " {'id': 119,\n",
       "  'seek': 99300,\n",
       "  'start': 1008.0,\n",
       "  'end': 1018.0,\n",
       "  'text': \" In this case, we'll continue to encourage using introspection, the tab completion, and Google through to fulfill the following exercises.\",\n",
       "  'tokens': [51114,\n",
       "   682,\n",
       "   341,\n",
       "   1389,\n",
       "   11,\n",
       "   321,\n",
       "   603,\n",
       "   2354,\n",
       "   281,\n",
       "   5373,\n",
       "   1228,\n",
       "   560,\n",
       "   2635,\n",
       "   19997,\n",
       "   11,\n",
       "   264,\n",
       "   4421,\n",
       "   19372,\n",
       "   11,\n",
       "   293,\n",
       "   3329,\n",
       "   807,\n",
       "   281,\n",
       "   13875,\n",
       "   264,\n",
       "   3480,\n",
       "   11900,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.05551497730208032,\n",
       "  'compression_ratio': 1.551219512195122,\n",
       "  'no_speech_prob': 0.0074313003569841385},\n",
       " {'id': 120,\n",
       "  'seek': 101800,\n",
       "  'start': 1018.0,\n",
       "  'end': 1025.0,\n",
       "  'text': \" Let's find a way to obtain a list that contains all of the column names in the DataFrame unemployment region.\",\n",
       "  'tokens': [50364,\n",
       "   961,\n",
       "   311,\n",
       "   915,\n",
       "   257,\n",
       "   636,\n",
       "   281,\n",
       "   12701,\n",
       "   257,\n",
       "   1329,\n",
       "   300,\n",
       "   8306,\n",
       "   439,\n",
       "   295,\n",
       "   264,\n",
       "   7738,\n",
       "   5288,\n",
       "   294,\n",
       "   264,\n",
       "   11888,\n",
       "   40305,\n",
       "   529,\n",
       "   17438,\n",
       "   4458,\n",
       "   13,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0883508215145189,\n",
       "  'compression_ratio': 1.6523605150214593,\n",
       "  'no_speech_prob': 0.09200091660022736},\n",
       " {'id': 121,\n",
       "  'seek': 101800,\n",
       "  'start': 1025.0,\n",
       "  'end': 1029.0,\n",
       "  'text': \" Let's use the plotting method, the plot method, to make a bar plot.\",\n",
       "  'tokens': [50714,\n",
       "   961,\n",
       "   311,\n",
       "   764,\n",
       "   264,\n",
       "   41178,\n",
       "   3170,\n",
       "   11,\n",
       "   264,\n",
       "   7542,\n",
       "   3170,\n",
       "   11,\n",
       "   281,\n",
       "   652,\n",
       "   257,\n",
       "   2159,\n",
       "   7542,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0883508215145189,\n",
       "  'compression_ratio': 1.6523605150214593,\n",
       "  'no_speech_prob': 0.09200091660022736},\n",
       " {'id': 122,\n",
       "  'seek': 101800,\n",
       "  'start': 1029.0,\n",
       "  'end': 1035.0,\n",
       "  'text': ' What does this look like now and how does it compare to the plot we created with the series?',\n",
       "  'tokens': [50914,\n",
       "   708,\n",
       "   775,\n",
       "   341,\n",
       "   574,\n",
       "   411,\n",
       "   586,\n",
       "   293,\n",
       "   577,\n",
       "   775,\n",
       "   309,\n",
       "   6794,\n",
       "   281,\n",
       "   264,\n",
       "   7542,\n",
       "   321,\n",
       "   2942,\n",
       "   365,\n",
       "   264,\n",
       "   2638,\n",
       "   30,\n",
       "   51214],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0883508215145189,\n",
       "  'compression_ratio': 1.6523605150214593,\n",
       "  'no_speech_prob': 0.09200091660022736},\n",
       " {'id': 123,\n",
       "  'seek': 101800,\n",
       "  'start': 1035.0,\n",
       "  'end': 1045.0,\n",
       "  'text': \" Let's use .loc to select the unemployment data for the northeast and west in the years 1995, 2005, 2011, and 2015.\",\n",
       "  'tokens': [51214,\n",
       "   961,\n",
       "   311,\n",
       "   764,\n",
       "   2411,\n",
       "   5842,\n",
       "   281,\n",
       "   3048,\n",
       "   264,\n",
       "   17438,\n",
       "   1412,\n",
       "   337,\n",
       "   264,\n",
       "   40984,\n",
       "   293,\n",
       "   7009,\n",
       "   294,\n",
       "   264,\n",
       "   924,\n",
       "   22601,\n",
       "   11,\n",
       "   14394,\n",
       "   11,\n",
       "   10154,\n",
       "   11,\n",
       "   293,\n",
       "   7546,\n",
       "   13,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0883508215145189,\n",
       "  'compression_ratio': 1.6523605150214593,\n",
       "  'no_speech_prob': 0.09200091660022736},\n",
       " {'id': 124,\n",
       "  'seek': 104500,\n",
       "  'start': 1045.0,\n",
       "  'end': 1050.0,\n",
       "  'text': \" And let's run the code unemploymentregion.d types below.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   718,\n",
       "   311,\n",
       "   1190,\n",
       "   264,\n",
       "   3089,\n",
       "   17438,\n",
       "   3375,\n",
       "   313,\n",
       "   13,\n",
       "   67,\n",
       "   3467,\n",
       "   2507,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13408660888671875,\n",
       "  'compression_ratio': 1.5324074074074074,\n",
       "  'no_speech_prob': 0.04014256224036217},\n",
       " {'id': 125,\n",
       "  'seek': 104500,\n",
       "  'start': 1050.0,\n",
       "  'end': 1058.0,\n",
       "  'text': ' What does this give you? How does this compare to the same thing on the series operation?',\n",
       "  'tokens': [50614,\n",
       "   708,\n",
       "   775,\n",
       "   341,\n",
       "   976,\n",
       "   291,\n",
       "   30,\n",
       "   1012,\n",
       "   775,\n",
       "   341,\n",
       "   6794,\n",
       "   281,\n",
       "   264,\n",
       "   912,\n",
       "   551,\n",
       "   322,\n",
       "   264,\n",
       "   2638,\n",
       "   6916,\n",
       "   30,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13408660888671875,\n",
       "  'compression_ratio': 1.5324074074074074,\n",
       "  'no_speech_prob': 0.04014256224036217},\n",
       " {'id': 126,\n",
       "  'seek': 104500,\n",
       "  'start': 1058.0,\n",
       "  'end': 1061.0,\n",
       "  'text': \" Welcome back. So let's go over the answers.\",\n",
       "  'tokens': [51014,\n",
       "   4027,\n",
       "   646,\n",
       "   13,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   352,\n",
       "   670,\n",
       "   264,\n",
       "   6338,\n",
       "   13,\n",
       "   51164],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13408660888671875,\n",
       "  'compression_ratio': 1.5324074074074074,\n",
       "  'no_speech_prob': 0.04014256224036217},\n",
       " {'id': 127,\n",
       "  'seek': 104500,\n",
       "  'start': 1061.0,\n",
       "  'end': 1065.0,\n",
       "  'text': \" So there's a property of the DataFrame called .colums.\",\n",
       "  'tokens': [51164,\n",
       "   407,\n",
       "   456,\n",
       "   311,\n",
       "   257,\n",
       "   4707,\n",
       "   295,\n",
       "   264,\n",
       "   11888,\n",
       "   40305,\n",
       "   529,\n",
       "   1219,\n",
       "   2411,\n",
       "   8768,\n",
       "   8099,\n",
       "   13,\n",
       "   51364],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13408660888671875,\n",
       "  'compression_ratio': 1.5324074074074074,\n",
       "  'no_speech_prob': 0.04014256224036217},\n",
       " {'id': 128,\n",
       "  'seek': 104500,\n",
       "  'start': 1065.0,\n",
       "  'end': 1071.0,\n",
       "  'text': ' And notice this just corresponds to the equivalent of .index but for the column data.',\n",
       "  'tokens': [51364,\n",
       "   400,\n",
       "   3449,\n",
       "   341,\n",
       "   445,\n",
       "   23249,\n",
       "   281,\n",
       "   264,\n",
       "   10344,\n",
       "   295,\n",
       "   2411,\n",
       "   471,\n",
       "   3121,\n",
       "   457,\n",
       "   337,\n",
       "   264,\n",
       "   7738,\n",
       "   1412,\n",
       "   13,\n",
       "   51664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13408660888671875,\n",
       "  'compression_ratio': 1.5324074074074074,\n",
       "  'no_speech_prob': 0.04014256224036217},\n",
       " {'id': 129,\n",
       "  'seek': 107100,\n",
       "  'start': 1071.0,\n",
       "  'end': 1078.0,\n",
       "  'text': ' And notice it gives us northeast, Midwest, Southwest, and National.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   3449,\n",
       "   309,\n",
       "   2709,\n",
       "   505,\n",
       "   40984,\n",
       "   11,\n",
       "   33483,\n",
       "   11,\n",
       "   31708,\n",
       "   11,\n",
       "   293,\n",
       "   4862,\n",
       "   13,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11006030582246326,\n",
       "  'compression_ratio': 1.7619047619047619,\n",
       "  'no_speech_prob': 0.04076329991221428},\n",
       " {'id': 130,\n",
       "  'seek': 107100,\n",
       "  'start': 1078.0,\n",
       "  'end': 1085.0,\n",
       "  'text': \" We can make the bar plot and notice what it's done is it's plotted each of the columns for each year.\",\n",
       "  'tokens': [50714,\n",
       "   492,\n",
       "   393,\n",
       "   652,\n",
       "   264,\n",
       "   2159,\n",
       "   7542,\n",
       "   293,\n",
       "   3449,\n",
       "   437,\n",
       "   309,\n",
       "   311,\n",
       "   1096,\n",
       "   307,\n",
       "   309,\n",
       "   311,\n",
       "   43288,\n",
       "   1184,\n",
       "   295,\n",
       "   264,\n",
       "   13766,\n",
       "   337,\n",
       "   1184,\n",
       "   1064,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11006030582246326,\n",
       "  'compression_ratio': 1.7619047619047619,\n",
       "  'no_speech_prob': 0.04076329991221428},\n",
       " {'id': 131,\n",
       "  'seek': 107100,\n",
       "  'start': 1085.0,\n",
       "  'end': 1092.0,\n",
       "  'text': ' So the x-axis is still the indexes but now we have five different bars for each year.',\n",
       "  'tokens': [51064,\n",
       "   407,\n",
       "   264,\n",
       "   2031,\n",
       "   12,\n",
       "   24633,\n",
       "   307,\n",
       "   920,\n",
       "   264,\n",
       "   8186,\n",
       "   279,\n",
       "   457,\n",
       "   586,\n",
       "   321,\n",
       "   362,\n",
       "   1732,\n",
       "   819,\n",
       "   10228,\n",
       "   337,\n",
       "   1184,\n",
       "   1064,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11006030582246326,\n",
       "  'compression_ratio': 1.7619047619047619,\n",
       "  'no_speech_prob': 0.04076329991221428},\n",
       " {'id': 132,\n",
       "  'seek': 107100,\n",
       "  'start': 1092.0,\n",
       "  'end': 1099.0,\n",
       "  'text': ' And the colors correspond to the Northeast, Midwest, Southwest, and National.',\n",
       "  'tokens': [51414,\n",
       "   400,\n",
       "   264,\n",
       "   4577,\n",
       "   6805,\n",
       "   281,\n",
       "   264,\n",
       "   42150,\n",
       "   11,\n",
       "   33483,\n",
       "   11,\n",
       "   31708,\n",
       "   11,\n",
       "   293,\n",
       "   4862,\n",
       "   13,\n",
       "   51764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11006030582246326,\n",
       "  'compression_ratio': 1.7619047619047619,\n",
       "  'no_speech_prob': 0.04076329991221428},\n",
       " {'id': 133,\n",
       "  'seek': 109900,\n",
       "  'start': 1099.0,\n",
       "  'end': 1107.0,\n",
       "  'text': ' To select the subset of data we talked about, we can do as we discussed and pass two lists.',\n",
       "  'tokens': [50364,\n",
       "   1407,\n",
       "   3048,\n",
       "   264,\n",
       "   25993,\n",
       "   295,\n",
       "   1412,\n",
       "   321,\n",
       "   2825,\n",
       "   466,\n",
       "   11,\n",
       "   321,\n",
       "   393,\n",
       "   360,\n",
       "   382,\n",
       "   321,\n",
       "   7152,\n",
       "   293,\n",
       "   1320,\n",
       "   732,\n",
       "   14511,\n",
       "   13,\n",
       "   50764],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.15684746333530972,\n",
       "  'compression_ratio': 1.563953488372093,\n",
       "  'no_speech_prob': 0.007638425566256046},\n",
       " {'id': 134,\n",
       "  'seek': 109900,\n",
       "  'start': 1107.0,\n",
       "  'end': 1113.0,\n",
       "  'text': ' And with d types, notice we were told float64 but it now gives us a series.',\n",
       "  'tokens': [50764,\n",
       "   400,\n",
       "   365,\n",
       "   274,\n",
       "   3467,\n",
       "   11,\n",
       "   3449,\n",
       "   321,\n",
       "   645,\n",
       "   1907,\n",
       "   15706,\n",
       "   19395,\n",
       "   457,\n",
       "   309,\n",
       "   586,\n",
       "   2709,\n",
       "   505,\n",
       "   257,\n",
       "   2638,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.15684746333530972,\n",
       "  'compression_ratio': 1.563953488372093,\n",
       "  'no_speech_prob': 0.007638425566256046},\n",
       " {'id': 135,\n",
       "  'seek': 109900,\n",
       "  'start': 1113.0,\n",
       "  'end': 1124.0,\n",
       "  'text': ' So it says the Northeast column has all float data, the Midwest column has all float data, et cetera.',\n",
       "  'tokens': [51064,\n",
       "   407,\n",
       "   309,\n",
       "   1619,\n",
       "   264,\n",
       "   42150,\n",
       "   7738,\n",
       "   575,\n",
       "   439,\n",
       "   15706,\n",
       "   1412,\n",
       "   11,\n",
       "   264,\n",
       "   33483,\n",
       "   7738,\n",
       "   575,\n",
       "   439,\n",
       "   15706,\n",
       "   1412,\n",
       "   11,\n",
       "   1030,\n",
       "   11458,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.15684746333530972,\n",
       "  'compression_ratio': 1.563953488372093,\n",
       "  'no_speech_prob': 0.007638425566256046},\n",
       " {'id': 136,\n",
       "  'seek': 112400,\n",
       "  'start': 1125.0,\n",
       "  'end': 1134.0,\n",
       "  'text': ' So in the previous exercise we asked you to run the commands, unemployment.dtype and unemploymentregions.dtypes and to think about the outputs.',\n",
       "  'tokens': [50414,\n",
       "   407,\n",
       "   294,\n",
       "   264,\n",
       "   3894,\n",
       "   5380,\n",
       "   321,\n",
       "   2351,\n",
       "   291,\n",
       "   281,\n",
       "   1190,\n",
       "   264,\n",
       "   16901,\n",
       "   11,\n",
       "   17438,\n",
       "   13,\n",
       "   67,\n",
       "   20467,\n",
       "   293,\n",
       "   17438,\n",
       "   3375,\n",
       "   626,\n",
       "   13,\n",
       "   67,\n",
       "   874,\n",
       "   5190,\n",
       "   293,\n",
       "   281,\n",
       "   519,\n",
       "   466,\n",
       "   264,\n",
       "   23930,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13262562318281693,\n",
       "  'compression_ratio': 1.7290836653386454,\n",
       "  'no_speech_prob': 0.05511924996972084},\n",
       " {'id': 137,\n",
       "  'seek': 112400,\n",
       "  'start': 1134.0,\n",
       "  'end': 1139.0,\n",
       "  'text': \" As we've already discussed, they return the types of the values inside of each column.\",\n",
       "  'tokens': [50864,\n",
       "   1018,\n",
       "   321,\n",
       "   600,\n",
       "   1217,\n",
       "   7152,\n",
       "   11,\n",
       "   436,\n",
       "   2736,\n",
       "   264,\n",
       "   3467,\n",
       "   295,\n",
       "   264,\n",
       "   4190,\n",
       "   1854,\n",
       "   295,\n",
       "   1184,\n",
       "   7738,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13262562318281693,\n",
       "  'compression_ratio': 1.7290836653386454,\n",
       "  'no_speech_prob': 0.05511924996972084},\n",
       " {'id': 138,\n",
       "  'seek': 112400,\n",
       "  'start': 1139.0,\n",
       "  'end': 1143.0,\n",
       "  'text': ' If we do this with a series, it will just give us a single type.',\n",
       "  'tokens': [51114,\n",
       "   759,\n",
       "   321,\n",
       "   360,\n",
       "   341,\n",
       "   365,\n",
       "   257,\n",
       "   2638,\n",
       "   11,\n",
       "   309,\n",
       "   486,\n",
       "   445,\n",
       "   976,\n",
       "   505,\n",
       "   257,\n",
       "   2167,\n",
       "   2010,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13262562318281693,\n",
       "  'compression_ratio': 1.7290836653386454,\n",
       "  'no_speech_prob': 0.05511924996972084},\n",
       " {'id': 139,\n",
       "  'seek': 112400,\n",
       "  'start': 1143.0,\n",
       "  'end': 1153.0,\n",
       "  'text': ' And if we do this with a DataFrame, it gives us a series that maps the column name into what type of data is stored inside of that column.',\n",
       "  'tokens': [51314,\n",
       "   400,\n",
       "   498,\n",
       "   321,\n",
       "   360,\n",
       "   341,\n",
       "   365,\n",
       "   257,\n",
       "   11888,\n",
       "   40305,\n",
       "   529,\n",
       "   11,\n",
       "   309,\n",
       "   2709,\n",
       "   505,\n",
       "   257,\n",
       "   2638,\n",
       "   300,\n",
       "   11317,\n",
       "   264,\n",
       "   7738,\n",
       "   1315,\n",
       "   666,\n",
       "   437,\n",
       "   2010,\n",
       "   295,\n",
       "   1412,\n",
       "   307,\n",
       "   12187,\n",
       "   1854,\n",
       "   295,\n",
       "   300,\n",
       "   7738,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.13262562318281693,\n",
       "  'compression_ratio': 1.7290836653386454,\n",
       "  'no_speech_prob': 0.05511924996972084},\n",
       " {'id': 140,\n",
       "  'seek': 115300,\n",
       "  'start': 1153.0,\n",
       "  'end': 1158.0,\n",
       "  'text': \" So it's important that you often check what type of data you're reading into.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   309,\n",
       "   311,\n",
       "   1021,\n",
       "   300,\n",
       "   291,\n",
       "   2049,\n",
       "   1520,\n",
       "   437,\n",
       "   2010,\n",
       "   295,\n",
       "   1412,\n",
       "   291,\n",
       "   434,\n",
       "   3760,\n",
       "   666,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1014703901687471,\n",
       "  'compression_ratio': 1.5529411764705883,\n",
       "  'no_speech_prob': 0.0013911083806306124},\n",
       " {'id': 141,\n",
       "  'seek': 115300,\n",
       "  'start': 1158.0,\n",
       "  'end': 1167.0,\n",
       "  'text': ' Pandas? Because you can get some things where if data is not of the type you think it is, that it will misbehave.',\n",
       "  'tokens': [50614,\n",
       "   16995,\n",
       "   296,\n",
       "   30,\n",
       "   1436,\n",
       "   291,\n",
       "   393,\n",
       "   483,\n",
       "   512,\n",
       "   721,\n",
       "   689,\n",
       "   498,\n",
       "   1412,\n",
       "   307,\n",
       "   406,\n",
       "   295,\n",
       "   264,\n",
       "   2010,\n",
       "   291,\n",
       "   519,\n",
       "   309,\n",
       "   307,\n",
       "   11,\n",
       "   300,\n",
       "   309,\n",
       "   486,\n",
       "   3346,\n",
       "   650,\n",
       "   24284,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1014703901687471,\n",
       "  'compression_ratio': 1.5529411764705883,\n",
       "  'no_speech_prob': 0.0013911083806306124},\n",
       " {'id': 142,\n",
       "  'seek': 115300,\n",
       "  'start': 1167.0,\n",
       "  'end': 1172.0,\n",
       "  'text': \" So let's see for example, so DataFrames will only distinguish between a few types.\",\n",
       "  'tokens': [51064,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   536,\n",
       "   337,\n",
       "   1365,\n",
       "   11,\n",
       "   370,\n",
       "   11888,\n",
       "   40305,\n",
       "   1632,\n",
       "   486,\n",
       "   787,\n",
       "   20206,\n",
       "   1296,\n",
       "   257,\n",
       "   1326,\n",
       "   3467,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1014703901687471,\n",
       "  'compression_ratio': 1.5529411764705883,\n",
       "  'no_speech_prob': 0.0013911083806306124},\n",
       " {'id': 143,\n",
       "  'seek': 115300,\n",
       "  'start': 1172.0,\n",
       "  'end': 1180.0,\n",
       "  'text': \" They can recognize Booleans, floating point numbers, integers, dates, which we'll talk more about soon, categorical data,\",\n",
       "  'tokens': [51314,\n",
       "   814,\n",
       "   393,\n",
       "   5521,\n",
       "   23351,\n",
       "   24008,\n",
       "   11,\n",
       "   12607,\n",
       "   935,\n",
       "   3547,\n",
       "   11,\n",
       "   41674,\n",
       "   11,\n",
       "   11691,\n",
       "   11,\n",
       "   597,\n",
       "   321,\n",
       "   603,\n",
       "   751,\n",
       "   544,\n",
       "   466,\n",
       "   2321,\n",
       "   11,\n",
       "   19250,\n",
       "   804,\n",
       "   1412,\n",
       "   11,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.1014703901687471,\n",
       "  'compression_ratio': 1.5529411764705883,\n",
       "  'no_speech_prob': 0.0013911083806306124},\n",
       " {'id': 144,\n",
       "  'seek': 118000,\n",
       "  'start': 1180.0,\n",
       "  'end': 1185.0,\n",
       "  'text': ' and then everything else, including strings, is stored as an object.',\n",
       "  'tokens': [50364,\n",
       "   293,\n",
       "   550,\n",
       "   1203,\n",
       "   1646,\n",
       "   11,\n",
       "   3009,\n",
       "   13985,\n",
       "   11,\n",
       "   307,\n",
       "   12187,\n",
       "   382,\n",
       "   364,\n",
       "   2657,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07901814676100208,\n",
       "  'compression_ratio': 1.6485355648535565,\n",
       "  'no_speech_prob': 0.027216440066695213},\n",
       " {'id': 145,\n",
       "  'seek': 118000,\n",
       "  'start': 1185.0,\n",
       "  'end': 1191.0,\n",
       "  'text': ' In the future, we will continue to refer to the data type of data stored in a column as its dtype.',\n",
       "  'tokens': [50614,\n",
       "   682,\n",
       "   264,\n",
       "   2027,\n",
       "   11,\n",
       "   321,\n",
       "   486,\n",
       "   2354,\n",
       "   281,\n",
       "   2864,\n",
       "   281,\n",
       "   264,\n",
       "   1412,\n",
       "   2010,\n",
       "   295,\n",
       "   1412,\n",
       "   12187,\n",
       "   294,\n",
       "   257,\n",
       "   7738,\n",
       "   382,\n",
       "   1080,\n",
       "   274,\n",
       "   20467,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07901814676100208,\n",
       "  'compression_ratio': 1.6485355648535565,\n",
       "  'no_speech_prob': 0.027216440066695213},\n",
       " {'id': 146,\n",
       "  'seek': 118000,\n",
       "  'start': 1191.0,\n",
       "  'end': 1196.0,\n",
       "  'text': \" So let's look more closely at an example of when having an incorrect dtype can cause problems.\",\n",
       "  'tokens': [50914,\n",
       "   407,\n",
       "   718,\n",
       "   311,\n",
       "   574,\n",
       "   544,\n",
       "   8185,\n",
       "   412,\n",
       "   364,\n",
       "   1365,\n",
       "   295,\n",
       "   562,\n",
       "   1419,\n",
       "   364,\n",
       "   18424,\n",
       "   274,\n",
       "   20467,\n",
       "   393,\n",
       "   3082,\n",
       "   2740,\n",
       "   13,\n",
       "   51164],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07901814676100208,\n",
       "  'compression_ratio': 1.6485355648535565,\n",
       "  'no_speech_prob': 0.027216440066695213},\n",
       " {'id': 147,\n",
       "  'seek': 118000,\n",
       "  'start': 1196.0,\n",
       "  'end': 1207.0,\n",
       "  'text': ' So suppose that we imported this unemployment data from somewhere else, but instead the data for the South column came as a string.',\n",
       "  'tokens': [51164,\n",
       "   407,\n",
       "   7297,\n",
       "   300,\n",
       "   321,\n",
       "   25524,\n",
       "   341,\n",
       "   17438,\n",
       "   1412,\n",
       "   490,\n",
       "   4079,\n",
       "   1646,\n",
       "   11,\n",
       "   457,\n",
       "   2602,\n",
       "   264,\n",
       "   1412,\n",
       "   337,\n",
       "   264,\n",
       "   4242,\n",
       "   7738,\n",
       "   1361,\n",
       "   382,\n",
       "   257,\n",
       "   6798,\n",
       "   13,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07901814676100208,\n",
       "  'compression_ratio': 1.6485355648535565,\n",
       "  'no_speech_prob': 0.027216440066695213},\n",
       " {'id': 148,\n",
       "  'seek': 120700,\n",
       "  'start': 1207.0,\n",
       "  'end': 1214.0,\n",
       "  'text': \" So notice what we've done here is we've created, we've taken the column of the South,\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   3449,\n",
       "   437,\n",
       "   321,\n",
       "   600,\n",
       "   1096,\n",
       "   510,\n",
       "   307,\n",
       "   321,\n",
       "   600,\n",
       "   2942,\n",
       "   11,\n",
       "   321,\n",
       "   600,\n",
       "   2726,\n",
       "   264,\n",
       "   7738,\n",
       "   295,\n",
       "   264,\n",
       "   4242,\n",
       "   11,\n",
       "   50714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09199254153525993,\n",
       "  'compression_ratio': 1.598802395209581,\n",
       "  'no_speech_prob': 0.01499343290925026},\n",
       " {'id': 149,\n",
       "  'seek': 120700,\n",
       "  'start': 1214.0,\n",
       "  'end': 1220.0,\n",
       "  'text': \" and we've turned it into a string data type, and then we've saved it back into that column.\",\n",
       "  'tokens': [50714,\n",
       "   293,\n",
       "   321,\n",
       "   600,\n",
       "   3574,\n",
       "   309,\n",
       "   666,\n",
       "   257,\n",
       "   6798,\n",
       "   1412,\n",
       "   2010,\n",
       "   11,\n",
       "   293,\n",
       "   550,\n",
       "   321,\n",
       "   600,\n",
       "   6624,\n",
       "   309,\n",
       "   646,\n",
       "   666,\n",
       "   300,\n",
       "   7738,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09199254153525993,\n",
       "  'compression_ratio': 1.598802395209581,\n",
       "  'no_speech_prob': 0.01499343290925026},\n",
       " {'id': 150,\n",
       "  'seek': 120700,\n",
       "  'start': 1220.0,\n",
       "  'end': 1228.0,\n",
       "  'text': ' And now when we look at string unemployment d types, we see float64, float64, and object.',\n",
       "  'tokens': [51014,\n",
       "   400,\n",
       "   586,\n",
       "   562,\n",
       "   321,\n",
       "   574,\n",
       "   412,\n",
       "   6798,\n",
       "   17438,\n",
       "   274,\n",
       "   3467,\n",
       "   11,\n",
       "   321,\n",
       "   536,\n",
       "   15706,\n",
       "   19395,\n",
       "   11,\n",
       "   15706,\n",
       "   19395,\n",
       "   11,\n",
       "   293,\n",
       "   2657,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.09199254153525993,\n",
       "  'compression_ratio': 1.598802395209581,\n",
       "  'no_speech_prob': 0.01499343290925026},\n",
       " {'id': 151,\n",
       "  'seek': 122800,\n",
       "  'start': 1229.0,\n",
       "  'end': 1234.0,\n",
       "  'text': ' So if we just look at the data frame, notice everything looks okay.',\n",
       "  'tokens': [50414,\n",
       "   407,\n",
       "   498,\n",
       "   321,\n",
       "   445,\n",
       "   574,\n",
       "   412,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   11,\n",
       "   3449,\n",
       "   1203,\n",
       "   1542,\n",
       "   1392,\n",
       "   13,\n",
       "   50664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10299312637512942,\n",
       "  'compression_ratio': 1.6009852216748768,\n",
       "  'no_speech_prob': 0.031843509525060654},\n",
       " {'id': 152,\n",
       "  'seek': 122800,\n",
       "  'start': 1234.0,\n",
       "  'end': 1245.0,\n",
       "  'text': \" You don't notice that South is of type string, but if we try to do something like compute the sum, you'll get something weird.\",\n",
       "  'tokens': [50664,\n",
       "   509,\n",
       "   500,\n",
       "   380,\n",
       "   3449,\n",
       "   300,\n",
       "   4242,\n",
       "   307,\n",
       "   295,\n",
       "   2010,\n",
       "   6798,\n",
       "   11,\n",
       "   457,\n",
       "   498,\n",
       "   321,\n",
       "   853,\n",
       "   281,\n",
       "   360,\n",
       "   746,\n",
       "   411,\n",
       "   14722,\n",
       "   264,\n",
       "   2408,\n",
       "   11,\n",
       "   291,\n",
       "   603,\n",
       "   483,\n",
       "   746,\n",
       "   3657,\n",
       "   13,\n",
       "   51214],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10299312637512942,\n",
       "  'compression_ratio': 1.6009852216748768,\n",
       "  'no_speech_prob': 0.031843509525060654},\n",
       " {'id': 153,\n",
       "  'seek': 122800,\n",
       "  'start': 1245.0,\n",
       "  'end': 1254.0,\n",
       "  'text': \" So notice for the Northeast, Midwest, West, and National, it's performed the sum as we might expect, but when we summed the South,\",\n",
       "  'tokens': [51214,\n",
       "   407,\n",
       "   3449,\n",
       "   337,\n",
       "   264,\n",
       "   42150,\n",
       "   11,\n",
       "   33483,\n",
       "   11,\n",
       "   4055,\n",
       "   11,\n",
       "   293,\n",
       "   4862,\n",
       "   11,\n",
       "   309,\n",
       "   311,\n",
       "   10332,\n",
       "   264,\n",
       "   2408,\n",
       "   382,\n",
       "   321,\n",
       "   1062,\n",
       "   2066,\n",
       "   11,\n",
       "   457,\n",
       "   562,\n",
       "   321,\n",
       "   2408,\n",
       "   1912,\n",
       "   264,\n",
       "   4242,\n",
       "   11,\n",
       "   51664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10299312637512942,\n",
       "  'compression_ratio': 1.6009852216748768,\n",
       "  'no_speech_prob': 0.031843509525060654},\n",
       " {'id': 154,\n",
       "  'seek': 125400,\n",
       "  'start': 1254.0,\n",
       "  'end': 1259.0,\n",
       "  'text': \" it's just taken all of the unemployment rates, and it's tied the strings together.\",\n",
       "  'tokens': [50364,\n",
       "   309,\n",
       "   311,\n",
       "   445,\n",
       "   2726,\n",
       "   439,\n",
       "   295,\n",
       "   264,\n",
       "   17438,\n",
       "   6846,\n",
       "   11,\n",
       "   293,\n",
       "   309,\n",
       "   311,\n",
       "   9601,\n",
       "   264,\n",
       "   13985,\n",
       "   1214,\n",
       "   13,\n",
       "   50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0663644654410226,\n",
       "  'compression_ratio': 1.7366255144032923,\n",
       "  'no_speech_prob': 0.031958211213350296},\n",
       " {'id': 155,\n",
       "  'seek': 125400,\n",
       "  'start': 1259.0,\n",
       "  'end': 1269.0,\n",
       "  'text': ' And this happens because dot sum is just calling plus between each of the rows in a column, and when we add two strings, the result is the two strings being concatenated.',\n",
       "  'tokens': [50614,\n",
       "   400,\n",
       "   341,\n",
       "   2314,\n",
       "   570,\n",
       "   5893,\n",
       "   2408,\n",
       "   307,\n",
       "   445,\n",
       "   5141,\n",
       "   1804,\n",
       "   1296,\n",
       "   1184,\n",
       "   295,\n",
       "   264,\n",
       "   13241,\n",
       "   294,\n",
       "   257,\n",
       "   7738,\n",
       "   11,\n",
       "   293,\n",
       "   562,\n",
       "   321,\n",
       "   909,\n",
       "   732,\n",
       "   13985,\n",
       "   11,\n",
       "   264,\n",
       "   1874,\n",
       "   307,\n",
       "   264,\n",
       "   732,\n",
       "   13985,\n",
       "   885,\n",
       "   1588,\n",
       "   7186,\n",
       "   770,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0663644654410226,\n",
       "  'compression_ratio': 1.7366255144032923,\n",
       "  'no_speech_prob': 0.031958211213350296},\n",
       " {'id': 156,\n",
       "  'seek': 125400,\n",
       "  'start': 1269.0,\n",
       "  'end': 1273.0,\n",
       "  'text': \" And so right here, all we've done is concatenate all of the strings.\",\n",
       "  'tokens': [51114,\n",
       "   400,\n",
       "   370,\n",
       "   558,\n",
       "   510,\n",
       "   11,\n",
       "   439,\n",
       "   321,\n",
       "   600,\n",
       "   1096,\n",
       "   307,\n",
       "   1588,\n",
       "   7186,\n",
       "   473,\n",
       "   439,\n",
       "   295,\n",
       "   264,\n",
       "   13985,\n",
       "   13,\n",
       "   51314],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0663644654410226,\n",
       "  'compression_ratio': 1.7366255144032923,\n",
       "  'no_speech_prob': 0.031958211213350296},\n",
       " {'id': 157,\n",
       "  'seek': 125400,\n",
       "  'start': 1273.0,\n",
       "  'end': 1281.0,\n",
       "  'text': \" So these types of errors will pop up unexpectedly, and it's useful to check what type your data is.\",\n",
       "  'tokens': [51314,\n",
       "   407,\n",
       "   613,\n",
       "   3467,\n",
       "   295,\n",
       "   13603,\n",
       "   486,\n",
       "   1665,\n",
       "   493,\n",
       "   40452,\n",
       "   11,\n",
       "   293,\n",
       "   309,\n",
       "   311,\n",
       "   4420,\n",
       "   281,\n",
       "   1520,\n",
       "   437,\n",
       "   2010,\n",
       "   428,\n",
       "   1412,\n",
       "   307,\n",
       "   13,\n",
       "   51714],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0663644654410226,\n",
       "  'compression_ratio': 1.7366255144032923,\n",
       "  'no_speech_prob': 0.031958211213350296},\n",
       " {'id': 158,\n",
       "  'seek': 128100,\n",
       "  'start': 1282.0,\n",
       "  'end': 1287.0,\n",
       "  'text': \" Now let's go ahead and talk about how we can change the data inside of a data frame in various ways.\",\n",
       "  'tokens': [50414,\n",
       "   823,\n",
       "   718,\n",
       "   311,\n",
       "   352,\n",
       "   2286,\n",
       "   293,\n",
       "   751,\n",
       "   466,\n",
       "   577,\n",
       "   321,\n",
       "   393,\n",
       "   1319,\n",
       "   264,\n",
       "   1412,\n",
       "   1854,\n",
       "   295,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   294,\n",
       "   3683,\n",
       "   2098,\n",
       "   13,\n",
       "   50664],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.059418104943774995,\n",
       "  'compression_ratio': 1.6367924528301887,\n",
       "  'no_speech_prob': 0.009825562126934528},\n",
       " {'id': 159,\n",
       "  'seek': 128100,\n",
       "  'start': 1287.0,\n",
       "  'end': 1296.0,\n",
       "  'text': \" In particular, we'll talk about adding new columns, renaming the index labels or column names, and altering the data inside of the data frame.\",\n",
       "  'tokens': [50664,\n",
       "   682,\n",
       "   1729,\n",
       "   11,\n",
       "   321,\n",
       "   603,\n",
       "   751,\n",
       "   466,\n",
       "   5127,\n",
       "   777,\n",
       "   13766,\n",
       "   11,\n",
       "   8124,\n",
       "   5184,\n",
       "   264,\n",
       "   8186,\n",
       "   16949,\n",
       "   420,\n",
       "   7738,\n",
       "   5288,\n",
       "   11,\n",
       "   293,\n",
       "   11337,\n",
       "   278,\n",
       "   264,\n",
       "   1412,\n",
       "   1854,\n",
       "   295,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.059418104943774995,\n",
       "  'compression_ratio': 1.6367924528301887,\n",
       "  'no_speech_prob': 0.009825562126934528},\n",
       " {'id': 160,\n",
       "  'seek': 128100,\n",
       "  'start': 1296.0,\n",
       "  'end': 1302.0,\n",
       "  'text': \" Some of these will be discussed at length later in the class, but we'll just give a brief introduction.\",\n",
       "  'tokens': [51114,\n",
       "   2188,\n",
       "   295,\n",
       "   613,\n",
       "   486,\n",
       "   312,\n",
       "   7152,\n",
       "   412,\n",
       "   4641,\n",
       "   1780,\n",
       "   294,\n",
       "   264,\n",
       "   1508,\n",
       "   11,\n",
       "   457,\n",
       "   321,\n",
       "   603,\n",
       "   445,\n",
       "   976,\n",
       "   257,\n",
       "   5353,\n",
       "   9339,\n",
       "   13,\n",
       "   51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.059418104943774995,\n",
       "  'compression_ratio': 1.6367924528301887,\n",
       "  'no_speech_prob': 0.009825562126934528},\n",
       " {'id': 161,\n",
       "  'seek': 130200,\n",
       "  'start': 1302.0,\n",
       "  'end': 1315.0,\n",
       "  'text': \" So to create a new column, you can take the data frame, we'll often abbreviate our data frames as DF, and we can give it a new name and strings, and assign it to the values.\",\n",
       "  'tokens': [50364,\n",
       "   407,\n",
       "   281,\n",
       "   1884,\n",
       "   257,\n",
       "   777,\n",
       "   7738,\n",
       "   11,\n",
       "   291,\n",
       "   393,\n",
       "   747,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   11,\n",
       "   321,\n",
       "   603,\n",
       "   2049,\n",
       "   35839,\n",
       "   473,\n",
       "   527,\n",
       "   1412,\n",
       "   12083,\n",
       "   382,\n",
       "   48336,\n",
       "   11,\n",
       "   293,\n",
       "   321,\n",
       "   393,\n",
       "   976,\n",
       "   309,\n",
       "   257,\n",
       "   777,\n",
       "   1315,\n",
       "   293,\n",
       "   13985,\n",
       "   11,\n",
       "   293,\n",
       "   6269,\n",
       "   309,\n",
       "   281,\n",
       "   264,\n",
       "   4190,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07108812706143249,\n",
       "  'compression_ratio': 1.7532467532467533,\n",
       "  'no_speech_prob': 0.007111303973942995},\n",
       " {'id': 162,\n",
       "  'seek': 130200,\n",
       "  'start': 1315.0,\n",
       "  'end': 1317.0,\n",
       "  'text': \" So let's see how we could do that.\",\n",
       "  'tokens': [51014, 407, 718, 311, 536, 577, 321, 727, 360, 300, 13, 51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07108812706143249,\n",
       "  'compression_ratio': 1.7532467532467533,\n",
       "  'no_speech_prob': 0.007111303973942995},\n",
       " {'id': 163,\n",
       "  'seek': 130200,\n",
       "  'start': 1317.0,\n",
       "  'end': 1331.0,\n",
       "  'text': ' So we could create unemployment region, a new column called unweighted mean, and we could do that by adding the Northeast, Midwest, South, and West unemployment regions, and then dividing by four.',\n",
       "  'tokens': [51114,\n",
       "   407,\n",
       "   321,\n",
       "   727,\n",
       "   1884,\n",
       "   17438,\n",
       "   4458,\n",
       "   11,\n",
       "   257,\n",
       "   777,\n",
       "   7738,\n",
       "   1219,\n",
       "   517,\n",
       "   12329,\n",
       "   292,\n",
       "   914,\n",
       "   11,\n",
       "   293,\n",
       "   321,\n",
       "   727,\n",
       "   360,\n",
       "   300,\n",
       "   538,\n",
       "   5127,\n",
       "   264,\n",
       "   42150,\n",
       "   11,\n",
       "   33483,\n",
       "   11,\n",
       "   4242,\n",
       "   11,\n",
       "   293,\n",
       "   4055,\n",
       "   17438,\n",
       "   10682,\n",
       "   11,\n",
       "   293,\n",
       "   550,\n",
       "   26764,\n",
       "   538,\n",
       "   1451,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.07108812706143249,\n",
       "  'compression_ratio': 1.7532467532467533,\n",
       "  'no_speech_prob': 0.007111303973942995},\n",
       " {'id': 164,\n",
       "  'seek': 133100,\n",
       "  'start': 1332.0,\n",
       "  'end': 1343.0,\n",
       "  'text': ' And notice, if we do that, we now have the national mean, which is going to be population weighted, and the unweighted mean, which is what we just performed right here.',\n",
       "  'tokens': [50414,\n",
       "   400,\n",
       "   3449,\n",
       "   11,\n",
       "   498,\n",
       "   321,\n",
       "   360,\n",
       "   300,\n",
       "   11,\n",
       "   321,\n",
       "   586,\n",
       "   362,\n",
       "   264,\n",
       "   4048,\n",
       "   914,\n",
       "   11,\n",
       "   597,\n",
       "   307,\n",
       "   516,\n",
       "   281,\n",
       "   312,\n",
       "   4415,\n",
       "   32807,\n",
       "   11,\n",
       "   293,\n",
       "   264,\n",
       "   517,\n",
       "   12329,\n",
       "   292,\n",
       "   914,\n",
       "   11,\n",
       "   597,\n",
       "   307,\n",
       "   437,\n",
       "   321,\n",
       "   445,\n",
       "   10332,\n",
       "   558,\n",
       "   510,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0686046991003565,\n",
       "  'compression_ratio': 1.5931372549019607,\n",
       "  'no_speech_prob': 0.007564261555671692},\n",
       " {'id': 165,\n",
       "  'seek': 133100,\n",
       "  'start': 1343.0,\n",
       "  'end': 1346.0,\n",
       "  'text': ' You can also change the values inside of a data frame.',\n",
       "  'tokens': [50964,\n",
       "   509,\n",
       "   393,\n",
       "   611,\n",
       "   1319,\n",
       "   264,\n",
       "   4190,\n",
       "   1854,\n",
       "   295,\n",
       "   257,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0686046991003565,\n",
       "  'compression_ratio': 1.5931372549019607,\n",
       "  'no_speech_prob': 0.007564261555671692},\n",
       " {'id': 166,\n",
       "  'seek': 133100,\n",
       "  'start': 1346.0,\n",
       "  'end': 1353.0,\n",
       "  'text': \" Again, you should do this very sparingly because you don't want to change the data that you're using.\",\n",
       "  'tokens': [51114,\n",
       "   3764,\n",
       "   11,\n",
       "   291,\n",
       "   820,\n",
       "   360,\n",
       "   341,\n",
       "   588,\n",
       "   637,\n",
       "   1921,\n",
       "   356,\n",
       "   570,\n",
       "   291,\n",
       "   500,\n",
       "   380,\n",
       "   528,\n",
       "   281,\n",
       "   1319,\n",
       "   264,\n",
       "   1412,\n",
       "   300,\n",
       "   291,\n",
       "   434,\n",
       "   1228,\n",
       "   13,\n",
       "   51464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.0686046991003565,\n",
       "  'compression_ratio': 1.5931372549019607,\n",
       "  'no_speech_prob': 0.007564261555671692},\n",
       " {'id': 167,\n",
       "  'seek': 135300,\n",
       "  'start': 1354.0,\n",
       "  'end': 1363.0,\n",
       "  'text': \" So in this case, we're going to take the 1995 unweighted mean value, and we're going to assign it to zero.\",\n",
       "  'tokens': [50414,\n",
       "   407,\n",
       "   294,\n",
       "   341,\n",
       "   1389,\n",
       "   11,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   747,\n",
       "   264,\n",
       "   22601,\n",
       "   517,\n",
       "   12329,\n",
       "   292,\n",
       "   914,\n",
       "   2158,\n",
       "   11,\n",
       "   293,\n",
       "   321,\n",
       "   434,\n",
       "   516,\n",
       "   281,\n",
       "   6269,\n",
       "   309,\n",
       "   281,\n",
       "   4018,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10463353670560396,\n",
       "  'compression_ratio': 1.5633802816901408,\n",
       "  'no_speech_prob': 0.043282948434352875},\n",
       " {'id': 168,\n",
       "  'seek': 135300,\n",
       "  'start': 1363.0,\n",
       "  'end': 1371.0,\n",
       "  'text': ' And so now, in the year 1995, the unweighted mean has been reset to the value zero.',\n",
       "  'tokens': [50864,\n",
       "   400,\n",
       "   370,\n",
       "   586,\n",
       "   11,\n",
       "   294,\n",
       "   264,\n",
       "   1064,\n",
       "   22601,\n",
       "   11,\n",
       "   264,\n",
       "   517,\n",
       "   12329,\n",
       "   292,\n",
       "   914,\n",
       "   575,\n",
       "   668,\n",
       "   14322,\n",
       "   281,\n",
       "   264,\n",
       "   2158,\n",
       "   4018,\n",
       "   13,\n",
       "   51264],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10463353670560396,\n",
       "  'compression_ratio': 1.5633802816901408,\n",
       "  'no_speech_prob': 0.043282948434352875},\n",
       " {'id': 169,\n",
       "  'seek': 135300,\n",
       "  'start': 1371.0,\n",
       "  'end': 1374.0,\n",
       "  'text': ' And we can also rename columns.',\n",
       "  'tokens': [51264, 400, 321, 393, 611, 36741, 13766, 13, 51414],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10463353670560396,\n",
       "  'compression_ratio': 1.5633802816901408,\n",
       "  'no_speech_prob': 0.043282948434352875},\n",
       " {'id': 170,\n",
       "  'seek': 137400,\n",
       "  'start': 1374.0,\n",
       "  'end': 1386.0,\n",
       "  'text': \" One of my favorite examples that's entirely frustrating is the Bureau of Labor Statistics here in the United States, names their regional unemployment rate according to the following protocol.\",\n",
       "  'tokens': [50364,\n",
       "   1485,\n",
       "   295,\n",
       "   452,\n",
       "   2954,\n",
       "   5110,\n",
       "   300,\n",
       "   311,\n",
       "   7696,\n",
       "   16522,\n",
       "   307,\n",
       "   264,\n",
       "   19738,\n",
       "   295,\n",
       "   17250,\n",
       "   49226,\n",
       "   510,\n",
       "   294,\n",
       "   264,\n",
       "   2824,\n",
       "   3040,\n",
       "   11,\n",
       "   5288,\n",
       "   641,\n",
       "   10964,\n",
       "   17438,\n",
       "   3314,\n",
       "   4650,\n",
       "   281,\n",
       "   264,\n",
       "   3480,\n",
       "   10336,\n",
       "   13,\n",
       "   50964],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11383875211079915,\n",
       "  'compression_ratio': 1.3617021276595744,\n",
       "  'no_speech_prob': 0.1813802868127823},\n",
       " {'id': 171,\n",
       "  'seek': 138600,\n",
       "  'start': 1386.0,\n",
       "  'end': 1395.0,\n",
       "  'text': ' LASRD 9100000003.dot.dot.',\n",
       "  'tokens': [50364,\n",
       "   441,\n",
       "   3160,\n",
       "   49,\n",
       "   35,\n",
       "   1722,\n",
       "   6879,\n",
       "   628,\n",
       "   1360,\n",
       "   18,\n",
       "   13,\n",
       "   43494,\n",
       "   13,\n",
       "   43494,\n",
       "   13,\n",
       "   50814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.12812433960617228,\n",
       "  'compression_ratio': 1.5672268907563025,\n",
       "  'no_speech_prob': 0.3793981969356537},\n",
       " {'id': 172,\n",
       "  'seek': 138600,\n",
       "  'start': 1395.0,\n",
       "  'end': 1399.0,\n",
       "  'text': \" And you're supposed to know that that's the unemployment rate for the Northeast.\",\n",
       "  'tokens': [50814,\n",
       "   400,\n",
       "   291,\n",
       "   434,\n",
       "   3442,\n",
       "   281,\n",
       "   458,\n",
       "   300,\n",
       "   300,\n",
       "   311,\n",
       "   264,\n",
       "   17438,\n",
       "   3314,\n",
       "   337,\n",
       "   264,\n",
       "   42150,\n",
       "   13,\n",
       "   51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.12812433960617228,\n",
       "  'compression_ratio': 1.5672268907563025,\n",
       "  'no_speech_prob': 0.3793981969356537},\n",
       " {'id': 173,\n",
       "  'seek': 138600,\n",
       "  'start': 1399.0,\n",
       "  'end': 1408.0,\n",
       "  'text': \" So they have reasons internally for doing this because they have so many variables, but it makes our job really difficult because we'll need to type it repeatedly.\",\n",
       "  'tokens': [51014,\n",
       "   407,\n",
       "   436,\n",
       "   362,\n",
       "   4112,\n",
       "   19501,\n",
       "   337,\n",
       "   884,\n",
       "   341,\n",
       "   570,\n",
       "   436,\n",
       "   362,\n",
       "   370,\n",
       "   867,\n",
       "   9102,\n",
       "   11,\n",
       "   457,\n",
       "   309,\n",
       "   1669,\n",
       "   527,\n",
       "   1691,\n",
       "   534,\n",
       "   2252,\n",
       "   570,\n",
       "   321,\n",
       "   603,\n",
       "   643,\n",
       "   281,\n",
       "   2010,\n",
       "   309,\n",
       "   18227,\n",
       "   13,\n",
       "   51464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.12812433960617228,\n",
       "  'compression_ratio': 1.5672268907563025,\n",
       "  'no_speech_prob': 0.3793981969356537},\n",
       " {'id': 174,\n",
       "  'seek': 138600,\n",
       "  'start': 1408.0,\n",
       "  'end': 1415.0,\n",
       "  'text': \" And so what we'll do is we'll often rename these columns by passing a dictionary to the rename method.\",\n",
       "  'tokens': [51464,\n",
       "   400,\n",
       "   370,\n",
       "   437,\n",
       "   321,\n",
       "   603,\n",
       "   360,\n",
       "   307,\n",
       "   321,\n",
       "   603,\n",
       "   2049,\n",
       "   36741,\n",
       "   613,\n",
       "   13766,\n",
       "   538,\n",
       "   8437,\n",
       "   257,\n",
       "   25890,\n",
       "   281,\n",
       "   264,\n",
       "   36741,\n",
       "   3170,\n",
       "   13,\n",
       "   51814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.12812433960617228,\n",
       "  'compression_ratio': 1.5672268907563025,\n",
       "  'no_speech_prob': 0.3793981969356537},\n",
       " {'id': 175,\n",
       "  'seek': 141500,\n",
       "  'start': 1415.0,\n",
       "  'end': 1426.0,\n",
       "  'text': ' And what you do is you take a dictionary and you take the value that it currently is, in this case, Northeast, Midwest, South, and West, and that will be the key of your dictionary.',\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   437,\n",
       "   291,\n",
       "   360,\n",
       "   307,\n",
       "   291,\n",
       "   747,\n",
       "   257,\n",
       "   25890,\n",
       "   293,\n",
       "   291,\n",
       "   747,\n",
       "   264,\n",
       "   2158,\n",
       "   300,\n",
       "   309,\n",
       "   4362,\n",
       "   307,\n",
       "   11,\n",
       "   294,\n",
       "   341,\n",
       "   1389,\n",
       "   11,\n",
       "   42150,\n",
       "   11,\n",
       "   33483,\n",
       "   11,\n",
       "   4242,\n",
       "   11,\n",
       "   293,\n",
       "   4055,\n",
       "   11,\n",
       "   293,\n",
       "   300,\n",
       "   486,\n",
       "   312,\n",
       "   264,\n",
       "   2141,\n",
       "   295,\n",
       "   428,\n",
       "   25890,\n",
       "   13,\n",
       "   50914],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11080916555304277,\n",
       "  'compression_ratio': 1.763819095477387,\n",
       "  'no_speech_prob': 0.003693335223942995},\n",
       " {'id': 176,\n",
       "  'seek': 141500,\n",
       "  'start': 1426.0,\n",
       "  'end': 1429.0,\n",
       "  'text': ' And the value of your dictionary will be the new name.',\n",
       "  'tokens': [50914,\n",
       "   400,\n",
       "   264,\n",
       "   2158,\n",
       "   295,\n",
       "   428,\n",
       "   25890,\n",
       "   486,\n",
       "   312,\n",
       "   264,\n",
       "   777,\n",
       "   1315,\n",
       "   13,\n",
       "   51064],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11080916555304277,\n",
       "  'compression_ratio': 1.763819095477387,\n",
       "  'no_speech_prob': 0.003693335223942995},\n",
       " {'id': 177,\n",
       "  'seek': 141500,\n",
       "  'start': 1429.0,\n",
       "  'end': 1435.0,\n",
       "  'text': ' So in this case, we have NEMWS and W.',\n",
       "  'tokens': [51064,\n",
       "   407,\n",
       "   294,\n",
       "   341,\n",
       "   1389,\n",
       "   11,\n",
       "   321,\n",
       "   362,\n",
       "   426,\n",
       "   6683,\n",
       "   12508,\n",
       "   293,\n",
       "   343,\n",
       "   13,\n",
       "   51364],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11080916555304277,\n",
       "  'compression_ratio': 1.763819095477387,\n",
       "  'no_speech_prob': 0.003693335223942995},\n",
       " {'id': 178,\n",
       "  'seek': 141500,\n",
       "  'start': 1435.0,\n",
       "  'end': 1440.0,\n",
       "  'text': ' And so if we look at our data frame, it looks like things have been renamed.',\n",
       "  'tokens': [51364,\n",
       "   400,\n",
       "   370,\n",
       "   498,\n",
       "   321,\n",
       "   574,\n",
       "   412,\n",
       "   527,\n",
       "   1412,\n",
       "   3920,\n",
       "   11,\n",
       "   309,\n",
       "   1542,\n",
       "   411,\n",
       "   721,\n",
       "   362,\n",
       "   668,\n",
       "   40949,\n",
       "   13,\n",
       "   51614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.11080916555304277,\n",
       "  'compression_ratio': 1.763819095477387,\n",
       "  'no_speech_prob': 0.003693335223942995},\n",
       " {'id': 179,\n",
       "  'seek': 144000,\n",
       "  'start': 1440.0,\n",
       "  'end': 1445.0,\n",
       "  'text': ' Well, maybe not. So what happened here?',\n",
       "  'tokens': [50364, 1042, 11, 1310, 406, 13, 407, 437, 2011, 510, 30, 50614],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10557937622070312,\n",
       "  'compression_ratio': 1.5906735751295338,\n",
       "  'no_speech_prob': 0.02234898880124092},\n",
       " {'id': 180,\n",
       "  'seek': 144000,\n",
       "  'start': 1445.0,\n",
       "  'end': 1450.0,\n",
       "  'text': \" Why doesn't the data frame show that why does the data frame still show the old column names?\",\n",
       "  'tokens': [50614,\n",
       "   1545,\n",
       "   1177,\n",
       "   380,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   855,\n",
       "   300,\n",
       "   983,\n",
       "   775,\n",
       "   264,\n",
       "   1412,\n",
       "   3920,\n",
       "   920,\n",
       "   855,\n",
       "   264,\n",
       "   1331,\n",
       "   7738,\n",
       "   5288,\n",
       "   30,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10557937622070312,\n",
       "  'compression_ratio': 1.5906735751295338,\n",
       "  'no_speech_prob': 0.02234898880124092},\n",
       " {'id': 181,\n",
       "  'seek': 144000,\n",
       "  'start': 1450.0,\n",
       "  'end': 1455.0,\n",
       "  'text': ' The reason is most panda operations are going to create a copy of your data.',\n",
       "  'tokens': [50864,\n",
       "   440,\n",
       "   1778,\n",
       "   307,\n",
       "   881,\n",
       "   46685,\n",
       "   7705,\n",
       "   366,\n",
       "   516,\n",
       "   281,\n",
       "   1884,\n",
       "   257,\n",
       "   5055,\n",
       "   295,\n",
       "   428,\n",
       "   1412,\n",
       "   13,\n",
       "   51114],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10557937622070312,\n",
       "  'compression_ratio': 1.5906735751295338,\n",
       "  'no_speech_prob': 0.02234898880124092},\n",
       " {'id': 182,\n",
       "  'seek': 144000,\n",
       "  'start': 1455.0,\n",
       "  'end': 1464.0,\n",
       "  'text': ' And to specify that you meant to overwrite your data frame, you have to use the in place option.',\n",
       "  'tokens': [51114,\n",
       "   400,\n",
       "   281,\n",
       "   16500,\n",
       "   300,\n",
       "   291,\n",
       "   4140,\n",
       "   281,\n",
       "   670,\n",
       "   21561,\n",
       "   428,\n",
       "   1412,\n",
       "   3920,\n",
       "   11,\n",
       "   291,\n",
       "   362,\n",
       "   281,\n",
       "   764,\n",
       "   264,\n",
       "   294,\n",
       "   1081,\n",
       "   3614,\n",
       "   13,\n",
       "   51564],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10557937622070312,\n",
       "  'compression_ratio': 1.5906735751295338,\n",
       "  'no_speech_prob': 0.02234898880124092},\n",
       " {'id': 183,\n",
       "  'seek': 146400,\n",
       "  'start': 1464.0,\n",
       "  'end': 1473.0,\n",
       "  'text': \" We recommend that you avoid using this too much until you understand exactly what you're changing because you don't want to overwrite your data.\",\n",
       "  'tokens': [50364,\n",
       "   492,\n",
       "   2748,\n",
       "   300,\n",
       "   291,\n",
       "   5042,\n",
       "   1228,\n",
       "   341,\n",
       "   886,\n",
       "   709,\n",
       "   1826,\n",
       "   291,\n",
       "   1223,\n",
       "   2293,\n",
       "   437,\n",
       "   291,\n",
       "   434,\n",
       "   4473,\n",
       "   570,\n",
       "   291,\n",
       "   500,\n",
       "   380,\n",
       "   528,\n",
       "   281,\n",
       "   670,\n",
       "   21561,\n",
       "   428,\n",
       "   1412,\n",
       "   13,\n",
       "   50814],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06396867116292318,\n",
       "  'compression_ratio': 1.595,\n",
       "  'no_speech_prob': 0.04113099351525307},\n",
       " {'id': 184,\n",
       "  'seek': 146400,\n",
       "  'start': 1473.0,\n",
       "  'end': 1480.0,\n",
       "  'text': \" And we'll often just create the new data frame and assign it back to something of the same name.\",\n",
       "  'tokens': [50814,\n",
       "   400,\n",
       "   321,\n",
       "   603,\n",
       "   2049,\n",
       "   445,\n",
       "   1884,\n",
       "   264,\n",
       "   777,\n",
       "   1412,\n",
       "   3920,\n",
       "   293,\n",
       "   6269,\n",
       "   309,\n",
       "   646,\n",
       "   281,\n",
       "   746,\n",
       "   295,\n",
       "   264,\n",
       "   912,\n",
       "   1315,\n",
       "   13,\n",
       "   51164],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06396867116292318,\n",
       "  'compression_ratio': 1.595,\n",
       "  'no_speech_prob': 0.04113099351525307},\n",
       " {'id': 185,\n",
       "  'seek': 146400,\n",
       "  'start': 1480.0,\n",
       "  'end': 1486.0,\n",
       "  'text': \" So in this case, we've renamed our columns and this creates a new data frame.\",\n",
       "  'tokens': [51164,\n",
       "   407,\n",
       "   294,\n",
       "   341,\n",
       "   1389,\n",
       "   11,\n",
       "   321,\n",
       "   600,\n",
       "   40949,\n",
       "   527,\n",
       "   13766,\n",
       "   293,\n",
       "   341,\n",
       "   7829,\n",
       "   257,\n",
       "   777,\n",
       "   1412,\n",
       "   3920,\n",
       "   13,\n",
       "   51464],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.06396867116292318,\n",
       "  'compression_ratio': 1.595,\n",
       "  'no_speech_prob': 0.04113099351525307},\n",
       " {'id': 186,\n",
       "  'seek': 148600,\n",
       "  'start': 1486.0,\n",
       "  'end': 1496.0,\n",
       "  'text': \" And then we've assigned it to unemployment short name and in the short name data frame, we now have the renamed columns.\",\n",
       "  'tokens': [50364,\n",
       "   400,\n",
       "   550,\n",
       "   321,\n",
       "   600,\n",
       "   13279,\n",
       "   309,\n",
       "   281,\n",
       "   17438,\n",
       "   2099,\n",
       "   1315,\n",
       "   293,\n",
       "   294,\n",
       "   264,\n",
       "   2099,\n",
       "   1315,\n",
       "   1412,\n",
       "   3920,\n",
       "   11,\n",
       "   321,\n",
       "   586,\n",
       "   362,\n",
       "   264,\n",
       "   40949,\n",
       "   13766,\n",
       "   13,\n",
       "   50864],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10442165027965199,\n",
       "  'compression_ratio': 1.4391891891891893,\n",
       "  'no_speech_prob': 0.019664930179715157},\n",
       " {'id': 187,\n",
       "  'seek': 148600,\n",
       "  'start': 1496.0,\n",
       "  'end': 1499.0,\n",
       "  'text': \" And that's all we have for this lecture.\",\n",
       "  'tokens': [50864, 400, 300, 311, 439, 321, 362, 337, 341, 7991, 13, 51014],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10442165027965199,\n",
       "  'compression_ratio': 1.4391891891891893,\n",
       "  'no_speech_prob': 0.019664930179715157},\n",
       " {'id': 188,\n",
       "  'seek': 148600,\n",
       "  'start': 1499.0,\n",
       "  'end': 1503.0,\n",
       "  'text': \" So we'll see you soon to talk about panda's basics.\",\n",
       "  'tokens': [51014,\n",
       "   407,\n",
       "   321,\n",
       "   603,\n",
       "   536,\n",
       "   291,\n",
       "   2321,\n",
       "   281,\n",
       "   751,\n",
       "   466,\n",
       "   46685,\n",
       "   311,\n",
       "   14688,\n",
       "   13,\n",
       "   51214],\n",
       "  'temperature': 0.0,\n",
       "  'avg_logprob': -0.10442165027965199,\n",
       "  'compression_ratio': 1.4391891891891893,\n",
       "  'no_speech_prob': 0.019664930179715157}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"segments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73dfe34-907d-40b5-8221-ce0b3ef6ce85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
