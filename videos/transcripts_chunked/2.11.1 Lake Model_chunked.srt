1
00:00:00.000 --> 00:00:46.000
So we're very excited for this lecture in which we're going to estimate a Markov chain using some of the employment data that we've talked about. And we'll try to be specific about what we mean by estimate, but some of these are details that we'll learn later on in the class. Okay, so let's start with a short digression. We have a friend named Jim Savage who's a statistician, a good one actually. And he has thought about the right way to do modern statistical work. And we think that many of the things that he's learned in thinking about this apply to economics as well.

2
00:00:38.000 --> 00:01:23.000
And we think that many of the things that he's learned in thinking about this apply to economics as well. So let's start by talking about what he envisions as a modern statistical workflow. So he's kind of created nine steps, which seems long, but they're pretty short. So step one would be to prepare and visualize your data. So this involves things like data cleaning or making some plots. We did lots of this in a previous lecture with the BLS data and looked at how employment has changed in response to the COVID recession.

3
00:01:13.000 --> 00:01:55.000
We did lots of this in a previous lecture with the BLS data and looked at how employment has changed in response to the COVID recession. Step two is to create a generative model. And what do we mean by generative model is all we mean is some probability distribution. Over outcomes. That are a function of some model parameters and he thinks it's very important that the first model created should be as simple as possible. And the next step is something that lots of people don't necessarily think to do because it's not necessarily natural.

4
00:01:47.000 --> 00:02:35.000
And the next step is something that lots of people don't necessarily think to do because it's not necessarily natural. But one of the first things you should do is take the model that you've written down. And you should now actually generate some data. So we generate a bunch of observations from our model, given a particular parameter that we've picked out of a hat. And then step four is let's see if we can back out what theta looks like. So we'll call theta hat. The parameter is that we get from our model sitting and see how this compares to theta.

5
00:02:24.000 --> 00:03:01.000
The parameter is that we get from our model sitting and see how this compares to theta. And you do that because what you want to know is in step five. You should check that you understand and that you're able to, you know, if you have certain parameters for your model that your fitting procedure is able to recover those parameters when you know them. Because if it does, if you can't get the parameters that you know. It's very unlikely that you'll be able to find the parameters that you don't know.

6
00:02:56.000 --> 00:03:45.000
It's very unlikely that you'll be able to find the parameters that you don't know. And he emphasizes that you possibly repeat steps three through five. So two through five with different methods and parameters to get an understanding, oh, sorry, to get an understanding of how your model works and what fitting procedure is seem to be successful. After this, so we've now worked with some fake data. We understand the fitting procedure and how the model works. Now, we move on to our real data. So we take the same procedure and do the same fitting procedures on the data that we observed in step one.

7
00:03:34.000 --> 00:04:15.000
So we take the same procedure and do the same fitting procedures on the data that we observed in step one. And then step seven and this makes me laugh. So he says, argue about the results with your friends and colleagues. So Jim's not a particularly. Hard guy to get along with. He's actually very friendly. And so his choice of the word argue. I think tells you a little bit about who you should be talking to about this. You don't want someone that's going to just tell you that it's nice work. You want someone that's going to be able to ask you questions and say, why did you do that?

8
00:04:10.000 --> 00:04:46.000
You want someone that's going to be able to ask you questions and say, why did you do that? Well, what if you did this instead? So you want someone that's going to push back a little bit. Step eight is then returned to step two and work with a slightly richer model and we repeat steps three, four, five, six and seven. And then once you've done this with a sufficiently rich model, you should think about what decisions are going to be made from this analysis. Figure out what your loss function will be, kind of what are the costs of getting this right or wrong.

9
00:04:39.000 --> 00:05:25.000
Figure out what your loss function will be, kind of what are the costs of getting this right or wrong. And then you should perform make decisions based on statistical decision analysis. And so later in the semester, we'll talk more formally about what we mean by fitting your model and the work that goes along with it. But right now, anytime we say fit, you should just imagine some type of a procedure that allows us to take a generative model that depends on parameter's data. And data and try to get a guess at what those data parameters are if we could not see them.

10
00:05:15.000 --> 00:06:05.000
And data and try to get a guess at what those data parameters are if we could not see them. And so we're going to go ahead and do some of these steps on the labor data to see if we can work through this process. So what's our plan for this lecture? So we're going to actually skip steps one. We've already mostly done step one. We created some graphs. We cleaned the data. So we're going to take that as given. And then the plan for the remainder of this class and this probably will go into our next class is to work through some of the remaining steps of a modern statistical workflow.

11
00:05:51.000 --> 00:06:32.000
And then the plan for the remainder of this class and this probably will go into our next class is to work through some of the remaining steps of a modern statistical workflow. And we're going to develop a generative model of employment and unemployment. We're going to use that model to simulate some fake data from our generative model. We're then going to fit that model using simulated data and we'll tell you explicitly how we're going to do that fitting. Then we're going to explore some options that we might have had for having for how we could choose to fit the data.

12
00:06:24.000 --> 00:07:02.000
Then we're going to explore some options that we might have had for having for how we could choose to fit the data. So we're going to fit the model with the BLS data. And finally we're going to examine what our model implies for the effects of COVID on employment and unemployment. So again, we won't get through all of these today and it's an ambitious lecture, but I think we're going to have a lot of fun along the way. Okay, so let's develop a generative model. So Jim's recommendation is that the first model created should be as simple as possible.

13
00:06:55.000 --> 00:07:43.000
So Jim's recommendation is that the first model created should be as simple as possible. So in the spirit of as simple as possible, we're going to return to the model that we talked about in class when we talked about mark-off chains. So we're going to consider a single individual. This individual is going to move between two states, employment and unemployment. When an individual is unemployed, they're going to find a new job with probability alpha. So we'll call this the job finding rate. And while an individual is employed, they're going to lose their job with probability beta.

14
00:07:36.000 --> 00:08:26.000
And while an individual is employed, they're going to lose their job with probability beta. So we'll call this the job separation rate. And that's it. That's our model. So our model is entirely defined by two parameters and alpha, which is the job finding rate and a beta, which is the job separation rate. Okay, well, we now have a generative model. So the next step is for us to actually simulate data from this generative model. And we're going to do this in two steps. So first, we're going to take an individual's state of employment or unemployment.

15
00:08:16.000 --> 00:08:59.000
And we're going to do this in two steps. So first, we're going to take an individual's state of employment or unemployment. And the transition probabilities, which are our parameters. And then we're going to draw from tomorrow's state. And determine whether that person will be employed or unemployed. Next, once we have what we're going to call the one step transition, we're going to specify, we're going to create a function that takes an initial state and the parameters. And that can then simulate the entire history of employment or unemployment using our one step function.

16
00:08:51.000 --> 00:09:37.000
And that can then simulate the entire history of employment or unemployment using our one step function. So let's start doing this. So how can we simulate the one step function? So we're going to specify again, we highly encourage everyone to win their writing code, follow good coding habits and write documentation. So our function called the next state is going to take three arguments. It's going to take an ST, which is an individual's current state. And we're going to specify ST equals zero is going to map to unemployment. And ST equals one is going to be employment.

17
00:09:28.000 --> 00:10:25.000
And we're going to specify ST equals zero is going to map to unemployment. And ST equals one is going to be employment. There's going to be the alpha, which is the job finding probability. And there's going to be the beta, which is the job separation. And so what it's going to return, this is actually, I should have written this documentation previously, but it's going to return an ST plus one, which is also an integer. And it's going to be the individuals employment state and period T plus one. Okay, so what are we going to do? The first thing is there's going to be some random randomness in whether an individual transitions from one state to another.

18
00:10:14.000 --> 00:11:03.000
Okay, so what are we going to do? The first thing is there's going to be some random randomness in whether an individual transitions from one state to another. So we're going to draw our random number up front just that we don't have to do it multiple times. Then, and so this random number is going to be drawn from from a uniform zero one. So this is coming from uniform zero one. So if an individual is unemployed, so this is ST equals zero, then we want to know whether they become unemployed or employed.

19
00:10:48.000 --> 00:11:42.000
So if an individual is unemployed, so this is ST equals zero, then we want to know whether they become unemployed or employed. Well, they find a job with probability alpha. So there's lots of ways that we could potentially do this, but the easiest is what's another event that has probability alpha. Well, if X is distributed uniform zero one, then the probability that X is less than alpha is equal to alpha. And so what we do is we say if an individual is unemployed and our random number is less than alpha. So this is our probability alpha.

20
00:11:31.000 --> 00:12:21.000
And so what we do is we say if an individual is unemployed and our random number is less than alpha. So this is our probability alpha. Then they transition from unemployment to employment. Similarly, if an individual is employed, so this is ST equals one, then they could either become unemployed or employed. And they do this with probability beta, they become unemployed. And so we check whether the number was less than beta. And if so, then they become unemployed. So if they're state, state is the same, it doesn't change. So this means either U.T. was greater than alpha or U.T. was greater than beta.

21
00:12:11.000 --> 00:13:13.000
So if they're state, state is the same, it doesn't change. So this means either U.T. was greater than alpha or U.T. was greater than beta. Or you gave us a state that was incorrect. So we also could have added a check at the front to make sure that S sub T was less than or equal to one. But we're going to be the ones using this code, so it's probably okay. So one thing we wanted to point out was that this function actually depends on the markoff property, which we've previously discussed. In case you don't remember, the markoff property states that the probability of ST plus one given ST, so the probability of a state tomorrow given the state today, is the same as the probability of ST plus one given ST ST minus one dot dot S zero.

22
00:12:53.000 --> 00:13:38.000
In case you don't remember, the markoff property states that the probability of ST plus one given ST, so the probability of a state tomorrow given the state today, is the same as the probability of ST plus one given ST ST minus one dot dot S zero. And what this means is other than the transition probabilities, we only need to give the function the previous state and not an entire history. And so that's why in this function we can specify that ST is just an integer, just a single integer. And again, we've started with the simplest model we could have, and so this is just purely by assumption.

23
00:13:30.000 --> 00:14:23.000
And again, we've started with the simplest model we could have, and so this is just purely by assumption. Okay, well, now we've written some code and did we run the code, I'm not sure, so just double check. And whenever you write a function that will be used kind of frequently, oh, you forgot code from even earlier. There we go, so we forgot to import our packages. Let's just read that. Okay, perfect. So once you've written a function that's going to be used somewhat frequently and just in general if you write a function, it's a good idea to create some simple test cases.

24
00:14:08.000 --> 00:14:44.000
Let's just read that. Okay, perfect. So once you've written a function that's going to be used somewhat frequently and just in general if you write a function, it's a good idea to create some simple test cases. So what we're doing here is we have an individual who's starting in state zero and they're transitioning to unemployment with about probability one half. So we should expect to see about half ones and half zeros and, you know, modular randomness that seems to be the case. Okay, so what other checks can we do?

25
00:14:41.000 --> 00:15:27.000
Okay, so what other checks can we do? If we set the probability of going from unemployed to employed, so if we set the alpha to zero, then the individual should stay unemployed. Well, that works. Likewise, if we set the job separation probability to zero, then an individual who is currently employed should stay employed. And likewise, we can set the job finding probability to one and then an individual who is unemployed should always find a job. That works. And we can set the job separation probability to one and check whether an individual who is currently employed will become unemployed next time.

26
00:15:18.000 --> 00:16:03.000
And we can set the job separation probability to one and check whether an individual who is currently employed will become unemployed next time. And even if we ran all of these things, because we've set the probability to one, they're not going to change. Okay, great. So now we can simulate a single step. And so all we have to do is now be able to simulate an entire history. We do want to advise you, eventually we're going to allow alpha and beta to change over time. So we want you to think of them as constant for now, but we're going to write this code in a way that allows them to fluctuate period by period.

27
00:15:55.000 --> 00:16:30.000
So we want you to think of them as constant for now, but we're going to write this code in a way that allows them to fluctuate period by period. And the way we'll do that, notice you can see this in the documentation is alpha and beta. Our both numpy of rays and they're going to have type of float. And each element is going to be a probability that an individual finds a job or loses a job. And then the other thing we're going to specify is the initial state of unemployment or employment that an individual is in.

28
00:16:23.000 --> 00:17:02.000
And then the other thing we're going to specify is the initial state of unemployment or employment that an individual is in. So in order to be able to take our first step, we need to know whether someone was employed or unemployed. So we're going to do some test checking first. So we're going to make sure that alpha and beta were the same length. And we're going to call their length T. And then we're going to make room for a history of employment or unemployment. We're going to set the first value to our initial state.

29
00:16:56.000 --> 00:17:41.000
We're going to set the first value to our initial state. And then we're going to do T steps. So how do we do that is we compute the next state by giving it the previous state. And what the transition probabilities for that period are, which are alpha, T and beta, T. And then we're going to save the new value back into S naught. And this is just to kind of keep a minimum number of variables here. So then we'll save that value into our array. And then we'll return our history of employment or unemployment.

30
00:17:35.000 --> 00:18:24.000
And then we'll return our history of employment or unemployment. Okay, well, let's check the output of this function. So we're going to set the job finding rate at 25, 0.25, and the job separation at 0.025. And let's see what happens. Well, an individual who is employed is likely to instead employ it for a long time. So we see it takes a long time to lose a job because you're only losing it with probability 0.025. Well, if we run it again, now let's start an individual as unemployed and see what happens.

31
00:18:13.000 --> 00:19:03.000
Well, if we run it again, now let's start an individual as unemployed and see what happens. And it doesn't take very long for them to find a job. And that's because the probability of finding a job is 0.25. And so you should kind of expect this to take two, five, six. This person, I guess, was really unlucky. And then continue to be unlucky because they eventually became unemployed. But you get the idea and the output of this function seems to be sensible. So we'll call that a success. Great. So we've now completed step two, which is defined a generative model.

32
00:18:58.000 --> 00:19:39.000
So we've now completed step two, which is defined a generative model. And step three, which is simulate data from your generative model. And now we're going to come to step four, which is going to be Picture model to fake data. There's a lot of different procedures that we could have chosen to map our data into the implied parameters. So remember, fitting our model is just going to be a procedure in which we uncover Unknown parameters of our model. The way we're going to do this for our mark-off chain is just by counting the frequencies of transitions.

33
00:19:31.000 --> 00:20:17.000
The way we're going to do this for our mark-off chain is just by counting the frequencies of transitions. So let's start by thinking about the general case. So consider an end state mark-off chain. The parameters of an end state mark-off chain are going to be the elements of the transition matrix, cat bottle P. So what do each of these P's stand for? So remember that P sub one one stands for the probability that an individual in state one is in state one next period. P one two is the probability that an individual in state one is in state two next period.

34
00:20:09.000 --> 00:21:00.000
P one two is the probability that an individual in state one is in state two next period. That that that. So let why not, why one to y t be a sequence of generations that are generated from our end state mark-off chain. Then the proposed fitting procedure we're going to use is going to assign the following value to Pij. We're going to sum up over all of the T. So let's have a sample. So again, we're going to use a two state mark-off chain at first. Okay, so we have seven. That's enough. Okay, so it's going to sum up over our seven values.

35
00:20:56.000 --> 00:22:04.000
Okay, so it's going to sum up over our seven values. And the one in bold like this is a function. And this is, why, equals S. It's called the indicator function. And it takes the value one if the condition below is true and zero otherwise. So let's go ahead and compute P zero zero according to this formula. Okay, so we're going to look for places where that subti observation is equal to zero. So this one is equal to zero. This one is equal to zero. And this one is equal to zero. And so now we want to know, so this is what going to be one times is YT plus one equal to

36
00:21:54.000 --> 00:22:46.000
And so now we want to know, so this is what going to be one times is YT plus one equal to also zero. Nope. So that could say zero. These will all be zero's just because they're not zero's. And then we're now going to do zero. So that's good so one. And T plus one is also equal to zero. So that gets a one. So this is, and then the last one is going to transition from zero to one. So it will get a zero. And if we sum up over all of the places that YT was equal to zero, we get one plus one plus one.

37
00:22:40.000 --> 00:23:32.000
we get one plus one plus one. So this comes out to one over three. And so we think there's a lot of potential intuition in understanding this. And we believe this procedure is intuitive. And if it's not necessarily intuitive, I think the right place to start is to compute the sum across the Pijs for given I, where these Pijs are defined by this procedure. And you should think about what value do you get? And why do you think you get that value? You might already see it. But if you don't, this is kind of a, we find something,

38
00:23:28.000 --> 00:24:16.000
But if you don't, this is kind of a, we find something, things like this to be useful exercises and understanding how the fitting procedure's work. Okay, so let's write some code that can count frequencies. So our function is going to compute the transition probabilities for a two state mark-off chain. So we're going to specialize. The input is going to be a history with the state values of the two state mark-off chain. It's going to return two values and alpha and a beta. Where the alpha is going to be the probability of transitioning from state zero to state one.

39
00:24:10.000 --> 00:24:59.000
Where the alpha is going to be the probability of transitioning from state zero to state one. And the beta is from state one to state zero. So how do we do this? Well, the first thing is let's check what the length of the history is. So how many observations did we see total? And then we're going to create this little IDX so the stands for index. And it's just going to be a counter. So this will take the values zero, one, two, two, T. And now what we're going to do is let's find all of the places where

40
00:24:49.000 --> 00:25:36.000
And now what we're going to do is let's find all of the places where the history was equal to zero. So this is going to give us some truths and falses. It's right here based on whether that element of the history was zero or not. And we want to make sure we're not looking at the last step because we won't be able to see what comes after it. And we do the same things for the ones. And so what does our counting procedure look like here? Well, let's go ahead and sum up all of the, So this is kind of clever.

41
00:25:32.000 --> 00:26:35.000
So this is kind of clever. So let's talk through it. So zero, one, one, zero, zero. So we've created zero IDX's true and true. Zero, one, two, three, four, five, six, seven. Okay, so let's talk through what each of these things are. So IDX is these numbers from zero to T. History equal equal zero is going to give us true, false, true, false, false true, true. And that and IDX less than T minus one is going to give us true, true, true, true, true, true, false. And so then if we index with those truths and falsees,

42
00:26:30.000 --> 00:27:30.000
And so then if we index with those truths and falsees, we're going to get the numbers zero, two and six. Because these are values in which the history is equal to zero and are not the very last value in our observation. Okay, now we're going to sum up the history zero IDX is plus one. So the IDX is so now we're moving them from zero to one, two to three and six to seven. So that's going to be this value, this value, and this value. And we're going to sum them up and so we get two and we're going to divide it by the length of these indexes, which was three.

43
00:27:20.000 --> 00:27:59.000
And we're going to sum them up and so we get two and we're going to divide it by the length of these indexes, which was three. And we see that's because we moved from the state zero to one, two times and from zero to zero, one time. And likewise we do the same thing to compute the beta, except now we're looking at the indexes that are ones. And so we want to know when they were zeroes. And so to get that we do one minus, because if we do one minus zero, it will be equal to one, so they will each count as one.

44
00:27:51.000 --> 00:28:37.000
And so to get that we do one minus, because if we do one minus zero, it will be equal to one, so they will each count as one. And when it's one, it will be one minus one, so it will get set to zero. So if you didn't quite follow that, you should take a minute and break out each of these pieces of the function. And think about what's happening. And I often, when I write code, I break my code into as natural steps of possible. So in this case, I'm thinking about this is kind of pre analysis. In this step, I'm finding out where the zeroes and the ones are.

45
00:28:32.000 --> 00:29:17.000
In this step, I'm finding out where the zeroes and the ones are. And then this step, I'm using where the zeroes and the ones are to figure out what the corresponding counts should be. Okay. So now what we're going to do is we're going to use this count frequencies individual to check the accuracy of our fit. Let's see if we know what our parameters are. Can we uncover them if we pretend that we don't know them? So we're going to use this function and it's just going to be a length of simulation. And some parameters for alpha and beta.

46
00:29:14.000 --> 00:29:49.000
And some parameters for alpha and beta. It's going to create the arrays that we pass into our simulation. We'll then simulate to create an employment history and we'll always start the individual as unemployed in this case. So you can think about this as someone who's graduated from college and is initially unemployed. We're then going to count the frequencies and we'll call these alpha hat and beta hat. And then we're going to print out what the true alpha was and what the fitted value was. Okay.

47
00:29:48.000 --> 00:30:31.000
Okay. So let's check the accuracy. Let's simulate for 10,000 periods with the same probabilities that we've been using. And what we find is we get rather accurate results. So you'll find that all of the numbers we've had so far have been within a percentage or so. So that's good news. So it means that if we have enough data, our fitting procedure is recovering the true parameter. Well, let's now think about what the interpretation of this is. So we're viewing this as monthly transitions in employment.

48
00:30:26.000 --> 00:31:10.000
So we're viewing this as monthly transitions in employment. So if we observe 10,000 months of employment history for someone, that's a very long time. We're unlikely to have that much data on anyone. And so that's just not going to be what our data looks like. So what happens if we have less data? So maybe we have a lifetime of employment transition. So this is someone who works for 45 years. For 12 months a year with the same parameters. And you'll immediately notice that before all of our values were in the 0.24, 0.26.

49
00:31:04.000 --> 00:31:47.000
And you'll immediately notice that before all of our values were in the 0.24, 0.26. I think we had a 0.27. And now we're at 0.15 when the true value is 0.25. I wouldn't call that a disaster, but you're certainly not as accurate. We could get a 0.3. And you see it just really depends on what the particular history is. This one's particularly inaccurate. It just depends on what sequence of draws you get. And you can't quite tell. But again, it's unlikely that we have an entire lifetime of employment transitions for very many people.

50
00:31:41.000 --> 00:32:23.000
But again, it's unlikely that we have an entire lifetime of employment transitions for very many people. And you might not even want to do that because you think their structural changes and how employment has worked. And so what if we just look at two years? What's going to happen? Well, it's not terrible. We're getting the job separation rate very wrong. If we re-run this, so this person over two years. So they became employed. And once they were employed, they never became unemployed again. And what you see is our fitting procedure gets very noisy.

51
00:32:17.000 --> 00:33:06.000
And what you see is our fitting procedure gets very noisy. So what could we do to deal with this? How could we fix this? So the BLS doesn't actually base their transition probabilities off of a single individual. Instead, they're using an entire cross section of individuals. So can we do the same thing if we use a cross section rather than a single individual's history? The answer is yes. But in order for our frequency counting frequency counting to work, what we need is we need job finding and job separation to be independent across individuals.

52
00:32:58.000 --> 00:33:40.000
what we need is we need job finding and job separation to be independent across individuals. So in our previous case, all of the work was being done by the Markov property. And we simply assumed that the transition from one state to the next today was independent of the transition from one state to the next tomorrow, conditioning on what the current state was. So what happens? So we could this break. So again, so formally what we're saying for independence is that the joint distribution over SIT plus one and SJT plus one,

53
00:33:30.000 --> 00:34:22.000
So again, so formally what we're saying for independence is that the joint distribution over SIT plus one and SJT plus one, where I and J are individuals in our cross section, conditional on their previous states and the parameters. Independence is going to be that joint distribution is equal to the product of their marginal distributions. So what's the probability of SIT plus one given SIT and the parameters? Times the probability of SJT plus one given SJT. So what could violate this? Well, we thought of three examples.

54
00:34:19.000 --> 00:35:02.000
Well, we thought of three examples. I'm sure there's lots of others that someone could think of. But one example might be there's a changing government policy for one year. That results in a job guarantee. Well, then if two individuals are unemployed, then finding a job is not necessarily an independent event. You could have a technological change that results in the destruction of an entire industry. And if that happens, then there's going to be lots of correlated job loss. So everyone who worked in that industry is likely to lose their job.

55
00:34:56.000 --> 00:35:31.000
So everyone who worked in that industry is likely to lose their job. And if temporarily, you may also see a recession. And this recession could cause increased firing across the country. As a spoiler alert, some of these problems, some of these are going to be problems that are active in the data. And this is actually why we're going to allow for the fact for alpha and beta to move each period. Because we think, at least, roughly, the transition from employment to unemployment or unemployment to employment

56
00:35:23.000 --> 00:36:06.000
Because we think, at least, roughly, the transition from employment to unemployment or unemployment to employment is close to independent on a period by period basis. And I think you could show examples of where that's not true, but we're just going to assume in our model for now, that it's true. So how could we simulate a cross-section? So we're going to give it, give our function, an alpha and beta. And if these are going to be the same function of same arrays that they were before, then we're going to give it an S0,

57
00:35:57.000 --> 00:36:36.000
And if these are going to be the same function of same arrays that they were before, then we're going to give it an S0, but we're going to change what S0 is and what the interpretation is. S0 is now going to be an array with two elements that represent the fraction of the population that begins in each employment state. And then we're going to specify a number of individuals that will be in our cross-section. And the output is going to be an N by T matrix that contains individual histories of employment along each row.

58
00:36:27.000 --> 00:37:23.000
And the output is going to be an N by T matrix that contains individual histories of employment along each row. So what we're going to see is we're going to get the history. So we'll call this person 0 and period 0, then we're going to see person 0 in period 1, all the way until person 0 at period T, where T is again going to be the length of alpha and beta. Then there's going to be person 1. We're going to observe them in period 0. We're going to observe them in period 1, all the way up to T. And we're going to store this in a big matrix.

59
00:37:15.000 --> 00:38:06.000
And we're going to store this in a big matrix. Okay, so how do we do this? We check sizes again. We're going to check to make sure that you gave us two fractions that add up to 1. And then we're going to figure out based on the number of individuals that the fraction of individuals that should be employed. How many should start as an employed? And so the first NZ individuals will all start as an employed. We're going to allocate room for us to store these employment histories. And we're going to set all of the individuals who start employed.

60
00:38:01.000 --> 00:38:38.000
And we're going to set all of the individuals who start employed. We're going to set their states to 1. And then this turns out to be pretty easy because we've already done the work. Where now we can simulate the employment history and we've already explored this function. It just simulates a single individual's history. And we're going to give them the initial state that they'll start in. And we're just going to store those values into the array that we've allocated. And then we're going to return it.

61
00:38:35.000 --> 00:39:26.000
And then we're going to return it. So let's see what happens when we simulate 10 individuals for two periods. And we specified that roughly 35% of the individuals should be starting as unemployed. So we've rounded down and we got three. And so two of these three found jobs. One did not. And of the seven that were employed, all of them stayed employed. So this seems plausible. So this is just with one transition. We could increase the number of transitions. And so now we have 0, 1, 2, 3. So that function seems to be working.

62
00:39:21.000 --> 00:40:00.000
So that function seems to be working. Just as a matter of practice and kind of keeping with our data. If we when we import real data into Python, we're typically going to import this data into a data frame. So let's go ahead and keep our simulated data in a data frame as well. So we're going to allow to give it a data frame that's going to have the alpha and beta. So the job finding and separation probabilities. We're going to specify again the S naught. It's the fraction of the population that begins in each population.

63
00:39:57.000 --> 00:40:58.000
It's the fraction of the population that begins in each population. Stay in each employment state. And we're going to have an N, which is the number of individuals. In our cross section. And so all we're going to do is we'll make sure that our alpha's and beta's are ordered by their date. We'll extract the alpha and beta arrays. We'll simulate our entire cross section. And we'll dump the output into a data frame. Okay. So let's see whether this function works. So we're going to simulate. Let's initially just do six months.

64
00:40:53.000 --> 00:41:44.000
Let's initially just do six months. And we're going to have alpha equal to the same thing. It's been forever. Data are going to start in January 2018. And we're going to go forward six months. And let's see what comes out. So we have a single person. Oh, we just are looking at the head. So let's look at 12. So we're going to see this person, person zero. We're going to observe their employment history for six months. So this person took five months of being unemployed. And in their six months, they found a job.

65
00:41:40.000 --> 00:42:31.000
And in their six months, they found a job. Person one was unemployed for two months. And then found and held a job for the remainder of their simulated history. Obviously, we can increase how many simulations we do. And it continues to work. So this is good news. Okay. So just to keep it interesting, let's actually pretend that we are the BLS. And so the BLS has their hands tied. And are actually not able to observe in individuals entire employment history. So what we're going to do is we're going to take our full employment

66
00:42:27.000 --> 00:43:13.000
So what we're going to do is we're going to take our full employment history and we're going to restrict it. And we're going to pretend to ask the individuals from our generative model, the CPS questions. So if you remember, so if we have January, February, March, April, May, June, July, August, September, October, November, December. So if an individual, and we'll call this year one and year two, if an individual begins interviewing with the CPS and February of year one, they're interviewed in February, March, April, and May.

67
00:43:06.000 --> 00:43:54.000
they're interviewed in February, March, April, and May. And then they are left out of the survey for eight months. And they're interviewed again in February, March, April, and May. And then they're no longer interviewed. Okay. So how could we do that? So what we're going to do is this is going to take a single individual, and then we're going to simulate an employment history, and it's going to interview that individual. So the inputs are going to be a data frame that have the columns person ID, so who are we talking to?

68
00:43:51.000 --> 00:44:39.000
so who are we talking to? DT, which is in what month are we talking to them and employment? And then we're going to have start year and start month as when the interviews occur. And so now the CPS is going to be a version of this data frame, but it's only going to contain the observations that would correspond to the CPS schedule for someone who starts interviewing in start year, start month. So the way we're going to do this is we're going to create some date ranges, and we're going to talk about these when we talk about pandas, dates,

69
00:44:32.000 --> 00:45:21.000
and we're going to talk about these when we talk about pandas, dates, and how to work with them. And we're just going to put these together. So we're going to start the four months of observations in our first year, and then we're going to start four months of observations in our second year. And then we're only going to keep data that's in those two sets of dates. And so how do we interview someone? Is we're going to pass this function, so we're going to do CPS interviews, we're going to give it some data frame,

70
00:45:18.000 --> 00:46:08.000
we're going to give it some data frame, and then we're going to randomly sample from our year. Let's go ahead and make this an X. And then we're going to group by the person IDs, and so we're going to randomly choose a year and an integer, and then we're going to group by the person IDs, and we're going to interview that person and get a subset of the data, and then we're just going to drop some of the extra indexes that show up. So this will take a minute or so to run. Okay, so now that we've let this finish running,

71
00:46:03.000 --> 00:47:05.000
Okay, so now that we've let this finish running, let's go ahead and see what this data looks like. So let's revert, let's revert, let me room to see about 25 observations. So individual zero was interviewed in October of 2019. I see, so this person, we stopped interviewing them in 2020, because we haven't seen those dates yet. Okay, so, yeah, because we started interviewing in 2018, zero one zero one, and we simulated two years of history. So because we interviewed this person in October of 2019,

72
00:47:00.000 --> 00:47:52.000
So because we interviewed this person in October of 2019, we stopped all of our interviews in January of 2020, so this person's employment history was cut short. But this person started in April of 2018. So let's go ahead and look at what happened to them. So person one was interviewed in April, May June, July of 2018, and then April, May June, July in 2019. So this looks, this looks accurate. Let's go ahead and see how many individuals are we observing per month. So if I remember, I believe we interviewed 5,000 individuals overall.

73
00:47:41.000 --> 00:48:39.000
So if I remember, I believe we interviewed 5,000 individuals overall. So we're going to interview 5,000 individuals over the course of two years. And so what you see is, as we start interviewing people, there's not very much data, because this is when all of our employment history starts. And we're going to add about 200 people per month. And because of that, by the time we get to April of 2019, we're adding 200 people a month, which means that we have 200 people times four months, because you could happen in any of four months that you could start in any of four months

74
00:48:32.000 --> 00:49:15.000
because you could happen in any of four months that you could start in any of four months and be interviewed in April. So if you started being interviewed in January, then you were still being interviewed in April, same with February, March, and April. And so that gives us 800, and there's about another 800 that come from individuals who started being interviewed in the four months from last year. So once our interview is started, we expect to see about 1,600 people per interview, and that seems to line up with what we're seeing.

75
00:49:10.000 --> 00:50:00.000
and that seems to line up with what we're seeing. So all of this is good. So now what we're going to do is we're going to go ahead and fit to the cross section. So our data looks exactly like what the BLS uses. So how can we modify our frequency of transition concept to account for the fact that we're now observing cross sectional data? So the parameters of the Markov chain are still going to be the elements of our transition matrix P. But now our data is going to be, remember, Y0, 0, Y0, 1, Y0, 2, Y0,

76
00:49:51.000 --> 00:50:48.000
But now our data is going to be, remember, Y0, 0, Y0, 1, Y0, 2, Y0, Y10, Y11, Y12, and that's what we get from here. So Y10, Y11, dot dot dot. So the way that we're going to do this is we're simply going to add an additional sum. So if we look at this equation here, we're doing the exact same sum as before. But now we're going to add a sub m to stand for individual. And we're going to check whether that individual experience state i during period t and state j in t plus 1. And then we'll do the same thing for the bottom.

77
00:50:42.000 --> 00:51:34.000
And then we'll do the same thing for the bottom. So not much new there. So how could we do this? There's some clever pieces of this function. So I'm going to walk through it. But again, I'd invite you to come back and think carefully about each step of this function. So our input is now just going to be a sample of individuals from our CPS survey. So this is going to have columns dt, pid, and employment. And the output of this function is going to be alpha and beta, which is the job finding and job separation rates.

78
00:51:23.000 --> 00:52:13.000
And the output of this function is going to be alpha and beta, which is the job finding and job separation rates. So the way that we're going to do this is the first thing we'll do is we'll put date and person ID on the index. Then we're going to extract the date values. So this is just going to give us all of the date values. And we're going to do dot shift 1 by month. And so what is this going to do? Oh, don't do that. What is this going to do? If this is going to transform 2020, 01, that's 01 to 01.

79
00:52:06.000 --> 00:53:10.000
If this is going to transform 2020, 01, that's 01 to 01. So we're simply going to move things forward one month. And you'll see why we do that in a second. Next thing we do is just extract all of the associated person IDs. So these are two vectors that are the same shape, same height as the original data. Then we're going to create another index that has the shifted dates and the original people, person IDs. And now we're going to reindex the original data by using this new index that we've created.

80
00:52:59.000 --> 00:54:13.000
And now we're going to reindex the original data by using this new index that we've created. And so then we're going to reassign the column employment to unemployment T plus 1. So what this is going to give us is we're going to have DT and PID and employment, which will eventually be named Employment T plus 1. And it's going to associate. So we'll have 2020, 01, 01, 02, 01, person 0, and their employment value in February of 2020. And what's going to happen is we won't have a value for January. But this will be the same size as our original data frame.

81
00:54:06.000 --> 00:54:59.000
But this will be the same size as our original data frame. So now we're going to reset the index on both data frames. So DT and PID will become columns. And then we're just going to concat them horizontally. So we're going to set access equals 1 and we're going to keep DT and PID and employment from the original data frame. And we're only going to keep employment T plus 1 from the new data frame from the data T plus 1. And then we're going to drop any missing values because these are associated with when individual stopped being interviewed.

82
00:54:50.000 --> 00:55:42.000
And then we're going to drop any missing values because these are associated with when individual stopped being interviewed. And then we're going to make sure things are integers again. And so what we're going to have is we're going to have data frame that has DT, PID, employment and employment T plus 1. And this is going to have a date, a person ID and an employment status. So let's say that the individual was unemployed and that in T plus 1 they were employed. And then we're going to see the next month's date.

83
00:55:37.000 --> 00:56:31.000
And then we're going to see the next month's date. So this is going to be February 1. Now we're going to have the same person ID. Except their new employment must line up with what the T plus 1 set. So that's what all of this code is going to do. And then this could be in 1, which would mean that there would be a 1 here until we get, we had a missing value. But we would have dropped it that row. So that's roughly what all of this code does. Once we have the T's and the T plus 1's, we can just do exactly the same thing we did with the other frequency counting function, where we find all of the zero states.

84
00:56:19.000 --> 00:57:24.000
Once we have the T's and the T plus 1's, we can just do exactly the same thing we did with the other frequency counting function, where we find all of the zero states. And we take the mean across just the one values, or the mean of 1 minus the zero values. So we're also going to write another function to check for the accuracy of our cross sectional work. So what happens when we have 1,000 individuals? We observe them for up to two years. And these are our alpha and beta. Well, there we go. So that looks pretty accurate. This will again take another second.

85
00:57:12.000 --> 00:58:06.000
Well, there we go. So that looks pretty accurate. This will again take another second. So that's not so bad. It's not as good as when we had 10,000 observations, but it's pretty good. And when we do it with 500 observations, we see some success. We're going to see that this is less successful than with a thousand, but it performs relatively well. And by the time we get down to 100, our data starts being pretty noisy again. If you think about this, the BLS is interviewing approximately 60,000 individuals per month.

86
00:57:58.000 --> 00:58:42.000
If you think about this, the BLS is interviewing approximately 60,000 individuals per month. And so if this generative model is correct, we should expect that what they're doing is pretty accurate that they're uncovering the right transition probabilities. So I think this is good news in terms of what we've learned for our generative model and our fitting process. And so the last thing we're going to do is actually we're going to go ahead and take our model and we're going to fit it with real CPS data.

87
00:58:32.000 --> 00:59:24.000
And so the last thing we're going to do is actually we're going to go ahead and take our model and we're going to fit it with real CPS data. So we've downloaded and cleaned for you. You're welcome. A subset of CPS data for the years 2018 and 2019. So let's see what our constant parameter model does with this data. So let's go ahead and load our data. We'll take a look. Notice this looks very similar to the data we've been generating. We have a date time. We have a person ID, but they create their CPS creates their person ID slightly differently in that the ID is generated by using the first for the first four digits is generated by the year that their interviews begin.

88
00:59:06.000 --> 00:59:44.000
We have a date time. We have a person ID, but they create their CPS creates their person ID slightly differently in that the ID is generated by using the first for the first four digits is generated by the year that their interviews begin. And the next two digits are generated by the month in which they are first interviewed. So their person IDs are more meaningful than our numbers zero one two, etc. So let's find some employment histories. So what we're going to do is we're going to group by the person ID.

89
00:59:36.000 --> 01:00:18.000
So let's find some employment histories. So what we're going to do is we're going to group by the person ID. And we're going to count how many times a DT shows up and we're going to sum up how many periods they were employed for it. And then we're going to sort by these counts and look at the bottom. So we're going to just grab one of these identifiers and we can see an individual's employment history. So this person started being interviewed in June 2018. They were employed and they continued to be employed for all four months in which they were observed for 2018.

90
01:00:11.000 --> 01:00:54.000
They were employed and they continued to be employed for all four months in which they were observed for 2018. And then when we return in 2019, this individual is still employed and continues employed throughout the remainder of their history. So let's try and find someone that's not employed for the entire time. So let's search for IDs in which we have eight observations for date time. But the sum of the employment states is less than eight. So previously I picked a this ID out of a hat. It looks like there's plenty of others that we could have chosen.

91
01:00:46.000 --> 01:01:28.000
So previously I picked a this ID out of a hat. It looks like there's plenty of others that we could have chosen. And so what we see is this person was wasn't employed in 2018 in July was when they began their interviews. And they were employed for all four months that we observed them in 2018. But fast forward to 2019. We now see that this person is employed in July, but they lose their job from July to August. And during September and October, they continued as an unemployed individual. Let's see if there's someone who finds a job.

92
01:01:24.000 --> 01:02:05.000
Let's see if there's someone who finds a job. Oh yeah, so here's someone they have an unfortunate history I guess. So this person was employed for all of their 2018 interviews. They were employed for their first 2019 interview. Became unemployed. Then became employed again. And then became unemployed again. So I don't know what it happened. But there we go. And so now since we've done all of the work and we've our data is in our clean format. All we have to do is apply our CPS count frequencies function and it's going to return to us two numbers.

93
01:01:58.000 --> 01:02:41.000
All we have to do is apply our CPS count frequencies function and it's going to return to us two numbers. It's going to return an alpha and a beta. And the alpha is going to be the job finding probability. So notice in 2018 and 2019 these were relatively good economic times. And so there was lots of job finding. So the probability of finding a job if you were unemployed was about 0.37. And the probability of losing your job was only about 0.01. And so that kind of wraps up what we're going to do today.

94
01:02:36.000 --> 01:03:15.000
And so that kind of wraps up what we're going to do today. We're going to continue this lecture. So recall one of the goals that we have with this lecture is to kind of build a model, fit it. And then do some kind of forecasting or prediction where we start thinking about what could happen in the future. And to do that we're going to talk about a model in which these transition probabilities, the job finding in job loss rates can fluctuate over time. And we're going to see what comes out of that model next time. So talk soon.

