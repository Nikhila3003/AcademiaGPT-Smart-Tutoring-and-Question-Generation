1
00:00:00.000 --> 00:00:39.000
The final topic we'll cover today is on Data Merchink. So prior to this lecture, you should have heard us talk about reshaping, and if you were here 10 minutes ago, you heard all about it. After this lecture, you should be able to understand the different pandas routines for combining data sets. No win and how to apply concat merge and join. We'll be using two data sets during today's lecture. We'll use data on the GDP components, population, and square miles of various countries. And we'll use data from a website called Goodreads that allows you to rate different books.

2
00:00:33.000 --> 00:01:10.000
And we'll use data from a website called Goodreads that allows you to rate different books. And so there will be over 6 million ratings in our data set. Let's get started. So we often want to perform analysis on data that originates from different sources. For example, if you were trying to analyze data on regional sales for a company, you might want to include things like industry aggregates or demographic information from each reason, or if you have product level data, maybe it's split into product information

3
00:01:05.000 --> 00:01:50.000
reason, or if you have product level data, maybe it's split into product information in one data set and data about sales and other things in another data set. And you often need to take these data sets and put them together to do the type of analysis that you'd like. So we're going to be using, like I said, data on GDP and some other variables. So we're going to read this in from the World Development Index. So what we see is we have WETI data from 2017. And we have government expenditures, consumption, exports, imports, and GDP.

4
00:01:41.000 --> 00:02:38.000
And we have government expenditures, consumption, exports, imports, and GDP. And all of this is measured in trillions of US dollars. We'll also get a version of this data set with 2016 and 2017 data. The next data set we're going to read in is data on the land area of different countries. So the United States covers 3.8 square millions of square miles. Canada also 3.8, Germany 0.13, and Russia 6.6. This is our second data set. And our third data set is going to be data on the population measured in millions of people

5
00:02:32.000 --> 00:03:13.000
And our third data set is going to be data on the population measured in millions of people for each country. And each one of these data sets is going to be data on GDP. So given the data sets we've just introduced, we have data on GDP, we have data on land area, and we have data on population. Suppose we were just compute as to compute a number of statistics. Suppose we were asked to compute a measure of land usage. So what is consumption per square mile? Or what is the GDP per capita for each country in each year?

6
00:03:08.000 --> 00:03:47.000
Or what is the GDP per capita for each country in each year? About consumption per person. And what is the population density of each country and how does it change over time? So the answer to each of these questions is going to require us to use data from multiple data frames. And so the three methods as we've kind of hinted at that will be learning about our concat, merge, and join. We'll look at each one independently. And we'll start with concat. The concat function is used to stack data frames, and it's particularly useful when you

7
00:03:41.000 --> 00:04:15.000
The concat function is used to stack data frames, and it's particularly useful when you have something like a year of data that stored in one month files. And you need them to go on top of one another. That should actually remind you of the word string concat nation where you literally just tie it together. So there's nothing creative or interesting going on. It's literally just taking to rate data frames and wishing them together. With as with many other pandas functions, we're going to have some options about how to

8
00:04:11.000 --> 00:04:51.000
With as with many other pandas functions, we're going to have some options about how to do that. So we can choose whether we're stacking on by rows, which would set be setting access equal to zero, or by columns, which would be setting access equals to one. So we'll look at each case separately, and we'll start by looking at rows. So let's consider two of our data sets. So we have the WDI 2017, which remember is just a data set. Notice the country is on the index. The columns are these different classifications of government of the expenditure report.

9
00:04:44.000 --> 00:05:25.000
The columns are these different classifications of government of the expenditure report. And we have square miles, which has country on the index as well. And so if we stack these at top of another with access equals zero, what you'll see is that literally it has just stacked these two data frames on top of one another. They didn't have any of the same columns. And so anywhere that didn't have a shared column is going to get a man. So square miles only had data on square miles, and not on government expenditures or

10
00:05:20.000 --> 00:05:54.000
So square miles only had data on square miles, and not on government expenditures or anything else. And this government expenditures consumption, the WDI data frame, didn't have information on square miles. So we can kind of see what it's doing. But I think this tells us that this kind of wasn't the right operation to do in this case. So the number of rows is the total number of rows and all inputs. And the labels are going to come from the original data frames. They're each going to be distinct.

11
00:05:52.000 --> 00:06:44.000
They're each going to be distinct. And for columns that appeared only in one input, the value for all labels originating from the different input is set to missing. So maybe what we would prefer to do is concat with access equals one. And so what you can see is it's actually matched up. It notices, so let's look at our two data frames. It notices that there's data for Canada in both data sets. And so when it combines these, notice it's taken 0.37.1.86. And it's gotten found where Canada was. And so it matches on the access that your concat needing.

12
00:06:38.000 --> 00:07:27.000
And so it matches on the access that your concat needing. Similarly for Germany, it's done this. So it has these columns match and it's correctly found to the right square miles, et cetera. So concat is pretty simple. It's a bit of a brute force way to combine data sets. But there's many cases in which it's the right answer. So now that we know how to do this, we actually can answer one of our questions that we mentioned. So what is the consumption per square mile? So here we go. Well, this is Modulo, the right units.

13
00:07:23.000 --> 00:08:19.000
Well, this is Modulo, the right units. So I guess we would have multiplied this one by 1 trillion. 1,000 million, billion, trillion, trillion. And multiplied this one by 1 million. But let's ignore that for now because I'm going to get something wrong. And let's just assume that it's in the units we want. And now we've got a measure of consumption per square mile. Now what you see is that in terms of square, consumption per square mile, the United Kingdom in Germany look really good. And a lot of that is being driven by the fact that they have such a small land area.

14
00:08:11.000 --> 00:09:00.000
And a lot of that is being driven by the fact that they have such a small land area. We can't answer this question for Russia because we don't have any of their GDP or consumption data. Great. So that covers our concatenance. And now we'll talk about another operation that you'll probably use slightly more, which is merge. Merge operates on two data frames at a time. And it's primarily used to bring columns from one data set into another. And it aligns the data based on one or more key columns. So remember, concat aligned on the axis that you were concatenating on, but you were restricted

15
00:08:52.000 --> 00:09:38.000
So remember, concat aligned on the axis that you were concatenating on, but you were restricted in the sense that it either had to be a column already or on the index. Merge is going to allow us to be a little bit more flexible. So let's just see an example. If we take the two data sets that we just concat it, so these same two data sets, and now we're going to merge them, we actually get something that looks really similar. So what have we done here? We'll talk more about that in a second. So let's do some examples.

16
00:09:35.000 --> 00:10:38.000
So let's do some examples. So we can merge on 2016 and 2017. And square miles. And reason this is interesting is that this data set had each of these countries for two years rather than just one. And I actually, I kind of, so this is learning in real time. So we're going to see what happens when we can cut these. I actually don't know what it'll do. So zero kind of done does what we expect, but it takes the two levels and combines them into one, which is a great. And access equals one kind of gives this absolute garbage.

17
00:10:33.000 --> 00:11:18.000
And access equals one kind of gives this absolute garbage. So we could not have used concat in this example, which I think is what we wanted to demonstrate. I guess the one problem is, so the data is copied over exactly as it is. And the additional column from square miles was added and matched on the country. So what you'll see is we have two observations for Canada that correspond to 2016 and 2017. And they both were given square miles. Unfortunately, we lost the information about here because it was on the index.

18
00:11:13.000 --> 00:12:05.000
Unfortunately, we lost the information about here because it was on the index. And we'll talk about how we could have gotten it back, and how we could have kept it in the merge. So how can we fix this? If we take our data set and we reset the index, so remember we have this, oh, 17. Resetting the index, which we learned about in our reshape, is simply going to take the index and move it back into columns with the data frame. So we now have columns country and the year. So if we specify that we want to match on country and it's a column rather than an index,

19
00:11:58.000 --> 00:12:37.000
So if we specify that we want to match on country and it's a column rather than an index, notice it is kept the column year. And it still has matched up correctly where square miles goes. Additionally, we sometimes need to merge on multiple columns. For example, our population data and our WDI data, both have observations organized by country and year. So to properly merge these data sets, we will need to align the data on both the country and the year. So notice we can pass our list country and year.

20
00:12:32.000 --> 00:13:08.000
So notice we can pass our list country and year. It's going to head and put that on the index for us. If we did not want it on the index, we could have reset the index to take it out of the index and it still kept both country and year. And this puts us in a position where we can now answer one of our other questions. What is the GDP per capita for each country in each year? How about consumption per person? And so now if we look at GDP per capita, it's not so different in all these countries, actually.

21
00:13:01.000 --> 00:13:42.000
And so now if we look at GDP per capita, it's not so different in all these countries, actually. I think there's a difference between 0.54 and 0.42, but it's maybe not so big. And the consumption difference is actually quite a bit different. This is the United States concerns, consumes almost a third more than the other countries in our data set. OK, so let's use our new merge skills that we've learned to answer the final question that we asked originally. What is the population density of each country?

22
00:13:39.000 --> 00:14:20.000
What is the population density of each country? How much does it change over time? And we'll go ahead and give you two or three minutes to answer this question. OK, welcome back. So let's see how I did this problem. There's more than one way to solve this. So I took the two data sets and noticed I took the population data set and I reset the index so that we wouldn't lose any information when we merged the two data sets. We merged it on country and then we created a new column called P-Dens for population density.

23
00:14:11.000 --> 00:14:53.000
We merged it on country and then we created a new column called P-Dens for population density. And then I just did a pivot table to put it in a format that was easy to read. And I put the year on the index, the columns were the countries and the values were the population density. And what you see is that the population density has increased in the United States and in Canada. But it's been roughly constant in Germany and you've seen a slight increase in United Kingdom. OK, so merge takes many different optional arguments.

24
00:14:48.000 --> 00:15:30.000
OK, so merge takes many different optional arguments. So we've talked about some of the most commonly used ones so far. In particular, we've talked about the argument on and this just specifies what columns you're trying to align the two data sets on. And the default is actually that it will merge on any of the columns that appear in both data frames. So in this case, we've reset the index on both of the data frames and we've tried emerging it and what it found was it identified on its own that country was a common column and so it merged on that column.

25
00:15:18.000 --> 00:15:58.000
So in this case, we've reset the index on both of the data frames and we've tried emerging it and what it found was it identified on its own that country was a common column and so it merged on that column. In the example above, we could use the on argument because the column name was the same in both data frames. Turns out that sometimes you won't have data frames that have the same name for a column. For example, one data frame might call the identifier product ID while the other calls it ID or product.

26
00:15:50.000 --> 00:16:32.000
For example, one data frame might call the identifier product ID while the other calls it ID or product. And in that case, we can use left on and right on arguments and press the proper column names to align the data. So again, in this case, it's somewhat trivial because they're the same, but we could do left on country, right on country, and it succeeds. Additionally, the data may be on the index instead of on one of the columns in which case you could do some and you can mix all of these so you can do left on country and right index equals true.

27
00:16:17.000 --> 00:16:55.000
Additionally, the data may be on the index instead of on one of the columns in which case you could do some and you can mix all of these so you can do left on country and right index equals true. And that means it will try and merge the values in the column country with the values that are on the square miles index. I should point out, I don't think I've said this yet, that we're using the we're calling the first argument to the merge function, the left data frame and the second argument, the right data frame.

28
00:16:43.000 --> 00:17:19.000
I should point out, I don't think I've said this yet, that we're using the we're calling the first argument to the merge function, the left data frame and the second argument, the right data frame. And this lines up with how people discuss the merges and see what about to see why that is. Great. So the argument we haven't discussed yet that you will typically, that you will find to be one of the most powerful arguments is the how argument. And so there's four possible values for how you do your merging.

29
00:17:14.000 --> 00:17:54.000
And so there's four possible values for how you do your merging. The first is a left merge and what a left merge does is it finds all of the values that are in your left data frame. And it keeps those that you're merging on to and it keeps those as the inputs to the new data frame. Right does the same and so it'll ignore any values that are in the right data frame that are not in the left data frame. Right does the exact opposite where it keeps the values from the right data frame and ignores any in the left data frame that are not in the right data frame.

30
00:17:44.000 --> 00:18:23.000
Right does the exact opposite where it keeps the values from the right data frame and ignores any in the left data frame that are not in the right data frame. And there only thinks about values that are in both data frames and outer thinks about any of the values that can merge on in both in both the left and right data frame. So let's see some some examples. So what we're going to do is we're going to create two new data frames. One is going to have is the going to be the WDI data and we're going to drop the United States and the others will be square miles data.

31
00:18:15.000 --> 00:19:00.000
One is going to have is the going to be the WDI data and we're going to drop the United States and the others will be square miles data. We'll drop Germany. So let's walk through all of the possible options. So we can merge two data sets by going on the left. So if we change this to the no US data set. You'll notice that the United States disappears and that's because we'll see that again. So when we do left it's going to find Canada, Germany, United Kingdom United States. And it's going to find all of those same values inside of the square miles.

32
00:18:54.000 --> 00:19:41.000
And it's going to find all of those same values inside of the square miles. When we do the WDI, no US, it's not going to find the United States in this data set, but it will find it in this data set. And it chooses to leave it out because it's not in the left data frame. When we do this merge with the WDI and square miles, but we use right. Notice Russia has been included because Russia was in the square miles data set, but not in the WDI data set. So it's just going to put a missing anywhere that it doesn't have data for it.

33
00:19:33.000 --> 00:20:36.000
So it's just going to put a missing anywhere that it doesn't have data for it. When we do the inner join with the no US data, notice this one has, does not have the US or Russia. And this one has both and it chooses to do to ignore both of them because they're not in both. If we move back to the original WDI 2017, then the US will be in the WDI, but not in square miles. So let's see this. Oh, WDI 2017. And square miles. So what do we see here? They have Canada, Germany, United States, and United Kingdom.

34
00:20:30.000 --> 00:21:16.000
So what do we see here? They have Canada, Germany, United States, and United Kingdom. This one has Russia, and so when we do enter, it's going to leave Russia out because it's not in both. And finally, when we use the outer argument, it goes ahead and includes all five countries, even though there's no square miles data for Germany, and there's no WDI data for either the US or Russia in the WDI 2017, no US. Okay, so you should come back and we'll make sure to include this in your assignment. But you should study these different how methods.

35
00:21:13.000 --> 00:21:53.000
But you should study these different how methods. It's important to understand kind of what operation you're actually doing when you merge data. And there's a lot of flexibility in what you can do because of these many arguments. The final thing we'll say about this is just that rather than write pd.murge, you could just write a data frame dot merge and use the merge method rather than the function. And it has all the same arguments. It's an equivalent thing to do. Okay, the last merge method we'll talk about is called join.

36
00:21:48.000 --> 00:22:32.000
Okay, the last merge method we'll talk about is called join. And it's very similar to the merge method, but what's going to happen is it only allows you to use the index of the right data frame as the join key. So left dot join right on country is equivalent to calling pd.murge left right left on country right index equals true. The actual implementation of join is going to call merge internally, but it just sets the left on and right index for you. You can do anything with df.join that you can do with df.murge, but sometimes it's more convenient to use if the keys are in the right index.

37
00:22:22.000 --> 00:23:14.000
You can do anything with df.join that you can do with df.murge, but sometimes it's more convenient to use if the keys are in the right index. And we can just do this here. Okay, so what we're going to do now is we're actually going to do put all of the tools we've learned today into practice. And we're going to do a case study using this book data that I described earlier. So on your computers, this will probably take a minute to load the first time. Because it's a relatively large data set. So if we look at ratings, it has approximately six million entries.

38
00:23:03.000 --> 00:23:56.000
Because it's a relatively large data set. So if we look at ratings, it has approximately six million entries. And inside of this rating data, there is a user ID that maps from that maps to different users. There's a book ID that will map two different books. And then what happens is when a user rates a particular book, they give it a rating on a scale of one to five, five being the best. And so you know how a particular user rated a particular book. So let's see what we can do with this. So I think kind of the most obvious first thing to do is to just see how many of each rating there are in our data set.

39
00:23:47.000 --> 00:24:30.000
So I think kind of the most obvious first thing to do is to just see how many of each rating there are in our data set. So what we see is that there's approximately two million five star ratings, a little over two million four star, one and a half three star, half million two stars, and you know maybe a hundred thousand one star books. So the data certainly seems a little bit skewed to higher ratings. The next thing we could probably do is let's see how many users have rated N books. So let's think about why how we're going to do this.

40
00:24:24.000 --> 00:25:20.000
So let's think about why how we're going to do this. So we want to know how many of these different users have rated a certain number of books. So if we count how many times a user ID shows up, that will give us, so let's go ahead and do that. What that's going to do is that's going to say user. Okay, so user 3094 has provided 200 ratings. And so if we want to know how many people have provided 200 ratings, what we're going to do is actually apply value counts again. So that's what we're doing down here.

41
00:25:18.000 --> 00:25:51.000
So that's what we're doing down here. So we're doing dot value counts to figure out how many ratings each user has done. And then we're going to do dot value counts again, which is going to show us how many users provided a certain number of ratings. Then we're going to sort our index, because remember our index are going to be these numbers potentially from one to 200. Or going to move them out of the index. And then we're going to rename them to N ratings and users. And let's see what that gives us.

42
00:25:48.000 --> 00:26:35.000
And let's see what that gives us. So what we see here is one user has provided 19 ratings when user has provided 20 ratings dot dot dot dot. So let's look at some statistics on this data we've created. So we can use the dot describe method, and it will show us that there were 181 separate users in our data set. The mean number of ratings that were given was 100. So it seems people are relatively active. The minimum number of ratings given was 19. And the median number of ratings was also about 109.

43
00:26:30.000 --> 00:27:12.000
And the median number of ratings was also about 109. And the max ratings was 200 of which almost a thousand users gave 200 book ratings. And we can just describe this in our in a box plot. So this is the same information we just saw, but now in a graph. So let's practice using the Want operator. So let's determine whether there's a relationship between the number of ratings a user has written. And the distribution of ratings. One reason you might do this is if you're an author and you're trying to inflate your ratings.

44
00:27:07.000 --> 00:27:46.000
One reason you might do this is if you're an author and you're trying to inflate your ratings. And you wonder whether you should try and get more experience. Could we use this to rate your book or focus on getting first time users to rate your book. This is going to be kind of a funny I like the answer here. So let's apply the Want operator, which means we're going to start at a result and work our way backwards. So we can answer a question if we have two data frames. First we have all ratings by the top end users with the most ratings and all ratings by the end users with the least number of ratings.

45
00:27:36.000 --> 00:28:17.000
First we have all ratings by the top end users with the most ratings and all ratings by the end users with the least number of ratings. Okay, so how are we going to get that? So we need to extract the rows of ratings with user ID associated with the end most and least prolific readers. So to get that we need to find the most and least active user IDs. And so to get that information we can just do what we did a few minutes ago where we count how many times that user shows up in the data set. So we have four steps and the first one is kind of trivia.

46
00:28:12.000 --> 00:28:59.000
So we have four steps and the first one is kind of trivia. So each step is trivial, which is to means we've kind of applied the Want operator correctly. And let's start going backwards. So end ratings if we just value count we now know user 3944 has provided 200 ratings. 52036 has a private at a hundred and ninety nine ratings, et cetera. Excellent. So now we need the end largest and in smallest. So let's go ahead and change this to end. So we're going to look at this series we've created in ratings and the method in largest selects the end largest values.

47
00:28:50.000 --> 00:29:34.000
So we're going to look at this series we've created in ratings and the method in largest selects the end largest values. And we want the indexes associated with that because there is a user IDs and we're going to move it into a list. And let's go ahead and print one of these lists so we know it. So we can look at what it looks like. Excellent. So now we have these 25 users. Okay. So now let's see what ratings were given by these particular users. So we're going to find we're going back to our original data set, the ratings.

48
00:29:28.000 --> 00:30:08.000
So we're going to find we're going back to our original data set, the ratings. So we're going to figure out when these radars are end. So when the user ID is in this list of the most prolific users, the most frequent users of the data set. And we're going to keep all of the columns and then we're going to also do the same thing for the less common users. And maybe we want to look at activating stuff ahead. Okay. So now we have a new data set that's going to give us a user ID, a book ID and a rating.

49
00:30:01.000 --> 00:30:44.000
So now we have a new data set that's going to give us a user ID, a book ID and a rating. But it's only going to include user IDs for the most active or least active users. Okay. Now we can get to our answers. So what we're going to do is we're going to plot the distribution of the most active users. And plot the distribution of ratings for the least active users. And so what do we see? We see that more active users have a median probably in the mean of like three and a half to four. Whereas I think the mean of the active users is closer to four.

50
00:30:39.000 --> 00:31:54.000
Whereas I think the mean of the active users is closer to four. And I guess we could actually tell ourselves what this what that is. So inactive ratings rating. Oh, that's interesting. I was wrong. So you get a higher mean from active users. So this is fun. This is a learning experience. This is why you do things like this. So you get a higher mean from the active users. That happens to the median. Okay, if I have the same median. Interesting. Okay. So you get a higher average rating for more active users.

51
00:31:49.000 --> 00:32:31.000
Okay. So you get a higher average rating for more active users. And the reason is that they assign a very small fraction of their ratings to be one and two. You get a lower standard deviation because they're focusing on these three four and five ratings. For inactive users, you get a higher fraction of five. You get the same median, which is four. But you have a higher standard deviation. So I guess your strategy probably depends on what you think. If you think you're a relatively strong book, then an active user might still rate you as a four.

52
00:32:25.000 --> 00:33:00.000
If you think you're a relatively strong book, then an active user might still rate you as a four. But maybe you have a better chance of getting a five if you appeal to the exact of users. So this isn't the answer I thought we had. So new users are more likely to leave five star ratings. But your average rating is actually higher if you go through the active users. So we learned something that I didn't know. Okay. So we've kind of started learning a little bit about this data set. And you're probably thinking, wasn't this supposed to be emerging?

53
00:32:56.000 --> 00:33:25.000
And you're probably thinking, wasn't this supposed to be emerging? Why are we only using one data set? I heard you. Okay. So what we're going to do is we're going to load up another data set that contains information on books. So we're only going to keep a couple of columns. So we're going to look at the book ID, which you might suspect is because we're going to merge and you'd be right. We're going to look at the authors and we're going to look at the title. So again, this will probably take a minute to run.

54
00:33:21.000 --> 00:34:11.000
So again, this will probably take a minute to run. So now we have a book ID and author and the book title. And so if we want to do interesting things with the book status set, we could. But what we're going to do is we're simply going to merge these two data sets together. So we look at rated books. ahead. And now we have the user ID, the book ID, the rating. I noticed it's given us the author and the title. And we did not pass on, but we could choose to do on book ID. And we might actually want to do left that we're not loading up a bunch of random books.

55
00:34:05.000 --> 00:34:53.000
And we might actually want to do left that we're not loading up a bunch of random books. Okay. So let's start ahead. I guess let's see how the. So if we do left, we get about six million, which is what we should. What happens if we do enter. Into the outer. Okay. It looks like the book status set is. It's contains all of the books that are in ratings. So none of the methods are going to make a difference. Okay. So let's see. So now we could have done this before where we counted which books were the most often rated.

56
00:34:49.000 --> 00:35:23.000
So now we could have done this before where we counted which books were the most often rated. But we would have only known what book ID was most rated. Now we can actually look at which titles were the most rated. So we take our rated books. Book ID.value counts. So we're going to tell us how many times a particular book was rated. And now let's look at the 10 largest. And we're going to grab the index, which is going to be the corresponding book IDs. We're then going to look at the rated books from the most rated IDs.

57
00:35:17.000 --> 00:35:57.000
We're then going to look at the rated books from the most rated IDs. And let's go ahead and look at what these titles are. So these are all books that are. So good reads is mostly US based. I don't know whether they call it a social media site. It's a book rating site. And so you might not be surprised that the Harry Potter books are some of the most read. And most rated at the Hunger Games and Twilight. I'm kind of so to kill a mockingbird was actually one of my father's favorite books. And so I'm pleased to see that it made the cut of one of the most rated books.

58
00:35:53.000 --> 00:36:32.000
And so I'm pleased to see that it made the cut of one of the most rated books. It's actually a really nice book. So these are the most rated books on in our good reads data set. So now we could use our knowledge of pivot table that we learned about previously to get an average rating for each of these books. So the average rating, let's go ahead and look to sort it by the rating. So the highest rated books were Harry Potter and the Prisoner of Ascabam. And the sorcerers stone. Then to kill a mockingbird again, I'm pleased about this.

59
00:36:29.000 --> 00:37:04.000
Then to kill a mockingbird again, I'm pleased about this. And then kind of Hunger Games Harry Potter. Twilight comes in last which is kind of funny. I suspect this is a function of who the audience of Twilight's readers is. They may not be the people who are most involved on good reads. I could be wrong. I don't know. I don't know enough about kind of the literary world to comment intelligently on all of these insights. Which brings us to a slightly different point. When you're analyzing data, there's actually a lot of value in domain knowledge.

60
00:37:00.000 --> 00:37:33.000
When you're analyzing data, there's actually a lot of value in domain knowledge. So you'll see a lot in the world that data scientists or sometimes statisticians or even economists. They'll wander into a topic that they don't know very much about. And they'll make some rather broad and strong claims. And someone with domain expertise will say kind of, well, what you're saying isn't true. This is actually a short coming of the data. And this is why what you're saying is kind of a, this kind of weaker kind of wrong.

61
00:37:31.000 --> 00:38:12.000
this kind of weaker kind of wrong. And so there's a lot of value in your working on business problems of teaming up with someone who has a lot of domain expertise. Interesting, you know, if you have a client, trusting that your client knows more about what they're doing than you do. And then tying their knowledge into your kind of statistical and economic insights. And really mixing these two things is the best. But okay, getting off my soapbox now. So we could compute the average number of ratings for each book in our sample.

62
00:38:03.000 --> 00:38:50.000
So we could compute the average number of ratings for each book in our sample. Sorry, yeah. So this is rated books, pivot table ratings. I see. So what we've done is we've computed the average rating in that average number of ratings. And so the best, one of the most highly rated books is Calvin and Hobbes, which is a comic. It's quite good. And it shows up lots of times actually. So there's one, two, three, four, five Calvin and Hobbes in the top ten. And so what is the overall distribution of ratings look like?

63
00:38:45.000 --> 00:39:27.000
And so what is the overall distribution of ratings look like? We plot the kernel density using our data frame method. And then we also plot the histogram. And it tells roughly the same thing. Looks like the majority of books are rated just below four. Okay. So that's going to end our case study for now. If we really wanted to kind of do a deep dive and learn some really deep insights about the literary business world, we would have needed to bring someone in that knows that world. So this was just kind of fun and a chance to show off some of the methods that we've learned.

64
00:39:20.000 --> 00:40:11.000
So this was just kind of fun and a chance to show off some of the methods that we've learned. And the final thing we just wanted to do was we've put together. And by we, I mean the Royal Wii and the synths Spencer has put together some more beautiful gifts about how these different merge methods work. So we create a toy dataset, the F left and the F right. And we just look at how these work. So notice our concat with access equals zero. It's just going to stack these on top of each other. So you're taking these values and you're just going to put them into the C3 column.

65
00:40:04.000 --> 00:40:55.000
So you're taking these values and you're just going to put them into the C3 column. The key is going to get matched because it's the same in both datasets. And then these two will just come straight over. But then you have missing data here because there's no C3 column. And you have missing data here because in the other data frame, there's no C1 or C2. For access equals one, notice it's doing something similar. But in this case, there's no, there's actually no overlap. So because the index on the first data frame and the index on the second data frame are completely separate.

66
00:40:45.000 --> 00:41:30.000
So because the index on the first data frame and the index on the second data frame are completely separate. So it's just copying over a bunch of data. This would not be the merge method you wanted for the dataset we're using. So now we have our merge. So, and this is going to be by default. Remember left. And so what it does is it basically copies all of this data first because we're only going to keep these values. And we're merging on key. And so it's going to say key equals A. So let's put 100 there and there.

67
00:41:27.000 --> 00:42:19.000
So let's put 100 there and there. Key equals B. Let's put a 200 and the key equals B. And then there's not going to be a D. There is a C. So let's put the C in the right spot. But there's no D. So it's not going to show up. We could do the same thing for the right. But now notice what it's done is first it copies this data frame over. And now it looks for A. It finds two of them. So it duplicates this particular row. And then fills in with 1 and 10. And 3 and 30. Then it finds the B. And it goes and it looks up what the B was here.

68
00:42:16.000 --> 00:42:48.000
And it goes and it looks up what the B was here. And it sees 2 and 20. Then it looks at the C and C is 4 and 40. And there's no values for D. So it simply keeps them as missing. And that's all we have for our merging lecture. So we'll give you your assignment. And please feel free to reach out and ask us if you have any questions. Bye-bye.

