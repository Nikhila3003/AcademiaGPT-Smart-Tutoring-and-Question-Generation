1
00:00:00.000 --> 00:00:34.000
This is Chase, and today we're going to be talking about some of the basic functionality that we have in pandas. At the end of the lecture today, you should have been familiar with what is a datetime object, and you also should have learned how to use aggregation functions, transformation functions, and scalar transformation functions, and apply built-in and create customized versions of these to make transformations to your data. We'll also talk about how to select subsets of your data using Boolean selection,

2
00:00:28.000 --> 00:01:13.000
We'll also talk about how to select subsets of your data using Boolean selection, and we'll introduce you to what we call the WANT operator, and we'll teach you how to apply it. The data that we'll be using for today's course comes from the US State Unemployment Data from the Bureau of Labor Statistics. So this is our outline, and let's go ahead and get started. So as usual, we're going to import our pandas as PD, and we're going to activate our custom plotting theme. Now, whenever we do a pandas lecture, this is our first one, we'll typically read in some kind of a data set that we'll use as motivation for the things that we do in class.

3
00:01:02.000 --> 00:01:44.000
Now, whenever we do a pandas lecture, this is our first one, we'll typically read in some kind of a data set that we'll use as motivation for the things that we do in class. Again, in this case, we're going to be reading in State Unemployment, and you'll want to take note that there's going to be a column named date, and we're going to specify that that column should be read in as a date object. And so if we look at this, we can see the structure of our data. It has a column for date, a column for state, a column for how many people are in the labor force in that state, and an unemployment rate.

4
00:01:33.000 --> 00:02:10.000
It has a column for date, a column for state, a column for how many people are in the labor force in that state, and an unemployment rate. And you won't have to worry about this, but what we want to do is we want to have a column for each state, and then along the index we would like to have the dates, so one per month in our case, and the values that we'd like inside of our data frame will be the unemployment rates. And this is just some reshaping that we'll learn to do later in this course to make sure that we have the data that we want.

5
00:02:03.000 --> 00:02:37.000
And this is just some reshaping that we'll learn to do later in this course to make sure that we have the data that we want. So again, you should just take note that the states are the columns, the date is the index, and the values are unemployment rates. And to keep the data manageable, we're just going to focus on seven states. We're going to focus on Arizona, California, Florida, Illinois, Michigan, New York, and Texas. So this unimp data frame that we have will be the data frame we work with for the rest of this course.

6
00:02:31.000 --> 00:03:11.000
So this unimp data frame that we have will be the data frame we work with for the rest of this course. So the first thing we can do is we can plot the data. And we talked briefly about this in the last video, that it will create this plot for us. The thing I want to highlight right now is that it knows that the unemployment, that the date column has been moved to the index, and so when we plot, it places this on the index for us and labels it. Additionally, it will create one line for each of the columns, and this is why we chose to have seven columns instead of 50,

7
00:03:04.000 --> 00:03:53.000
Additionally, it will create one line for each of the columns, and this is why we chose to have seven columns instead of 50, and it's going to label these for us. So this purple line is Arizona. This really low unemployment rate is from Texas. And so we see that we get some of this nice plotting features for free. So let's go ahead and look at what index we have on our data frame. You'll notice that it has these nice formatted text, so 2000, 01, 01. And that's because we specified when we read the data in that column, that data was a datetime object.

8
00:03:45.000 --> 00:04:34.000
And that's because we specified when we read the data in that column, that data was a datetime object. And so Pandas has recognized that this is a datetime object because we told it, and it's going to allow us to do some special things with it. So for example, we can index into a particular date. So if we choose to look at January 1st from the year 2000, we see that Arizona had an unemployment rate of 4.1, Texas had an unemployment rate of 4.6, and California had an unemployment rate of 5. We could also choose all of the days between New Year's Day in June 1st and the year 2000 just by putting 01, 01, 2000.

9
00:04:24.000 --> 00:05:07.000
We could also choose all of the days between New Year's Day in June 1st and the year 2000 just by putting 01, 01, 2000. And notice we can pass dates in in different formats and going to June 1st, 2000. And that will now give us a data frame with six rows where each of the rows corresponds to a month. So we'll talk much more about this in a future lecture, but we just wanted to highlight that you have this functionality available to you. So the next topic we'll talk about are data frame aggregations.

10
00:05:02.000 --> 00:05:40.000
So the next topic we'll talk about are data frame aggregations. And loosely speaking, aggregation is an operation that takes multiple values and turns it into a single value. The example that we should all be familiar with is computing a mean. So taking the three numbers 01 and 02 and computing their mean returns a single number, 1. We're going to use aggregations extensively in this course, and we're very grateful that Pandas will make this easy for us. So Pandas has some of the most frequently used aggregations already built in.

11
00:05:36.000 --> 00:06:23.000
So Pandas has some of the most frequently used aggregations already built in. For example, mean, variance, standard deviation, finding the minimum, medium, max, etc. We do want to note here this is not all of them. There's many other aggregations Pandas has that we have not listed. And we just encourage you to use a little bit of tab completion. So if you take a data frame, so again, we can take the mean. If you hit tab, notice it brings all of these options up. And let's see an aggregation that we might do is standard deviation.

12
00:06:16.000 --> 00:06:59.000
And let's see an aggregation that we might do is standard deviation. So if we just type ST and hit tab, it brings up STD. And if we wanted to know what that was, we could put a question mark and return sample standard deviation or requested axis normalized by n minus one by default. So it tells us a little bit more. And we could then use that to get the standard deviation. So by default, as you might have noticed, aggregation operates on the columns. We can use the axis argument though to specify whether we like to do aggregations by rows.

13
00:06:53.000 --> 00:07:40.000
We can use the axis argument though to specify whether we like to do aggregations by rows. So remember the zero axis is going, you collapse the zero axis, which is going to collapse over the rows. But if we specify axis equals one, we can collapse over the columns. So when we take the variance with respect to axis equals one, we're getting the variance of the unemployment rate in the year 2000. In January 2000 or February 2000, etc. So the built-in aggregations get us pretty far in our analysis, but sometimes you need a little bit more flexibility.

14
00:07:32.000 --> 00:08:09.000
So the built-in aggregations get us pretty far in our analysis, but sometimes you need a little bit more flexibility. We can have pandas perform custom aggregations by following the next two steps. One, you write a Python function that takes a series as an input and outputs a single value. And then you can call the ag method with the function as an argument. So let's do an example below where we classify states as low unemployment or high unemployment based on whether their mean unemployment level is above or below 6.5.

15
00:07:57.000 --> 00:08:47.000
So let's do an example below where we classify states as low unemployment or high unemployment based on whether their mean unemployment level is above or below 6.5. So step one, let's write the aggregation function. So higher low is a function that takes s, which is going to be a pandas series object. And if the mean of s is less than 6.5, we'll return low. And if it's higher than 6.5, which is in the else, then we'll return high. And so now step two, all we have to do is call dot ag. And notice we've now classified Arizona as a low unemployment state, California as a high unemployment state, etc.

16
00:08:37.000 --> 00:09:30.000
And notice we've now classified Arizona as a low unemployment state, California as a high unemployment state, etc. We also can get the axis equals one. And we can see which of the months were low or high unemployment. Now all of the ones we can see are low unemployment because they were in times of growth. But if we looked at, so let's go ahead and do our fancy date indexing. So let's go from January 2008 to October of 2008. And we notice that at the end of 2008, we started seeing high unemployment rates.

17
00:09:21.000 --> 00:10:07.000
And we notice that at the end of 2008, we started seeing high unemployment rates. And we just did this. Great. And the other thing you might notice is that ag can accept multiple functions. So here we're passing in three functions. We're passing in the function min, max, and then higher low. And what this will return is a data frame now, where we have the columns the same as the data frame that we passed in. And now we have a row for each of the functions. So the min unemployment rate in Texas was 3.9.

18
00:10:01.000 --> 00:10:45.000
So the min unemployment rate in Texas was 3.9. The max unemployment rate in Michigan was 14.6. And then we have our high low classifier that's telling us whether each of the states is a high or low unemployment state. So now let's take a pause and let's do an exercise to see whether we understood the concepts that were just presented. So at each state, at each date, we'd like you to find the minimum minimum unemployment rate across all of the states in our sample. We'd like you to find what was the median unemployment rate in each state.

19
00:10:40.000 --> 00:11:14.000
We'd like you to find what was the median unemployment rate in each state. What was the maximum unemployment rate across the states in our sample. What state did it happen in and in what month or year was this achieved. And then we'd like you to classify each state as being high or low volatility based on whether the variance of their unemployment is above or below four. And we'll go ahead and pause for five to ten minutes, this five minutes, and see how we see how we do. Let's review the answers.

20
00:11:12.000 --> 00:11:53.000
Let's review the answers. So these exercises asked first at each date, what is the minimum unemployment rate across all of the states in our sample. And the way we can find that is for each month, we want to find the minimum unemployment rate, which means we want to take the minimum across the columns or the axis. So axis equals one. Then the second was what was the median unemployment rate in each state. Since this is the median across within a column is we're going to just take the median, which will default to axis equals zero, which collapses the rows.

21
00:11:41.000 --> 00:12:14.000
Since this is the median across within a column is we're going to just take the median, which will default to axis equals zero, which collapses the rows. Next, we're going to try and find what the maximum unemployment rate was across each of the states. What state did this maximum unemployment rate happen in and when was it achieved. And the way we're going to do that is first we're going to take the maximum across all of the unemployment, which will tell us the maximum unemployment rate in each state.

22
00:12:05.000 --> 00:12:37.000
And the way we're going to do that is first we're going to take the maximum across all of the unemployment, which will tell us the maximum unemployment rate in each state. And then the max will tell us what the maximum unemployment rate was. IDX max, again, the first part is the same is going to tell us which state that occurred in. And then we're going to take the maximum according to axis equals one to find the maximum unemployment rate in each month. And if we take the IDX max that will tell us when it occurred.

23
00:12:34.000 --> 00:13:18.000
And if we take the IDX max that will tell us when it occurred. And so the maximum unemployment rate in our sample was 14.6%, which happened in Michigan in June of 2009. And the last question in this exercise was to classify each state as high or low volatility based on whether the variance of their unemployment was above or below four. So we define a new function, unemployment rate volatility classify, which takes a series as an input. And then it returns either the string high or low, depending on whether the variance is above or below four.

24
00:13:08.000 --> 00:13:46.000
And then it returns either the string high or low, depending on whether the variance is above or below four. And so what we see is Arizona, California, Florida and Michigan are high variance unemployment rate states and Illinois, New York and Texas are low variance unemployment rate states. Hopefully these lined up with what you did. Otherwise, you should review after class and feel free to ask us questions. Great. So we're done talking about aggregations for now. And next we're going to talk about what we call transforms.

25
00:13:42.000 --> 00:14:24.000
And next we're going to talk about what we call transforms. And transforms are going to be analytical operations of some sort that do not necessarily involve an aggregation. So the output of a transform would be a series that is mapped to another series. Some examples of what this might be are the percentage change in unemployment rate from month to month, or the cumulative sum of the elements in some column. So there's lots of built in transforms for us, just like there were aggregations, we can take the cumulative sum, the cumulative min, the cumulative max, the cumulative product, we can take the difference.

26
00:14:10.000 --> 00:14:45.000
So there's lots of built in transforms for us, just like there were aggregations, we can take the cumulative sum, the cumulative min, the cumulative max, the cumulative product, we can take the difference. We can do element wise additions, retraction, multiplication or division, percent change, value counts, or absolute values. And again, we encourage you to explore what functions are available using tab completion. So let's go ahead and do a couple of examples. So recall, this is the data that we have.

27
00:14:38.000 --> 00:15:19.000
So let's go ahead and do a couple of examples. So recall, this is the data that we have. The first we're going to do is computing the percent change. Notice it can't compute the percent change for the first element because there's nothing that precedes it. And then we see that unemployment rate did not change in most states, but in Michigan it went down by 3% and it computes these percent changes for us. Additionally, we can compute the difference, which will instead of taking the percent change, it's just going to take the difference.

28
00:15:12.000 --> 00:15:52.000
Additionally, we can compute the difference, which will instead of taking the percent change, it's just going to take the difference. And again, the first row is going to be all missing because you can't take the difference between something before the first row. So we classify transforms into a few main categories. The first is going to be a series transforms, and that's a function that takes in one series and produces another series. And this will result in the index of the input and the output not necessarily being the same.

29
00:15:46.000 --> 00:16:21.000
And this will result in the index of the input and the output not necessarily being the same. Scalar transforms are going to be functions that take a single value and produce a single value. So one example would be the absolute value method, which we're adding a constant to each value of a series. So these are going to take a particular value and do an operation to that one value rather than to the entire series. So just like we could write custom aggregation functions, we can also write custom transformations.

30
00:16:14.000 --> 00:16:45.000
So just like we could write custom aggregation functions, we can also write custom transformations. The steps are going to be the same as we're going to write a Python function that will take a series and output a new series. And now rather than pass our new function as an argument to the ag method for aggregation, we're going to pass it to the apply method. So as an example, what we're going to do is we're going to standardize all of the unemployment data to have mean zero and standard deviation one.

31
00:16:37.000 --> 00:17:12.000
So as an example, what we're going to do is we're going to standardize all of the unemployment data to have mean zero and standard deviation one. And we'll do this so that we can classify which date the unemployment rate differs most from normal times in each state. So step one is to write a function that can standardize the data. So it takes in a series and it's going to take the mean of that series and the standard deviation. And then it's going to create a new series that's going to be the original series minus the mean divided by the standard deviation.

32
00:17:03.000 --> 00:18:03.000
And then it's going to create a new series that's going to be the original series minus the mean divided by the standard deviation. Next, we can apply this to our data. And what we'll see is that in the early 2000s, we had negative values for our, they call this a Z transform. So our standardized data is going to have minus one, which means that the data was abnormally low, which in unemployment is a good thing. And then finally, we're going to take the absolute value of all of the elements. And again, this is a scalar transformation because it's going to take one value and map it into one new value and do this for each value in the data frame.

33
00:17:50.000 --> 00:18:37.000
And again, this is a scalar transformation because it's going to take one value and map it into one new value and do this for each value in the data frame. And now let's go ahead and find out when the unemployment rate was most different than normal from for each state. And so what we see is that the 2008, 2009, 2010 recession really did strange things to unemployment rates and those were the most abnormal unemployment rates in our series. And in addition to doing customized series transformations, we can also do customized scalar transformations.

34
00:18:29.000 --> 00:19:03.000
And in addition to doing customized series transformations, we can also do customized scalar transformations. And the way that you do this is again, the first step will always be to find a Python function. And in this case, it will take a scaler as an input and produce a new scalar. And second, it's going, you're going to pass this function as an argument to the apply map method. So let's go ahead and look at this. So imagine we want to determine whether unemployment rate was high greater than 6.5, medium or low.

35
00:18:55.000 --> 00:19:33.000
So imagine we want to determine whether unemployment rate was high greater than 6.5, medium or low. So we can write a Python function that takes a single number as an input and outputs a single string. And then we're going to pass this function to apply map. So one question you should ask yourself is why are you passing this to apply map and not aggregate or apply? And think about why neither aggregate or apply would work. So here's our function. We define unemployment classifier and if it's above 6.5, we write high.

36
00:19:29.000 --> 00:20:15.000
We define unemployment classifier and if it's above 6.5, we write high. It's above 4.5, but below 6.5, we return medium. Otherwise, we return low. And now when we do apply map, what happens? We're going to get back a new data frame and notice every value in this data frame is either going to be the same. So it's not going to be low, medium or high, because those were the values that we could potentially put in. So now we're going to move on. We're doing these as examples rather than exercises. So let's use another transformation on unemployment bins, which was the data frame we just created,

37
00:20:09.000 --> 00:20:40.000
So let's use another transformation on unemployment bins, which was the data frame we just created, to figure out how many times each state had each of the three classifications. So, if you were doing this on your own, which I'd encourage you to try and do after class, you should think about, will this value counting function be a series or a scalar transform? And Google, pandas count unique value, or something similar to find which transformation. We're then going to construct a horizontal bar chart of the number of occurrences of

38
00:20:37.000 --> 00:21:23.000
We're then going to construct a horizontal bar chart of the number of occurrences of each level with one bar per state and classification. Okay, so the method we were looking for is one called value count. And so what we're going to do is we're going to take unemployment bins, which remember, looks like this. And we're then going to count each of the times that the occurrence of value occurs for each column. And then we'll go ahead and look at what this looks like. So now we have for Arizona, the high unemployment, Arizona had high unemployment 75 times,

39
00:21:12.000 --> 00:22:16.000
So now we have for Arizona, the high unemployment, Arizona had high unemployment 75 times, low unemployment 44 times, and medium unemployment 97 times, etc. And we're going to plot the transpose so that it does something. So that's what this dot capital T was. And let's go ahead and reorder this data a little bit so that we can have low, medium, high. Here we go. And so if the blue bar is high, it means that state has a low unemployment a lot of time. Whereas if the yellow bar is high, it means that that state has high unemployment frequently.

40
00:22:08.000 --> 00:22:50.000
Whereas if the yellow bar is high, it means that that state has high unemployment frequently. So from this chart, you might not be particularly keen on California or Florida, whereas somewhere like Texas or New York or Florida might be much better in terms of their unemployment rates. So finally, we're going to repeat the previous step, but now we're going to count how many states had each classification and each month. So we're going to do almost the same thing, but now we're going to count across the columns.

41
00:22:44.000 --> 00:23:19.000
So we're going to do almost the same thing, but now we're going to count across the columns. So we're going to use axis equals one. And whenever there's a missing value, which just means that none of the states had a particular value, we're going to fill it with a zero, because it just means that there were zero states with that value. And here we go. This is what it looks like. So notice in the early 2000s, no states had high unemployment. Most states had low unemployment and a few states had medium unemployment.

42
00:23:14.000 --> 00:24:03.000
Most states had low unemployment and a few states had medium unemployment. And so now which state, which month had the most states with high unemployment? You might be unsurprised when we say 2009, which when was the most medium unemployment? 2001. And when was there lots of low unemployment? The beginning of 2000 or late in the year 2000. And we found these with IDX max, which is going to search for the maximum about the index associated with the maximum value. Great. So now we've covered aggregations, transforms and scalar transformations.

43
00:23:58.000 --> 00:24:38.000
So now we've covered aggregations, transforms and scalar transformations. Another thing that we'll often want to do is Boolean selection, which is we're going to frequently want to select pieces of data based on whether certain conditions are met. For example, if you were in marketing, you might be interested in a particular target audience. And so you might subset your data to only include adults between 18 and 32, if you had a particular product for adults, young adults. You might also be looking at data that corresponds to a particular time period, or some of the work we did while we were, well, Spencer and I were at NYU,

44
00:24:29.000 --> 00:25:05.000
You might also be looking at data that corresponds to a particular time period, or some of the work we did while we were, well, Spencer and I were at NYU, as we looked at data that corresponded to recessions. So we'll be able to do this by using a series or list of Boolean values to index into a series or data frame. So we're going to take a small view of our unemployment data frame. So we took the dot head, which is going to be only five rows of our data. And we're doing this so that we can see what's happening as we do these operations.

45
00:24:59.000 --> 00:25:46.000
And we're doing this so that we can see what's happening as we do these operations. So we can use Booleans to select particular rows. So in this case, we need seven values or five values associated with let's be explicit here. So five values that are going to be associated with our five rows. So in this case, we'll select the first three, true, true, true, and disregard the last two where they were falses. We can also use Booleans to select both rows and columns. So in this case, we're again selecting the first three rows, but now we're only going to select the first and last two columns.

46
00:25:38.000 --> 00:26:14.000
So in this case, we're again selecting the first three rows, but now we're only going to select the first and last two columns. Now, we did these selections with lists, but obviously you're not going to be able to write by hand a list with a million values in it. And so what we're going to do next is we're going to use conditional statements to construct new series of Booleans from our data. And this will look a lot like some of what we covered in the conditional section of our introduction to Python.

47
00:26:07.000 --> 00:26:50.000
And this will look a lot like some of what we covered in the conditional section of our introduction to Python. So what can we do here? So we're looking at the unemployment small data frame, the Texas column, and we're asking when the unemployment in Texas was less than 4.5. And what this says is that unemployment was above 4.5 for the first three months of the year, but then in April and May, the unemployment fell below 4.5. And once we have that, we can start selecting subsets of our data. So notice we have unemployment small Texas less than 4.5, which is going to create a series that has trues and false in it, says in it.

48
00:26:42.000 --> 00:27:25.000
So notice we have unemployment small Texas less than 4.5, which is going to create a series that has trues and false in it, says in it. And we can select all of the columns. And this just gives us the last two rows like we expected. We could check for when the unemployment in New York was higher than the unemployment in Texas. And in this small data set, that always happens to be true. And in the Boolean section of our basic Python lecture, we saw that we could use the words and or or to combine multiple Booleans into a single Boolean.

49
00:27:14.000 --> 00:27:58.000
And in the Boolean section of our basic Python lecture, we saw that we could use the words and or or to combine multiple Booleans into a single Boolean. Just as a quick refresher, we use and and or in the mathematical sense, which means true and false results in false. True and true results in true. False and false results in false. True or false is true. True or true is true. And false or false is false. We can do something similar in Pandas, but rather than writing bool one and bool two, we write bool series one and sign bool series two.

50
00:27:44.000 --> 00:28:30.000
We can do something similar in Pandas, but rather than writing bool one and bool two, we write bool series one and sign bool series two. Likewise, rather than writing bool one or bool two, we use the pipe symbol to denote or. Let's see this. So in this case, we're going to look at when unemployment rate was below 4.7 in Texas and the unemployment rate was below 4.7 in New York. Notice the ampersand. And this is going to say this is false for the first two months and true for the last three months.

51
00:28:21.000 --> 00:29:05.000
And this is going to say this is false for the first two months and true for the last three months. And if we select using this, notice that the unemployment rates in both New York and Texas will be below 4.7 for every month. Sometimes we'll want to check whether a data takes on one of several fixed values. The way we could do this right now is we could write df of a particular column x is equal to the value one or df column x is equal to value two, etc. But because this is a frequent operation, Pandas provides us a better way.

52
00:29:00.000 --> 00:29:43.000
But because this is a frequent operation, Pandas provides us a better way. So in this case, we're going to look at the Michigan column and we're going to ask whether the values are in 3.3 and 3.2. And so the first four months of the year 2000, the value of unemployment in Michigan was either 3.3 or 3.2. And we can confirm that here. And next we're going to talk about any and all methods. And as you may remember from our first Boolean and conditional lecture, that the Python functions any and all are functions that take a collection of Booleans and return a single Boolean.

53
00:29:33.000 --> 00:30:07.000
And as you may remember from our first Boolean and conditional lecture, that the Python functions any and all are functions that take a collection of Booleans and return a single Boolean. Any returns true whenever at least one of the inputs is true, while all returns true only when all of the inputs are true. Series and data frames have, with a d type bool, have dot any and dot all methods that can apply this logic. So what we're going to do is we're going to count, we're going to learn this by counting how many months all of the states in our sample had high unemployment.

54
00:29:59.000 --> 00:30:33.000
So what we're going to do is we're going to count, we're going to learn this by counting how many months all of the states in our sample had high unemployment. And as we work through this example, we think this is a great opportunity to talk about the want operator, which is a helpful concept that Spencer and I learned from Tom. So here's how the want operator works. You start by writing want followed by what we want to accomplish. In this case, we would write, we want to count the number of months in which all states in our sample had unemployment above 6.5%.

55
00:30:24.000 --> 00:31:01.000
In this case, we would write, we want to count the number of months in which all states in our sample had unemployment above 6.5%. And now let's work backwards to identify a sequence of steps necessary to accomplish our goal. So to count the number of months in which all states in our sample had unemployment above 6.5, we would just want to sum the number of true values in a series indicating dates for which all series had high unemployment. So now how do we get a series that has true values when the states had high unemployment?

56
00:30:53.000 --> 00:31:42.000
So now how do we get a series that has true values when the states had high unemployment? We would, the one way to do this is to build a series by using the dot all method on a data frame containing booleans indicating whether each state had high unemployment at each date. So what this means is we want to know when every state in our sample had high unemployment, which we can do using the dot all method with axis equals one. Finally, how do we get to one that can do this? We can just build a data frame by using a greater than comparison to compare whether the data was above 6.5%.

57
00:31:34.000 --> 00:32:15.000
We can just build a data frame by using a greater than comparison to compare whether the data was above 6.5%. So here is a plan and to apply the want operator, all we're going to do is start at step three and work our way back. And the reason we find this helpful is because if you start knowing what you want, it's easier to figure out what you need to get one step ahead and you keep stepping backwards inductively. So, great, step three, unemployment greater than 6.5. This tells us, well, the early 2000s were good years for the unemployment rate.

58
00:32:09.000 --> 00:32:55.000
This tells us, well, the early 2000s were good years for the unemployment rate. So in this case, they're all false. In step two, we're going to use the dot all method with axis equal one, which will check whether every single column is equal to true for, and we're going to apply this to every row. So high dot all axis equals one will give us a new series and now it's going to have a true or a false based on whether all of the states had high unemployment rates. And now finally, simply summing this series will tell us when all states had high unemployment.

59
00:32:48.000 --> 00:33:11.000
And now finally, simply summing this series will tell us when all states had high unemployment. And so out of 216 months, only 41 months had high unemployment in all states. And this ties up our Pandos basics lecture. Thanks for being here. Look forward to seeing you soon. Bye.

