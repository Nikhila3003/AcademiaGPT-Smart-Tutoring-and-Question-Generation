page_content="So we're very excited for this lecture in which we're going to estimate a Markov chain using some of the employment data that we've talked about. And we'll try to be specific about whatspecific about what we mean by estimate, but some of these are details that we'll learn later on in the class. Okay, so let's start with a short digression. We have a friend named Jim Savage who's a sJim Savage who's a statistician, a good one actually. And he has thought about the right way to do modern statistical work. And we think that many of the things that he's learned in thinking about thin thinking about this apply to economics as well. So let's start by talking about what he envisions as a modern statistical workflow. So he's kind of created nine steps, which seems long, but they'res long, but they're pretty short. So step one would be to prepare and visualize your data. So this involves things like data cleaning or making some plots. We did lots of this in a previous lecture wiprevious lecture with the BLS data and looked at how employment has changed in response to the COVID recession. Step two is to create a generative model. And what do we mean by generative model is alnerative model is all we mean is some probability distribution. Over outcomes. That are a function of some model parameters and he thinks it's very important that the first model created should be asreated should be as simple as possible. And the next step is something that lots of people don't necessarily think to do because it's not necessarily natural. But one of the first things you should dothings you should do is take the model that you've written down. And you should now actually generate some data. So we generate a bunch of observations from our model, given a particular parameter thaicular parameter that we've picked out of a hat. And then step four is let's see if we can back out what theta looks like. So we'll call theta hat. The parameter is that we get from our model sittingm our model sitting and see how this compares to theta. And you do that because what you want to know is in step five. You should check that you understand and that you're able to, you know, if you hayou know, if you have certain parameters for your model that your fitting procedure is able to recover those parameters when you know them. Because if it does, if you can't get the parameters that yoe parameters that you know. It's very unlikely that you'll be able to find the parameters that you don't know. And he emphasizes that you possibly repeat steps three through five. So two through fiveSo two through five with different methods and parameters to get an understanding, oh, sorry, to get an understanding of how your model works and what fitting procedure is seem to be successful. Afterbe successful. After this, so we've now worked with some fake data. We understand the fitting procedure and how the model works. Now, we move on to our real data. So we take the same procedure and dome procedure and do the same fitting procedures on the data that we observed in step one. And then step seven and this makes me laugh. So he says, argue about the results with your friends and colleagfriends and colleagues. So Jim's not a particularly. Hard guy to get along with. He's actually very friendly. And so his choice of the word argue. I think tells you a little bit about who you shouldbout who you should be talking to about this. You don't want someone that's going to just tell you that it's nice work. You want someone that's going to be able to ask you questions and say, why did ys and say, why did you do that? Well, what if you did this instead? So you want someone that's going to push back a little bit. Step eight is then returned to step two and work with a slightly richerh a slightly richer model and we repeat steps three, four, five, six and seven. And then once you've done this with a sufficiently rich model, you should think about what decisions are going to be madare going to be made from this analysis. Figure out what your loss function will be, kind of what are the costs of getting this right or wrong. And then you should perform make decisions based on stacisions based on statistical decision analysis. And so later in the semester, we'll talk more formally about what we mean by fitting your model and the work that goes along with it. But right now, any. But right now, anytime we say fit, you should just imagine some type of a procedure that allows us to take a generative model that depends on parameter's data. And data and try to get a guess at whao get a guess at what those data parameters are if we could not see them. And so we're going to go ahead and do some of these steps on the labor data to see if we can work through this process. So whathis process. So what's our plan for this lecture? So we're going to actually skip steps one. We've already mostly done step one. We created some graphs. We cleaned the data. So we're going to take th're going to take that as given. And then the plan for the remainder of this class and this probably will go into our next class is to work through some of the remaining steps of a modern statisticalmodern statistical workflow. And we're going to develop a generative model of employment and unemployment. We're going to use that model to simulate some fake data from our generative model. We're thtive model. We're then going to fit that model using simulated data and we'll tell you explicitly how we're going to do that fitting. Then we're going to explore some options that we might have had fowe might have had for having for how we could choose to fit the data. So we're going to fit the model with the BLS data. And finally we're going to examine what our model implies for the effects of COor the effects of COVID on employment and unemployment. So again, we won't get through all of these today and it's an ambitious lecture, but I think we're going to have a lot of fun along the way. Okan along the way. Okay, so let's develop a generative model. So Jim's recommendation is that the first model created should be as simple as possible. So in the spirit of as simple as possible, we're gos possible, we're going to return to the model that we talked about in class when we talked about mark-off chains. So we're going to consider a single individual. This individual is going to move betws going to move between two states, employment and unemployment. When an individual is unemployed, they're going to find a new job with probability alpha. So we'll call this the job finding rate. Andb finding rate. And while an individual is employed, they're going to lose their job with probability beta. So we'll call this the job separation rate. And that's it. That's our model. So our model isdel. So our model is entirely defined by two parameters and alpha, which is the job finding rate and a beta, which is the job separation rate. Okay, well, we now have a generative model. So the next smodel. So the next step is for us to actually simulate data from this generative model. And we're going to do this in two steps. So first, we're going to take an individual's state of employment or unof employment or unemployment. And the transition probabilities, which are our parameters. And then we're going to draw from tomorrow's state. And determine whether that person will be employed or unll be employed or unemployed. Next, once we have what we're going to call the one step transition, we're going to specify, we're going to create a function that takes an initial state and the parameteate and the parameters. And that can then simulate the entire history of employment or unemployment using our one step function. So let's start doing this. So how can we simulate the one step functionhe one step function? So we're going to specify again, we highly encourage everyone to win their writing code, follow good coding habits and write documentation. So our function called the next statelled the next state is going to take three arguments. It's going to take an ST, which is an individual's current state. And we're going to specify ST equals zero is going to map to unemployment. And Sunemployment. And ST equals one is going to be employment. There's going to be the alpha, which is the job finding probability. And there's going to be the beta, which is the job separation. And so wseparation. And so what it's going to return, this is actually, I should have written this documentation previously, but it's going to return an ST plus one, which is also an integer. And it's going ter. And it's going to be the individuals employment state and period T plus one. Okay, so what are we going to do? The first thing is there's going to be some random randomness in whether an individuawhether an individual transitions from one state to another. So we're going to draw our random number up front just that we don't have to do it multiple times. Then, and so this random number is goingndom number is going to be drawn from from a uniform zero one. So this is coming from uniform zero one. So if an individual is unemployed, so this is ST equals zero, then we want to know whether theyo know whether they become unemployed or employed. Well, they find a job with probability alpha. So there's lots of ways that we could potentially do this, but the easiest is what's another event thats another event that has probability alpha. Well, if X is distributed uniform zero one, then the probability that X is less than alpha is equal to alpha. And so what we do is we say if an individual iy if an individual is unemployed and our random number is less than alpha. So this is our probability alpha. Then they transition from unemployment to employment. Similarly, if an individual is employindividual is employed, so this is ST equals one, then they could either become unemployed or employed. And they do this with probability beta, they become unemployed. And so we check whether the numbeck whether the number was less than beta. And if so, then they become unemployed. So if they're state, state is the same, it doesn't change. So this means either U.T. was greater than alpha or U.T. wthan alpha or U.T. was greater than beta. Or you gave us a state that was incorrect. So we also could have added a check at the front to make sure that S sub T was less than or equal to one. But we'real to one. But we're going to be the ones using this code, so it's probably okay. So one thing we wanted to point out was that this function actually depends on the markoff property, which we've previy, which we've previously discussed. In case you don't remember, the markoff property states that the probability of ST plus one given ST, so the probability of a state tomorrow given the state today,ven the state today, is the same as the probability of ST plus one given ST ST minus one dot dot S zero. And what this means is other than the transition probabilities, we only need to give the functid to give the function the previous state and not an entire history. And so that's why in this function we can specify that ST is just an integer, just a single integer. And again, we've started withwe've started with the simplest model we could have, and so this is just purely by assumption. Okay, well, now we've written some code and did we run the code, I'm not sure, so just double check. Andst double check. And whenever you write a function that will be used kind of frequently, oh, you forgot code from even earlier. There we go, so we forgot to import our packages. Let's just read that.t's just read that. Okay, perfect. So once you've written a function that's going to be used somewhat frequently and just in general if you write a function, it's a good idea to create some simple teseate some simple test cases. So what we're doing here is we have an individual who's starting in state zero and they're transitioning to unemployment with about probability one half. So we should expef. So we should expect to see about half ones and half zeros and, you know, modular randomness that seems to be the case. Okay, so what other checks can we do? If we set the probability of going fromility of going from unemployed to employed, so if we set the alpha to zero, then the individual should stay unemployed. Well, that works. Likewise, if we set the job separation probability to zero, thbability to zero, then an individual who is currently employed should stay employed. And likewise, we can set the job finding probability to one and then an individual who is unemployed should alwaysloyed should always find a job. That works. And we can set the job separation probability to one and check whether an individual who is currently employed will become unemployed next time. And even ifxt time. And even if we ran all of these things, because we've set the probability to one, they're not going to change. Okay, great. So now we can simulate a single step. And so all we have to do is nl we have to do is now be able to simulate an entire history. We do want to advise you, eventually we're going to allow alpha and beta to change over time. So we want you to think of them as constantof them as constant for now, but we're going to write this code in a way that allows them to fluctuate period by period. And the way we'll do that, notice you can see this in the documentation is alphocumentation is alpha and beta. Our both numpy of rays and they're going to have type of float. And each element is going to be a probability that an individual finds a job or loses a job. And then ths a job. And then the other thing we're going to specify is the initial state of unemployment or employment that an individual is in. So in order to be able to take our first step, we need to know whewe need to know whether someone was employed or unemployed. So we're going to do some test checking first. So we're going to make sure that alpha and beta were the same length. And we're going to cald we're going to call their length T. And then we're going to make room for a history of employment or unemployment. We're going to set the first value to our initial state. And then we're going to doen we're going to do T steps. So how do we do that is we compute the next state by giving it the previous state. And what the transition probabilities for that period are, which are alpha, T and beta,e alpha, T and beta, T. And then we're going to save the new value back into S naught. And this is just to kind of keep a minimum number of variables here. So then we'll save that value into our arrayvalue into our array. And then we'll return our history of employment or unemployment. Okay, well, let's check the output of this function. So we're going to set the job finding rate at 25, 0.25, andte at 25, 0.25, and the job separation at 0.025. And let's see what happens. Well, an individual who is employed is likely to instead employ it for a long time. So we see it takes a long time to losea long time to lose a job because you're only losing it with probability 0.025. Well, if we run it again, now let's start an individual as unemployed and see what happens. And it doesn't take very lonoesn't take very long for them to find a job. And that's because the probability of finding a job is 0.25. And so you should kind of expect this to take two, five, six. This person, I guess, was reall, I guess, was really unlucky. And then continue to be unlucky because they eventually became unemployed. But you get the idea and the output of this function seems to be sensible. So we'll call thatSo we'll call that a success. Great. So we've now completed step two, which is defined a generative model. And step three, which is simulate data from your generative model. And now we're going to coow we're going to come to step four, which is going to be Picture model to fake data. There's a lot of different procedures that we could have chosen to map our data into the implied parameters. So reed parameters. So remember, fitting our model is just going to be a procedure in which we uncover Unknown parameters of our model. The way we're going to do this for our mark-off chain is just by counhain is just by counting the frequencies of transitions. So let's start by thinking about the general case. So consider an end state mark-off chain. The parameters of an end state mark-off chain are gmark-off chain are going to be the elements of the transition matrix, cat bottle P. So what do each of these P's stand for? So remember that P sub one one stands for the probability that an individualy that an individual in state one is in state one next period. P one two is the probability that an individual in state one is in state two next period. That that that. So let why not, why one to y tnot, why one to y t be a sequence of generations that are generated from our end state mark-off chain. Then the proposed fitting procedure we're going to use is going to assign the following value tofollowing value to Pij. We're going to sum up over all of the T. So let's have a sample. So again, we're going to use a two state mark-off chain at first. Okay, so we have seven. That's enough. Okay,That's enough. Okay, so it's going to sum up over our seven values. And the one in bold like this is a function. And this is, why, equals S. It's called the indicator function. And it takes the valueit takes the value one if the condition below is true and zero otherwise. So let's go ahead and compute P zero zero according to this formula. Okay, so we're going to look for places where that subtices where that subti observation is equal to zero. So this one is equal to zero. This one is equal to zero. And this one is equal to zero. And so now we want to know, so this is what going to be one tat going to be one times is YT plus one equal to also zero. Nope. So that could say zero. These will all be zero's just because they're not zero's. And then we're now going to do zero. So that's goodero. So that's good so one. And T plus one is also equal to zero. So that gets a one. So this is, and then the last one is going to transition from zero to one. So it will get a zero. And if we sum upro. And if we sum up over all of the places that YT was equal to zero, we get one plus one plus one. So this comes out to one over three. And so we think there's a lot of potential intuition in undersintuition in understanding this. And we believe this procedure is intuitive. And if it's not necessarily intuitive, I think the right place to start is to compute the sum across the Pijs for given I,he Pijs for given I, where these Pijs are defined by this procedure. And you should think about what value do you get? And why do you think you get that value? You might already see it. But if you done it. But if you don't, this is kind of a, we find something, things like this to be useful exercises and understanding how the fitting procedure's work. Okay, so let's write some code that can countcode that can count frequencies. So our function is going to compute the transition probabilities for a two state mark-off chain. So we're going to specialize. The input is going to be a history witho be a history with the state values of the two state mark-off chain. It's going to return two values and alpha and a beta. Where the alpha is going to be the probability of transitioning from state ztioning from state zero to state one. And the beta is from state one to state zero. So how do we do this? Well, the first thing is let's check what the length of the history is. So how many observatiohow many observations did we see total? And then we're going to create this little IDX so the stands for index. And it's just going to be a counter. So this will take the values zero, one, two, two,ero, one, two, two, T. And now what we're going to do is let's find all of the places where the history was equal to zero. So this is going to give us some truths and falses. It's right here based onright here based on whether that element of the history was zero or not. And we want to make sure we're not looking at the last step because we won't be able to see what comes after it. And we do ther it. And we do the same things for the ones. And so what does our counting procedure look like here? Well, let's go ahead and sum up all of the, So this is kind of clever. So let's talk through it. Ss talk through it. So zero, one, one, zero, zero. So we've created zero IDX's true and true. Zero, one, two, three, four, five, six, seven. Okay, so let's talk through what each of these things are. Sthese things are. So IDX is these numbers from zero to T. History equal equal zero is going to give us true, false, true, false, false true, true. And that and IDX less than T minus one is going to gus one is going to give us true, true, true, true, true, true, false. And so then if we index with those truths and falsees, we're going to get the numbers zero, two and six. Because these are valuesse these are values in which the history is equal to zero and are not the very last value in our observation. Okay, now we're going to sum up the history zero IDX is plus one. So the IDX is so now we'he IDX is so now we're moving them from zero to one, two to three and six to seven. So that's going to be this value, this value, and this value. And we're going to sum them up and so we get two and wso we get two and we're going to divide it by the length of these indexes, which was three. And we see that's because we moved from the state zero to one, two times and from zero to zero, one time. Ato zero, one time. And likewise we do the same thing to compute the beta, except now we're looking at the indexes that are ones. And so we want to know when they were zeroes. And so to get that we doo to get that we do one minus, because if we do one minus zero, it will be equal to one, so they will each count as one. And when it's one, it will be one minus one, so it will get set to zero. So ifset to zero. So if you didn't quite follow that, you should take a minute and break out each of these pieces of the function. And think about what's happening. And I often, when I write code, I breakwrite code, I break my code into as natural steps of possible. So in this case, I'm thinking about this is kind of pre analysis. In this step, I'm finding out where the zeroes and the ones are. And tthe ones are. And then this step, I'm using where the zeroes and the ones are to figure out what the corresponding counts should be. Okay. So now what we're going to do is we're going to use this couoing to use this count frequencies individual to check the accuracy of our fit. Let's see if we know what our parameters are. Can we uncover them if we pretend that we don't know them? So we're goinghem? So we're going to use this function and it's just going to be a length of simulation. And some parameters for alpha and beta. It's going to create the arrays that we pass into our simulation. We'our simulation. We'll then simulate to create an employment history and we'll always start the individual as unemployed in this case. So you can think about this as someone who's graduated from collegraduated from college and is initially unemployed. We're then going to count the frequencies and we'll call these alpha hat and beta hat. And then we're going to print out what the true alpha was ande true alpha was and what the fitted value was. Okay. So let's check the accuracy. Let's simulate for 10,000 periods with the same probabilities that we've been using. And what we find is we get rathefind is we get rather accurate results. So you'll find that all of the numbers we've had so far have been within a percentage or so. So that's good news. So it means that if we have enough data, our fe enough data, our fitting procedure is recovering the true parameter. Well, let's now think about what the interpretation of this is. So we're viewing this as monthly transitions in employment. So ifin employment. So if we observe 10,000 months of employment history for someone, that's a very long time. We're unlikely to have that much data on anyone. And so that's just not going to be what our ding to be what our data looks like. So what happens if we have less data? So maybe we have a lifetime of employment transition. So this is someone who works for 45 years. For 12 months a year with thenths a year with the same parameters. And you'll immediately notice that before all of our values were in the 0.24, 0.26. I think we had a 0.27. And now we're at 0.15 when the true value is 0.25. I wovalue is 0.25. I wouldn't call that a disaster, but you're certainly not as accurate. We could get a 0.3. And you see it just really depends on what the particular history is. This one's particularlys one's particularly inaccurate. It just depends on what sequence of draws you get. And you can't quite tell. But again, it's unlikely that we have an entire lifetime of employment transitions for vertransitions for very many people. And you might not even want to do that because you think their structural changes and how employment has worked. And so what if we just look at two years? What's goiwo years? What's going to happen? Well, it's not terrible. We're getting the job separation rate very wrong. If we re-run this, so this person over two years. So they became employed. And once they weed. And once they were employed, they never became unemployed again. And what you see is our fitting procedure gets very noisy. So what could we do to deal with this? How could we fix this? So the BLSfix this? So the BLS doesn't actually base their transition probabilities off of a single individual. Instead, they're using an entire cross section of individuals. So can we do the same thing if we ue same thing if we use a cross section rather than a single individual's history? The answer is yes. But in order for our frequency counting frequency counting to work, what we need is we need job find is we need job finding and job separation to be independent across individuals. So in our previous case, all of the work was being done by the Markov property. And we simply assumed that the transitmed that the transition from one state to the next today was independent of the transition from one state to the next tomorrow, conditioning on what the current state was. So what happens? So we couldhappens? So we could this break. So again, so formally what we're saying for independence is that the joint distribution over SIT plus one and SJT plus one, where I and J are individuals in our crossiduals in our cross section, conditional on their previous states and the parameters. Independence is going to be that joint distribution is equal to the product of their marginal distributions. So whdistributions. So what's the probability of SIT plus one given SIT and the parameters? Times the probability of SJT plus one given SJT. So what could violate this? Well, we thought of three examples.of three examples. I'm sure there's lots of others that someone could think of. But one example might be there's a changing government policy for one year. That results in a job guarantee. Well, thenuarantee. Well, then if two individuals are unemployed, then finding a job is not necessarily an independent event. You could have a technological change that results in the destruction of an entire iction of an entire industry. And if that happens, then there's going to be lots of correlated job loss. So everyone who worked in that industry is likely to lose their job. And if temporarily, you maytemporarily, you may also see a recession. And this recession could cause increased firing across the country. As a spoiler alert, some of these problems, some of these are going to be problems that ao be problems that are active in the data. And this is actually why we're going to allow for the fact for alpha and beta to move each period. Because we think, at least, roughly, the transition from ehe transition from employment to unemployment or unemployment to employment is close to independent on a period by period basis. And I think you could show examples of where that's not true, but we'renot true, but we're just going to assume in our model for now, that it's true. So how could we simulate a cross-section? So we're going to give it, give our function, an alpha and beta. And if thesebeta. And if these are going to be the same function of same arrays that they were before, then we're going to give it an S0, but we're going to change what S0 is and what the interpretation is. S0 ierpretation is. S0 is now going to be an array with two elements that represent the fraction of the population that begins in each employment state. And then we're going to specify a number of individa number of individuals that will be in our cross-section. And the output is going to be an N by T matrix that contains individual histories of employment along each row. So what we're going to see iwe're going to see is we're going to get the history. So we'll call this person 0 and period 0, then we're going to see person 0 in period 1, all the way until person 0 at period T, where T is again g, where T is again going to be the length of alpha and beta. Then there's going to be person 1. We're going to observe them in period 0. We're going to observe them in period 1, all the way up to T. Al the way up to T. And we're going to store this in a big matrix. Okay, so how do we do this? We check sizes again. We're going to check to make sure that you gave us two fractions that add up to 1. Athat add up to 1. And then we're going to figure out based on the number of individuals that the fraction of individuals that should be employed. How many should start as an employed? And so the firsyed? And so the first NZ individuals will all start as an employed. We're going to allocate room for us to store these employment histories. And we're going to set all of the individuals who start empiduals who start employed. We're going to set their states to 1. And then this turns out to be pretty easy because we've already done the work. Where now we can simulate the employment history and we'ment history and we've already explored this function. It just simulates a single individual's history. And we're going to give them the initial state that they'll start in. And we're just going to st're just going to store those values into the array that we've allocated. And then we're going to return it. So let's see what happens when we simulate 10 individuals for two periods. And we specifiedds. And we specified that roughly 35% of the individuals should be starting as unemployed. So we've rounded down and we got three. And so two of these three found jobs. One did not. And of the seven t. And of the seven that were employed, all of them stayed employed. So this seems plausible. So this is just with one transition. We could increase the number of transitions. And so now we have 0, 1,o now we have 0, 1, 2, 3. So that function seems to be working. Just as a matter of practice and kind of keeping with our data. If we when we import real data into Python, we're typically going to imppically going to import this data into a data frame. So let's go ahead and keep our simulated data in a data frame as well. So we're going to allow to give it a data frame that's going to have the alpoing to have the alpha and beta. So the job finding and separation probabilities. We're going to specify again the S naught. It's the fraction of the population that begins in each population. Stay inpopulation. Stay in each employment state. And we're going to have an N, which is the number of individuals. In our cross section. And so all we're going to do is we'll make sure that our alpha's andthat our alpha's and beta's are ordered by their date. We'll extract the alpha and beta arrays. We'll simulate our entire cross section. And we'll dump the output into a data frame. Okay. So let's see. Okay. So let's see whether this function works. So we're going to simulate. Let's initially just do six months. And we're going to have alpha equal to the same thing. It's been forever. Data are goiorever. Data are going to start in January 2018. And we're going to go forward six months. And let's see what comes out. So we have a single person. Oh, we just are looking at the head. So let's lookhead. So let's look at 12. So we're going to see this person, person zero. We're going to observe their employment history for six months. So this person took five months of being unemployed. And in tunemployed. And in their six months, they found a job. Person one was unemployed for two months. And then found and held a job for the remainder of their simulated history. Obviously, we can increasely, we can increase how many simulations we do. And it continues to work. So this is good news. Okay. So just to keep it interesting, let's actually pretend that we are the BLS. And so the BLS has thed so the BLS has their hands tied. And are actually not able to observe in individuals entire employment history. So what we're going to do is we're going to take our full employment history and we'rent history and we're going to restrict it. And we're going to pretend to ask the individuals from our generative model, the CPS questions. So if you remember, so if we have January, February, March, A, February, March, April, May, June, July, August, September, October, November, December. So if an individual, and we'll call this year one and year two, if an individual begins interviewing with thenterviewing with the CPS and February of year one, they're interviewed in February, March, April, and May. And then they are left out of the survey for eight months. And they're interviewed again in Fterviewed again in February, March, April, and May. And then they're no longer interviewed. Okay. So how could we do that? So what we're going to do is this is going to take a single individual, and tle individual, and then we're going to simulate an employment history, and it's going to interview that individual. So the inputs are going to be a data frame that have the columns person ID, so who aperson ID, so who are we talking to? DT, which is in what month are we talking to them and employment? And then we're going to have start year and start month as when the interviews occur. And so nowws occur. And so now the CPS is going to be a version of this data frame, but it's only going to contain the observations that would correspond to the CPS schedule for someone who starts interviewingstarts interviewing in start year, start month. So the way we're going to do this is we're going to create some date ranges, and we're going to talk about these when we talk about pandas, dates, and hpandas, dates, and how to work with them. And we're just going to put these together. So we're going to start the four months of observations in our first year, and then we're going to start four montg to start four months of observations in our second year. And then we're only going to keep data that's in those two sets of dates. And so how do we interview someone? Is we're going to pass this funing to pass this function, so we're going to do CPS interviews, we're going to give it some data frame, and then we're going to randomly sample from our year. Let's go ahead and make this an X. And thke this an X. And then we're going to group by the person IDs, and so we're going to randomly choose a year and an integer, and then we're going to group by the person IDs, and we're going to interviere going to interview that person and get a subset of the data, and then we're just going to drop some of the extra indexes that show up. So this will take a minute or so to run. Okay, so now that we'kay, so now that we've let this finish running, let's go ahead and see what this data looks like. So let's revert, let's revert, let me room to see about 25 observations. So individual zero was intervdual zero was interviewed in October of 2019. I see, so this person, we stopped interviewing them in 2020, because we haven't seen those dates yet. Okay, so, yeah, because we started interviewing in 2ed interviewing in 2018, zero one zero one, and we simulated two years of history. So because we interviewed this person in October of 2019, we stopped all of our interviews in January of 2020, so thiuary of 2020, so this person's employment history was cut short. But this person started in April of 2018. So let's go ahead and look at what happened to them. So person one was interviewed in April,terviewed in April, May June, July of 2018, and then April, May June, July in 2019. So this looks, this looks accurate. Let's go ahead and see how many individuals are we observing per month. So if Iper month. So if I remember, I believe we interviewed 5,000 individuals overall. So we're going to interview 5,000 individuals over the course of two years. And so what you see is, as we start interv, as we start interviewing people, there's not very much data, because this is when all of our employment history starts. And we're going to add about 200 people per month. And because of that, by theause of that, by the time we get to April of 2019, we're adding 200 people a month, which means that we have 200 people times four months, because you could happen in any of four months that you couldonths that you could start in any of four months and be interviewed in April. So if you started being interviewed in January, then you were still being interviewed in April, same with February, March,ith February, March, and April. And so that gives us 800, and there's about another 800 that come from individuals who started being interviewed in the four months from last year. So once our intervieSo once our interview is started, we expect to see about 1,600 people per interview, and that seems to line up with what we're seeing. So all of this is good. So now what we're going to do is we're gong to do is we're going to go ahead and fit to the cross section. So our data looks exactly like what the BLS uses. So how can we modify our frequency of transition concept to account for the fact thaunt for the fact that we're now observing cross sectional data? So the parameters of the Markov chain are still going to be the elements of our transition matrix P. But now our data is going to be, rea is going to be, remember, Y0, 0, Y0, 1, Y0, 2, Y0, Y10, Y11, Y12, and that's what we get from here. So Y10, Y11, dot dot dot. So the way that we're going to do this is we're simply going to add an aly going to add an additional sum. So if we look at this equation here, we're doing the exact same sum as before. But now we're going to add a sub m to stand for individual. And we're going to check w're going to check whether that individual experience state i during period t and state j in t plus 1. And then we'll do the same thing for the bottom. So not much new there. So how could we do this?w could we do this? There's some clever pieces of this function. So I'm going to walk through it. But again, I'd invite you to come back and think carefully about each step of this function. So our infunction. So our input is now just going to be a sample of individuals from our CPS survey. So this is going to have columns dt, pid, and employment. And the output of this function is going to be alon is going to be alpha and beta, which is the job finding and job separation rates. So the way that we're going to do this is the first thing we'll do is we'll put date and person ID on the index. ThID on the index. Then we're going to extract the date values. So this is just going to give us all of the date values. And we're going to do dot shift 1 by month. And so what is this going to do? Oh,his going to do? Oh, don't do that. What is this going to do? If this is going to transform 2020, 01, that's 01 to 01. So we're simply going to move things forward one month. And you'll see why we doou'll see why we do that in a second. Next thing we do is just extract all of the associated person IDs. So these are two vectors that are the same shape, same height as the original data. Then we'real data. Then we're going to create another index that has the shifted dates and the original people, person IDs. And now we're going to reindex the original data by using this new index that we've crindex that we've created. And so then we're going to reassign the column employment to unemployment T plus 1. So what this is going to give us is we're going to have DT and PID and employment, whichd employment, which will eventually be named Employment T plus 1. And it's going to associate. So we'll have 2020, 01, 01, 02, 01, person 0, and their employment value in February of 2020. And what'sof 2020. And what's going to happen is we won't have a value for January. But this will be the same size as our original data frame. So now we're going to reset the index on both data frames. So DT anata frames. So DT and PID will become columns. And then we're just going to concat them horizontally. So we're going to set access equals 1 and we're going to keep DT and PID and employment from the omployment from the original data frame. And we're only going to keep employment T plus 1 from the new data frame from the data T plus 1. And then we're going to drop any missing values because these alues because these are associated with when individual stopped being interviewed. And then we're going to make sure things are integers again. And so what we're going to have is we're going to have da're going to have data frame that has DT, PID, employment and employment T plus 1. And this is going to have a date, a person ID and an employment status. So let's say that the individual was unemployividual was unemployed and that in T plus 1 they were employed. And then we're going to see the next month's date. So this is going to be February 1. Now we're going to have the same person ID. Exceptme person ID. Except their new employment must line up with what the T plus 1 set. So that's what all of this code is going to do. And then this could be in 1, which would mean that there would be a 1t there would be a 1 here until we get, we had a missing value. But we would have dropped it that row. So that's roughly what all of this code does. Once we have the T's and the T plus 1's, we can jusplus 1's, we can just do exactly the same thing we did with the other frequency counting function, where we find all of the zero states. And we take the mean across just the one values, or the mean oflues, or the mean of 1 minus the zero values. So we're also going to write another function to check for the accuracy of our cross sectional work. So what happens when we have 1,000 individuals? We ob0 individuals? We observe them for up to two years. And these are our alpha and beta. Well, there we go. So that looks pretty accurate. This will again take another second. So that's not so bad. It's's not so bad. It's not as good as when we had 10,000 observations, but it's pretty good. And when we do it with 500 observations, we see some success. We're going to see that this is less successfulis less successful than with a thousand, but it performs relatively well. And by the time we get down to 100, our data starts being pretty noisy again. If you think about this, the BLS is interviewine BLS is interviewing approximately 60,000 individuals per month. And so if this generative model is correct, we should expect that what they're doing is pretty accurate that they're uncovering the rire uncovering the right transition probabilities. So I think this is good news in terms of what we've learned for our generative model and our fitting process. And so the last thing we're going to dog we're going to do is actually we're going to go ahead and take our model and we're going to fit it with real CPS data. So we've downloaded and cleaned for you. You're welcome. A subset of CPS data fsubset of CPS data for the years 2018 and 2019. So let's see what our constant parameter model does with this data. So let's go ahead and load our data. We'll take a look. Notice this looks very similhis looks very similar to the data we've been generating. We have a date time. We have a person ID, but they create their CPS creates their person ID slightly differently in that the ID is generated bhe ID is generated by using the first for the first four digits is generated by the year that their interviews begin. And the next two digits are generated by the month in which they are first interviey are first interviewed. So their person IDs are more meaningful than our numbers zero one two, etc. So let's find some employment histories. So what we're going to do is we're going to group by theing to group by the person ID. And we're going to count how many times a DT shows up and we're going to sum up how many periods they were employed for it. And then we're going to sort by these countsort by these counts and look at the bottom. So we're going to just grab one of these identifiers and we can see an individual's employment history. So this person started being interviewed in June 201erviewed in June 2018. They were employed and they continued to be employed for all four months in which they were observed for 2018. And then when we return in 2019, this individual is still employedal is still employed and continues employed throughout the remainder of their history. So let's try and find someone that's not employed for the entire time. So let's search for IDs in which we have es in which we have eight observations for date time. But the sum of the employment states is less than eight. So previously I picked a this ID out of a hat. It looks like there's plenty of others thatlenty of others that we could have chosen. And so what we see is this person was wasn't employed in 2018 in July was when they began their interviews. And they were employed for all four months that wl four months that we observed them in 2018. But fast forward to 2019. We now see that this person is employed in July, but they lose their job from July to August. And during September and October, tember and October, they continued as an unemployed individual. Let's see if there's someone who finds a job. Oh yeah, so here's someone they have an unfortunate history I guess. So this person was empthis person was employed for all of their 2018 interviews. They were employed for their first 2019 interview. Became unemployed. Then became employed again. And then became unemployed again. So I donoyed again. So I don't know what it happened. But there we go. And so now since we've done all of the work and we've our data is in our clean format. All we have to do is apply our CPS count frequenciCPS count frequencies function and it's going to return to us two numbers. It's going to return an alpha and a beta. And the alpha is going to be the job finding probability. So notice in 2018 and 20otice in 2018 and 2019 these were relatively good economic times. And so there was lots of job finding. So the probability of finding a job if you were unemployed was about 0.37. And the probability ond the probability of losing your job was only about 0.01. And so that kind of wraps up what we're going to do today. We're going to continue this lecture. So recall one of the goals that we have withls that we have with this lecture is to kind of build a model, fit it. And then do some kind of forecasting or prediction where we start thinking about what could happen in the future. And to do thature. And to do that we're going to talk about a model in which these transition probabilities, the job finding in job loss rates can fluctuate over time. And we're going to see what comes out of thatt comes out of that model next time. So talk soon." metadata={'source': 'transcripts_tiny/2.11.1 Lake Model.srt'}