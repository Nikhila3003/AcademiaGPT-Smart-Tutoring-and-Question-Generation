page_content="Hello, this is Spencer Lyon and today we'll be talking about different storage formats and their pros and cons for storing and accessing data from within Python and pandas. Our goals forandas. Our goals for today will be to understand the various different file formats available to us. We'll also understand where we can read more documentation and get help on how to get data in and oto get data in and out of our Python programs. And finally we will learn some rules of thumb for when to use each of the following file formats. CSV, Excel, Feather and SQL. The outline of our discusstline of our discussion today will be that we'll first discuss what the different file formats are and talk about their relative strengths and weaknesses. We'll then show some examples of how you canples of how you can take existing data you already have in pandas and then write them to these different file formats. Next we'll learn how to read them back from the file system into a Python sessionnto a Python session and we'll end with some practice. We'll start by importing Python or sorry importing pandas as PD and numpy as np as we'll use these throughout the lecture today. Let's talk aboutay. Let's talk about file formats. As you may know data can be saved to your hard drive or a permanent storage location in a variety of different formats. Pandas understands how to read these differenread these different file formats and construct a data frame from their contents as well as write the contents of an existing data frame to a file formatted in these special ways. We will strongly enWe will strongly encourage you to look at the official documentation for getting data in and out of pandas for a more complete understanding of the different options and features that are available.that are available. Our goal today will be to cover the most widely used formats for storing your data so that you can understand them when you come across them as well as be able to use them in yourto use them in your day to day work. The file type we'll talk about is called CSV which means comma separated values. Data stored in the CSV file is stored as plain text or a Python string. If we weren string. If we were to open a CSV file with a text editor we'd be able to read its contents without any problem. The way a data frame would be stored in the CSV file is as follows. Each row of our da. Each row of our data frame would be represented as a single line in the CSV file. The different columns in the row would be written out directly in their string representation and each column is sepd each column is separated by a comma. This is where the name comma separated values comes from. Some of the benefits of using a CSV file format are that they are widely used you're probably already fe probably already familiar with it. If not no worries you will be by the end of this course, but it's likely that you've come across them before. Another benefit is that it's a plain text file that cain text file that can be opened by any computer does not require any specialized drivers or software to open. Which makes it in some sense future proof you know you'll be able to open your files in topen your files in the future and access your data. And finally the last major pro for a CSV file is that it is understood by almost all data software. From spreadsheet programs like Excel to programmke Excel to programming languages like Python or R they all have very strong support for using CSV files. However there are some downsides using a CSV. The first main one is that it's not the most efft's not the most efficient way to store or to retrieve data. In the second slightly more subtle but perhaps more pernicious issue is that there's no formal standard for how a CSV file should be writteile should be written. So there's a little room for interpretation on how to handle some educations. For example would be how do you handle a data value that itself contains a comma. Perhaps you're wra. Perhaps you're writing currencies and you're having a comma separating the thousands. How should that be encoded in a CSV file when there's already used a comma is being used to separate columns. Tseparate columns. There are ways around this and some very common workgrounds or common implementations. It can become be too worried about this problem but everyone's in a while you'll find that oneyou'll find that one data set that's produced by a particular software won't read nicely into another software because of a lack of a standard. When should you use a CSV file. My real of them is thateal of them is that it's a really great default option for most use cases. I would use it if I don't have too much data meaning if I have data less than a million rows for example and less than maybeand less than maybe 30 or 40 columns see this view would be a great option. Now let's talk about another very common format called excel s or excel sx. This is a binary file format that is used by sprthat is used by spreadsheet programs like Microsoft Excel. When I say binary instead of plain text like a CSV what we mean is that if you were to open an excel sx file in your text editor you wouldn't editor you wouldn't see normal characters in your language you would instead see a computer representation of binary representation or bite representation of the data. Some of the benefits to the exe benefits to the excel sx file format are that it's very commonly used in a variety of industries. In particular the finance industry uses a lot of spreadsheets and a lot of their data ends up livingdata ends up living in the excel sx file format. Another benefit is that because excel is very widely used in some of these industries it's very easy to collaborate with colleagues that are already us that are already using excel. If you can exchange data using the excel sx file format. An example would be if you do some analysis in Python with your new programming skills you could then save thecould then save the output as the file ready to be read by excel. Send that to a colleague and make it open immediately in their spreadsheet to analyze what your output looks like. Some of the cons abSome of the cons about an excel sx file format are that it can be very slow to read a lot of data from one of these files and popular data frame as well as it is very slow to write a large amount ofe a large amount of data. From a data frame into an excel file will show you an example of this later so you have some intuition for the order of magnitude we're talking about. Another issue which somther issue which some may term as a feature. Is that the spreadsheet programs will store both the data itself as well as what I'll call metadata. Things like what color should this column or cell be ocolumn or cell be or what charts should be included in there. And the reason this is a problem is that this metadata is not always easily understood by other programs. You're sometimes locking yoursetimes locking yourself in to using a spreadsheet based program if you include all of this alongside your data. The last major issue would be that it's not human readable if you were to open this in yoe to open this in your text editor. You wouldn't be able to interpret what's going on immediately. So when should you use it? I think the two main use cases that we would recommend are if you're alreaare if you're already working with somebody else and there are using an excel based workflow and unable to learn a different environment at the time. You should use what's easy for collaboration andr collaboration and in this case might be an excel file. Another option would be if you're trying to present data and you would like to apply a special type of formatting or display to the data excely to the data excel is the only option we'll talk about today that gives you that choice. Next one we've onto the par k file system or file type. So par k is a custom binary format that was speciallythat was specially designed for reading and writing data stored as columns and it was designed to make that operation is efficient and fast as possible on modern computer hardware. This fits nicely w. This fits nicely with the notion of a data frame as we've learned because a data frame is a way to collect multiple columns or series of data alongside one another. What are some of the benefits ofof the benefits of the par k file system first it is extremely fast relative to CSV or excel sx you will often see. 100 or 1000 time improvements in the speed for writing or reading a large data setng a large data set between pandas and a file system. The second major benefit of the par k file system or file format is that it naturally understands all of the data types that are commonly used inre commonly used in pandas including a multi index data frame. The reason this is significant is that you can store the data in a par k file format from a data frame and with the data par k will rememata par k will remember what each of the types were for every column. Then later when you go to read this back into pandas during a subsequent analysis. Data types will also be read in and your data wd in and your data will be immediately usable and you can resume exactly where you left off. In contrast if you are using a CSV or xlsx file format you might have to remind pandas that a column shouldthat a column should be a date for example or that a column full of numbers should actually be integer instead of floats. This type of issue doesn't happen with par k. Another benefit is that it's a vfit is that it's a very common it's almost the common denominator for big data systems like adup or spark. And finally the last point here we've written that it supports various compression algorithmsmpression algorithms. What this means is that it will take the data that you are storing and if there's a way to make the file size for this representation on your hard drive smaller it will make thater it will make that happen and this will happen automatically for you. The only major con for a par k file format is that it is a binary storage format meaning that you can't open it up and easily init up and easily inspect. It's contents without using software specifically written for handling the park a types of files. When should you use it my rule of thumb is that I have a not small not necea not small not necessarily big but. 100 megabytes or more amount of data that doesn't change very often but that I might read multiple times for repeated analysis. I'll often store it in a par k filee it in a par k file. Also if I have a lot of data that I need to store and the compression. I'll use it in a larger of par k matters to me. It's often a reason why I'll choose it and then finally. Ifand then finally. If I know that I will be reading this data from many different systems or if I need to write out multiple data sets. The speed of par k can be a very large benefit over one of the ott over one of the other file types. Now we'll talk about a sibling or a cousin of the par k file format called feather. So again feather is a custom binary file format designed for efficient reading aefficient reading and writing of data stored in columns sounds familiar. That's just what we heard about par k. However, there are a couple differences so first it is extremely fast even faster thanst even faster than par k and one of the benefits for this is that or one of the reasons for this is that it doesn't do compression. It will store the data in an ideal format for reading it for writineading it for writing it very, very quickly to your hard drive and then reading it back out as fast as possible. In fact, the main author or creator of this file format is named as west McKinney, who'west McKinney, who's also the original creator of pandas. And when he finished writing the feather file format and was doing some tests to see how fast it was. And how well it used all of the resourcd all of the resources on his computer. He said he was not surprised at all the find that it completely saturated the input output capabilities of his CPU on the first try. It was designed specificalldesigned specifically for that. Some of the cons are that it can only be read or written from Python and a handful of other languages. It's a relatively new file format only about four and a half yearfour and a half years old and so most files out there won't be in this format to begin with often you may download them say as a CSV and then if you want this extremely fast reading and writing for lag and writing for later you can store it yourself as a feather file but it won't come that way. And finally, it only supports the standard Python index or shri pandas index that goes starting at zeroes starting at zero and then increasing integers up to the number of values minus one. So in order to store your data in a feather format first you need to call reset index to get that default one andthat default one and then once you read it back you also have to call set index to restore the index you are working with. So when should you use this I kind of think of it as an alternative to parquelternative to parquet when that last bit of efficiency and speed really matters. This will happen when I have a very stable data set that I'm not changing very much over time, but it's quite large andit's quite large and could take a significant amount of time to read in a less optimized file format. And finally you should only use this when you know that you need to access the data from a programdata from a program that supports the feather format. Three languages that are common you use in data analytics and scientific modeling our Python are in Julia and they all do have good support for te good support for the feather format. So if you're working with colleagues on a project using some some set of these languages you're okay to use the feather one. Okay we're going to continue we're nto continue we're nearly finished with the different types of file formats we have one more dimension here and it's called SQL SQL is an acronym that means structured query language. It itself isn'tge. It itself isn't really a file format but it's often associated with and talked about as a way for reading and storing data. It's used for interacting with what are known as relational databases orational databases or relational data management systems. And if you want more information we encourage you to follow this link we won't talk too much about this because SQL SQL is a very large topic aa very large topic and we don't have time to cover that in class. Some of the benefits are that it's well established in industry and much of the data in the world it's very likely that the vast majorthat the vast majority of data lives inside a SQL database. That's not some fact that I've done research on invalidated but given my experience in industry and talking to many people who have workedple who have worked in industry for a while SQL is kind of a needed skill in today's work environment. The major con is that it's complicated just like Python was a new language you needed to learn inu needed to learn in order to do this course SQL or SQL is another language you need to learn in order to extract data or save it into these systems. You should use it when you were workflow already ie workflow already incorporate SQL or you know that you'll need the benefits of a full database system and we'll maybe talk more about that in a future lecture time permitting. Okay the next one wouldy the next one would be what's called json or the JavaScript object notation. JavaScript serialized object notation and this is a very common way to store data and it's especially common when gettingcommon when getting data from an API. And the way that you a Python programmer would maybe want to think about json is that it's stored as plain text and looks very much like constructing a dictionaryructing a dictionary by hand using the curly brace notation. The pros of being familiar with and using the json file format are that it's extremely common and very frequently used when dealing with thwhen dealing with the web. Another benefit is that it naturally maps into a Python dictionary a data type that you're likely already familiar with. Another benefit is that it's very easy to read for hy easy to read for humans and the final benefit and this is in contrast to the other file formats we've talked about today. Is that it can store non tabular and on rectangular data you can have differyou can have different columns or keys in each of the objects of a json file. As well as you can have nested objects and in this sense json is the most flexible of the file formats we've talked abouts we've talked about so far. Some of the cons of using this file format are that it's pretty inefficient. For example, if I had a data frame with five columns and 500 rows. The standard way of writingndard way of writing the json would be one large array or list where each entry of this array is looks like a Python dictionary. And I will list out that column a in the first row has a particular valhas a particular value and then column b column c column d column e. And I'm actually going to have to repeat the column labels a b c d e for each of 500 rows for my data set. And so you'll see that Iso you'll see that I'm repeating data somewhat unnecessarily and this is kind of the flip side of json's flexibility. Is that because I'm allowed to have different keys or columns from one row to thefrom one row to the next. I have to specify what the keys columns are to make it unambiguous. In addition to being inefficient in terms of storage capacity, it's also inefficient in terms of reading an terms of reading and writing speed. And the final downside to json is that it is ambiguous for how to represent certain data. The example I walked you through would be an example of having an arrayof having an array or a list of records. But you could also think about it a different way and you could have a single record or object or dictionary filled with arrays. And the five column example yive column example you might imagine that there is a single dictionary and the key a standing for column a would represent a list would be represented by a list or an array of all the values in columnthe values in column a. You could move on to b c d and so on. This would be a more efficient way to store the data. But which one is chosen is somewhat ambiguous and you have to really inspect the datally inspect the data before you can tell us inside of it. The rule of thumb that I follow for when to use the json data is when I'm going to be interacting with the web and using a web API. And alsoa web API. And also if you want to store a relatively small amount of data that may not have a uniform rectangular structure. Json is a great option for those two use cases. Okay, now that we've discunow that we've discussed the various different file formats that Python and pandas are aware of let's talk about how we can take data that we have inside of a data frame and store it using one of thesit using one of these file formats. The rule of thumb to keep in mind here is that if I have a data frame named df and I would like to save the data in the file format. I would use the df dot to foo mthe df dot to foo method so I would call the two foo method on my data frame. For example, if I wanted to use a CSV I would write df dot to CSV and that would be the method that can allow me to writean allow me to write a CSV. Let's see this in practice. So first we're going to create a few data frames. And they're going to be filled with totally random data and the first one is going to have fouis going to have four columns called abcd. And then it's going to be filled with random integer between zero and 100. The second data frame is going to be filled with random floating point numbers andng point numbers and it's going to have 100,000 rows and then we're going to target that the output data frame will be will require 10 megabytes of storage. If you wanted to change this from 10 to saythis from 10 to say five or 200 just to experiment with the different efficiency properties of the file formats feel free to come back to this cell change the line that I've highlighted right here. Aighted right here. And we will construct for you a data frame of random numbers that is approximately this size. So you'll see here when I ran this cell with a size of 10 I got a data frame that was aata frame that was about 9.91 megabytes large. Okay, so let's start with df dot to CSV. You'll notice that this heading here is a link and if you wanted to look at the documentation for all the differn for all the different options you can follow this link. This will be true for each of the different data frame dot to methods we will be discussing. So first if we just call the dot to CSV method widot to CSV method without any additional arguments pandas will return to us a string containing our data frame in CSV format. You notice here as we discussed earlier this is just plain text it's a Pytlain text it's a Python string that we can read by hand. Notice also that our four columns are all represented here a bc and d and they're separated by a comma. The first column here is blank on the fre is blank on the first row because this is where the name of the index would go if we named it. And you'll see here that the values after this first row of column labels in this first column we havefirst column we have zero through nine which is our standard index on our data frame. All of the actual data would be included after this first column in each of these rows and it's just the random ins just the random integers that pandas created for us. Now if we do pass an argument to the two CSV method pandas will use the first argument as a file name. Instead of returning to us a string with to us a string with the CSV version of our data frame it will actually construct that string and save it to a file with the name we pass it in the first argument. So here we're going to say two CSV andg to say two CSV and we'd like to name the file df1.se sv. We could have this happens to match the name of our data frame object but we could have chosen anything we wanted here we could have writtencould have written my nice data frame and it would have worked just as well. We can actually use Python's file system integration to check to see if our df1.csv file exists or if you're working on ayou're working on a Linux or Mac based system you could also do exclamation point ls which will list the files in the directory. Notice that we have this df1.csv we also have the my nice data frames Cy nice data frames CSV that we saved to the second time. And these are the different Python notebooks we've been working through today. So now if you remember we have df2 which is about 10 megabytes labout 10 megabytes large as 100,000 rows and enough columns to take up 10 megabytes we can go ahead and see how long it takes Python to write this data frame out to a file. We're going to use here theoing to use here the percent percent time this is the ipython or Jupiter way of calling what they call a sell magic in this case what it does is this is not actually run as code. But it will tell Jupiut it will tell Jupiter that the entire cell. Should then be executed and we would like for Jupiter to keep track of how long it takes. So we could have as many lines as we needed to after the percentto after the percent percent time and Jupiter will record the total amount of time our computer spends running all of this code here it's the only line is to save df2 to a CSV. And here we see that ithere we see that it takes 1.37 seconds not too bad but this is only 10 megabytes of data you can imagine having a much larger data set. And then this time could could increase accordingly. Keep it inordingly. Keep it in mind though as we're going to compare this to the other file formats coming up. The next file format we talked about was the XL as x file format and this is what's used by the excat's used by the excel spreadsheet program. So pandas has a method to underscore excel. Now when we call this what happens is the first argument is the name of the file and the second argument is goinond argument is going to be the name of the sheet. The worksheet inside the spreadsheet inside the workbook and here we're going to say we'd like the file to be named df1 dot excel sx and then let's nsx and then let's name the the worksheet let's name this first data frame. So we run this and is created the XL file for us which we can verify if we were to run L s again now we have this df1 dot exhave this df1 dot excel sx. And if we were to open this in our spreadsheet program we would see that there's a sheet called first data frame whose contents match that of df1. One other benefit or featther benefit or feature of the excel file format is that you could have multiple sheets of data. In order to tell pandas to have a single file containing multiple sheets of data we use the pandas exceuse the pandas excel writer class. Here's how it works. We will say with pd dot excel writer and we'll pass the name of the file we'd like pandas to create. As writer. Column and we'll have an indentwe'll have an indented block here and we'll say right the df1 data frame to excel using this file writer so now instead of writing the file name here we pass this writer object. The second argument ise second argument is still the name of the sheet and then we can do another data frame here let's add 10 to our data frame. Right it to excel using our excel writer and we'll name this sheet df1 plusthis sheet df1 plus 10. If you haven't seen the with as syntax and Python before this is known as a context manager. What it's used for in this scenario is. Any the file after this line is executed thline is executed this file will be created and open for the pandas process to interact with. Then while the file is open all of the lines in the indented block will be executed in this case we're goithis case we're going to write these two sheets into the file and then when the indented block stops or closes the file will be closed also. So this is a way for pandas and Python to manage opening ato manage opening and closing the file without us having to do that ourselves. There's a additional but additional benefits to using the context manager in this way but that's effectively what's happectively what's happening under the hood. Here's where we describe what that is and. There's some more slides that you can look back on to understand it so now what we'll do is I'm going to start writgoing to start writing this cell and we'll talk a little more about it so we're now going to take our very large. How much larger 10 megabyte data set contained in DF2 and we're going to try to writeoing to try to write this out to a file called DF2 dot excel sx. As you can see since we arrived on this slide and I started it's still running that took my computer which has a very good CPU and veryry good CPU and very fast memory 18.8 seconds when I ran this on a different machine it took 25.7 seconds. Compare this to this CSV version which took 1.37 seconds on my computer as you can see the exs you can see the excel file format was. Quite a bit slower more than 10 times lower than the CSV for saving this larger data frame. So now we talked above about the feather file format and it was. Inormat and it was. Invented or created specifically to make writing data frames either in Python and are as efficient as possible. The best support for this file format comes from a package called piepackage called pie arrow is not installed by default so if you don't have it you should go ahead and uncomment this line by removing the first you characters and then run this cell. I'll do this it'sl. I'll do this it's going to tell me that I've already have it installed but if you don't yet in your current environment you'll see some additional output here. So now when we call the pie arrow feall the pie arrow feather dot right feather function we now have to pass the DF1 as the first argument this is a little different than the other methods we saw because it's not a method on our data frathod on our data frame. It's actually just a function so in order to make it aware of which data we'd like it to write you have to pass that is the first argument. So we can write the first data framethe first data frame as the F1 dot feather and then let's check how long it takes. Hi arrow to construct a data frame containing our 10 megabyte DF2 if we run this it took 18 milliseconds and I'm actuseconds and I'm actually going to use a different so magic here called time it which will run the code many times and take an average for us so we get a sense of on average how long is this taking. Thg is this taking. This should complete very soon so it says it was able to run that block a code in that couple seconds. 7 times per run and it did a hundred times so it actually constructed this fileonstructed this file 700 times each time taking on average 14 milliseconds. So now we had that the CSV file the CSV time was 1.37 seconds and the feather time was 0.014 seconds or 14 milliseconds. It14 milliseconds. It is almost a hundred times faster and this is what we were talking about earlier when we said that the feather file format is extremely efficient and fast for reading and writing daading and writing data. We constructed a table of the results for these three file formats when we initially ran this on a different machine we can see a similar pattern the CSV was about 10 times fass about 10 times faster than the Excel which was and then the CSV was about 50 to a hundred times slower than the feather file format. Now let's talk about how to read data from a file into pandas jusfile into pandas just like we had the DF dot 2 underscore methods we also have a similar pandas dot read underscore family of methods for reading data from a file into a data frame. Note that the diff. Note that the difference here is that the two methods are usually defined as a data frame method but because when we're trying to read data we don't have a data frame yet the reading routines are deding routines are defined as functions within the pandas namespace. Now the reading methods have quite a few more options because. Depending on the preferences or behaviors of the person and softwareperson and software that created the file there may be different edge cases that need to be handled. So there are going to be many more arguments on average for the read family of methods relative tomethods relative to the two family methods. We'll explore these more in the future when we talk about how to clean a messy data set and for now we're just going to show you how you can read in the idecan read in the idea case for a few of these file formats. And to do this we're actually just going to read the files that we just created so that we can ensure that what we get back matches the dataack matches the data frame that we started with. One more time we started with a data frame DF1 we did dot 2 CSV for example. And now we're going to try to read the data frame from the DF1 dot CSV filthe DF1 dot CSV file and make sure that what we get back matches the DF1 inside of our panda session. First we're going to read the DF1 underscore CSV we're going to construct a new data frame by reaew data frame by reading the CSV file. If you remember for the first column was filled with our index so we're going to say pandas the index is contained in the first column which pandas starts countipandas starts counting at zero and let's just look at the first five rows for now that looks pretty good. And check the same thing for the Excel file. And if you remember we changed the name of that fd the name of that file of that sheet when we saved it so let's go back and we called the sheet name first data frame. So this is what we need to use when we read it. We'll change the DF1 here to firsthe DF1 here to first data frame and there we go we get back data frame that we started with we can see that this matches the first five rows of the F1 showing the head of that data frame also. Next wa frame also. Next we can need the feather file that we saved and notice we didn't have to do the index call and the reason for this is that feather only ever saves. Data frames that have the defaultat have the default range index starting from zero and going up so there were never actually saved it to the file so when we read it back in feather knows that because it didn't save the index and itve the index and it was the default one when it reads the file it can just attach a default index and it will match what we gave it. So a little shortcut they take to preserve both space and time whenspace and time when reading and writing the file. One very nice feature of the read family of methods is that in addition to passing a file name for a file that lives on your local hard drive. You cal hard drive. You can also pass a URL for a file that lives somewhere on the internet. In this case we will pass the we will construct a URL for where you can find our DF1 data frame. In the GitHub reme. In the GitHub repository for this course will then tell pandas to read CSV from that URL and we'll again remind it that the first column should be treated as the index. When we run this we see tharun this we see that the data again matches the F1 this happens because we saved. Before the lecture I saved it to the F1 to a file uploaded it to GitHub so that when we read it it will match what wet will match what we have. The key here is to know that we can pass a URL here and pandas will do the necessary network operations to go find the file and use it. Okay my turns over now is time to prar now is time to practice what I'll do is I'll set up and walk stuck you through a problem that will have you work on and then we'll be able to stop the video and have you work on the problem yourselfthe problem yourself. So in the cell below you're going to have a variable called URL and this points to a file on the internet containing the result of all the united states NFL football. The first tootball. The first thing that we do is to read the data from the game from September of 1920 all the way through February of 2017 the NFL stands for the national football league and is the official prd is the official professional American football league in the United States. This data is to do the following first you need to use the PD or pandas dot read CSV function to read the data from this Uthe data from this URL from the file pointed to by the URL into a data frame that you'll call NFL. You'll then print out the shape and the column names of NFL so that you can see what the data looks lhat the data looks like. Then we'd like for you to save this data to a file on your local hard drive called NFL dot excel sx and we'd like for you to use the appropriate. Two method that matches the xd that matches the xlx file format if you need to look back at what we just work through together feel free to do that. And then finally if possible we'd like for you to open this spreadsheet on yourspreadsheet on your own computer if you have spreadsheets software install. If you're able to work through that really quickly and it doesn't take you long maybe you can try doing some analysis we donsome analysis we don't have any hard expectations for what you need to do here. But here are some suggestions for what you might try these four bullet points contain them and you can go ahead and lookan go ahead and look at what they are and do your best to evaluate that. That's the end of this lecture though and we please spend time working through this exercise and best of luck. Thank you." metack. Thank you." metadata={'source': 'transcripts_tiny/2.2.2 data storage formats with pandas.srt'}