page_content="Hi, this is Tom and today I'm going to talk with you about the finite dimensional Markov chains. And my text for this for which I'm going to basically provide a reader's guide is the Quar's guide is the Quanty Con lecture on Markov chains, which is on the Quanty Club website. Okay, so basically the roadmap is we're going to define a Markov chain. See how to construct it. We'll talk auct it. We'll talk about simulations and we'll construct marginal distributions in it. And then we'll talk about these concepts of irreducibility and non-periodic markov chains. And those are going tod those are going to be important because how they relate to stationary distributions. We'll talk about the concept of stationary distributions. There's notion of stationary and irreducibility. We'llreducibility. We'll talk about those. And we'll talk about how to approximate expectations. So let's get rolling. Okay, so what you're going to find is a Markov chain is just widely used in economics.y used in economics. In the arts machine learning actually encryption. And it's the work force of building dynamic models with is some randomness. So as powerful as they are, we already have the toolsready have the tools to study them. The key tools are going to be a little bit of linear algebra. And basic probability theory. So this lecture is accompanied by a notebook, which you would be able toyou would be able to run a Jupyter notebook. So as usual, here's our Quanty kind of various things we download. We import if we're going to be using Python as we are. Okay, so we'll just start with sol just start with some definitions. Key definition is there's going to be a matrix. And it's going to be a stochastic matrix. It's p, we'll call it p, and it's an n by n. So it's a square matrix. Eachsquare matrix. Each element, Pij is strictly positive. Well, actually non-negative. We'll see non-negative because it's going to be a probability. And this is going to be a conditional probability. Sional probability. So it's a matrix full of conditional probabilities. And each the row sums. So if we sum across it for every row, this is going to be one. So each row of Pij is a probability distribprobability distribution in itself. It's itself a probability distribution over n possible outcomes because J goes from one to n. And we're going to talk about what these mean. And we call this a stod we call this a stochastic matrix if it satisfies one into. So that's how to read that. So it's worthwhile staring at that. So we have a set of non-negative matrices. It's implied by this. This actuaby this. This actually implies. These two things will imply that Pij itself is a probability. It's some number of less than or equal to one greater than or equal to zero. That's for all IJ. So each eor all IJ. So each element of the matrix is a probability. And what you can find is, this is what this if P is a stochastic matrix is a stochastic matrix. What implies P to the K is also for any K gres also for any K greater than equal to one. Well, actually greater yeah, greater than equal to one. So that's what it's matrix is. So a stochastic matrix is it's a key thing that it's a big part of at's a big part of a mark-off chain. So one thing to define a mark-off chain is to define a mark-off chain. We need a stochastic matrix P plus something else. And we'll see what that something else is.t something else is. So to begin with, we're going to define some set of elements x1 through xn. And we're going to call that a state space. And these are the state values. So that's what the set set's what the set set is is. So the mark-off chain is defined on a state space s. And what it is, it's a sequence of random variables. So it's a sequence of random variables. On s, that have something chat have something called the mark-off property. The key thing is what the mark-off property is. And the mark-off property is something about conditional probabilities. It's a restriction on conditionriction on conditional probabilities. So here goes. So we're going to have the where we're going to do this is x, that t is the random variable. Remember this sequence at t. And it could possibly takecould possibly take on various values. It's going to take on some value. It's going to belong. It's going to take on a value inside this set, the x1 to xn. So the mark-off property is this. It's thaty is this. It's that the probability that xt plus 1 is equal to a particular value y, condition on xt, that random variable, is equal to the probability that it takes on the value y, condition on noty, condition on not only xt, but past values of xt. So only xt carries information about future values of the random variable x. So stare at this. And this is intimately connected with this is the reawith this is the reason why we give this name, xt is the state variable. xt is the state variable. It completely summarizes the current position of the system. So we can write this. The probability, i. The probability, if we write the probability of xt plus 1 is equal to y, given xt equals x, as we write this is matrix, the p of xy for all xy. And yes, that's going to give rise to a matrix. And whto a matrix. And what this means is pxy. It's the probability of going from x to y in one unit of time, one step. So this is the one step transition probability. That's what this is called. So one stis called. So one step transition probability. And actually, if we go back to where we started, these pijs, those are just the one step transition probabilities from going from i to j. So here we go.o j. So here we go. Pij is the probability that we go from xy to xj in one step. And the i's and j's both go from 1 to n. That feels out of matrix. So here's how we to complete a description of a markescription of a markoff chain. We need p. And we also need something else. We need a probability over states at time equals 0. We need this pair. And a markoff chain is going to be, it's going to cons, it's going to consist of that pair. And the way we can generate wealth. So what this says is we're going to draw x0 from some initial distribution, from this distribution. And then for each subsequeen for each subsequent t, we're going to draw xt plus 1 from the transition probability. So this is how a markoff chain works." metadata={'source': 'transcripts_tiny/2.10.1 Markov Chains 1 (1 of 4).srChains 1 (1 of 4).srt'}