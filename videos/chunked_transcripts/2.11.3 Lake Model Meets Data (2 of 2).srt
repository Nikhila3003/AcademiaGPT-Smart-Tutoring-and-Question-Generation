page_content="The models we've considered thus far have many things in common. One of them is our treatment of the state S at time t equals 0. What we've done so far is exaginously shift or adjust theshift or adjust the state at t equals 0 to match the data observed in the COVID era. And this exaginously shift happened in a very literal way. What we've been doing is starting a simulation at t equsimulation at t equals minus 35. Letting the model dynamics apply to that state and generate its values up until time t equal 1. And then we pause the simulation. While the simulation's paused, we retion's paused, we reach into the model. You shuffle around some workers such that there are fewer employed workers at going in to period t equals 0. Then ended at t minus 1 such that the starting statat the starting state at time 0 matches what we observed at the low point in the COVID era recession. Once we're done with our reshuffling, we take our hand out of the model, we press play again and less play again and let the model dynamics run and march the state forward from that point on. Now we might want to be able to answer the following question. What change in alpha or beta at time minusbeta at time minus 1 could have generated the drop and employment seen at time 0. In other words, is there a way we can by only adjusting model parameters and not artificially moving the state generaing the state generate dynamics that are consistent with the data? Now thinking about our model, there are in principle two ways might happen. Either a decrease in the job finding rate or an increaserate or an increase in the job separation rate could lead to a lower percentage of employed workers. In order to understand how much lower kind of the magnitude of the shift, let's take a look at thetake a look at the BLS data for COVID era, just in periods minus 1 and 0. We see here that at time minus 1 in COVID era, we had about 95% of workers employed. And then in periods 0, we had only 85% o0, we had only 85% or there was a 10 percentage point drop in the number of within the percentage of employed workers between periods minus 1 and 0. Let's keep that in mind and think more about how wenk more about how we can use our model parameters to replicate this. So let's recall that the steady state job finding rate, alpha bar is about 37%. And the steady state job separation rate is about 1tion rate is about 1%. Now suppose that we believe that this sharp change in employment with dribbling driven entirely by a shift in the job finding rate. Now the most extreme shift we could considert we could consider would be to shift the job finding rate at time minus 1 from steady state value of 37%. So let's go ahead and do that and then we'll iterate one period using the model dynamics andmodel dynamics and see what the percent of employed workers would be if alpha minus 1 was set to zero. So what we'll do is we'll begin here and we'll say that the percent of employed workers at minusyed workers at minus 1 and here we use the sub-tree. The letter m to represent minus. We'll start that out at 0.955, which is what we just saw from the data on the previous slide. Well then say that tWell then say that the employed the percent of workers that are employed at period zero is going to be that's percent at period minus 1 times all the people that did not lose their job. Plus all the pjob. Plus all the people that were employed times the number who found a job and this was where the alpha minus 1 is set and notice here we're setting this exactly equal to zero and that's where we ind that's where we impose our hypothesis the alpha minus 1 is equal to zero. And we do this we see here that alpha minus 1 was at zero.955 if nobody found a job in between period minus 1 and zero theninus 1 and zero then the percent of employed workers would be at 94.5%. So shifting this job finding rate from 37% to zero only had a 1% point percentage point impact on the share of workers that haveof workers that have a job where if you recall we were looking for a 10 percentage point difference. What this means is that our shift. In this means is that in the context of our model this shift ofmodel this shift of 10 percentage points could not have been generated entirely from a shift in the job finding rate. In other words we've ruled out the hypothesis that only changing alpha could haveng alpha could have generated the data we saw in the covid area. So our other lever we have to pull in this model is changing the job separation rate beta. Now suppose that we set this to its most extthis to its most extreme value we go from 1% of jobs are separated to 100% of jobs and then we'll keep alpha at its steady state value. When we do this we apply the same law of motion we see that if eion we see that if everybody lost their job and only 37% of previously unemployed workers found a job we would move all the way from 95% of people being employed to only having 1.6% of people being em% of people being employed. So this validates that it is possible within the context of our model for a change in the job separation rate to be consistent with the covid area data. So this leads us toSo this leads us to a question. So suppose that we want to keep alpha fixed at its steady state value and then determine what value of the job separation rate at time minus 1 could have generated thed have generated the shift from 95% of workers being employed to 85% at time zero. Whatever this value is at time minus 1 that moves us from and forgive the type of here from 95% of workers being emplf workers being employed to only 85 we're going to call this value of job separation beta till. So we can rearrange the law of motion for the fraction of workers that are employed in order to solve foin order to solve for beta till back and it's going to be in terms of the percentage of employed workers for targeting at time zero. The percentage of them zero workers we observed at time minus 1 andat time minus 1 and the steady state value of the job finding rate. So when we compute this we see that the job separation rate. At time minus 1 would have to move from a steady state value of 1% alltate value of 1% all the way up to a value of 12%. If we do this and we've done our algebra correctly, but we should see is that without reaching into the model and artificially moving the state arounving the state around at time t will zero. If we just set the time minus 1 value of beta equal to 12% instead of 1% we should see the same. Quick drop in employment but the data show. Let's implementow. Let's implement this in a function. So here we're going to have a new model and it's the solution to its direct problem. We're going to start out with the state initially at its steady state. We'rs steady state. We're going to say that alpha and beta are fixed at their steady state values for all time except that at time t equal minus 1 we move beta away from beta bar and to beta till there. To beta till there. That's it. There's no manipulation of the state going on. We're just having a one period shift in beta and then we'll simulate and return our output. We do this. We can go ahead andWe can go ahead and we can run this and we can look at the outcome and we see here that the model is able to generate this very sharp drop and employment just like we observed in the data. The MSE isthe data. The MSE is between four and five which is what we saw back in model one where we artificially reached into the model and set the state from its steady state down here. And so what we've doned so what we've done is instead of externally moving the state we've allowed the model parameters to move the state for us. And now this rebalancing of employment to unemployment is happening all insis happening all inside the model. So if the outcome of the modeling experiment is the same in model one where we always keep the parameters fixed to their steady state values but reach in and change teach in and change the state. Compared to model three where we have a one time deviation in the parameter beta. If those two frameworks or models generate the same outcome have we really improved or wreally improved or what benefit every we gained. And the benefit comes because when we allow the shift in the time zero state to come from within the model instead of without we can do experiments. Ann do experiments. And we can start to ask questions of our model and then try to learn what other outcomes might have happened so one of the experience we might run. Is suppose that the government hadt the government had the capacity to save one third of the jobs that were lost at times zero. Yeah, how could it do this will maybe it was able to subsidize a certain sector or subset of the economy sset of the economy such that employers didn't have to fire or separate from as many of their workers. And we might want to say well if this was possible for the government what impact with that of hadact with that of had going forward if we stay entirely within the model we're able to answer questions like that. And in this way we're able to use the model kind of as a vehicle for doing these countor doing these counterfactual exercises. And and this is actually in the spirit of what were really after when we're doing computational social science what we do is we build a model that captures a fel that captures a feature or features of data that we'd like to better understand. And we use this model so that we can analyze the impact of decisions on the variables of interest. We can't set up ft. We can't set up for ourselves a physical laboratory in which to perform these experiments so we have to do so using our models. With that in mind let's now consider a model where we shift both betae we shift both beta at time minus one to generate this rebalancing of employment and we're also going to slow down the recovery of the model like we did in model two. In some sense we're combining boe we're combining both models two and model three. So to do this we have another function here that takes the following parameters we have a beta tilde and this represents what happens at time periodpens at time period minus one. We then have the three parameters go the three extra parameters from model one or model two which is the alpha hat and beta hat values that are going to be applied fromto be applied from time t equals zero through time t equal n. Once we've taken in these parameters are solution to the direct problem is this follows we start off with the steady state distribution ostate distribution of workers across employment and unemployment. We construct a vector of parameters that's the length that we would like our time series to be that is always fixed at the steady statd at the steady state. We then apply the adjustment from model three where we shock the time minus one value of beta to equal beta tilde. We then apply the. Concept from model two where we shot or remwhere we shot or remove alpha zero through alpha n to be equal to alpha hat and we'll repeat that for beta. And then once we get past n we're going to be back in this period where alpha and beta are talpha and beta are their steady state values. Once we have these vectors of parameters we can go ahead and simulate and then combine with the BLS data to prepare for plotting and evaluating the MSE. Ovaluating the MSE. Okay, so we're going to be using the four parameter values we had from above we had beta tilde had to be about 12% in order to generate that sharp drop in the percentage of employedrcentage of employed workers. And then to slow down the recovery to steady state we have these three parameters here data hat of 2% alpha hat of 25% and those values apply for four periods. We do thatperiods. We do that. This model where we've combined both the shift of beta minus one to generate this sharp drop in the number of employed workers and then the. temporary but prolonged shift in alphlonged shift in alpha and beta between zero and equal four has slow recovery that matches the coveterra and we're now able to. Construct a model which again is our sequence of parameter vectors alphameter vectors alpha and beta that can match the coveterra dynamics and we're set up to be able to explore and ask this model more questions. We'll speak more about this in the future but for now we'rere but for now we're actually going to transition into something slightly different so our analysis so far has focused on models that for the most part keep alpha and beta at their steady state valuessteady state values. We've allowed for temporary but short deviations from this in order to get a sharp change in the employment configuration and then a quick recovery. And that helped us match thehelped us match the coveterra data. However, the great recession era data doesn't have a sharp decrease in rapid recovery in the number of employee in the percentage of employed workers. Rather has arkers. Rather has a gradual decline in the percent of employed workers followed by a gradual increase. And in order for our model to achieve something like that we would have to allow for alpha and below for alpha and beta to adjust. More than just a couple periods. And so what we'd like to do is answer the following questions. What values of alpha t and beta t were now t covers the entire horizons the entire horizon from minus 35 to plus 35. Would be consistent with or allow the model to generate great recession era dynamics. To answer this question, let's return to our data. So the the BLS dta. So the the BLS data frame it has columns E E U U E and U E. So these represent the rates of individuals moving between unemployment and employment status. And what these were of course bond to isf course bond to is governed by the first letter of employment and unemployment. So that E E column represents workers for that flow in any given month from being employed to still be employed. E U wobe employed. E U would be somebody who started the month employed and did unemployed. E would be people who started unemployed and then ended with a job. And finally, you would be unemployed at bothunemployed at both the start and then the period. So if we think carefully about what the interpretation of our model jar will see that the U E column from the BLS data frame represents the job findiesents the job finding rate. This is the rate at which unemployed workers transition from being unemployed to being employed. Similarly, the EU column would represent the job separation rate or the raation rate or the rate at which employed workers move into unemployment. So what we'll do is we'll take these columns. E E and E U from the BLS data frame and we will use them as a time series of valutime series of values for alpha T and beta T. And then we'll simulate our model using those parameters. Let's now code up a solution to the direct problem for this model. So our function here will taunction here will take in three parameters. The first is our like model simulator. The second would be our data frame of data from the BLS. And then a third parameter S in it which represents the initrepresents the initial state. We'll set down here. We'll talk why we we about why we need this here shortly. Now what our function does is it will take the BLS data and it will grab only the rows corab only the rows corresponding to the Great Recession. And then sort those rows by the months from column to make sure they're all in order. We'll then save that as DF underscore GR. We'll then get ouR. We'll then get our vector of alpha parameter values directly from the column you eat and the vector of beta values directly from the column EU. We'll then make sure that the state is set to the indte is set to the indicated value will simulate and return. When we evaluate this function and then run it we'll see here that our model is now able to generate dynamics that look very much like the Grery much like the Great Recession. Let's see here that the model now starts out of place very near the Great Recession starting point. This is what we needed that S in it value for. And then it's able. And then it's able to gradually decrease until it gets to about time zero and then gradually increase just like we see in the Great Recession data. In addition to visually seen that our lake model ihat our lake model is capable of of of Mimicumic Great Recession we can see that the model is indeed a much better fit quantitatively by looking at the MSE. And our previous iterations of the model. Tions of the model. The MSE for the Great Recession was between 25 and 30 which always very high and now we have an MSE that is less than one. A strong indication that this version of the model is theof the model is the best version we've considered thus far at matching the dynamics observed during the Great Recession. In addition to seeing that the MSE for the Great Recession era fell we saw thatera fell we saw that the MSE and the Covid era rose we were down between one and two in model four and now we're back up to a 9.05 for the MSE Covid. Now the fact that the MSE for the Great Recessionthe Great Recession era fell so sharply and the MSE for the Covid era rose it demonstrates or illustrates a common curse and hidden blessing to all modeling exercises. And this is summarized by the stsummarized by the statement that each modeling decision we make it comes with trade offs. So our two state mark-off chain view of labor market fluctuations that cannot match both the Great Recession ehe Great Recession era dynamics and the Covid era dynamics with the same set of parameters. These two time periods in U.S. labor markets are sufficiently different that a model of this degree of complthis degree of complexity is not able to match both of them at the same time. However, the good side of this or kind of the blessing in disguise is that we are forced as modelers to be deliberate in so be deliberate in specifying our modeling goals. We have to at the onset of our analysis answer the question of whether we want the model to match the dynamics of Covid era labor markets or of Greatmarkets or of Great Recession era labor markets. This specificity in our goal for doing the modeling and the analysis will help all aspects of what we're working on the more specific and pointed targeic and pointed targeted we can be with what we're doing the more likely it is that we'll have success. The second aspect of a blessing coming from this trade off is that we can afford to use simpler mord to use simpler models. If we are willing to accept that we can either match Covid era data or Great Recession era data are two state mark of chain is a great workhorse model for studying both arear studying both areas of U.S. data. This is a fairly simple model built up from building blocks that we know and understand well and by making this deliberate decision on what our goal is we are allowgoal is we are allowed to use a simple model. And as a guiding rule we will always want the simplest model that allows us to achieve our modeling goal. Now there are many reasons for this desire for sor this desire for simplicity which we will cover in greater detail throughout our time together. But one of the strongest reasons is that the simpler the model the more likely it is we understand howis we understand how it works and then we're able to effectively use the model to solve whatever problem or to apply it to whatever task we are working on. Now close our next time together today let'stogether today let's pause and look ahead a little bit. We did a bunch of work in a previous lecture and in this one about building a model and you might be wondering why do we go through the hassle othrough the hassle of doing this. And as social scientists we rarely have the luxury of setting up controlled experiments. There are some settings in which very creative and brilliant researchers haveant researchers have been able to effectively create an experiment. To test a social or economic theory but in general this just isn't possible because we study society and people setting up controllesetting up controlled testing environments is often unethical and almost always impossible. So in order to do real science and employ the scientific method we must create a laboratory for ourselves. Aory for ourselves. And instead of laboratories created with physical components you might observe in physics or chemistry or hard science. Our laboratories are constructed from the mathematical equatimathematical equations and the statistical structure that we used to build our models. A model allows us to consider what if scenarios that we call counterfactuals. And in these scenarios we're ablecenarios we're able to use the model as a lens or as a way to study trade-offs to potential decisions. So for example you might imagine that our lake model that helps us understand the flows from emplthe flows from employment to unemployment. Might be part of a larger economic framework. Now this larger framework could include other things such as the workers or the households making decisions limaking decisions like whether or not to save the income they generate from working as employees or to spend it on consumption right now and gain some happiness from that. And also include a governmeninclude a government that has to raise taxes to finance some expenditures on behalf of its constituents. And then in act policies for maybe labor market relief during time distress. You might also has. You might also have international trading partners that could both increase the diversity of goods available to consumers as well as help smooth out labor shocks. Our economies experiencing difficuexperiencing difficulties in the labor market but a different economy isn't. Maybe production and resources should shift to the more healthy economy for a short time. And this could happen through intd happen through international trading partners. These are all extra modeling components that could work alongside the lake model. And building up a rich framework by connecting these well understoodese well understood pieces allows us to have a very powerful laboratory for doing experiments and apply in the scientific method. We can apply a change in one component. For example, maybe we want toe, maybe we want to assume that the government was able to subsidize the labor market. And then we can see the corresponding impact on other parts of the economy. Maybe this would result in a lower seresult in a lower separation rate or increased household consumption because they still have jobs. And if their consumption is higher than maybe they're happy Mr. their welfare is higher. And by connehigher. And by connecting these components. We're able to create a laboratory of statistics and of equations that is our model. We can apply the solution to the direct problem to have the model generaave the model generate simulated data for us that we can study and compare against actual data to see if our model parameters are accurate. And then we can use the inferences we make and the things weke and the things we learned from that simulation. To apply to our solution to the indirect problem where we then refine the choices we make about the parameters. And we saw this balance plain off todalance plain off today. We did a lot of simulating which was solving the direct problem that allowed us to make different decisions about how to tweak model parameters which was the way we were solvine way we were solving the indirect problem. And kind of throughout this course we're going to be putting together our data programming, math and modeling tools that we've built up. In order to build mIn order to build models and perform experiments like this. And in that sense we'll be able to understand the society in which we live and operate. Appreciate you for being here and we look forward tnd we look forward to more exciting lectures here coming up." metadata={'source': 'transcripts_tiny/2.11.3 Lake Model Meets Data (2 of 2).srt'}