00:00:00 --> 00:01:25
Hello, this is Spencer Lyon and in this lecture we're going to continue talking about the Lake Model of unemployment dynamics and we're going to seek to understand how we can use the model and tie its parameters to US data so that we can have the data teach us something about the models parameters. Before we dive in on the Lake Model we want to remind ourselves of some of the higher level concepts we've been working with. In particular we want to review some of the standard notation and language that we're using so that we can keep a perspective of how this particular example fits into the larger framework that we've been building throughout this course. So first we have the notion of a statistical model which is a joint probability density F over some random variable some data Y and some parameters theta. Now with this statistical model we might think about solving two distinct but related problems. The first problem is called the direct problem and the direct problem is to draw a random

00:01:18 --> 00:02:52
The first problem is called the direct problem and the direct problem is to draw a random example Y and notice here our notation we use the lower case little Y to mean a draw from our model F and the upper case big Y to represent the data. Now again the direct problem is to draw a sample Y from the statistical model represented by F for some assumed or fixed value of the parameter vector theta. On the other hand the inverse problem will be to take a set of data which we'll call Y tilde and we're going to make an assumption or an assertion that this data was in fact generated from the model F. We will then use this data and the assumption about it being drawn from F and F's functional form and we'll use that combination in order to make some inferences or to learn something about an unknown parameter vector theta. But in slightly different terms the direct problem is to construct a simulation of our model F for a given parameter vector theta. Other names for this simulated data set again

00:02:44 --> 00:04:19
model F for a given parameter vector theta. Other names for this simulated data set again noted a little lower case Y might be artificial data or fake data. The inverse problem is also called the parameter estimation problem. This name comes from the fact that when we're solving the inverse problem we are taking some observed data, assuming that it was generated from F and then trying to learn something about our estimate values of our parameter vector theta. A third way to think about this is that the direct problem takes parameters theta as inputs and then it generates artificial or fake data as outputs. The inverse problem takes data, often real world data as its input, often we call this the observations or the observed data and with these as an input it will then generate statements or inferences about the parameters theta as the output. The inverse problem consumes data and returns something about our parameters as an output. One key thread underlying both of these model these problems are that they use the same statistical model F of a given data.

00:04:11 --> 00:05:38
these model these problems are that they use the same statistical model F of a given data. There are some tools that allow us to solve the direct problem which again relates to simulating artificial data and these tools in fact turn out to be helpful when solving the inverse problem or learning something about the parameter vector theta. We're going to be continuing in this general framework of a statistical model the direct problem in the inverse problem as we talk today about the lake model of unemployment that we've been working with and we want to make sure that as we work through the materials in this lecture that you keep this overall framework in mind and our goal today will be to use the lake model and its parameters and inferences we can make about its parameters given real world data from the BLS and it will help us understand the unemployment and employment rates over time and how they may change in various settings or in various economic climates. This first code cell here will just import some of the tools we'll be using.

00:05:27 --> 00:06:48
various economic climates. This first code cell here will just import some of the tools we'll be using. We're going to import pandas, numpy, and map plotlib. We'll just going to apply some style to our figures and then we also import the built-in random library so that we can set the random seed. And we do this just so that when we come back and we execute this code or this notebook in the future we'll be able to see the same outputs and our work will be reproducible. So let's review some facts that we've observed about the data. In particular we want to look at the unemployment rate in the United States in the past two recessions. We obtained data from the Bureau of Labor Statistics or BLS and as noted in previous lectures we saw a large shift in the employment and unemployment numbers in the United States in the beginning of the year 2020. And let's remind ourselves what these data look like. We have prepared a CSV file called BLS Reset Recessions Data that will read into a data frame using pandas read CSV function.

00:06:42 --> 00:08:05
Reset Recessions Data that will read into a data frame using pandas read CSV function. We'll then construct a few extra columns. So we have as columns of this data frame a column named Employed which represents the total number of persons that were employed when the survey, when the underlying survey was given as well as the total number of persons that were unemployed at the time the survey was administered. If we sum these two up we'll call this the Labor Supply. Then with this Labor Supply in hand we're just going to compute the fraction of workers that participated in the survey that were employed at the time of the survey and then unemployed. So we'll go ahead we'll import the data, create these in columns and we'll take a look at the first few rows. So the columns of note that we should point out here there's a column called Recession. This takes on two values one is GR which is short hand for the great recession. There's also another value in this column called COVID which again represents the early 2020 months when there was a large shift in labor market dynamics

00:07:54 --> 00:09:15
which again represents the early 2020 months when there was a large shift in labor market dynamics due to the COVID-19 pandemic. We then see the employed and unemployed columns talked about before as well as our new columns, Labor Supply employed percent and unemployed percent over here on the right. We'll talk about some of these other columns, EEE and UU and a little bit and the other thing to keep in mind is there is this column called Months from and what this represents is it's always an integer and it represents the number of months from the peak of unemployment associated with the given recession. So this first row corresponds to the great recession and Months from has a value of minus 35. So this would be 35 months prior to the peak unemployment observed during the great recession. So this is about three years before the peak unemployment. We'll actually be able to visualize this data here on the next page. So what we have here, the function and we'll go back and look at it in a moment but this is the type of, this is the plot generated by our function.

00:09:09 --> 00:10:33
and look at it in a moment but this is the type of, this is the plot generated by our function. So we see here we have two subvellers in our graph. The first subplot up at the top shows the percent of employed workers given number of months before and after the the trial for the low point in the labor market during the recession. So you'll see here that the blue line that lives up here and then has the large spike downward represents the COVID era unemployment dynamics and the red line that is more gradual represents the great recession era employment dynamics. This top subplot would represent the share of persons that are employed and it's why access ranges from about 85 percent to about 96 percent. The bottoms of graph would just be exactly one minus the top one, which is why you see kind of the mirror pattern down here on the bottom. It's almost as if it was reflected about this space in between the two charts. Now let's look at the code that generated this just so we have a sense of what it does. So what you can read the doc string and we encourage you

00:10:28 --> 00:11:41
just so we have a sense of what it does. So what you can read the doc string and we encourage you to take a look at this on your own time. But we'll talk through the high point right now. So this function will consume a data frame and then what we will do is it will construct a 2 by 1 figure of subplots and it will plot both the employed percent and unemployed percent. The employed percent columns get put on the first subplot and the unemployed percent get put on the bottoms of flat. Then what we do is we will group by the recession column of the data frame and we will plot this group so this would contain only data for a single instance of that recession column, or a single value of the recession column. And we'll plot on the horizontal axis the months from column, which you see down here and then why is going to be either employed percent or unemployed and then we do a little bit of work to kind of clean up the look for the chart and we return it. And what this function will allow us to do is if we happen to have say three different

00:11:36 --> 00:12:58
And what this function will allow us to do is if we happen to have say three different groups in the data frame of the recession column we would see three lines on each of these charts. And we'll use this throughout the lecture today as we're going to add a third set of rows to a data frame which will represent the model output and we'll see that here shortly. So we just had a reminder and one thing actually let's go back to this to one more look. So we noticed a couple patterns here first before the the low point of the current COVID related unemployment. Things where I are fairly steady or fairly smooth there was roughly 4% unemployment and then very quickly within the matter of one period there is a sharp decline in employment all the way down to about 85% and then a rather quick recovery or at least that's what the data looks like up until the end of the data set. In contrast the great recession looks quite a bit different. So the three years prior to the low point of employment during the

00:12:49 --> 00:14:11
looks quite a bit different. So the three years prior to the low point of employment during the greater session did not see a steady value like we did in the COVID era but rather a slow decline and slow loss of jobs that finally culminated right here at the worst point or the lowest point of the employed percent and then a very slow and gradual ascension in the number of in the percentage of employed workers representing a slow or gradual recovery. Now throughout the lecture we will use the lake model to capture kind of both of these dynamics but keep in mind that the great recession and the COVID era data from the BLS they show a very different pattern from one another. Now that we've reminded ourselves with the data look like let's remind ourselves what the model is that we've been working with. So the lake model in each period has two parameters. First parameter we've called alpha and this is the probability of an unemployed worker finding a job. We're going to refer to this as the job finding rate.

00:14:05 --> 00:15:26
unemployed worker finding a job. We're going to refer to this as the job finding rate. The second parameter is beta. This is the probability of an employed worker losing their job. This is going to be called the job separation rate in today's lecture. So these are the two parameters. This would be the container divector theta from our description at the beginning of this lecture. There's also one state variable. We've been calling it S and S is a two by one vector of percentages of unemployed and employed workers respectively. So these are two numbers between zero and one that sum to one. One thing to point out is that if the values of alpha and beta remain constant is not possible for our model to generate sudden and large changes in the unemployment rate like those showcased in the COVID era BLS data. Keep this in mind it's something we're going to try to address throughout our lecture today. So let's take a quick look. We took some of the routines from a former lecture and we put them in a

00:15:21 --> 00:16:36
So let's take a quick look. We took some of the routines from a former lecture and we put them in a standalone pipeline file called v1 employmentmodel.py. We're going to import these functions under the alias emp and we're also going to keep numpy around as we'll be using that throughout our code today. What we'll do is we'll first create a new Python class that we'll use throughout our lesson today. And the purpose for this is that the class will allow us to keep track of the model parameters that alpha and the beta. We'll also have routines for simulating a panel of workers and it will help us combine the simulated data with the data from the BLS so that we can plot results and compute statistics from our simulation. Now the Python class it looks quite long but most of the lines are actually documentation and comments so they can be helpful as yourself studying after this lecture's over. We'll review each of the functions each of the methods that make up the class and it should be clear once we're finished or I walk through what the class does.

00:16:31 --> 00:17:55
and it should be clear once we're finished or I walk through what the class does. So here we go. The class is going to be called Lake Model Simulator. It's a knit method. This is the method that we used to create a new instance or a new version of our simulator. It has just two parameters. One is called N and this will just represent the number of individuals that we would like to include in our simulated data. The second parameter here is called S0. This will represent the initial or starting steady state or starting state value for beginning our simulations. Here we've gone ahead and we've set default values for both of these parameters. By default, our simulator will simulate 5,000 workers at every time step and it will set the initial state equal to about 2.3% unemployment and then about 97.6% unemployment. Sorry, 97.6% employment. We'll go ahead and all the admit method does in this case is it just stores these values for later. We then have a block of code right here that defines what's called

00:17:48 --> 00:19:08
stores these values for later. We then have a block of code right here that defines what's called a property. So these parameters here, the N as well as the underscore S0, our call of properties. In our case, we wanted to put a little bit more structure around how the property S0 without the leading underscore would work. By default, when we try to get S0, meaning when we try to type the lakemodel dot S underscore zero, what's going to happen is it's going to actually run this function and all we're going to be doing is returning the current value of underscore S0. This is the same as the behavior we would get from here from just using, for if we are trying to access N. So we're not really leveraging the fact that this is a property with custom behavior. The custom behavior comes in the second half of the property, which is called the set method or the setter. Now the set method, it needs to take in a new proposal for the initial state. And the reason we're defining this as a custom property is because we want

00:19:01 --> 00:20:25
the initial state. And the reason we're defining this as a custom property is because we want to make sure that whatever anybody passes in is an array that has two values and we want to also assert that they sum to one. This just allows us to make sure that whatever the user of our class wants to change the starting state before a new simulation, they set it properly by having a two element vector that sums to one. If those two both paths, we will then just set this underscore S0 property to the array that was passed in and then whenever somebody uses LM dot S0, this will be returned to them via our get function above. So once we have these defined, we just need to connect them using the special property function built into Python. So this code right here allows us to have custom behavior for setting the S0 property. If I didn't have a two element array or it didn't sum to one, this attempt to set a new value for S0 would be rejected. This is just a showcase. One of the features that Python affords us to make sure that we don't

00:20:17 --> 00:21:45
This is just a showcase. One of the features that Python affords us to make sure that we don't have an inconsistent version of our model. Okay, the next method will be to combine with the data from the BLS. So this takes two parameters. One, we've called DFSM, which is short for the data frame that we obtain via simulation. And then the second one is called DFBLS. We've set a default value for this to the DFBLS. We imported above from the CSV file and used to construct our plots. Now this, the main part of this function is just to concatenate these two and stack them on top of each other. However, there's one thing we need to do, which is right here. We need to adjust the month from column to begin at negative 35. So by default, our simulation routines will set the lowest value of the month from column to zero. It will just simulate from time zero, forward. And in order for everything to line up properly with the data obtained from the BLS and how we've ordered it, we need to subtract 35 so that our simulation also begins at negative 35.

00:21:38 --> 00:23:03
ordered it, we need to subtract 35 so that our simulation also begins at negative 35. So once we've done that, we'll just stack them together and then return. Next, we have a method called simulate panel. So here, this is going to be kind of a wrapper function around the simulate employment cross section method that we saw in our last lecture together. Our last lecture on the lake model. The two parameters that are consumed by this method are alpha and beta. These are going to be arrays that contain the value of alpha and beta for every time step of the simulation. So they will have lake the T and they will be passed into the simulate method from the employment file. We're going to then pass in the S0 that's part of the class right now as the initial state as well as the number of individuals that's also a property of our class. This will be an N by T array filled with entirely the number 0 and 1. We're 0 indicates a worker being unemployed in that period and a 1 indicates a worker being employed.

00:22:57 --> 00:24:20
We're 0 indicates a worker being unemployed in that period and a 1 indicates a worker being employed. Well then extract the shape of the number of columns which are present this number T so that we can use it again now below. Then once we have all these workers, we'll simply sum over all the N in each period to get a total number of workers that were employed. Finally the last thing this does is it puts it in a data frame and this data frame will have four columns. The first is month from and as mentioned before this will just start at 0 and increment by 1 in each row and it will end at T minus 1. So it has T elements. Then we have a column called employed percent and importantly this is the same column name that showed up in the BLS data frame from above and this will be the number of employees. There's the number of workers that were employed every period which we computed above divided by the total number of employees in our panel. There's a total number of workers in our panel and finally we'll also compute the unemployed percent which is going to be 1 minus that

00:24:13 --> 00:25:53
panel and finally we'll also compute the unemployed percent which is going to be 1 minus that and we will set the recession column to the constant model so that when we use our plotting routine to find a above we will have a legend entry that reads model. This will be kind of the workhorse of our our work today and this solves the direct problem. So notice that as an input to this function or this method we receive parameters. This is this alpha and beta would be like the the parameter vector theta that we spoke about at the beginning and the output of this method is a simulation from the model f. The model f is encapsulated in how we use the parameters alpha and beta in order to produce a simulation of the data. So our core simulation routine here allows us to solve the direct problem. We then have one more helper method in this class where we are labeling an easy routine for simulating a panel of workers where alpha and beta are fixed at their time zero values and so all we'll do here is we'll construct a vector of alpha

00:25:47 --> 00:27:17
at their time zero values and so all we'll do here is we'll construct a vector of alpha which will be the constant alpha zero of vector of beta which is the constant beta zero and we'll use the function or the method to find above to run a simulation and return a data frame. So we'll evaluate the cell to define our function and we'll be ready to start using that here shortly. Okay we're now ready to put our new class and its simulation routines to work. So to begin we will first simulate a a quote normal time or non-recession version of the data and we will refer to these parameters as alpha and beta but with a bar over it. So alpha bar is 0.37 which represents a job finding rate for the unemployed of about 37%. Meanwhile the normal time or steady state version of the beta parameter or the job separation rate is about 1%. And again we're going to refer to these parameters with the bar over them as the steady state parameters and we'll talk soon about why we we choose this term steady state and we'll remind you that these values were computed using

00:27:09 --> 00:28:45
why we we choose this term steady state and we'll remind you that these values were computed using data from the CPS in an earlier lecture. If you remember our simulation routine also required that we pass a value for the initial state of employed and unemployed workers will refer to this as S bar and we'll note that it has about 2.37% of unemployed and about 97.6% of workers that were employed. Again these were also previously estimated using the CPS data. So we'll define these three constants right here and then in the next cell we will use them to simulate we use them with our simulator to produce a chart that includes the data from the BLS as well as our simulated data. So these blue and red lines are the same that we saw before. They represent the COVID era in blue and the great recession era in red. But now we have a new yellow line that represents kind of the the model generated data using the steady state parameters. So notice that we passed in an unemployment rate, a starting unemployment rate of about 2.3% which is right here in the yellow line and because

00:28:36 --> 00:30:08
rate, a starting unemployment rate of about 2.3% which is right here in the yellow line and because we were in steady state the model stayed roughly at that place. We didn't see large we didn't see gradual decline in employment or increase and we didn't see a large spike as we do in these other two lines. This is one of the reasons we call the set of parameters the steady state parameters because it's a a place of aggregate rest for the model. So visually we can see that this steady state version of the model. It doesn't match features of either recession. So the COVID recession had that really sharp drop in employment which wasn't replicated in the steady state model simulation. And again the great recession data set had that gradual decline and then after the trough was reached a gradual increase in employment which again was inconsistent with the model being very constant and flat over time. Our goal in the rest of our lecture today will be to discover deviations from the model where we set alpha equal to alpha bar for our periods and beta

00:30:00 --> 00:31:36
discover deviations from the model where we set alpha equal to alpha bar for our periods and beta equal to beta bar for our periods. There will have a lake model to do a better job of describing times of stress in the US labor market. In order to understand the effectiveness of our modeling decisions we need to talk about the concept of a last function. So a last function is a deterministic mathematical function that measures the difference between a model's output and its target. The last function is a key component to any statistical optimization routine or practice and it is heavily used in machine learning in order to evaluate the goodness of fit of a particular model. In today's lecture we're going to be using a very common last function called the mean squared error last function or written MSE for short. It's defined as follows given a sequence of targets and model outputs y and y hat. The MSE is defined as the average 1 over n of summing up the difference between the target and the model output squared. So we'll compute the residual or the loss for

00:31:26 --> 00:32:55
between the target and the model output squared. So we'll compute the residual or the loss for each observation. We'll square that loss and then we'll compute the average of all those squared losses. That's what is meant by the average or mean squared error. This is where it gets its name. For the current exercise that we're undergoing the model output will be the time series of the percentage of workers that are employed. The target will be the corresponding time series from the BLS and we will compute the last function independently for both the great recession error data as well as the COVID error data. We've defined here a function that takes in a data frame that has the same shape and features as the data frame we're plotting and it will compute the MSE between the model and the great recession error data and the model and the COVID error data. So to do this we use some of our reshaping tools we've learned. First we construct a pivot table by setting the index to be the months from column. Then across the columns we're going to have

00:32:48 --> 00:34:03
by setting the index to be the months from column. Then across the columns we're going to have different values of recession. In this case we'll have GR for great recession as a column. COVID will be another column and model will be another column. The values that are referred to by this row and column label will just come from the employee percent column of the input data frame. We're going to multiply each of these by 100 to put them in percentage units and this is actually we'll just make the MSE bigger and a little easier to compare across models because we won't be dealing with very small numbers. Once we have this pivot table computing the MSE looks very similar to its formulaic representation. So what we'll do is for each row or each value of months from we will compute the difference between the model level of employed percentage and the great recession level of employed percentage and then we'll square that difference. Once we've done this for each row we'll have a pandas series after this code is evaluated

00:33:57 --> 00:35:16
Once we've done this for each row we'll have a pandas series after this code is evaluated and then we'll just compute the mean or the average of each of these squared error terms. We'll repeat this for both the great recession column and the COVID column. And the reason we used a pivot table there were two reasons one by setting the index two months from we're allowing pandas to align the operations here where we do model minus great recession and pandas will align the observations to make sure that we're subtracting a month from of 10 in the model to a month from of 10 in the great recession. And the second reason was so that we could have these three versions of our data, great recession, model and COVID each becomes so they're easy to operate. Once we do that we can define the function and use it to evaluate the efficacy of our steady state. And just kind of keep these numbers in mind as we move forward. So the the mean squared error, the average squared deviation of our steady state model for our good recession data

00:35:11 --> 00:36:31
the average squared deviation of our steady state model for our good recession data ended up being about 29%. Or value 29%age points. Remember that was the units we chose. So the mean squared error was about 29 and for the COVID data it was about 12. So just kind of keep these numbers in the back your head as these are going to be our benchmark for evaluating one model versus another one. And in particular the use of a loss function allows us to be precise about what we mean when we say a model is better or has improved upon a different model. It allows us to move from a qualitative notion of how good a model fits the data to a very quantitative version. So for example instead of saying model 2 appears to do better than model 1 that matching the spike in the COVID error unemployment. We could say that the MSC COVID from model 2 is 5.1 relative to the value of 11.5 from model 1. So we've moved from a qualitative statement here to a quantitative numerical statement down below. And this will make it easier to have

00:36:26 --> 00:37:57
to a quantitative numerical statement down below. And this will make it easier to have interpretable results that are completely objective. We'll now begin putting in a different models on the table and seeing what they can help us learn about unemployment data in the United States. So let's first consider the hypothesis that the spike observed in the COVID-19 era unemployment data was only due to a redistribution of workers in the initial period. But it didn't actually change the job creation or separation rates. In terms of our model parameters, this hypothesis would mean that alpha t is equal to its steady state value alpha bar for alt t. And similarly beta t is always equal to its steady state value of beta bar. This is what we meant by the hypothesis saying that the job creation and job separation rates were not changed. Then we'll say that we can begin our simulation at time t equals 0 at the steady state or S bar. Then we will say that s t is the shocked value of s. We'll talk about this soon

00:37:46 --> 00:39:12
or S bar. Then we will say that s t is the shocked value of s. We'll talk about this soon where the subscript t or the date is just the date of the shock. Then for all tau greater than t, the value of the state in period tau will just be dictated by the lake model and the parameters alpha and beta. So we'll be using again in order to compute what the state is in period's greater than t will be using our solution to the direct problem by fixing the parameters alpha and beta and simulating our model forward to obtain the state in periods after the shock. We'll make one note here that for any tau sufficiently greater than t, you will have that the state in period tau is going to be approximately equal to the steady state state S bar. This is actually what is meant by a steady state. It is a place of rest according to the dynamics of the model. If the model is at the steady state today and the parameters remain at their steady state values, the model will be at the parameter to steady state again in the next period.

00:39:07 --> 00:40:34
their steady state values, the model will be at the parameter to steady state again in the next period. We actually have all the tools we need to inspect this hypothesis visually and we'll lay out a few steps for how we'll do it. The first step to visually inspect this hypothesis would be to set the initial state equal to the steady state value. Then we will fix the parameters alpha and beta at their steady state values for all time periods. Now because we want to inspect visually and compare against the data from the BLS, we know how long we need to simulate before the shock. So what we'll do is starting from the steady state and keeping parameters fixed, we'll simulate for 35 periods. This would be from period minus 35 to minus 1 inclusive. Then at time t equals zero, we will set the state. By hand, we'll reach into the model and we'll reshuffle our workers so that there are 14% of our workers will now be unemployed and then the balance or 86% will be employed. This is approximately the value that was observed in the spring of 2020 because of COVID-19.

00:40:28 --> 00:42:00
This is approximately the value that was observed in the spring of 2020 because of COVID-19. Then we can from that point forward simulate for the other 36 periods that correspond to the length of the BLS time series and finally we'll stitch together these two halves of the simulation, the pre-shock half and the post-shock half. Should notice that this is very similar to what we did when we'd simulated the steady state. We're the only differences here at step four at period zero. We the model, we reach our hand into the model and we cut some of the employed worker jobs, we separate them from their jobs and move them into the unemployed bucket. And that's it and then we let the model continue to apply its dynamics going forward using the direct problem and our solution to it and we obtain our simulation. So in code we follow these same steps. Here we make sure that the initial state is S bar and we simulate for the first 35 periods from zero to 34. Then we will need to apply the shock, this is step four. And the way we do this is we will set

00:41:48 --> 00:43:25
Then we will need to apply the shock, this is step four. And the way we do this is we will set the S0 parameter of our Lake model equal to the shock value which we pass in as an argument here. And the reason we're setting the says S0 equal to ST is because we're effectively starting a brand new simulation where the shock value will become the initial state for the simulation from this point on. We'll then go ahead and simulate the second half. We'll need to make sure that we adjust the months from column to make sure that it picks up exactly where this data frame before left off. And then we'll just concatenate the two and stick them together. And finally we combine them with the BLS data so that we're ready to plot or compute the MSC straight away. Sure, our solution is to simulate in two halves. We first simulate from the steady state without changing anything. We then shock the state, simulate afterwards and stitch the output together. We'll go ahead and we'll do that and we'll call the value DF shock and we'll plot the results

00:43:20 --> 00:45:00
We'll go ahead and we'll do that and we'll call the value DF shock and we'll plot the results alongside the model from the data. And we'll see here that we do have a steady state up at the top all the way until period T-1. Then at period zero we immediately drop down to about the value of that COVID recession which would be 86% of the workers employed. And then the model dynamics kick in and as the job finding rate is at a steady state value of 37%. The number of the percent of workers employed goes up up but it continues to climb until it reaches a steady state value and then stays roughly constant after that. So because the job finding rate is high at 37% and the job separation rate is low at 1%. This level of only 86% of them workers being employed is not sustainable by those model parameters and the number of employed workers are the percentage of employed workers continues to climb until it reaches its steady state value again. So one thing to point out here is that the dynamics before the shock and even up into it were fairly similar for the COVID era

00:44:51 --> 00:46:17
is that the dynamics before the shock and even up into it were fairly similar for the COVID era and the model. The model had a slightly higher stable or steady state unemployment or shy employment numbers but the pattern of being consistent and flat up until a large spike is the same across both. What is different however though is that the model holding that job finding and job separation rate constant causes a much quicker recovery, a quicker increase in the percentage of workers employed in the model than we see in the data. The regates to be a gap in the between these two lines that grows wider as we move forward in time. This is even more pronounced when we consider how quickly the model is able to recover relative to the great recession. So here it looks like the difference between about 97% and 86. If we split it right down the middle would be somewhere around here and it looks like it's getting there after just one or two periods. Contrast this with what happens in the great recession where if we want to go from say 95 to 90 you want to go back to 92 and a half

00:46:10 --> 00:47:37
great recession where if we want to go from say 95 to 90 you want to go back to 92 and a half this doesn't happen until almost 35 months after the the low point in employment from the great recession. So this recovery from the great recession was much slower than our model would be predicting. Let's go ahead and evaluate the last function. We'll see here that the MSC for the great recession didn't move very much before with the original model that didn't have the spike it was between 25 and 30 and it still is. However the MSC for the COVID era, what data has fallen significantly. This should be expected because when we reached in as the modeler and shifted the balance of employment and unemployment to artificially mimic this spike we should see that the model does a bit better because for these few periods were much closer to the blue line then we would have been had we stayed up here in the steady state. So the fact that the MSC fell is entirely a feature generated kind of in these 10 periods where we go from zero to when we've recovered which is at about

00:47:31 --> 00:48:53
generated kind of in these 10 periods where we go from zero to when we've recovered which is at about the MSC in this simulation is going to be smaller than it would have been had we stayed up here the entire time and that shows up right here with this number being about four instead of something close to 10 or 12. From the analysis we just completed. We've learned that even when we artificially adjust or shock the state at time t equals zero to match the great recession the steady state values of our job finding in separation rates are inconsistent with the market dynamics during the COVID era recession. This was manifest in our model featuring a faster recovery than the actual COVID era data from the BLS. As we think about this we would realize that our assumption that alpha and beta remain constant is likely too strong in addition to a redistribution of our workers between employment and unemployment there may also have been a shift in the job separation and finding rates during this period.

00:48:46 --> 00:50:13
there may also have been a shift in the job separation and finding rates during this period. We'll explore that hypothesis here in what we'll call model two. So we're going to relax the assumption that alpha and beta are always fixed at their steady state values but we're going to do so in a very disciplined and specific way. And here's how we're perceived. We're going to assume that before the time t equals zero shock occurs alpha and beta with the subscript t are going to be fixed and held at their steady state values. Then we're going to continue to assume that the state s t immediately jumps to the spiked COVID levels so far this is the same as what we did in the previous model. Now we're also going to assume that starting at times zero and continuing temporarily for n a finite number of periods alpha and beta will move to new values alpha hat and beta hat. Once those n periods are over alpha and beta will return to their steady state values of alpha bar and beta bar. Mathematically these assumptions are expressed as follows.

00:50:08 --> 00:51:37
bar and beta bar. Mathematically these assumptions are expressed as follows. Alpha sub t is going to be equal to alpha hat only when t is between zero and n. In all other periods including t less than zero and t greater than n, alpha will be equal to alpha bar. The same holds for beta. We're going to have beta hat only between zero and n and beta bar in all other periods. We will define a function that can do this for us. So here this function again takes our lake model simulator as an argument and then it needs to know the shocked values of alpha hat beta hat as well as how long to hold them n and it needs to know where to adjust the state at times zero. So once we take as an input these parameters our solution to the direct problem described above is as follows. Our before t equals zero code remains exactly the same and even up until t equals zero our movement of the state directly to the shocked value is going to mimic what we saw in the previous direct model for model one. What changes is what happens starting at t equals

00:51:27 --> 00:52:55
what we saw in the previous direct model for model one. What changes is what happens starting at t equals zero. So we're going to construct a vector of alpha which represent alpha sub t. We're going to have him have length 35 and we're going to start out sitting all of them equal to alpha bar. Then the first n values of this vector are going to be adjusted so that it equals alpha hat in those periods. This is how we apply this mathematical function that was part of our model up above. This is our direct model solution or direct problem solution to this modeling feature we described up here. We repeat the same thing for the beta parameter. We'll use our lake model simulate panel routine that can accept this vector of alpha's and datas. As we did before we have to adjust the months from in the second half of our panel to account for the fact that we've already simulated from periods by 34 up until zero and then we stitch the two together combine with the BLS data and return. So this code is going to be the same as what we saw before. We'll evaluate this

00:52:47 --> 00:54:14
BLS data and return. So this code is going to be the same as what we saw before. We'll evaluate this cell and we will test it out. So we're going to go ahead and put on our thinking caps and recognize that in order to slow down the recovery of our model we may need to decrease the job finding rate and that would mean moving it from zero point three seven is something smaller. Here we're just going to pick the values 0.28. We have no reason to believe that this is any better or worse than another value smaller than 0.37. We're just testing out one possible model which includes setting alpha hat to 0.28. Another thing that might slow down the recovery would be if the job separation rate rises. It's steady state value is at 1% and we'll just move it up to 5% and see what happens. And then we'll suppose or pause it at this last for three periods. We'll go ahead and we'll use our new simulation routine or our new solution to the direct problem. With the values of alpha hat at 0.28, beta hat at 0.05 that are held there for three periods

00:54:05 --> 00:55:34
With the values of alpha hat at 0.28, beta hat at 0.05 that are held there for three periods and then we'll plot it and compute the MSC. So we see here visually that the model went down to the same value but then it fell a little too far and it ended up reaching about the same place as the the COVID data about five months after the the low point unemployment and then it continued its recovery. But in this region from 0 to about five the model actually underestimated the recovery and employment. This was better than the previous model if you remember the previous model moved up in this region and quickly recovered and visually we can kind of see that that's better but precisely and numerically we can see that it's better because our MSC for the COVID era data went down from about four or four and a half to now it's about two and a half. So we've made an improvement in the mean squared error between the yellow line representing our model and the blue line representing the COVID era data. So these initial test values as we just

00:55:22 --> 00:56:55
model and the blue line representing the COVID era data. So these initial test values as we just noticed did improve the MSC but we didn't have any reason to believe that the values we picked were going to be the optimal or best values. There are infinite numbers of configurations of alpha hat, beta hat and end we could have tried or infinite number of other models we could have used. And so what we're going to do next is we're going to assume the role of the visual econometrition and hard goals will be to look at these graphs and graphs and the MSC to try to manually adjust alpha hat, beta hat and end to improve upon this MSC of 2.6. So we're going to utilize the ipy widgets library to create an interactive version of our chart. So here we see that we're going to wrap the simulate plot and MSC steps in a function and then we're going to use the intractor team and say how we'd like alpha hat to be manipulated, beta hat to be manipulated and then a starting value for end. And what this did was it returned our MSC like we had before and we see that we're still near

00:56:49 --> 00:58:17
And what this did was it returned our MSC like we had before and we see that we're still near that value of 2.6. Any deviation from exactly 2.6 is because there is randomness in the simulation. So we shouldn't expect to get the same value every time. It'll be slightly different each time we run it. So we have the MSC we have the plot but now we also have these three little widgets here that allow us to be the visual econometrition. If we slide this bottom widget, this will allow us to change the value of n or change the stickiness of the alpha hat and beta hat parameters. And you'll see here as we make n quite large but is doing to our plot here is it's making this region where the model has an output employment percentage of about 0.85. It's making that longer and longer. So we see here that n is currently at a value of 9, which corresponds visually to going from 0 to 9 here and it has this whole region. Then as soon as the model parameters are set back to their steady state value, we see this very sharp and quick or very smooth and quick recovery back to the steady state.

00:58:11 --> 00:59:39
we see this very sharp and quick or very smooth and quick recovery back to the steady state. So let's set this back to something a little more moderate. Maybe three. And let's see what happens if we play with the job finding rate alpha hat. As we make this bigger, so we would like to make our recovery a bit faster than it currently is. We'd like to make sure that we're able to move up and track this blue line better. So if we're going to change alpha hat, the job finding rate would need to make it bigger. So it would make it bigger and bigger and bigger and it looks like this current value of 42 percent would match the data pretty well. So if there was some way that we could raise the job finding rate from the steady state value of about 37 percent to about 42 percent, that might help the model match closely the data obtained from the BLS in the covid error. Now if we think that this is probably not what's happening during this time period that there's probably unlikely that more unemployed workers are finding work, this may not be what we want to do.

00:59:31 --> 01:01:00
probably unlikely that more unemployed workers are finding work, this may not be what we want to do. We might not want to be increasing the parameter alpha hat because that's inconsistent with our beliefs about what's happening in the economy. In addition to looking at this just raw data of the percentage of workers employed and unemployed, we could kind of use some of our other knowledge about the total claims of workers claiming unemployment rising to maybe discount or discard the hypothesis that the job finding rate is increasing to make this match. So what we'll do is we'll go and we'll make this a little bit smaller. In fact it's probably more likely that the job finding rate has fallen from a steady state value and we'll leave it here at something smaller than the 37 percent in steady state. Now what's probably true is that the relative to the steady state value of 0.01 which saw this two eager recovery from the model. We might believe that the job separation rate has increased. So let's go ahead and increase it from 1 percent to 2 percent and we'll see here that we're very

01:00:53 --> 01:02:23
So let's go ahead and increase it from 1 percent to 2 percent and we'll see here that we're very closely matching the data and only at this kind of period three here do we see that it starts to take off a little too fast. So let's go ahead and leave these values at 25 percent and 2 percent and move up N1 more and now we see here that if we allow these adjusted parameters to persist for four periods we're very closely the model's very closely tracking the COVID era data. So let's summarize what we did. We first started experimenting and understanding with what a change in N did and what it did is it delayed the rapid but smooth increase from whatever value of employment percentage was currently held back to the steady state. As we made N very large this value lengthened the duration over here until we see the rapid increase to the steady state. If we make N very small then the if we again 0 so there is no alpha-hatter beta-hat then we see an immediate sharp or immediate quick increase back to the steady state.

01:02:17 --> 01:03:40
then we see an immediate sharp or immediate quick increase back to the steady state. Then we thought about what if we only move alpha-hat what if we only change the job finding rate. We found that in order to match the dynamics observed in the COVID era data we would need to have a job finding rate that was higher than the steady state job finding rate. This was inconsistent with our beliefs as observant economists that more unemployed workers were finding our beliefs were that there were more people being unemployed and it was harder for them to find jobs. So trying to sort of have an increase in this job finding rate was inconsistent with that. So we then said well what if we move the job finding rate somewhere below the steady state? How much would we need to then increase the job separation rate to have the model match the COVID era data and that led us to increase the job separation rate from 1% to 2%. And this exercise that we just performed is kind of the cracks of doing computational social

01:03:32 --> 01:05:01
And this exercise that we just performed is kind of the cracks of doing computational social science. It included three main components. One was the data. That was this blue line that was the actual or real world data that we were trying to match and we were trying to have our model track. The second main component was the statistical model. It was the hypothesis that the model parameters alpha and beta shifted from their steady state values to alpha-hat for finite number of periods. We had a way to toggle these parameters and adjust them and that led to different statistical models that we had a direct solution to. A direct problem solution to and we were able to produce these simulations. So feature number one was the data. Feature number two was the model and then feature number three was our prior beliefs about which levels of parameters are plausible and that helped us applying these prior beliefs helped us arrive a set of parameters that both match the data and were consistent with our beliefs.

01:04:53 --> 01:06:17
a set of parameters that both match the data and were consistent with our beliefs. Had we not imposed that we may well have stopped when we found that alpha-hat of 45%. But our belief said that that was inconsistent with what we knew and observed about the world. Even though it was outside the model, there's nothing in our model about individuals claiming unemployment or businesses closing down. The fact that we knew those things were happening formed our prior beliefs and that led us to change the set of parameters we were willing to consider. Those three components data, model and prior beliefs are at the heart of a lot of computational social science and the heart of a lot of what we will be doing as we move forward in this lecture. In this lecture in this course. Okay, so we're going to summarize these three values. We had 25% job finding rate, a 2% job separation rate and that these were last for four periods after that initial shock to the state of the workers. When we did that, the MSC fell from about 2.4 to about 1.7.

