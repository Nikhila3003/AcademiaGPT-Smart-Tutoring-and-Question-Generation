1
00:00:00,000 --> 00:00:05,280
This is Chase, and today we're going to be talking about some of the basic functionality

2
00:00:05,280 --> 00:00:07,680
that we have in pandas.

3
00:00:07,680 --> 00:00:11,960
At the end of the lecture today, you should have been familiar with what is a date time

4
00:00:11,960 --> 00:00:18,120
object, and you also should have learned how to use aggregation functions, transformation

5
00:00:18,120 --> 00:00:24,560
functions, and scalar transformation functions, and apply built-in and create customized versions

6
00:00:24,560 --> 00:00:29,000
of these to make transformations to your data.

7
00:00:29,000 --> 00:00:34,640
We'll also talk about how to select subsets of your data using Boolean selection, and we'll

8
00:00:34,640 --> 00:00:39,880
introduce you to what we call the Wantokreator, and we'll teach you how to apply it.

9
00:00:39,880 --> 00:00:44,520
The data that we'll be using for today's course comes from the US State Unemployment Data

10
00:00:44,520 --> 00:00:48,600
from the Bureau of Labor Statistics.

11
00:00:48,600 --> 00:00:50,000
So this is our outline.

12
00:00:50,000 --> 00:00:52,840
Let's go ahead and get started.

13
00:00:52,840 --> 00:00:59,680
So as usual, we're going to import our pandas as PD, and we're going to activate our custom

14
00:00:59,680 --> 00:01:03,000
plotting theme.

15
00:01:03,000 --> 00:01:07,120
Now, whenever we do a pandas lecture, this is our first one.

16
00:01:07,120 --> 00:01:11,880
We'll typically read in some kind of a data set that we'll use as motivation for the things

17
00:01:11,880 --> 00:01:13,360
that we do in class.

18
00:01:13,360 --> 00:01:19,560
Again, in this case, we're going to be reading in State Unemployment, and we want to

19
00:01:19,560 --> 00:01:24,200
take note that there's going to be a column named date, and we're going to specify that

20
00:01:24,200 --> 00:01:28,640
that column should be read in as a date object.

21
00:01:28,640 --> 00:01:33,320
So if we look at this, we can see the structure of our data.

22
00:01:33,320 --> 00:01:39,120
It has a column for date, a column for state, a column for how many people are in the labor

23
00:01:39,120 --> 00:01:44,600
force in that state, and an unemployment rate.

24
00:01:44,600 --> 00:01:49,480
And you don't have to worry about this, but what we want to do is we want to have a column

25
00:01:49,480 --> 00:01:58,080
for each state, and then along the index, we would like to have the dates, so one per month

26
00:01:58,080 --> 00:02:02,280
in our case, and the values that we'd like inside of our data frame will be the unemployment

27
00:02:02,280 --> 00:02:03,280
rates.

28
00:02:03,280 --> 00:02:07,920
And this is just some reshaping that we'll learn to do later in this course to make sure

29
00:02:07,920 --> 00:02:10,560
that we have the data that we want.

30
00:02:10,560 --> 00:02:16,600
So again, you should just take note that the states are the columns, the date is the index,

31
00:02:16,600 --> 00:02:21,080
and the values are unemployment rates.

32
00:02:21,080 --> 00:02:25,120
And to keep the data manageable, we're just going to focus on seven states.

33
00:02:25,120 --> 00:02:32,120
We're going to focus on Arizona, California, Florida, Illinois, Michigan, New York, and Texas.

34
00:02:32,120 --> 00:02:36,360
So this unimp, data frame that we have will be the data frame we work with for the rest

35
00:02:36,360 --> 00:02:39,920
of this course.

36
00:02:39,920 --> 00:02:45,360
So the first thing we can do is we can plot the data, and we talked briefly about this

37
00:02:45,360 --> 00:02:50,240
in the last video that it will create this plot for us.

38
00:02:50,240 --> 00:02:55,800
The thing I want to highlight right now is that it knows that the unemployment that the

39
00:02:55,800 --> 00:03:02,200
date column has been moved to the index, and so when we plot, it places this on the index

40
00:03:02,200 --> 00:03:04,480
for us and labels it.

41
00:03:04,480 --> 00:03:09,280
Additionally, it will create one line for each of the columns, and this is why we chose

42
00:03:09,280 --> 00:03:13,840
to have seven columns instead of 50, and it's going to label these for us.

43
00:03:13,840 --> 00:03:16,600
So this purple line is Arizona.

44
00:03:16,600 --> 00:03:18,680
It's really low unemployment rate.

45
00:03:18,680 --> 00:03:22,640
It's from Texas.

46
00:03:22,640 --> 00:03:29,520
So we see that we get this nice plotting features for free.

47
00:03:29,520 --> 00:03:36,960
So let's go ahead and look at what index we have on our data frame.

48
00:03:37,360 --> 00:03:41,160
You'll notice that it has these nice formatted text.

49
00:03:41,160 --> 00:03:45,120
So 2,000, 0, 1, 0, 1.

50
00:03:45,120 --> 00:03:52,320
And that's because we specified when we read the data in, that column or that data was a

51
00:03:52,320 --> 00:03:53,640
date time object.

52
00:03:53,640 --> 00:03:59,200
And so pandas is recognized that this is a date time object because we told it, and

53
00:03:59,200 --> 00:04:03,600
it's going to allow us to do some special things with it.

54
00:04:03,640 --> 00:04:09,200
So for example, we can index into a particular date.

55
00:04:09,200 --> 00:04:15,600
So if we choose to look at January 1st from the year 2000, we see that Arizona had an unemployment

56
00:04:15,600 --> 00:04:16,800
rate of 4.1.

57
00:04:16,800 --> 00:04:19,440
Texas had an unemployment rate of 4.6.

58
00:04:19,440 --> 00:04:25,040
And California had an unemployment rate of 5.

59
00:04:25,040 --> 00:04:29,680
We could also choose all of the days between New Year's Day and June 1st in the year

60
00:04:29,760 --> 00:04:34,000
2000 just by putting 0,001, 2000.

61
00:04:34,000 --> 00:04:41,120
And notice we can pass dates in in different formats and going to June 1st, 2000.

62
00:04:41,120 --> 00:04:51,200
And that will now give us a data frame with six rows, where each of the rows corresponds to a month.

63
00:04:51,200 --> 00:04:56,480
So we'll talk much more about this in a future lecture, but we just wanted to highlight that.

64
00:04:56,480 --> 00:05:02,240
You have this functionality available to you.

65
00:05:02,240 --> 00:05:07,200
So the next topic we'll talk about are data frame aggregations.

66
00:05:07,200 --> 00:05:12,800
And loosely speaking, aggregation is an operation that takes multiple values and turns it into

67
00:05:12,800 --> 00:05:15,040
a single value.

68
00:05:15,040 --> 00:05:19,040
The example that we should all be familiar with is computing a mean.

69
00:05:19,040 --> 00:05:25,520
So taking the three numbers, 0, 1, and 2, and computing their mean returns a single number,

70
00:05:25,600 --> 00:05:26,800
1.

71
00:05:26,800 --> 00:05:32,400
We're going to use aggregations extensively in this course, and we're very grateful that

72
00:05:32,400 --> 00:05:35,600
pandas will make this easy for us.

73
00:05:35,600 --> 00:05:40,480
So pandas has some of the most frequently used aggregations already built in.

74
00:05:40,480 --> 00:05:48,240
For example, mean variance, standard deviation, finding the minimum, median, max, etc.

75
00:05:49,600 --> 00:05:51,760
We do want to note here, this is not all of them.

76
00:05:51,760 --> 00:05:58,240
There's many other aggregations pandas has that we have not listed, and we just encourage you to

77
00:05:58,240 --> 00:06:00,080
use a little bit of tab completion.

78
00:06:00,080 --> 00:06:07,280
So if you take a data frame, so again, we can take the mean.

79
00:06:09,840 --> 00:06:14,880
If you hit tab, notice it brings all of these options up.

80
00:06:15,120 --> 00:06:23,440
And let's see an aggregation that we might do is standard deviation.

81
00:06:23,440 --> 00:06:28,640
So if we just type ST and hit tab, it brings up STD.

82
00:06:28,640 --> 00:06:33,760
And if we wanted to know what that was, we could put a question mark and return sample standard deviation

83
00:06:33,760 --> 00:06:35,120
or requested access.

84
00:06:36,080 --> 00:06:42,560
Normalized by N minus 1 by default, so it tells us a little bit more, and we could then use that to get

85
00:06:42,640 --> 00:06:43,680
the standard deviation.

86
00:06:46,000 --> 00:06:52,240
So by default, as you might have noticed, aggregation operates on the columns.

87
00:06:53,120 --> 00:06:59,120
We can use the access argument though to specify whether we'd like to do aggregations by rows.

88
00:06:59,120 --> 00:07:07,200
So remember, the zero axis is going, you collapse the zero axis, which is going to

89
00:07:07,200 --> 00:07:14,160
collapse over the rows. But if we specify Axix equals 1, we can collapse over the columns.

90
00:07:14,160 --> 00:07:20,640
So when we take the variance with respect to Axix equals 1, we're getting the variance of the

91
00:07:20,640 --> 00:07:28,080
unemployment rate in the year 2000 in January 2000, or February 2000, etc.

92
00:07:31,840 --> 00:07:36,160
So the delta and aggregations get us pretty far in our analysis.

93
00:07:36,160 --> 00:07:42,080
But sometimes you need a little bit more flexibility. We can have pandas perform custom aggregations

94
00:07:42,080 --> 00:07:48,960
by following the next two steps. One, you write a Python function that takes a series of

95
00:07:48,960 --> 00:07:56,160
an input and outputs a single value, and then you can call the Ag method with the function as an argument.

96
00:07:57,360 --> 00:08:03,280
So let's do an example below where we classify states as low unemployment or high unemployment,

97
00:08:03,280 --> 00:08:07,120
based on whether there are mean unemployment level is above or below 6.5.

98
00:08:09,440 --> 00:08:16,560
So step one, let's write the aggregation function. So higher low is a function that takes S,

99
00:08:16,560 --> 00:08:23,520
which is going to be a pandas series object. And if the mean of S is less than 6.5,

100
00:08:23,520 --> 00:08:30,160
we'll return low, and if it's higher than 6.5, which is then the L's, then we'll return high.

101
00:08:30,480 --> 00:08:36,480
And so now, step two, all we have to do is call.ag.

102
00:08:37,840 --> 00:08:43,920
And notice we've now classified Arizona as a low unemployment state, California as a high unemployment

103
00:08:43,920 --> 00:08:54,320
state, etc. We also can get the access equals one. And we can see which of the months

104
00:08:54,320 --> 00:09:00,080
were low or high unemployment. Now all of the ones we can see are low unemployment, because they

105
00:09:00,080 --> 00:09:07,840
weren't times growth, but if we looked at, so let's go ahead and do our fancy date indexing.

106
00:09:07,840 --> 00:09:15,840
So let's go from January 2008 to October of 2008.

107
00:09:20,800 --> 00:09:25,120
And we noticed that at the end of 2008, we started seeing high unemployment rates.

108
00:09:30,480 --> 00:09:37,600
And we just did this great. And the other thing you might notice is that ag can accept multiple

109
00:09:37,600 --> 00:09:42,880
functions. So here we're passing in three functions. We're passing in the function min,

110
00:09:43,840 --> 00:09:53,040
max, and then higher low. And what this will return is a data frame now, where we have the columns,

111
00:09:53,120 --> 00:10:01,760
the same as the data frame that we passed in. And now we have a row for each of the functions.

112
00:10:01,760 --> 00:10:10,960
So the min unemployment rate in Texas was 3.9. The max unemployment rate in Michigan was 14.6.

113
00:10:11,680 --> 00:10:16,320
And then we have our high low classifier that's telling us whether each of the states is a

114
00:10:16,320 --> 00:10:26,640
high or low unemployment state. So now let's take a pause and let's do an exercise to see whether

115
00:10:26,640 --> 00:10:33,680
we understood the concepts that were just presented. So at each state, at each date, we'd

116
00:10:33,680 --> 00:10:38,480
like you to find the minimum unemployment rate across all of the states in our sample.

117
00:10:40,080 --> 00:10:43,520
We'd like you to find what was the median unemployment rate in each state?

118
00:10:44,480 --> 00:10:48,320
What was the maximum unemployment rate across the states in our sample?

119
00:10:49,040 --> 00:10:52,800
What state did it happen in? And in what month or a year was this achieved?

120
00:10:54,160 --> 00:10:59,040
And then we'd like you to classify each state as being high or low volatility.

121
00:10:59,040 --> 00:11:02,880
Based on whether the variance of their unemployment is above or below four.

122
00:11:03,600 --> 00:11:10,720
And we'll go ahead and pause for five to ten minutes, this five minutes, and see how we see how we do.

123
00:11:11,040 --> 00:11:19,040
Let's review the answers. So these exercises asked first at each date, what is the minimum

124
00:11:19,040 --> 00:11:24,880
unemployment rate across all of the states in our sample? And the way we can find that is for each

125
00:11:24,880 --> 00:11:29,760
month, we want to find the minimum unemployment rate, which means we want to take the minimum

126
00:11:29,760 --> 00:11:39,360
across the columns or the axis. So axis equals one. Then the second was what was the median unemployment

127
00:11:39,680 --> 00:11:46,960
rate in each state since this is the median across within a column is we're going to just take the

128
00:11:46,960 --> 00:11:51,680
median, which will default to axis equals zero, which collapses the rows.

129
00:11:53,680 --> 00:11:59,040
Next we're going to try and find what the maximum unemployment rate was across each of the states.

130
00:11:59,920 --> 00:12:04,080
What state did this maximum unemployment rate happen in? And when was it achieved?

131
00:12:05,040 --> 00:12:09,760
And the way we're going to do that is first we're going to take the maximum across all of

132
00:12:10,320 --> 00:12:14,800
the unemployment which will tell us the maximum unemployment rate in each state. And then the

133
00:12:14,800 --> 00:12:22,800
max will tell us what the maximum unemployment rate was. IDX max, again the first part is the same,

134
00:12:22,800 --> 00:12:28,240
is going to tell us which state that occurred in? And then we're going to take the maximum

135
00:12:29,200 --> 00:12:34,800
according to axis equals one to find the maximum unemployment rate in each month. And if we take the

136
00:12:34,800 --> 00:12:40,560
IDX max that will tell us what it occurred. And so the maximum unemployment rate in our sample was

137
00:12:40,560 --> 00:12:50,480
14.6%, which happened in Michigan in June of 2009. And the last question in this exercise was

138
00:12:50,480 --> 00:12:54,640
to classify each state as high or low volatility based on what did the variance of their

139
00:12:54,640 --> 00:13:04,560
unemployment was above or below four. So we define a new function an employment rate volatility

140
00:13:04,560 --> 00:13:13,360
classify, which takes a series an input. And then it returns either the string high or low depending

141
00:13:13,360 --> 00:13:23,040
on whether the variance is above or below four. And so what we see is Arizona, California, Florida,

142
00:13:23,040 --> 00:13:28,480
and Michigan are high variance unemployment rate states and Illinois, New York, and Texas are

143
00:13:28,480 --> 00:13:34,640
low variance unemployment rate states. Hopefully these lined up with what you did. Otherwise you should

144
00:13:34,640 --> 00:13:41,360
review after class and feel free to ask us questions. Great. So we're done talking about aggregations

145
00:13:41,360 --> 00:13:48,160
for now. And next we're going to talk about what we call transforms. And transforms are going to

146
00:13:48,160 --> 00:13:54,720
be analytical operations of some sort that do not necessarily involve an aggregation. So the output

147
00:13:54,720 --> 00:14:01,600
of a transform would be a series that is mapped to another series. Some examples of what this might

148
00:14:01,600 --> 00:14:07,360
be are the percentage change in unemployment rate from month to month or the cumulative sum of the

149
00:14:07,360 --> 00:14:16,000
elements in some column. So there's lots of built-in transforms for us just like there were aggregations.

150
00:14:16,080 --> 00:14:22,560
We can take the cumulative sum, the cumulative min, the cumulative max, the cumulative product,

151
00:14:22,560 --> 00:14:28,480
we can take the difference. We can do element-wise additions, subtraction, multiplication or division,

152
00:14:28,480 --> 00:14:35,040
percent change, value counts, or absolute values. And again, we encourage you to explore what

153
00:14:35,040 --> 00:14:43,680
functions are available using tab completion. So let's go ahead and do a couple of examples. So we call

154
00:14:43,760 --> 00:14:53,600
this as the data that we have. The first we're going to do is computing the percent change.

155
00:14:53,600 --> 00:14:58,480
Notice it can't compute the percent change for the first element because there's nothing that proceeds it.

156
00:15:00,000 --> 00:15:06,080
And then we see that unemployment rate didn't did not change in most of the dates, but in Michigan

157
00:15:06,080 --> 00:15:14,800
it went down by 3%. And it compute these percent changes for us. Additionally, we can compute

158
00:15:14,800 --> 00:15:19,840
the difference, which will instead of taking the percent change, it's just going to take the difference.

159
00:15:19,840 --> 00:15:26,480
And again, the first row is going to be all missing because you can't take the difference between

160
00:15:26,480 --> 00:15:35,360
something before the first row. So we classify transforms into a few main categories.

161
00:15:36,400 --> 00:15:43,440
The first is going to be a series transforms, and that's a function that takes in one series

162
00:15:43,440 --> 00:15:49,840
and produces another series. And this will result in the index of the input and the output,

163
00:15:49,840 --> 00:15:56,560
not necessarily being the same. Scalor transforms are going to be functions that take a single value

164
00:15:56,560 --> 00:16:02,480
and produce a single value. So one example would be the absolute value method, which we're adding

165
00:16:02,480 --> 00:16:09,040
a constant to each value of a series. So these are going to take a particular value and do an operation

166
00:16:09,040 --> 00:16:16,160
to that one value rather than to the entire series. So just like we could write custom

167
00:16:17,120 --> 00:16:23,200
aggregation functions, we can also write custom transformations. The steps are going to be the same

168
00:16:23,200 --> 00:16:27,360
is we're going to write a Python function that will take a series and output a new series.

169
00:16:28,320 --> 00:16:33,600
And now rather than pass our new function as an argument to the Ag method for aggregation,

170
00:16:33,600 --> 00:16:39,600
we're going to pass it to the Apply method. So as an example what we're going to do is we're

171
00:16:39,600 --> 00:16:45,040
going to standardize all of the unemployment data to have mean zero and standard deviation one.

172
00:16:45,760 --> 00:16:51,120
And we'll do this so that we can classify which date the unemployment rate differs most from normal

173
00:16:51,120 --> 00:16:58,320
times in each state. So step one is to write a function that can standardize the data. So it takes

174
00:16:58,320 --> 00:17:04,160
it a series and it's going to take the mean of that series and the standard deviation and then

175
00:17:05,040 --> 00:17:10,320
it's going to create a new series that's going to be the original series minus the mean

176
00:17:10,320 --> 00:17:20,720
divided by the standard deviation. Next we can apply this to our data and what we'll see is that

177
00:17:20,720 --> 00:17:30,320
in the early 2000s we had negative values for our, they call this a Z transform.

178
00:17:31,920 --> 00:17:39,360
So our standardize data is going to have minus one which means that the data was abnormally low

179
00:17:40,880 --> 00:17:48,400
which in unemployment is a good thing. And then finally we're going to take the absolute value

180
00:17:48,480 --> 00:17:57,040
of all of the elements and again this is a scalar transformation because it's going to take

181
00:17:57,040 --> 00:18:03,040
one value and map it into one new value and do this for each value in the data frame.

182
00:18:04,800 --> 00:18:10,240
And now let's go ahead and find out when the unemployment rate was most different than normal

183
00:18:10,320 --> 00:18:19,280
from for each state. And so what we see is that the 2008-2009, 2010 recession

184
00:18:20,080 --> 00:18:27,120
really did strange things to unemployment rates and those were the most abnormal unemployment rates in our series.

185
00:18:30,000 --> 00:18:36,320
And in addition to doing customized series transformations we can also do customized scalar

186
00:18:36,640 --> 00:18:41,520
transformations. And the way that you do this is again on the first step we'll always be to find a

187
00:18:41,520 --> 00:18:46,320
Python function and in this case it will take a scalar as an input and produce a new scalar.

188
00:18:47,040 --> 00:18:52,640
And second it's going to pass this function as an argument to the apply map method.

189
00:18:54,240 --> 00:18:59,440
So let's go ahead and look at this. So imagine we want to determine whether unemployment rate was

190
00:18:59,440 --> 00:19:07,200
high greater than 6.5 medium or low. So we can write a Python function that takes a single

191
00:19:07,200 --> 00:19:12,320
number as an input and outputs a single string. And then we're going to pass this function

192
00:19:13,040 --> 00:19:19,840
to apply map. So one question you should ask yourself is why are you passing this to apply map

193
00:19:19,840 --> 00:19:26,080
and not aggregate or apply? And think about why neither aggregate or apply will work.

194
00:19:26,160 --> 00:19:33,840
So here's our function we define unemployment classifier and if it's above 6.5 we write

195
00:19:33,840 --> 00:19:39,600
high, it's above 4.5 but below 6.5 we return medium otherwise we return low.

196
00:19:42,160 --> 00:19:53,040
And now when we do apply map what happens? We're going to get back a new data frame and notice

197
00:19:53,040 --> 00:19:57,840
every value in this data frame is either going to be low, medium or high because those with

198
00:19:57,840 --> 00:20:05,520
the values that we could potentially put in. So now we're going to move on. We're doing these as

199
00:20:05,520 --> 00:20:12,560
examples rather than exercises. So let's use another transformation on unemployment bins which was

200
00:20:12,560 --> 00:20:18,560
the data frame we just created to figure out how many times each state had one each of the three

201
00:20:18,560 --> 00:20:24,880
classifications. So if you're doing this on your own which I'd encourage you to try and do after

202
00:20:24,880 --> 00:20:30,240
class, you should think about will this value counting function be a series or scalar transform?

203
00:20:31,120 --> 00:20:36,320
And Google pandas count unique value or something similar to find which transformation.

204
00:20:36,960 --> 00:20:40,960
We're then going to construct a horizontal bar chart of the number of occurrences of each

205
00:20:40,960 --> 00:20:49,920
level with one bar per state and classification. Okay so the method we were looking for is one called

206
00:20:49,920 --> 00:20:54,240
value counts and so what we're going to do is we're going to take unemployment bins.

207
00:20:55,280 --> 00:21:02,640
What do you remember? Looks like this and we're then going to count each of the

208
00:21:03,600 --> 00:21:10,640
times a occurrence of value occurs for each column and then we'll go ahead and look at what

209
00:21:11,040 --> 00:21:19,040
this looks like. So now we have for Arizona the high unemployment

210
00:21:20,720 --> 00:21:28,720
Arizona had high unemployment 75 times low unemployment 44 times and medium unemployment 97 times

211
00:21:30,800 --> 00:21:38,960
etc etc etc. And we're going to plot the transpose so that it does something so that's what

212
00:21:39,040 --> 00:21:46,560
this dot capital T was and let's go ahead and reorder this data a little bit so that we can have

213
00:21:48,960 --> 00:21:51,760
low medium high.

214
00:22:00,720 --> 00:22:07,600
Here we go and so if the blue bar is high it means that state has a low unemployment a lot of

215
00:22:07,680 --> 00:22:16,000
time whereas if the yellow bar is high it means that that state has high unemployment frequently.

216
00:22:16,880 --> 00:22:23,360
So from this chart you might not be particularly keen on California or Florida

217
00:22:24,240 --> 00:22:33,680
whereas somewhere like Texas or New York or Florida might be much better in terms of their unemployment rates.

218
00:22:33,920 --> 00:22:41,040
So finally we're going to repeat the previous step but now we're going to count how many

219
00:22:41,040 --> 00:22:47,920
states had each classification and each month. So we're going to do almost the same thing but

220
00:22:47,920 --> 00:22:53,680
now we're going to count across the columns so we're going to use access equals one and whenever

221
00:22:53,680 --> 00:22:58,480
there's a missing value which just means that none of the states had a particular value we're going

222
00:22:58,480 --> 00:23:03,440
to fill it with a zero because it just means that there is zero states with that value.

223
00:23:08,080 --> 00:23:13,760
And here we go this is what it looks like so notice in the early 2000s no states had high unemployment

224
00:23:14,960 --> 00:23:18,640
most states had low unemployment and a few states had medium unemployment.

225
00:23:20,160 --> 00:23:25,920
And so now which state which month had the most states with high unemployment?

226
00:23:26,800 --> 00:23:33,760
You might be unsurprised when we say 2009 which when was the most medium unemployment?

227
00:23:35,600 --> 00:23:47,360
2001 and when was there lots of low unemployment the beginning of 2000?

228
00:23:49,200 --> 00:23:55,120
And we found these with IDX Max which is going to search for the maximum about the index associated

229
00:23:55,120 --> 00:24:03,040
with the maximum value. Great so now we've covered aggregations, transforms and scalar transformations

230
00:24:04,160 --> 00:24:10,240
another thing that we'll often want to do is Boolean selection which is we're going to frequently

231
00:24:10,240 --> 00:24:15,840
want to select pieces of data based on whether certain conditions are met. For example if you were

232
00:24:15,840 --> 00:24:20,480
in marketing you might be interested in a particular target audience and so you might

233
00:24:20,560 --> 00:24:28,640
subset your data to only include adults between 18 and 32 if you had a particular product for adults,

234
00:24:28,640 --> 00:24:33,920
young adults. You might also be looking at data that corresponds to a particular time period

235
00:24:34,800 --> 00:24:39,840
or some of the work we did while we were most Spencer and I were at NYU as we looked at data that

236
00:24:39,840 --> 00:24:47,120
of corresponded to recessions. So we'll be able to do this by using a series or list of Boolean values

237
00:24:47,200 --> 00:24:54,720
index into a series or data frame. So we're going to take a small view of our unemployment data frame

238
00:24:54,720 --> 00:25:00,720
so we took the dot head which is going to be only five rows of our data and we're doing this so

239
00:25:00,720 --> 00:25:09,280
that we can see what's happening as we do these operations. So we can use Boolean to select particular

240
00:25:09,280 --> 00:25:17,760
rows so in this case we need seven values or five values associated with let's be explicit here.

241
00:25:17,760 --> 00:25:23,040
So five values that are going to be associated with our five rows so in this case we'll select

242
00:25:23,040 --> 00:25:28,800
the first three true true true and disregard the last two whether we're falsist.

243
00:25:29,280 --> 00:25:40,720
We can also use Boolean to select both rows and columns. So in this case we're again selecting

244
00:25:40,720 --> 00:25:46,080
the first three rows but now we're only going to select the first and last two columns.

245
00:25:50,960 --> 00:25:56,800
Now we did these we did these selections with lists but obviously you're not going to be able to

246
00:25:57,280 --> 00:26:02,560
write by hand a list with a million values in it. And so what we're going to do next is we're

247
00:26:02,560 --> 00:26:08,160
going to use conditional statements to construction new series of Boolean from our data and this will

248
00:26:08,160 --> 00:26:13,920
look a lot like some of what we covered in the conditional section of our introduction to Python.

249
00:26:14,880 --> 00:26:21,120
So what can we do here? So we're looking at the unemployment small data frame. The Texas

250
00:26:21,120 --> 00:26:28,160
column we're asking when the unemployment in Texas was less than 4.5. And what this says is that

251
00:26:28,160 --> 00:26:34,080
unemployment was above 4.5 for the first three months of the year but then in April and May

252
00:26:34,080 --> 00:26:42,640
the unemployment fell below 4.5. And once we have that we can start selecting subsets of our data.

253
00:26:43,200 --> 00:26:49,120
So notice we have unemployment small Texas less than 4.5 which is going to create a series

254
00:26:49,120 --> 00:26:56,480
that has chosen false in it and we can select all of the columns. And this just gives us the last two

255
00:26:56,480 --> 00:27:04,640
rows like we expected. We could check for when the unemployment in New York was hired in the

256
00:27:04,640 --> 00:27:12,080
unemployment in Texas and in this small data set that always happens to be true.

257
00:27:12,720 --> 00:27:21,680
And in the Boolean section of our basic Python lecture we saw that we could use the words

258
00:27:21,680 --> 00:27:28,800
and or or to combine multiple Booleans into a single Boolean. Just as a quick refresher we use

259
00:27:28,800 --> 00:27:35,440
and and or in the mathematical sense which means true and false results in false. True and true

260
00:27:35,440 --> 00:27:43,040
results in true false and false results in false. True or false is true. True or true is true

261
00:27:43,040 --> 00:27:48,400
and false or false is false. We could do something similar in Panthers but rather than writing

262
00:27:48,400 --> 00:27:55,920
Boolean and Boolean 2 we write Boolean series 1 and sign Boolean series 2.

263
00:27:58,800 --> 00:28:03,920
Likewise rather than writing Boolean or Boolean 2 we use the Python

264
00:28:04,560 --> 00:28:12,960
to denote or let's see this. So in this case we're going to look at when unemployment rate

265
00:28:13,360 --> 00:28:20,560
was below 4.7 in Texas and the unemployment rate was below 4.7 in New York. Notice the ampersand

266
00:28:22,320 --> 00:28:27,120
and this is going to say this is false for the first two months and true for the last three months.

267
00:28:27,440 --> 00:28:36,160
And if we select using this notice that the unemployment rates in both New York and Texas will

268
00:28:36,160 --> 00:28:44,560
be below 4.7 for every month. Sometimes we'll want to check whether a data takes on

269
00:28:45,200 --> 00:28:50,320
one of several fixed values. The way we could do this right now is we could write

270
00:28:50,320 --> 00:28:59,680
DF of a particular column x is equal to the value 1 or DF column x is equal to value 2 etc.

271
00:29:00,720 --> 00:29:04,720
But because this is a frequent operation, Panthers provides us a better way.

272
00:29:05,440 --> 00:29:09,760
So in this case we're going to look at the Michigan column and we're going to ask whether the

273
00:29:09,760 --> 00:29:17,600
values are in 3.3 and 3.2. And so with the first four months of the year 2000 the value of

274
00:29:17,600 --> 00:29:26,240
unemployment in Michigan was either 3.3 or 3.2. And we can confirm that here.

275
00:29:30,720 --> 00:29:35,680
And next we're going to talk about any and all methods. You may remember from our first

276
00:29:35,680 --> 00:29:41,120
Boolean in conditional lecture that the Python functions any and all are functions that take

277
00:29:41,120 --> 00:29:46,640
a collection of Boolean's and return a single Boolean. Any returns true whenever at least one of the

278
00:29:46,720 --> 00:29:50,800
inputs is true while all returns true only when all of the inputs are true.

279
00:29:51,600 --> 00:29:58,000
Series and data frames have with a deep-type Boole have dot any and all methods that can apply

280
00:29:58,000 --> 00:30:02,640
these logic. This logic. So what we're going to do is we're going to count we're going to learn

281
00:30:02,640 --> 00:30:07,280
this by counting how many months all of the states in our sample have high unemployment.

282
00:30:08,320 --> 00:30:12,240
And as we work through this example we think this is a great opportunity to talk about

283
00:30:12,320 --> 00:30:16,640
the Want operator which is a helpful concept that spins our anilerin from Tom.

284
00:30:18,000 --> 00:30:24,480
So here's how the Want operator works. You start by writing Want followed by what we want to

285
00:30:24,480 --> 00:30:30,080
accomplish. In this case we would write we want to count the number of months in which all states

286
00:30:30,080 --> 00:30:38,160
in our sample had unemployment above 6.5%. And now let's work backwards to identify a sequence of steps

287
00:30:38,240 --> 00:30:44,240
necessary to accomplish our goal. So to count the number of months in which all states in our

288
00:30:44,240 --> 00:30:50,640
sample had unemployment above 6.5 we would just want to sum the number of true values in a series

289
00:30:50,640 --> 00:30:56,720
indicating dates for which all series had high unemployment. So now how do we get a series that has

290
00:30:56,720 --> 00:31:04,080
true values when the states had an high unemployment? We would the one way to do this is to build a

291
00:31:04,080 --> 00:31:11,600
series by using the all method on a data frame containing Boolean indicating whether each state

292
00:31:11,600 --> 00:31:22,400
had high unemployment at each date. So what this means is we want to know when every state in our sample

293
00:31:22,400 --> 00:31:28,640
had high unemployment which we can do using the dot all method with access equals one.

294
00:31:29,600 --> 00:31:37,360
Finally, how do we get to one that can do this? We can just build a data frame by using a

295
00:31:37,360 --> 00:31:45,360
greater than comparison to compare whether the data was above 6.5%. So here is a plan and to apply

296
00:31:45,360 --> 00:31:51,040
the Want operator all we're going to do is start at step three and work our way back. And the reason

297
00:31:51,040 --> 00:31:56,400
we find this helpful is because if you start knowing what you want it's easier to figure out what

298
00:31:56,400 --> 00:32:04,720
you need to get one step ahead and you keep stepping backwards inductively. So great step three

299
00:32:05,600 --> 00:32:14,720
unemployment greater than 6.5. This tells us well the early 2000s were good years for the unemployment

300
00:32:14,720 --> 00:32:22,720
rate so in this case they're all false. In step two we're going to use the dot all method with

301
00:32:22,880 --> 00:32:32,880
access equal one which will check whether every single column is equal to true for and we're going to

302
00:32:32,880 --> 00:32:40,720
apply this to every row. So high dot all access equals one will give us a new series and now it's

303
00:32:40,720 --> 00:32:46,960
going to have a true or a false based on whether all of the states had high unemployment rates.

304
00:32:47,840 --> 00:32:55,920
And now finally simply setting this series with LSU in all states had high unemployment. And so

305
00:32:56,640 --> 00:33:05,120
out of 216 months only 41 months had high unemployment in all states. And this ties up our

306
00:33:05,120 --> 00:33:10,960
Pandus-based lecture. Thanks for being here. But for you to see me soon. Bye.

