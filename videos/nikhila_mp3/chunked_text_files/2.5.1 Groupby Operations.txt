Hello, this is Spencer Lyon and I'm really excited about our topic today.Today we're talking about Group by Operations and Pandas, which in my opinion is one of the very coolest and most powerful operations we can do.
Let's take a stock of our Pandas journey thus far.We started out by learning about the core data types in Pandas.
This includes the series and the data frame.We then learned how we can do operations such as extracting values or subsets of values from our series and data frame objects.We learned about how we can do arithmetic, either on single values or on entire columns or entire data frames all at once.We then studied how we can organize our data in Pandas using the index and the column names.We saw how a careful selection of the index and columns names could help with analysis because Pandas will align the data for us using the index and column names.We then took some time to understand how to reshape data, how to maybe transform it from wide to long format, as well as how we can compute things like pivot tables and other summary forms of theforms of the data.Finally, we've learned how to merge two different data sets, different about related data sets, on one or more key columns.Today we continue with what in my view is the kind of crowning functionality of Pandas.
It will help us utilize all the tools we've developed thus far and do some really compelling analysis.And that functionality we'll learn about today is called GroupBuy.Our plan for today and the way the class will unfold is that we will be first understanding the split apply combined strategy for analyzing data.If you haven't heard of this before, don't worry. We're going to be learning a lot about it as we move through the lecture today.Well, then learn once we've split the data, we'll learn how we can use some of the built-in Pandas routines for computing aggregate values based on subsets or groups of our data.Some of these built-in routines we're already familiar with, they are things like the mean, median, or variance.We'll also understand how we can create our own custom aggregation operations by defining Python functions and then ask Pandas to apply our own functions to the grouped data.Finally, we'll understand how we can use multiple keys to group by more than one column.The data we'll be using for today comes from the United States Bureau of Transportation Statistics and contains detailed data on all delayed flights in the domestic United States from December 2016.We'll begin by importing our standard packages that we'll be using throughout the class today.If you haven't installed the QEDS or Quant Econ Data Science Library, you can uncomment that second line in this first code cell and have PIP install this for you.We've already done this, so we'll execute this cell, which will just move us past it.
And then we'll be able to actually import our libraries.Today we'll be using our well-known friends Pandas and NumPy.
In addition, we'll also be using Matplotlib to show some charts.We'll import the base Python Random Library and then we'll use QEDS to help us style our charts.
We'll run this cell to import the libraries now.Let's begin by talking about the Split Apply Combined Strategy.
As you might guess, there are three steps to this strategy.First is the Split Stage. Here, we take our entire data set, and based on the values in one or more columns, we will split the data set into different subsets.One subset is similar in the sense that these key columns all have the same value as any other row in the subset.After we've created these split data sets, we then apply some function or logic or operation on each of the subsets.This will work through each set one of the time and it will apply the chosen function to each of them.Finally, once we're done applying our operation to each subset of data, Pandas will then combine all the data sets or all the outputs for us into a final data frame that we can continue our analysisour analysis with.We're going to cover these concepts right now, one at the time, and we'll cover the basics and the concepts behind it.However, there's a lot of power and functionality here, and we won't have time to cover all of it in our time together today.So, as with other topics, we strongly encourage you to look at the official Pandas documentation for more information on what you can do using the group by machinery.So, in order to describe the operations, we're going to need some data.
And we're going to start with this artificial data set that we're creating right here to the side.Notice that the data frame we end up with has three columns, A, B, and C.
Columns A and B are filled with integers.
You can see here that column A has the integers 111, 222.Column B has the same numbers, but the order of the rows are different.
And then finally, column C has floating point numbers with a few missing values.We're going to use this example data set to demonstrate the three steps in split apply combined.
To begin, we'll start with the split step.In order to ask Pandas to split the data for us, we use the group by method of a data frame.
You see here that we're calling df.groupby and we're passing the string A.This instructs Pandas to construct groups of our data using the values from the A column.This is the most basic and often most used form of the group by method to split on the values of a single column.
We can check the type of this GBA object.And we see here a very long type name, but we're just going to refer to this as a group by for short.
Once we have a group by object, there are a few things we can do with it.One thing we could do is we could ask to get the subset of data for a particular group.
Here we're going to say gba.getgroup and we're going to pass one and then we'll pass two.Notice when we do this, that the data we get in return has all the rows of our original data set where column A is equal to one.
That's what we saw up there in our first example.And then all of the rows where the column A has the value of two is what we get in the second code cell.
This is not a numerical index that Python would start counting at 0, 1, 2, and so on.This is actually you give it a value from the data frame and it will return rows with that matching value.
So let's do an example.This could be an exercise, but we're going to do it here together in class.So once we have our group by object, in addition to selecting the rows that belong to a particular group, we can apply some of our favorite aggregation functions directly to the group by object.So let's go ahead and remind ourselves what the data frame looks like and then we will compute the gba.sum.
And when we do this, notice the following.So whenever a the values of a ended up being on the index of our data frame.
Notice here that there are only two rows, one with index a equal to one and the second with index a equal to two.We end up with two rows because there are only two distinct values in the a column.
Now in the output of the a column of the original data frame.Now in the output, notice that when a equal one b has a value of four in the output of gba.sum.This is because if we look carefully at the values of the b column for any row where the a column equals one, we see that those values are one, one, two.The sum of these three numbers is of course four, which is what we see here in the a equal one column b row of the output.
Similarly, we can do a similar of the same operation for the c column.We look at the value of the c column when a equal one and we see that we have the values 1.0, 2.0, 3.0.
If we sum these three things up, we get the answer 6.0.Now let's move down to the second row of our output.
Here we have a equal two and we see that the value for b is again four because we have the three numbers 2,1,1 from the array.Well, slightly more interesting is the value for c.
Notice here that we have the value of c equal to five.
This comes because in our data set, we had three values.
One, c is nann, five, and nann.And when pandas did these operations, it decided to ignore the nanns for us when we did the summation and only compute the sum of the single real valued number five.That's why we're left with the number five as our solution.
Now there's another example here that we're going to ask that you do as an exercise.So we'll describe what the exercise is and then we'll pause for a moment so that you can work through it.What we would like for you to do is take the GBA object that we computed earlier and use tap completion or introspection to see what other methods are available beyond just the sum.You may have already thought of some of these other aggregation methods that you've used on a data frame and chances are they also exist on the group by object, but here you have a chance tohave a chance to interact, interactively discover them.We'll go ahead and pause here and we'd like for you to find three methods and try applying them to your group by object.
We'll pause at this time so that you can complete this exercise.Okay, welcome back.
We were going to go ahead and we'll continue on. Hopefully we were able to find three methods. We'll continue on with the rest of the lecture.So in addition to grouping by a single column as we did with group by a last time, we can actually group by multiple columns.And the way we do this is instead of passing a single string with the column name in it, we can pass a list of strings with more than one column name.The result of applying this group by operation will be that the data frame will be split into collections of rows where there are unique combinations of multiple columns.Let's see what this looks like. So now we're going to do a GBAB, which is equal to the data frame dot group by both a and B in a list.Let's see here that the type matches the type of GBA that we saw before. So all the same type of operations that we were doing will still apply.
Let's try the get group one.So before we were calling get group one when we had just GBA and here because we've chosen to group by two columns, we need to pass two values and we'll do this in a tuple.This is similar to the indexing behavior when you have a multi index data frame in that you pass a tuple where each element of the tuple represents one level of your index.Here each element of the tuple represents one level of the grouping.We're going to pass one one which will extract pandas to give us all of the rows of the data frame for which column A is equal to one and column B is equal to one.We see here that that's what we have in return to us.
So we can still apply these aggregation methods like some mean count variance, maybe some of the ones that you found in the exercise.And notice what happens is on the index when we call count, we have two levels down. We have an A level and a B level.We were left with a data frame with only a single column because from our original data frame that had three columns, we used two of them for grouping and we were left with a single column C that hascolumn C that has values in it.Notice that the index for levels for A have values one and two and the same thing for B.
This is because the distinct values in columns A and B were both one and two.Now notice the values we have in the C column, it says 2110.What this means is that there were two rows in the original data frame where column A had a value equal to one and column B had a value equal to one.Then there was only one row where we had a equal one be equal to or a equal to be equal one.
That's what the second and third row of this output show.Finally, there were zero rows that had A equal to and B equal to.We have here down underneath the data frame a reminder that when you do an aggregation operation, the index you get back is going to be derived from the columns you grouped by and the values thatand the values that they took on in the original data frame.So far we've been applying some built in aggregation functions to our group by objects but this is only a part of the power.What really ends up being extremely useful and very common is to apply custom operations to each group.
In order to do this, there's two steps.First, we define a Python function that is supposed to receive a column and compute a single number.
This would be aggregating the values from that column into a scalar.Once we have defined this function, we then pass it as an argument to the Ag method of a group by object.
Let's see how this works.So let's define a function that counts the number of missing values in each column.So the way we would do that is we could utilize the is null method of a data frame or series and then compute the sum.
Notice here, I misspoke a moment ago.What we need to define is something that consumes a data frame that may have multiple columns and returns one number per column.So in this case, we are going to receive a data frame and at runtime, when we actually constructed the groups, this would be a data frame with all rows of a single group.And then we're going to compute is null will map over every element and check if it's null.And we call some that will sum each column and turn it into a series where the column names are on the index and the values are the values of the series.So let's define this function and we'll move to checking out how it works.So if we do num missing on the whole data frame, so this is not on the group by one, we'll see that there are no missing values in a or b, but that there are two missing values in c.This looks correct based on how we define the data frame at the start.
Now, let's go back to our GBA object and then we'll use the dot ag method and pass num missing.We'll see here that when a equal one, both b and c had no missing values.
This is consistent with what the raw data shows.
Now when a equal to b does have any missing values, but c had two.And so you see here that the value at index a equal to and column c has a value of two.
And here's a little bit more rules about what the function should do.Either it consumes a data frame in returns a series, which is what we showed, or it consumes a series in returns a scalar.
And this was what I had in mind when I first introduced this before.Sorry for the confusion, but hopefully this clears it up.
What happens then is pandas will call the function for each group.For a data frame, the function will be called separately once for each column.
Now in addition to doing what we call an aggregation, where we reduce a column or an array into a single number,there's something else called a transformation.
And this is a little bit more general.
We saw some transformations when we worked with data frames at the start,but now that we combine transformations with a group by object that become even more powerful.
Let's just see an example to try to understand how this works.So let's remind ourselves of what the data frame looks like.
And then we'll go ahead and we'll define a function.And what this function does, it will return the rows of the data frame corresponding to the two smallest rows.
Two smallest values in the column B.So one more time this is all rows, or turn all rows of the data frame corresponding to the two smallest values of B.
So when we apply the GBA, sorry, when we use GBA.apply and we pass this function,what happens is we get back a data frame, A is still on the index,
but now we have a second level of our index.
And what this is, is it's actually going to be the rows from the original index.We'll see what this means here in a minute. So a whole lot of that thought.
The values, though, are going to be B is one all the way along.
And if you remember back from the original data,we had B could be either one or two,
and it happened that when A was equal to one,
there were two instances of B equal one,
and then a single of A, B equal to.And then same thing when A was equal to two.
We had two rows with B equal one, and a third row with B equal to.
So what this in effect did was it extracted all the rows where B was equal to one.This was a feature of the particular data set we had,
but had we had values of B that were not just one and two.
We would have seen the two smallest rows of B for each group.Now here's a note about that index.
So we saw again here that the first layer of our index is equal to A.The second layer of our index is equal to the values on the index from the original data frame.
Here they are zero, one, four, and five.
Now why did this happen?So the reason for this is that the smallest by B function,
it actually kept the original index when it returned its value.And you'll notice here that at the top, and I'll put number 17 at the top of this cell,
we see that the values of the rows were B is equal to one.
The index is indeed zero, one, four, and five.And this kind of demonstrates a rule of how the group by works.
When you're doing a apply on a group by, and you return more than one row,whatever index is associated with your return value will be kept and not thrown away
when Pandas does the combining step at the very end.So in this instance, it had the index for A is equal to one and two,
but it kept the zero, one, four, five that we originally had.
Okay, so let's go ahead and work through this example together.We will now work through the solution to this exercise.
And you can see here down below that I have actually typed this out already.And I've defined a function called deviation from mean that consumes one argument.
And if we look at the documentation string, we'll see that the purpose of this functionis to compute the deviation from mean for an entire Pandas series or for each column of a data frame.
We'll go ahead and define our function.And the body of this function is quite simple. It's just x minus x dot mean.
We can then use our GBA object and call the apply method and pass in this deviation
from mean function we've just defined.We store the value or the output of this function call as deviations.
When we look at deviations, we see here that formerly B had values of one and two.And now it takes on values of either minus a third or positive two thirds.
This is because the mean of the B column was equal to one and one third.So when we subtract that from a value that equals equal to one, we end up at minus a third.
And then we subtract one and a third from two, we get positive two thirds.We can see a similar result for the C column.
Now the second half of this exercise asks us to combine the result of this deviation computation
with the actual values from our original data frame.So here we're going to call the data frame dot merge method.
And we'll pass in the deviations object as the right data frame.We can then say that we would like to use both the left index and the right index.
And finally this last argument here, the suffixes argument.What this does is we'll take a look at the output and then we'll talk about what happened here.
So we see here that on the output one moment.
Ah, excellent.Ah, excellent.
So we see here on the output we have two new columns, B deviation and C deviation,
where we have our original columns ABC.This came, this underscore deviation came from right here where we have the suffix argument.The first value in this tuple is the suffix that should be appended to the end of the columns from the left data frame.
In this case, DF.In this case, DF.
And then the second argument of the tuple here, underscore deviations, is what we would like to have appended to the end of the columns from the right data frame.Here are deviations data frame.
So this is how we would combine the results and then put them back alongside the original data.So we've seen some examples where the columns themselves contain the groups.
We saw this with A and we also saw it when we combine both A and B.
However, this isn't always the case.So sometimes you want a group by a column and a level of the index.
That could be something that's plausible or other times we may have a time series or a sequence of dates in a column.And we would like to group them at a particular frequency.
For example, suppose we have timestamps that include hour, minute and second.And we would like to form a calculation based on all the data for a particular hour.
In this case, we would have to instruct pandas that we'd like to use the timestamp column.But we'd like to group it in buckets of one hour at a time.
We could similarly ask it to group it in buckets of four hours or a day or a week.And this is not expressible by just passing the column name timestamp.
We also have to add in the frequency that we'd like.
And pandas does enable this behavior using the PD.gruper type.This is what we'll learn about now.
So in order to see it in action, we're going to make a copy of our data frame.
And we're going to move the a column to the index.And we're going to add a date column.
So let's just go ahead and show you what we end up with.So notice that the a column has that or sorry, the a column is now on the index with this original values of 111222.
The b and c columns are unchanged.But now we have a date column that has some dates between 2020.
Show between 2020 and 2022.
Okay.
Now that we have this data frame, we can use the PD.gruper to group by year.So let's take one more look here.
And we'll notice that there is one value in the year 2020.
But then for the year 2021, we have a value in March, June, September and December.So now there's going to be four rows that happen in the year 2021.
So over in this next cell, when we ask to group by the date column with the frequency equals a meaning annual,we'll then be able to count the number of non-null rows in each column.
Here we see that for the year ending, December 31st, 2020, we have one column, one item in the b column that's non-empty,and also one column in the c column that's not empty.
For the year ending 2021, we had four rows that were non-empty in the b column, but only three that were non-empty in the c column.And then finally for the year ending 2022, we only had a single row.
The b value is non-null, but the c value was indeed null.
Notice here the syntax of using the PD.gruper.
We pass two arguments.One is the key.
Here this needs to reference the column name.
So we pass key equal date.
The second argument here is the frequency.Here we're going to pass an abbreviation for the frequency that we like.
Here we pass a for annual.We could have passed something different like 2a for two years, or if we had timestamps, not just dates,
we could pass h for hour, m for month, and so on.But here we wanted the group by year, so we passed a as our frequency.
So we can also group by a level of the index.So remember when we created this df2, we shifted the column named a, and we brought it over to be the index.
So if we do df2.gruper by, we construct another PD.gruper.But here instead of setting the key argument, which specifies a column name, we set the level argument.And we say that level equals a, meaning we would like to use the level from the index that has the name a.Now if we do this, we call count, we're going to see that we have three columns in our result, b, c, and date.The b column has three and three, because there were no no values, same with the date column.
And then we'll notice here that the two no values in the c column, both came when a was equal to two.Now we can, we can get even more sophisticated here.
And when we're constructing our group by, we can pass a list.
So here this closing square bracket here ends our list.Before we passed a list of two strings for the two column names, a and b.
Now we're passing a list where we are grouping annually in this first argument.And then we're going to group by level a from the index.
We can do that and we'll see that the result has two levels on the index now, one for the date and one for the a.And then it has the b and c columns.
And we'll see here that it did compute the number of non-no values for each column b and c for the, the year and the level of a.Furthermore, in addition to combining one instance of pd.grouper with another, we can define a, we can combine a pd.grouper here, again grouping by the year annually, with a column b.When we do this, we'll see here that the result only has a column c, because we used the other two columns as part of our group by.
So they were both moved into the index.And then this will have computed the number of non-no values in the c column for each unique combination of the date and the annual frequency, as well as the column b.Okay. So at this point, we've been able to work through a number of examples of how the group by machinery works.The core framework we've been working with is called the split apply combine framework.The three steps are to split the data into subsets of the data frame where we have all rows collected that share some key values in particular columns.This is done by using the dot group by method on our data frame.The second step would be to apply a function, and this was done either by calling the name of an aggregation method pandas defined like count or mean or some.Another option was that we used the dot ag method to reduce a series into a single number, and we can use dot ag and pass in our own custom function.The third option would be to do the dot apply method after we've called group by which will allow us to not just summarize a column to a number, but transform a column into another column.Those were the ways that we worked through that second step of applying.And then the combined phase where we regroup all the results of applying this function group by group pandas took care of that for us.And we didn't actually have to write any code to do that combined step.We also worked through how we can get different groupings based on columns based on transformations of them levels of the index, and now we're ready to do a case study.So what we'd like to do now is load up a data set that was again collected from the US Bureau of Transportation Statistics.And we're going to utilize the QEDS library and the very first time you run this particular line of code on your machine, it'll take a little bit of time.So you see here I started the computation a minute ago, which we can see that over on the far side of the cell, there's a star and asterisk in between the square brackets.That's Jupiter's way of letting us know that this computation is running.But what happens is the QEDS library will go and we'll fetch the data from online, it will clean it up, prepare it for us, and then it will save it to a file on our computer.So the next time we run this cell, you see here that it finished, we now have a 33 in the square brackets instead of a star.If I were to run this again, it wouldn't take quite as long because it's just finding the file that it saved on my computer and reading it back into memory.And you'll see here this already finished, it says 34 instead of 33, letting us know that it's done.The first thing we'd like to do is compute the average delay in arrival time for all carriers in each week.
And we're going to do this in a number of steps.So first, we're going to begin with our airline December data frame, and then we're going to go right into a group by.And here we're going to group by two things. We're going to group by the date column at a weekly frequency.
We're also going to group by the carrier.The reason we chose these two arguments was because we wanted the average arrival time for each carrier in each week.
And grouping by these two levels will give us that grouping.Then we're extracting only the column that we're interested in. Here it's the arrival delay column.Then we're going to compute the mean. So it's the average arrival delay for each flight across for the whole carrier within a week.And then finally once we're done, what we end up with is we have two in two level index, which would be the date and the carrier.And then we're going to have a single level of column of where you have a single column, which is the airline delay.And what we'd like to do is we're just going to rotate this carrier level of the index.
We're going to rotate that up to become column names.So we're going to be left with the date going down the rows.
The columns are carriers and the values are the average delay.
Let's compute this and see what it looks like.So we see here the shape we just we expected.
We have dates going down the rows and along the columns we have carrier codes.
So these may look a little difficult to understand.These are just two digit codes that the airline industry uses to keep track of which airline.
This WN for example is Southwest Airlines and then way over at the other end the AA is American Airlines.And we'll see here that we have one observation for each airline and each week.
The dates over here on the index those represent the last day of a week.So this first row represents the average delay for all flights in the week ending on December the fourth.
The second row would be for the week ending December 11th, December 18th, and so on.Now it's a little hard to see the pattern by looking just at the draw table of numbers.
So let's go ahead and plot this data also and see what we can discover.So if we see here that we have plotted the following we have on the horizontal axis we have the date.
On the vertical axis we have the delay time and then each sub plot represents one airline.So we have here a sequence of 12 charts where each of them are showing us the average delay time for that airline.And it looks like for every single almost every single one of these charts with two exceptions.
One being this one and the other being the AS chart up here.But for every other chart the third bar corresponding to the week ending December 18th has the largest average delay time.So this seems somewhat systemic or something that we could dig into a little bit more and that's quite interesting.
So we'll go ahead and we'll see if we can analyze that more.The way we're going to proceed with our analysis is to utilize a few more columns contained in this data frame.So in addition to just the total delay in minutes we actually have a breakdown of what contributed to that delay from five different categories.These categories include things like was it the airline carrier fault, was it a weather delay, was it because there was a late aircraft, was it security and so on.So we'll go ahead and we'll create a list called delay calls containing these column names so that we don't have to type them out later.And our goal here is to understand what contributed to the high delays in the week ending December 18th.And we're going to be using these five categories and we're going to try to understand how much they each contributed to those delays.So we'll just go ahead and we're just going to construct a data frame that has the raw delay values here.
And we did kind of a lot in this step so we're going to break it down one of the time.The first thing we did up here in the code cell was that we extracted the data for only that week.We said we would like to get all the data where the date is greater than or equal to the twelfth and less than or equal to the 18th.We then have a new function that we defined a new aggregation function called positive.The purpose of this function is to compute the total number of rows where the value in a particular column was greater than zero.This will help us understand how many times a particular delay or a particular factor caused a delay.Then the final thing we're going to do is we're going to take this pre-Christmas data frame so containing only data from December 12th to the 18th.
We're going to group by the carrier.We're going to look at the delay columns and then we're going to call ag.Now before we were calling ag and passing it a single argument which then would take each column, use the ag function and reduce it to a single number and then move to the next group.Here we're passing a list of functions.
We want it to compute the sum, the mean, and we want it to use our positive function.
And so what we get back here, let's look at the shape of this output.You see here that the rows are equal to the carrier.
Sorry, the index is equal to the carrier.
This happened because that's what we chose to group by up here.Second, notice that we have two layers of column index.
We have one here at the top and then a second one down here.
This outer most layer of columns is actually coming from the delay columns.So this becomes the outer layer of our index and then the inner layer has positive, sum, and mean.
This comes from the functions we passed to the ag method.So we have the group by argument on the index, the columns become the outer most column and then each ag function becomes the inner most column level.
And we have a bunch of data here.Just like last time, it's a little hard to understand exactly what the patterns are just by staring at the numbers.
So we'll go ahead and we'll make a chart.So what is it that we would like to do or what is our want?
What we want to do is put the total average and number of each type of delay by carrier.And to do this, we're going to have to reshape this delay totals data frame a little bit.
So what we'd like to see is that we would like to have the delay type be the horizontal axis on our charts.And the way pandas does its plotting is anything on the index will become the horizontal axis or the x axis values.So the first step is to move the delay type from being this outer most column level.
We want to rotate it down to be on the index.The second thing we'd like to do is we'd like to have the aggregation method.Instead of being the bottom or the inner column level, we'd like to rotate it up so that it's the outer or top column labels.And then finally, we want to move the carrier names from being on the index.
We want to rotate them up and become the outer most column level.Or sorry, we've become the inner column level, not the outer one.
It's a lot and the code here is just one way to do it.
We'll show you the output and then we'll work through how it happened.So here we have the carriers, sorry, the delay types going down rows.We have the outer level of these columns as the aggregation type here is some and the inner level of the column labels is the carrier code.
So how do we do it?So how do we do it?
We started with our data and then we chose to stack it.
And what this did was it moved the aggregation method down and flipped it down.So before we had delay type aggregation method, but now we've rotated the aggregation method down to become a level on the index.So now we have carrier code and delay type on the index and then our column labels are now the aggregation type or sorry, the delay type.
So then we're going to do a transpose.So this dot t here in the second line will just invert that.
So then we're going to end up with the delay type on the index.And then we're going to have the carrier code on the bottom level of the columns and the aggregation type on the top actually vice versa.We're going to have the aggregation type close to the data and the carrier code on the outside.This happened because when before the transpose, the carrier code was on the outside and then the aggregation type was closer to the data.That is preserved. So the aggregation type in the columns is going to be the lower level and the carrier code will be the higher one.The next step after the transpose is to swap the level of the column layers.So we're going to swap so that we flip the delay type or sorry, the aggregation type to be on the top and the carrier code to be underneath it.And finally, we'll just sort by the index so that we have a sorted column labels.
That's a lot.
If you didn't totally follow, that's okay.I encourage you to go back on your own time and to study these commands and make sure that you understand how we can start from where we were over here on this previous slide.And we can start here, use these reshape commands to get down here.
So once we have this, we're going to go ahead and plot it.So what we're going to do is we're going to loop over the aggregation type, the mean, some, and positive.We're then going to extract all the columns for that one aggregation type and then we'll plot it.
So the actual instructions in the plot command are similar to what we saw before.So we won't focus too much on it.
Feel free to study this on your own later.
And what we end up with is three different charts.The first chart shows the values for the average contribution of each delay type.
Now this is not average across flights.
Remember this is average across non-zero values for that delay type.So we'll see here on the x-axis or the horizontal one, we have the delay type.On the subplots, we have carrier codes and then the value, the height of the bars is going to be the value of that aggregation type.We have the same thing for the sum as well as the positive, the total count.
So we'll see here a few patterns. Let's just talk about them.
So look down here at the number of positive delays.We see that Southwest Airlines down here at the bottom had very high counts of these three delay types.
It was the carrier delay and NAS delay.I'm not exactly sure what that one is. I'd have to look that up.
And then the late aircraft delay.
However, if we look at the average contribution, they don't seem quite as bad.The carrier delays are a bit smaller per occurrence than it is for other airlines.
This may be indicative of Southwest having many instances of that.So they know how to handle it and how to get back on track a little bit better than some other airlines who don't frequently have carrier delays.This NAS delay, the average contribution is quite low.
And then this late aircraft delay is still fairly high, but I would say lower on average than for other airlines.Another thing to notice is the height of this bar right here.
So area with airline with carrier code B6 had a very large average delay for a late aircraft.Now if we look at what happened here, we see that that didn't happen very often for this carrier.And so when it does, even though the sum is not that large, because the total number of occurrences is low, the bar is quite high.Because even though it doesn't happen often when it does, it contributes quite a bit to the delay.
Those are just some of the insights we can pick up.
And we actually have an exercise here.We would like for you on your own time to come back and think through these charts and try to understand what factors contributed to delays for different airlines.In particular, we'd like for you to answer questions like which type of delay was the most common or occurred the most, which type of delay caused the highest average, which type of, yeah, type ofof, yeah, type of delay caused the average......arrival delay to be the biggest.
And also do these answers and ones to related questions vary by airline.
We kind of saw a little bit of this variation by airline with Southwest before.Now they have a lot of delays, but they tend to be on average smaller than the corresponding delays for other airlines.Again, we'll encourage you to come back on your own time and think through this, maybe discuss it with one of your friends and write your thoughts down so that you can understand a bit more how toa bit more how to think through this analysis once we've done the computation.So let's take stock a little bit of what we've done.
We were able to compute for the month of December 2016 the average flight delay for all U.S. domestic flights by airline and by week.We then, just by plotting these numbers, we noticed that one week in particular seemed to have much higher delays than other weeks. This was the week ending in December 18th.Then we studied the contribution of delay from five different categories to try to understand what happened and you did some of this study on your own.Now this was a fairly good analysis in the sense that we were able to utilize a lot of our tools and we were the uncover insights about this industry, but it's not the only type of analysis we mayof analysis we may ever want to do.Suppose that after seeing our results, the somebody at the airline industry would say, hey, we loved your weekly analysis. Could you repeat it for us at the daily frequency?Sure enough, it wouldn't be that hard. What we could do is either go back through our code and change this W frequency when we grouped by to a D for a week.Sure, it should move from week to day and then repeat and execute all the cells again, but there's a better way.So what we would like to do is actually compute the analysis we did in a container or in a function so that we can call it repeatedly, maybe tweaking a few of these values at a time.And in particular, what we're going to do now is we're going to wrap this analysis above into two functions.
One, it will produce a set of bar charts for the average delays at a particular frequency.Then we're going to have a second set of bar charts for the sum, mean, and number of positive occurrences for each delay type again at the chosen frequency.Here, we're going to do it at the delay or sorry, at the daily frequency.
So let's go ahead and define these functions. So what we have here is we have a function that takes three arguments.One has a default value. The first argument is our raw data frame. This would be kind of the air underscore DEC or airline December data frame we've been working with.Then it takes a second argument calls freak or frequency.And here's what it does. The first step is to take the raw data frame and group by the date at the frequency passed into the function and the carrier.This looks just like the code we wrote above, but before we had hard coded a string W here for a frequency.
And now we're allowing that to change with the argument to this function.Once we've done this grouping, we're going to again take our arrival delay column, we'll compute the average and then we'll unstack it so that we're left with only the date on the index and thethe index and the carrier code has been moved from a level on the index up to be a level of the columns.So this is just the code we first saw when we analyze the airline data, but now we've made this frequency argument customizable with a function argument.Next we're going to repeat the plotting code. So all this the rest of this function does is it repeats the code necessary to compute that grid of plots for the average delay for each airline at theeach airline at the given frequency.We'll go ahead and define this and we'll show how to use it in just a moment.
The second thing we wanted to do was be able to understand a bit more what contributed to the delays.So here's what we're going to do. We're going to have a second function that takes three arguments.
The first argument is going to be the data frame containing our data.Second argument is a string representing the starting date and the third argument is a string representing the ending date.Once we have these arguments, we do the following. We first construct a subset of the data. We hear we call it sub DF, where we're going to say we want the data where the date column is at leastcolumn is at least start and the value of the date column is no more than end.This will give us a subset of the data in between start and end.
Then you'll see here that we have we've repeated that positive function.We're going to then pass positive alongside some and mean to this group by operation where we've grouped by the carrier.We're looking at only delay columns and then we're aggregating with some mean and positive.
Then we're going to do the reshaping and plotting code.So there's no new code here. All we've done is instead of hard coding December 18th for the end and December 12th for the start, we've made those be customizable via function arguments.We'll evaluate that cell to define this function and that leaves us to an exercise.What we'd like for you to do is we'd like for you to call the mean delay plot function to replicate the chart we had before where we had the average delay per airline per week.Then we'd like for you to call the second function the delay type plot to replicate the charts that we used to study the delays between December 12th and December 18th.So doing this type of check or replication exercise is a good practice.
So what we did is we wrote out some analysis once.We decided that we might want to encapsulate some of it in a function so we could reuse it.So we started with analysis, we put it in functions and now we're going to call the functions and this will allow us to compare the function output to the original analysis output.And if they match, then we have some confidence that we've written our functions correctly.So we'd like for you to take a moment to call these two functions and see if you can replicate the charts we already have.
Okay, welcome back. We'll see here and we'll work this together now.You'll see here that I've written out the code that I feel should be able to replicate the charts we saw before.
So what I'm going to do is I'm going to call the mean delay plot function.I'm going to give it our airline December data frame and then I'm going to pass as the frequency argument, the string W, meaning we want to compute average delays across airlines by week.When we do this, we end up with the chart that we saw before.
We see here that for almost all of these airlines this third week and even December 18th has the highest average delay.Great. The first part of our analysis was successfully replicated.
Second was this delay type chart.So what I've done here is I've called delay type plot and the three arguments I passed were one, the airline December data frame containing our data.Two, I passed a string representing the date, the 12th of December 2016 as the starting point of our window.
And then I did the 12th 18th of December 2016 as the ending date.When we call this function and we execute this, we get back those same three charts we saw before.
Here we have a chart for the average delay by delay type for airline.We have the total minutes delay per each delay type and then we have the number of positive or number of times that delay type contributed to a delay.So at this point we feel confident that we've successfully encapsulated our analysis in reusable functions.So now let's actually utilize this. So what we'll do is we'll now take this mean delay plot function and instead of only looking at a weekly frequency, let's go ahead and look at a daily frequency.So this is going to produce a lot more charts or a lot more numbers here on the x axis because now instead of just the five week endings we have all 31 day endings.But what we're able to see is that there are a few days here on December 17th and 18th that seem to have larger delays for all the different airlines.We see it over here in this middle one. We see it over there on that side and then we also see it right here.So the the the airline the average delays seem to be biggest on December the 17th and December the 18th.So let's go ahead and look at this what contributed to these delays. We're going to use our delay type plot function and try to understand a bit more about what happened.So the way we're going to call this is we're going to call delay type plot. We're going to give it both.
Oh, sure, we're going to pass our airline December and we need to start an end date.The start date we're choosing is December 17th 2016. The end date is December 18th 2016.
So here we're going to see the average delay pie delay type for both days together.We'll see here that it looks like what's causing most of the delays and we're going to look at the some chart for this is that this late aircraft one is causing a lot of problems kind of generallykind of generally across all these charts as well as the carrier delay.So these two types of delays seem to be troublesome. You also have a non zero delay being caused by weather.It seems like weather may be so there may be some bad weather in the United States at this time, which can make sense because we're looking at December which should be winter.And those are times when maybe the weather would cause more delays than in a more temperate seasons.Now, because we've written out our function, we were able to look at having a start and end date that were different.So December 17th and 18th and look at both days together. But now we're able to just use one line of code to generate these plots for only December 17th.It looks like on this day the total delays again were probably coming from these two columns, but there is some notion of the weather being a little bit high.Let's look what happens on December 18th.
We'll evaluate this cell. So now the start date is the 18th as well as the ending date.And now the weather delays are all but gone. We see here that the weather delays are just not apparent, especially here for Southwest.
And it's almost entirely due to late aircraft or carriers.So we'll see here that the type of delay was different between the 17th where we did have a reasonable amount of weather related delays to the 18th where we really just don't have any weather delays.So now the purpose of the exercise we just did was we really want to underscore the ability of utilizing Python and good programming practices so that we can automate or make reproducible ourreproducible our analysis.We were able to write a pair of functions that allowed us to easily repeat the exact same analysis on different subsets of the data or maybe even different data sets.We could if we had data for December 2019 or September 2020, we would be able to pass this data frame in to these functions and it would work without a problem.There is nothing particular about it being December 2016.Now these same principles of being able to write a reusable or reproducible analysis can be applied in many, many settings and we'll see this as a theme that continues to pop up throughout ourup throughout our studies together.So though the last part of our lecture today will be to introduce an extended problem that we would like for you to work on on your own.And here's the setting that we're working with the QEDS library. It contains some routines that can simulate the structure and distribution of data from some common sources.One of these sources is Shopify. This is an e-commerce platform or service used by many different retail companies in order to engage in online transactions with customers.When a firm utilizes the Shopify platform to sell their goods online, Shopify will keep a record of all the transactions and produce a report with a consistent format.And what we're going to do here is we're going to we were able to contain or obtain an actual Shopify report from a real retailer.And we were able to uncover some of the distributional facts as well as the structure of the data.And we took this distributional awareness and the structural awareness and we encoded it in a simulation routine.So now what we can do is simulate some random data that has the same structure and distribution of real life retailer data from the Shopify provider.And so what we've done here in order to do this, the first two lines here, they set the random generator seed.Now what this means is that if you were to come back and execute this cell again, you're going to get back the exact same set of randomly simulated data that you're going to get right now.Had we not put the seed here every time you run this cell be it today, two minutes from now, two weeks or two years from now, you'd end up with different data because it's randomly generated.But if we set the seed on the random generator, then it will kind of reset the randomness back to a known value and then all the numbers generated from that point will be consistent and we'll see theand we'll see the same ones today and then tomorrow.And every time we run this in the future.
Now let's take a look at what we have here.
So the Shopify reports will contain a day, the customer type and the customer ID.The customer ID is a unique identifier for this customer and the customer type could either be returning or new.For all returning customers, we will have seen this customer ID previously, maybe in a different month report or in a different data set.It may not appear in our data set because we're just taking a snapshot.But if we had the whole history of Shopify data, we would see the customer ID for all returning customers had already been seen.The orders column tells us how many orders did that customer place on that day of the year, on that date.
Well, then have a column for the total dollar value in the sales.This is the revenue generated by that customer on that day.
The returns is how much we owe the customer because they returned an object.Ordered quantity is the number of items purchased by the customer.
We have here a gross sales and a net sales.
So this would be the total sales net of any returns because there were no returns.These columns line up exactly with total sales.
We're not going to include shipping or tax for now.
That's a complication that we don't need in our analysis so far.And we're also not going to worry about returns or discounts.
We're going to try to keep it a little more simple, but it does have the structure of the full Shopify data set.Now, the exercise we want to do has to do with a customer cohort.
So we're going to define what that means or what we mean by customer cohort.So a customer cohort is identified or indexed by the month in which they place their first order.
The customer type column tells us whether it's a new or returning customer.So for every customer ID, we would like to identify which row there's only going to be one, which row has type new.And then we'll look at the month corresponding to that date and we'll see that that is the label or the index for that customer's cohort.
Let's think of an example.If I were to place an order today in October of 2020 and I had a customer type of returning and an ID of one, my cohort would not be October.If instead we look through the data set and see that customer with ID equal one had a new customer type in July of 2020, then July 2020 would be my cohort.If there were a customer with ID equal to whose new customer type occurred in October 2020, then this would be his cohorts label.So now here's the want that we would like to cover and I'm going to read it carefully and I'm going to read it twice.We want to compute the monthly total of orders total sales and total quantity separated by customer cohort and customer type.One more time we want to compute the monthly total number of orders total sales and total quantity separated by customer cohort and customer type.You can see here that we kind of have three groupings. We have the grouping on the dates, which is going to be at a monthly frequency.We have the grouping on the customer type and then we have a grouping on the customer cohort.
Now this is going to be kind of the heart of the exercise.What I would like for you to do is use the reshape and group by tools that you've learned so far to compute the want defined right here.Now we're not going to do this one together, but we have here and we'll let you read it on your own time.
We'll have here a snapshot of what the data should look like when you're done.And a note with some hints on how you can might do it.
We'll let you look through this on your own time. We won't spend the time to read it together right now.We'll encourage you to go through here. And so one kind of hint or note that Dr. Sargent would strongly encourage and so would Dr. Coleman and myself is don't skip steps.The hint here is quite helpful. So make sure you work from step one, just step two, think through the hints for how to do step two, and then move on to three, four, five.It seems like a lot and it's it's a fairly complicated operation. However, if you go one step at a time and you don't skip steps, you do have all the tools and knowledge that you need to successfullyto successfully complete this exercise.That's going to be it for our lecture today. And good luck on the exercise and we'll see you next time. Thank you.