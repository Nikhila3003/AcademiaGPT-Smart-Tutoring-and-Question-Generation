The models we've considered thus far have many things in common.
One of them is our treatment of the state s at time t equals zero.What we've done so far is exaginously shift or adjust the state at t equals zero to match the data observed in the COVID era.
And this exogenous shift happened in a very literal way.What we've been doing is starting a simulation at t equal minus 35, letting the model dynamics apply to that state and generate its values up until time t equal one.
And then we pause the simulation.While the simulation is paused, we reach into the model, we shuffle around some workers such that there are fewer employed workers at going in to period t equals zero, then ended at t minus one suchat t minus one such that the starting state at time zero matches what we observed at the low point in the COVID era recession.And as we're done with our reshuffling, we take our hand out of the model, we press play again and let the model dynamics run and march the state forward from that point on.Now we might want to be able to answer the following question.
What change in alpha or beta at time minus one could have generated the drop in an employment seen at time zero.In other words, is there a way we can by only adjusting model parameters and not artificially moving the state generate dynamics that are consistent with the data.Now thinking about our model, there are in principle two ways this might happen either a decrease in the job finding rate or an increase in the job separation rate could lead to a lower percentage oflower percentage of employed workers.In order to understand how much lower kind of the magnitude of this shift let's take a look at the BLS data for COVID era, just in periods minus one and zero.We see here that at time minus one in COVID era, we had about 95% of workers employed and then in period zero, we had only 85% where there was a 10 percentage point drop in the number of in thenumber of in the percentage of employed workers between periods minus one and zero.Let's keep that in mind and think more about how we can use our model parameters to replicate this.So let's recall that the steady state job finding rate alpha bar is about 37% and the steady state job separation rate is about 1%.Now suppose that we believe that this sharp change in employment was driven driven entirely by a shift in the job finding rate.Now the most extreme shift we could consider would be to shift the job finding rate at time minus one from steady state value of 37% all the way down to zero.So let's go ahead and do that and then we'll iterate one period using the model dynamics and see what the percent of employed workers would be if alpha minus one was set to zero.So what we'll do is we'll begin here and we'll say that the percent of employed workers at minus one and here we use the this a letter M to represent minus we'll start that out at 0.955, which isat 0.955, which is what we just saw from the data on the previous slide.We'll then say that the employed the percent of workers that are employed at period zero is going to be that's percent at period minus one times all the people that did not lose their job plus alltheir job plus all the people that were employed times the number who found a job.And this was where the alpha minus one is set and notice here we're setting this exactly equal to zero and that's where we impose our hypothesis that alpha minus one was equal to zero.And we do this we see here that alpha minus one was at 0.955 if nobody found a job in between period minus one and zero then the percent of employed workers would be at 94.5%.So shifting this job finding rate from 37% to zero only had a 1% point percentage point impact on the share of workers that have a job where if you recall we were looking for a 10 percentage point10 percentage point difference.What this means is that our shift in what this means is in the context of our model this shift of 10 percentage points could not have been generated entirely from a shift in the job finding rate.In other words we've ruled out the hypothesis that only changing alpha could have generated the data we saw in the covid area.So our other lever we have to pull in this model is changing the job separation rate beta.Now suppose that we set this to its most extreme value we go from 1% of jobs are separated to 100% of jobs and then we'll keep alpha at its steady state value.When we do this we apply the same motion we see that if everybody lost their job and only 37% of previously unemployed workers found a job we would move all the way from 95% of people being employedbeing employed to only having 1.6% of people being employed.So this validates that it is possible within the context of our model for a change in the job separation rate to be consistent with the covid era data.So this leads us to a question. So suppose that we want to keep alpha fixed at its steady state value and then determine what value of the job separation rate at time minus 1 could have generatedhave generated this shift from 95% of workers being employed to 85% at times 0.Whatever this value is at time minus 1 that moves us from and forgive the type of here from 95% of workers being employed to only 85. We're going to call this value of jobs separation beta till that.So we can rearrange the law of motion for the fraction of workers that are employed in order to solve for beta till that and it's going to be in terms of the percentage of employed workers forworkers for targeting at time 0.The percentage of observed workers we observed at time minus 1 and the steady state value of the job finding rate.So when we compute this we see that the job separation rate at time minus 1 would have to move from a steady state value of 1% all the way up to a value of 12%.If we do this and we've done our algebra correctly, but we should see is that without reaching into the model and artificially moving the state around at time t equals 0.If we just set the time minus 1 value of beta equal to 12% instead of 1% we should see the same quick drop in employment that the data show.So let's implement this in a function. So here we're going to have a new model and it's the solution to its direct problem coded up right here.We're going to start out with the state initially at its steady state.We're going to say that alpha and beta are fixed at their steady state values for all time except that at time t equal minus 1 we move beta away from beta bar and to beta till there.That's it. There's no manipulation of the state going on. We're just having a one period shift in beta and then we'll simulate and return our output.We do this. We can go ahead and we can run this and we can look at the outcome and we see here that the model is able to generate this very sharp drop in employment just like we observed in the data.The MSE is between four and five which is what we saw back in model one where we artificially reached into the model and set the state from its steady state down here.And so what we've done is instead of externally moving the state we've allowed the model parameters to move the state for us.And now this rebalancing of employment to unemployment is happening all inside the model.So if the outcome of the modeling experiment is the same in model one where we always keep the parameters fixed at their steady state values but reach in and change the state compared to model threeto model three where we have a one time deviation in the parameter beta.If those two frameworks or models generate the same outcome. Have we really improved or what benefit have we gained and the benefit comes because when we allow the shift in the time zero state totime zero state to come from within the model instead of without we can do experiments and we can start to ask questions of our model and then try to learn what other outcomes might have happened.And so what we've done is we've done experiments from might run is suppose that the government had the capacity to save one third of the jobs that were lost at time zero.Now how could it do this? Well, maybe it was able to subsidize a certain sector or subset of the economy such that employers didn't have to fire or separate from as many of their workers.So we want to say, well, if this was possible for the government, what impact would that have had going forward? If we stay entirely within the model, we're able to answer questions like that.And in this way, we're able to use the model kind of as a vehicle for doing these counterfactual exercises.And this is actually in the spirit of what we're really after when we're doing computational social science.What we do is we build a model that captures a feature or features of data that we'd like to better understand.And we use this model so that we can analyze the impact of decisions on the variables of interest.We can't set up for ourselves a physical laboratory in which to perform these experiments. So we have to do so using our models.With that in mind, let's now consider a model where we shift both beta at time minus one to generate this rebalancing of employment.And we're also going to slow down the recovery of the model like we did in model two.
In some sense, we're combining both models two and model three.So to do this, we have another function here that takes the following parameters. We have a beta tilde.
And this represents what happens at time period minus one.We then have the three parameters or the three extra parameters from model one or model two, which is the alpha hat and beta hat values that are going to be applied from time t equals zero throughequals zero through time t equal n.Once we've taken in these parameters, our solution to the direct problem is this follows.
We start off with the steady state distribution of workers across employment and unemployment.We construct a vector of parameters that's the length that we would like our time series to be that is always fixed at the steady state.We then apply the adjustment for model three where we shock the time minus one value of beta to equal beta tilde.We then apply the concept from model two where we shot where we move alpha zero through alpha n to be equal to alpha hat and we'll repeat that for beta.And then once we get past n, we're going to be back in this period where alpha and beta are their steady state values.Once we have these vectors of parameters, we can go ahead and simulate and then combine with the L.S. data to prepare for plotting and evaluating the mse.So we're going to be using the four parameter values we had from above. We had beta tilde had to be about 12% in order to generate that sharp drop in the percentage of employed workers.And then to slow down the recovery to steady state, we have these three parameters here, data hat of 2% alpha hat of 25% and those values apply for four periods.
We do that.We do that.
This model where we've combined both the shift of beta minus one to generate this sharp drop in the number of employed workers and then thetemporary, but prolonged shift in alpha and beta between zero and n equal four has a slow recovery that matches the covert era and we were now able to construct a model which again is our sequence ofis our sequence of parameter vectors alpha and beta that can match the covert era dynamics.And we're set up to be able to explore and ask this model more questions.
We'll speak more about this in the future, but for now, we're actually going to transition into something slightly different.So our analysis so far has focused on models that for the most part keep alpha and beta at their steady state values.We've allowed for temporary, but short deviations from this in order to get a sharp change in the employment configuration and then a quick recovery.
And that helped us match the covert era data.However, the great recession era data doesn't have a sharp decrease in rapid recovery in the number of employee in the percentage of employed workers, but rather has a gradual decline in the percentin the percent of employed workers followed by a gradual increase.And in order for our model to achieve something like that, we would have to allow for alpha and beta to adjust more than just a couple periods.And so what we'd like to do is answer the following questions. What values of alpha T and beta T, where now T covers the entire horizon from minus 35 to plus 35.And so what we'd be consistent with or allow the model to generate great recession era dynamics.
To answer this question, let's return to our data.So the BLS data frame, it has columns, E, EU, UE and you you.
These represent the rates of individuals moving between unemployment and employment status.And what these correspond to is governed by the first letter of employment and unemployment.So that E, column represents workers that flow in any given month from being employed to still being employed.
EU would be somebody who started the month employed and did unemployed.UE would be people who started unemployed and then ended with a job.
And finally, you would be unemployed at both the start and end of a period.So if we think carefully about what the interpretation of our models are, we'll see that the U, E column from the BLS data frame represents the job finding rate.This is the rate at which unemployed workers transition from being unemployed to being employed.Similarly, the EU column would represent the job separation rate or the rate at which employed workers move into unemployment.So what we'll do is we'll take these columns, you and you from the BLS data frame and we will use them as a time series of values for alpha T and beta T.And then we'll simulate our model using those parameters.
Let's now code up a solution to the direct problem for this model.
So our function here will take in three parameters.The first is our like model simulator.
The second would be our data frame of data from the BLS.
And then a third parameter SNIT, which represents the initial state, we'll set down here.We'll talk about why we need this here shortly.Now what our function does is it will take the BLS data and it will grab only the rows corresponding to the great recession and then sort those rows by the months from column to make sure they're allsure they're all in order.We'll then save that as df underscore gr.
We'll then get our vector of alpha parameter values directly from the column u e and the vector of beta values directly from the column EU.We'll then make sure that the state is set to the indicated value.
We'll simulate and return.When we evaluate this function and then rent it, we'll see here that our model is now able to generate dynamics that look very much like the great recession.See here that the model now starts at a place very near the great recession starting point.
This is what we needed that SNIT value for.And then it's able to gradually decrease until it gets to about time zero and then gradually increase just like we see in the great recession data.Now in order in addition to visually seen that our lake model is capable of of mimicking the great recession, we can see that the model is indeed a much better fit quantitatively by looking at theby looking at the mse.In our previous iterations of the model, the mse for the great recession was between 25 and 30, which always very high.
And now we have an mse that is less than one.A strong indication that this version of the model is the best version we've considered thus far at matching the dynamics observed during the great recession.In addition to seeing that the mse for the great recession era fell, we saw that the mse and the covid era rose.We were down between one and two in model four and now we're back up to a 9.05 for the mse covid.Now the fact that the mse for the great recession era fell so sharply and the mse for the covid era rose.
It demonstrates or illustrates a common curse and hidden blessing to all modeling exercises.And this is summarized by the statement that each modeling decision we make, it comes with trade offs.So our two state mark off chain view of labor market fluctuations that cannot match both the great recession era dynamics and the covid era dynamics with the same set of parameters.These two time periods in U.S. labor markets are sufficiently different that a model of this degree of complexity is not able to match both of them at the same time.However, the good side of this or kind of the blessing in disguise is that we are forced as modelers to be deliberate in specifying our modeling goals.We have to at the onset of our analysis answer the question of whether we want the model to match the dynamics of covid era labor markets or of great recession era labor markets.This specificity in our goal for doing the modeling and the analysis will help all aspects of what we're working on the more specific and pointed and targeted we can be with what we're doing the moredoing the more likely it is that we'll have success.The second aspect of a blessing coming from this trade off is that we can afford to use simpler models.If we are willing to accept that we can either match covid era data or great recession era data are two state mark off chain is a great workhorse model for studying both areas of U.S. data.This is a fairly simple model built up from building blocks that we know and understand well.
And by making this deliberate decision on what our goal is we are allowed to use a simple model.And as a guiding rule we will always want the simplest model that allows us to achieve our modeling goal.Now there are many reasons for this desire for simplicity which we will cover in greater detail throughout our time together.But one of the strongest reasons is that the simpler the model the more likely it is we understand how it works and then we're able to effectively use the model to solve whatever problem or to applyproblem or to apply it to whatever task we are working on.Now to close our time together today let's pause and look ahead a little bit.We did a bunch of work both in a previous lecture and in this one about building a model and you might be wondering why do we go through the hassle of doing this well as social scientist we rarelyscientist we rarely have the luxury of setting up control experiments.There are some settings in which very creative and brilliant researchers have been able to effectively create an experiment to test a social or economic theory.But in general this just isn't possible because we study society and people setting up control testing environments is often unethical and almost always impossible.So in order to do real science and employ the scientific method we must create a laboratory for ourselves.And instead of laboratories created with physical components you might observe in physics or chemistry or hard science our laboratories are constructed from the mathematical equations and theequations and the statistical structure that we use to build our models.A model allows us to consider what if scenarios that we call counterfactuals.
And in these scenarios we're able to use the model as a lens or as a way to study trade offs to potential decisions.So for example you might imagine that our lake model that helps us understand the flows from employment to unemployment might be part of a larger economic framework.Now this larger framework could include other things such as the workers or the households making decisions like whether or not to save the income they generate from working as employees or to spendor to spend it on consumption rate.And gain some happiness from that.We might also include a government that has to raise taxes to finance some expenditures on behalf of its constituents and then enact policies for maybe labor market relief during time to stress.You might also have international trading partners that could both increase the diversity of goods available to consumers as well as help smooth out labor shocks if our economies experiencingexperiencing difficulties in the labor market but a different economy isn't maybe production and resources should shift to the more healthy economy for a short time.And this could happen through international trading partners.These are all extra modeling components that could work alongside the lake model and building up a rich framework by connecting these well understood pieces allows us to have a very powerfula very powerful laboratory for doing experiments and applying the scientific method.We can apply a change in one component for example maybe we want to assume that the government was able to subsidize the labor market and then we can see the corresponding impact on other parts ofon other parts of the economy maybe this would result in a lower separation rate or increased household consumption because they still have jobs.And if their consumption higher then maybe their happiness or their welfare is higher.And by connecting these components we're able to create a laboratory of statistics and of equations that is our model.We can apply the solution to the direct problem to have the model generate simulated data for us that we can study and compare against actual data to see if our model parameters are accurate.And then we can use the inferences we make and the things we learn from that simulation to apply to our solution to the indirect problem where we then refine the choices we make about the parameters.And we saw this balance plane off today we did a lot of simulating which was solving the direct problem that allowed us to make different decisions about how to tweak model parameters which was a waywhich was a way we were solving the indirect problem.And kind of throughout this course we're going to be putting together our data programming math and modeling tools that we've built up in order to build models and perform experiments like this.And in that sense we'll be able to understand the society in which we live and operate.
Appreciate you for being here and we look forward to more exciting lectures here coming up.