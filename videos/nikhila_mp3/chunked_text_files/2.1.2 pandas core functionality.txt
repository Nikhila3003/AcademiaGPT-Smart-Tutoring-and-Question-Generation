This is Chase and today we're going to be talking about some of the basic functionality
that we have in pandas.
At the end of the lecture today you should have been familiar with what is a date timeobject and you also should have learned how to use aggregation functions, transformation
functions and scalar transformation functions and apply built-in and create customized versionsof these to make transformations to your data.
We'll also talk about how to select subsets of your data using Boolean selection andwe'll introduce you to what we call the WANT operator and we'll teach you how to apply
it.
The data that we'll be using for today's course comes from the US State UnemploymentData from the Bureau of Labor Statistics.
So this is our outline.
Let's go ahead and get started.
So as usual we're going to import our pandas as PD and we're going to activate our customplotting theme.
Now whenever we do a pandas lecture this is our first one.
We'll typically read in some kind of a data set that we'll use as motivation for the things
that we do in class.Again in this case we're going to be reading in state unemployment and you'll want to
take a note that there's going to be a column named date and we're going to specifythat that column should be read in as a date object.
And so if we look at this we can see the structure of our data.It has a column for date, a column for state, a column for how many people are in the labor
force in that state and an unemployment rate.You don't have to worry about this but what we want to do is we want to have a column
for each state and then along the index we would like to have the dates so one per monthin our case and the values that we'd like inside of our data frame will be the unemployment
rates.
And this is just some reshaping that we'll learn to do later in this course to make surethat we have the data that we want.
So again you should just take note that the states are the columns, the date is the
index and the values are unemployment rates.And to keep the data manageable we're just going to focus on seven states.
We're going to focus on Arizona, California, Florida, Illinois, Michigan, New York, and
Texas.Texas.
So this unimp data frame that we have will be the data frame we work with for the rest
of this course.
So the first thing we can do is we can plot the data and we talked briefly about thisin the last video that it will create this plot for us.
The thing I want to highlight right now is that it knows that the unemployment that thedate column has been moved to the index and so when we plot it places this on the index
for us and labels it.
Additionally it will create one line for each of the columns and this is why we choseto have seven columns instead of 50 and it's going to label these for us.
So this purple line is Arizona.
This really low unemployment rate is from Texas.And so we see that we get this some of this nice plotting features for free.
So let's go ahead and look at what index we have on our data frame.
You'll notice that it has these nice formatted text.So 2000, 0101.
And that's because we specified when we read the data in that that column or that data
was a date time object.And so Pandas has recognized that this is a date time object because we told it and
it's going to allow us to do some special things with it.
So for example we can index into a particular date.So if we choose to look at January 1st from the year 2000 we see that Arizona had an unemployment
rate of 4.1.
Texas had an unemployment rate of 4.6 in California at an unemployment rate of 5.We could also choose all of the days between New Year's Day and June 1st in the year 2000
just by putting 0101 2000.
And notice we can pass dates in in different formats and going to June 1st 2000.And that will now give us a data frame with six rows where each of the rows corresponds
to a month.
So we'll talk much more about this in a future lecture but we just wanted to highlight
that.that.
So you have this functionality available to you.
So the next topic we'll talk about are data frame aggregations.And loosely speaking, aggregation is an operation that takes multiple values and turns it into
a single value.
The example that we should all be familiar with is computing a mean.So taking the three numbers 0, 1 and 2 and computing their mean returns a single
number.
1.
We're going to use aggregations extensively in this course and we're very grateful thatPandas will make this easy for us.
So Pandas has some of the most frequently used aggregations already built in.
For example, mean variance, standard deviation, finding the minimum, median, max, etc.We do want to note here this is not all of them.
There's many other aggregations Pandas has that we have not listed.
And we just encourage you to use a little bit of tab completion.So if you take a data frame, so again, we can take the mean.
If you hit tab, notice it brings all of these options up.
And let's see an aggregation that we might do is standard deviation.So if we just type ST and hit tab, it brings up STD.
And if we wanted to know what that was, we could put a question mark and return samplestandard deviation over requested access normalized by n minus 1 by default.
So it tells us a little bit more and we could then use that to get the standard deviation.So by default, as you might have noticed, aggregation operates on the columns.
We can use the access argument though to specify whether we'd like to do aggregations by
rows.rows.
So remember, the zero axis is going, you collapse the zero axis, which is going to collapse over
the rows.
But if we specify axis equals 1, we can collapse over the columns.So when we take the variance with respect to axis equals 1, we're getting the variance
of the unemployment rate in the year 2000, in January 2000, or February 2000, etc.So the built in aggregations get us pretty far in our analysis.
But sometimes you need a little bit more flexibility.
We can have pandas perform custom aggregations by following the next two steps.Then you write a Python function that takes a series as an input and outputs a single
value.
And then you can call the Ag method with the function as an argument.So let's do an example below where we classify states as low unemployment or high unemployment
based on whether their mean unemployment level is above or below 6.5.So step one, let's write the aggregation function.
So higher low is a function that takes S, which is going to be a pandas series object.
And if the mean of S is less than 6.5, we'll return low.And if it's higher than 6.5, which is in the else, then we'll return high.
And so now step two, all we have to do is call got Ag.And notice we've now classified Arizona as a low unemployment state, California as
a high unemployment state, et cetera.
We also can get the access equals one.And we can see which of the months were low or high unemployment.
Now all of the ones we can see are low unemployment because they were in times of growth.But if we looked at, so let's go ahead and do our fancy date indexing.
So let's go from January 2008 to October of 2008.And we noticed that at the end of 2008, we started seeing high unemployment rates.
And we just did this.
Great.
And the other thing you might notice is that Ag can accept multiple functions.So here we're passing in three functions.
We're passing in the function min, max, and then higher low.
And what this will return is a data frame now where we have the columns the same as thedata frame that we passed in.
And now we have a row for each of the functions.
So the min unemployment rate in Texas was 3.9.
The max unemployment rate in Michigan was 14.6.And then we have our high low classifier that's telling us whether each of the states is
a high or low unemployment state.So now let's take a pause and let's do an exercise to see whether we understood the concepts
that were just presented.So at each state, at each state, we'd like you to find the minimum unemployment rate
across all of the states in our sample.
We'd like you to find what was the median unemployment rate in each state.What was the maximum unemployment rate across the states in our sample?
What state did it happen in and in what month or year was this achieved?And then we'd like you to classify each state as being high or low volatility based on
whether the variance of their unemployment is above or below 4.And we'll go ahead and pause for 5 to 10 minutes and see how we see how we do.
Let's review the answers.
So these exercises asked first at each state what is the minimum unemployment rate acrossall of the states in our sample.
And the way we can find that is for each month, we want to find the minimum unemployment
rate which means we want to take the minimum across the columns or the axis.So axis equals 1.
Then the second was what was the median unemployment rate in each state?
Since this is the median across within a column is we're going to just take the median whichwill default to axis equals 0 which collapses the rows.
Next we're going to try and find what the maximum unemployment rate was across each
of the states.of the states.
What state did this maximum unemployment rate happen in and when was it achieved?
And the way we're going to do that is first we're going to take the maximum across allof the unemployment which will tell us the maximum unemployment rate in each state.
And then the max will tell us what the maximum unemployment rate was.The IDX max, again the first part is the same, is going to tell us which state that occurred
in.
And then we're going to take the maximum according to axis equals 1 to find the maximumunemployment rate in each month.
And if we take the IDX max that will tell us when it occurred.
And so the maximum unemployment rate in our sample was 14.6 percent which happened in Michiganin June of 2009.
The last question in this exercise was to classify each state as high or low volatility
based on whether the variance of their unemployment was above or below 4.So we defined a new function, unemployment rate, volatility classify which takes a series
as an input.
And then it returns either the string high or low depending on whether the variance isabove or below 4.
And so what we see is Arizona, California, Florida, and Michigan are high variance unemploymentrate states and Illinois, New York, and Texas are low variance unemployment rate states.
Hopefully these lined up with what you did.Otherwise you should review after class and feel free to ask us questions.
Great.
So we're done talking about aggregations for now.
Next we're going to talk about what we call transforms.And transforms are going to be analytical operations of some sort that do not necessarily
involve an aggregation.
So the output of a transform would be a series that is mapped to another series.Some examples of what this might be are the percentage change in unemployment rate from
month to month or the cumulative sum of the elements in some column.So there's lots of built in transforms for us.
Just like there were aggregations, we can take the cumulative sum, the cumulative min,
the cumulative max, the cumulative product.We can take the difference.
We can do element wise additions, retraction, multiplication, or division, percent change,
value counts, or absolute values.And again, we encourage you to explore what functions are available using tab condition.
So let's go ahead and do a couple of examples.
So we call this is the data that we have.The first we're going to do is computing the percent change.
Notice it can't compute the percent change for the first element because there's nothing
that precedes it.that precedes it.
And then we see that unemployment rate did not change in most states, but in Michigan it
went down by 3% and it computes these percent changes for us.Additionally, we can compute the difference, which will instead of taking the percent change,
it's just going to take the difference.And again, the first row is going to be all missing because you can't take the difference
between something before the first row.
So we classify transforms into a few main categories.The first is going to be a series transforms.
And that's a function that takes in one series and produces another series.And this will result in the index of the input and the output not necessarily being the
same.
Scalar transforms are going to be functions that take a single value and produce a single
value.value.
So one example would be the absolute value method, which we're adding a constant to each
value of a series.value of a series.
So these are going to take a particular value and do an operation to that one value rather
than to the entire series.So just like we could write custom aggregation functions, we can also write custom transformations.
The steps are going to be the same as we're going to write a Python function that willtake a series and output a new series.
And now rather than pass our new function as an argument to the ag method for aggregation,
we're going to pass it to the apply method.So as an example, what we're going to do is we're going to standardize all of the unemployment
data to have mean zero and standard deviation one.And we'll do this so that we can classify which date the unemployment rate differs most
from normal times in each state.
So step one is to write a function that can standardize the data.So it takes in a series and it's going to take the mean of that series and the standard
deviation.
And then it's going to create a new series that's going to be the original series minusthe mean divided by the standard deviation.
Next we can apply this to our data.
And what we'll see is that in the early 2000s we had negative values for our, they call
this the Z transform.So our standardized data is going to have minus one which means that the data was abnormally
low, which in unemployment is a good thing.And then finally we're going to take the absolute value of all of the elements.
And again this is a scalar transformation because it's going to take one value and mapit into one new value and do this for each value in the data frame.
And now let's go ahead and find out when the unemployment rate was most different than
normal from for each state.And so what we see is that the 2008, 2009, 2010 recession really did strange things to
unemployment rates.
And those were the most abnormal unemployment rates in our series.And in addition to doing customized series transformations, we can also do customized
scalar transformations.And the way that you do this is again the first step will always be to find a Python function.
In this case it will take a scalar as an input and produce a new scalar.And second, it's going, you're going to pass this function as an argument to the apply
map method.
So let's go ahead and look at this.So imagine we want to determine whether unemployment rate was high, greater than 6.5, medium or low.
So we can write a Python function that takes a single number as an input and outputs asingle string.
And then we're going to pass this function to apply map.
So one question you should ask yourself is why are you passing this to apply map and
not aggregate or apply?And think about why neither aggregate or apply would work.
So here's our function.
We define unemployment classifier.
And if it's above 6.5, we write high.
It's above 4.5.It's above 4.5.
But below 6.5 we return medium.
So why is we return low?
And now when we do apply map, what happens?We're going to get back a new data frame and notice every value in this data frame is
either going to be low, medium or high because those were the values that we could potentially
put in.put in.
So now we're going to move on.
We're doing these as examples rather than exercises.
So let's use another transformation on unemployment bins, which was the data frame we just created,to figure out how many times each state had one, each of the three classifications.
So if you are doing this on your own, which I'd encourage you to try and do after class,you should think about will this value counting function be a series or a scalar transform?
And Google pandas count unique value or something similar to find which transformation.We're then going to construct a horizontal bar chart of the number of occurrences of each
level with one bar per state and classification.
Okay.Okay.
So the method we were looking for is one called value counts.
And so what we're going to do is we're going to take unemployment bins, which you remember,
looks like this.looks like this.
And we're then going to count each of the times that occurrence, a value occurs for each
column.
And then we'll go ahead and look at what this looks like.So now we have for Arizona, the high unemployment, Arizona had high unemployment 75 times, low
unemployment 44 times, and medium unemployment 97 times, et cetera, et cetera.And we're going to plot the transpose so that it does something.
So that's what this dot capital T was.
And let's go ahead and reorder this data a little bit so that we can have low, medium,
high.high.
Oh, here we go.
And so if the blue bar is high, it means that state has a low unemployment a lot of time.Whereas if the yellow bar is high, it means that that state has high unemployment frequently.
So from this chart, you might not be particularly keen on California or Florida.Whereas somewhere like Texas or New York or Florida might be much better in terms of
their unemployment rates.
So finally, we're going to repeat the previous step.But now we're going to count how many states had each classification and each month.
So we're going to do almost the same thing.
But now we're going to count across the columns.So we're going to use axis equals one.
And whenever there's a missing value, which just means that none of the states had a particularvalue, we're going to fill it with a zero because it just means that there are zero states
with that value.
And here we go.
This is what it looks like.So notice in the early 2000s, no states had high unemployment.
Most states had low unemployment and a few states had medium unemployment.And so now which state, which month had the most states with high unemployment?
You might be unsurprised when we say 2009, which when was the most medium unemployment?
2001.2001.
And when was there lots of low unemployment?
The beginning of 2000, late in the year 2000.
And we found these with IDX max, which is going to search for the maximum about the indexassociated with the maximum value.
Great.
So now we've covered aggregations, transforms, and scalar transformations.Another thing that we'll often want to do is Boolean selection, which is we're going
to frequently want to select pieces of data based on whether certain conditions are met.For example, if you were in marketing, you might be interested in a particular target audience.
And so you might subset your data to only include adults between 18 and 32 if you got a particularproduct for adults, young adults.
You might also be looking at data that corresponds to a particular time period or some of the workwe did while we were, the most Spencer and I were at NYU, was we looked at data that
corresponded to recessions.
So we'll be able to do this by using a series or list of Boolean values to index intoa series or data frame.
So we're going to take a small view of our unemployment data frame.
So we took the dot head, which is going to be only five rows of our data.And we're doing this so that we can see what's happening as we do these operations.
So we can use Booleans to select particular rows.So in this case, we need seven values or five values associated with, let's be explicit
here.
So five values that are going to be associated with our five rows.So in this case, we'll select the first three, true, true, true, and disregard the last
two, whether or false is.
We can also use Booleans to select both rows and columns.So in this case, we're again selecting the first three rows, but now we're only going
to select the first and last two columns.Now we did these, we did these selections with lists, but obviously you're not going
to be able to write by hand a list with a million values in it.And so what we're going to do next is we're going to use conditional statements to construct
new series of Booleans from our data.And this will look a lot like some of what we covered in the conditional section of our
introduction to Python.
So what can we do here?So we're looking at the unemployment small data frame, the Texas column, and we're asking
when the unemployment in Texas was less than 4.5.And what this says is that unemployment was above 4.5 for the first three months of
the year, but then in April and May, the unemployment fell below 4.5.And once we have that, we can start selecting subsets of our data.
So notice we have unemployment small Texas less than 4.5, which is going to create a seriesthat has trues and false sentences in it.
So we can select all of the columns.
And this just gives us the last two rows like we expected.We could check for when the unemployment in New York was higher than the unemployment
in Texas.
And in this small data set, that always happens to be true.And in the Boolean section of our basic Python lecture, we saw that we could use the words
and or to combine multiple Booleans into a single Boolean.Just as a quick refresher, we use and and or in the mathematical sense, which means true
and false, results and false, true and true, results and true, false and false, resultsand false, true or false is true, true or true is true, and false or false is false.
We can do something similar in pandas, but rather than writing Boolean 1 and Boolean
2, we write.2, we write.
Boolean series 1 and sign, Boolean series 2.
Likewise, rather than writing Boolean 1 or Boolean 2, we use the Pyke symbol to denote
or.
Let's see this.or.
Let's see this.
So in this case, we're going to look at when unemployment rate was below 4.7 in Texas,
and the unemployment rate was below 4.7 in New York.
Notice the ampersand.This is going to say this is false for the first two months and true for the last three
months.
And if we select using this, notice that the unemployment rates in both New York and Texaswill be below 4.7 for every month.
We'll want to check whether a data takes on one of several fixed values.
The way we could do this right now is we could write DF of a particular column axis equalto the value 1 or DF column axis equal to value 2, etc.
But because this is a frequent operation, pandas provides us a better way.So in this case, we're going to look at the Michigan column and we're going to ask
whether the values are in 3.3 and 3.2.And so in the first four months of the year 2000, the value of unemployment in Michigan
was either 3.3 or 3.2.
And we can confirm that here.
And next, we're going to talk about any and all methods.You may remember from our first Boolean in conditional lecture that the Python functions
any and all are functions that take a collection of Booleans and return a single Boolean.Any returns true whenever at least one of the inputs is true, while all returns true
only when all of the inputs are true.Series and data frames have, with the D type Boole, have dot any and dot all methods that
can apply these logic, this logic.So what we're going to do is we're going to count, we're going to learn this by counting
how many months all of the states in our sample have high unemployment.And as we work through this example, we think this is a great opportunity to talk about
the want operator, which is a helpful concept that Spencer and I learned from Tom.So here's how the want operator works.
You start by writing want, followed by what we want to accomplish.
In this case, we would write, we want to count the number of months in which all statesin our sample had unemployment above 6.5%.
And now let's work backwards to identify a sequence of steps necessary to accomplish
our goal.our goal.
So to count the number of months in which all states in our sample had unemployment above
6.5, we would just want to sum the number of true values in a series indicating datesfor which all series had high unemployment.
So now how do we get a series that has true values when the states had a high unemployment?So the one way to do this is to build a series by using the all method on a data frame,
containing Booleans indicating whether each state had high unemployment at each date.So what this means is we want to know when every state in our sample had high unemployment,
which we can do using the dot all method with access equals one.Finally, how do we get to one that can do this?
We can just build a data frame by using a greater than comparison to compare whether the
data was above 6.5%.
So here is a plan.So here is a plan.
And to apply the want operator, all we're going to do is start at step three and work our
way back.way back.
And the reason we find this helpful is because if you start knowing what you want, it's
easier to figure out what you need to get one step ahead and you keep stepping backward
inductively.inductively.
So great, step three, unemployment greater than 6.5.
This tells us, well, the early 2000s were good years for the unemployment rate, so in
this case they're all false.In step two, we're going to use the dot all method with access equal one, which will
check whether every single column is equal to true for, and we're going to apply this
to every row.to every row.
So high dot all, access equals one will give us a new series and now it's going to have
a true or a false based on whether all of the states had high unemployment rates.And now finally, simply summing this series will tell us when all states had high unemployment.
And so out of 216 months, only 41 months had high unemployment in all states.And this ties up our Pandas basic lecture.
Thanks for being here.
I look forward to seeing you soon.
Bye.