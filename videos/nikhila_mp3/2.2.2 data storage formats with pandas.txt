Hello, this is Spencer Lyon and today we'll be talking about different storage formats
and their pros and cons for storing and accessing data from within Python and pandas.
Our goals for today will be to understand the various different file formats available
to us.
We'll also understand where we can read more documentation and get help on how to get
data in and out of our Python programs.
And finally, we will learn some rules of thumb for when to use each of the following file
formats, CSV, Excel, feather and SQL.
The outline of our discussion today will be that we'll first discuss what the different
file formats are and talk about their relative strengths and weaknesses.
We'll then show some examples of how you can take existing data you already have in
pandas and then write them to these different file formats.
Next we will learn how to read them back from the file system into a Python session and
we'll end with some practice.
We'll start by importing Python or starting pandas as PD and NMP as NP as we'll use these
throughout the lecture today.
Let's talk about file formats.
As you may know, data can be saved to your hard drive or a permanent storage location
in a variety of different formats.
Pandas understands how to read these different file formats and construct a data frame from
their contents as well as write the contents of an existing data frame to a file formatted
in these special ways.
We will strongly encourage you to look at the official documentation for getting data
in and out of pandas for a more complete understanding of the different options and features
that are available.
Our goal today will be to cover the most widely used formats for storing your data so that
you can understand them when you come across them as well as be able to use them in your
day to day work.
The first file type we'll talk about is called CSV, which means comma separated values.
Data stored in the CSV file is stored as plain text or a Python string.
If we were to open a CSV file with a text editor, we'd be able to read its contents without
any problem.
The way a data frame would be stored in a CSV file is as follows.
Each row of our data frame would be represented as a single line in the CSV file.
The different columns in the row would be written out directly in their string representation
and each column is separated by a comma.
This is where the name comma separated values comes from.
Some of the benefits of using a CSV file format are that they are widely used.
You're probably already familiar with it.
If not, no worries.
You will be by the end of this course, but it's likely that you've come across them before.
Another benefit is that it's a plain text file.
It can be opened by any computer, does not require any specialized drivers or software to
open, which makes it in some sense future proof.
You'll be able to open your files in the future and access your data.
And finally, the last major pro for a CSV file is that it is understood by almost all data
software.
From spreadsheet programs like Excel to programming languages like Python or R, they all have very
strong support for using CSV files.
However, there are some downsides using a CSV.
The first main one is that it is not the most efficient way to store or to retrieve data.
In the second, slightly more subtle, but perhaps more pernicious issue is that there's
no formal standard for how a CSV file should be written.
So, there's a little room for interpretation on how to handle some educations.
One example would be how do you handle a data value that itself contains a comma?
Perhaps you're writing currencies and you're having a comma separating the thousands.
How should that be encoded in a CSV file when there's already a comma is being used to
separate columns?
There are ways around this and some very common workarounds or common implementations.
So we shouldn't be too worried about this problem, but every once in a while you'll find
that one data set that's produced by a particular software won't read nicely into another software
because of a lack of a standard.
One should you use a CSV file?
My rule of thumb is that it's a really great default option for most use cases.
I would use it if I don't have too much data, meaning if I have data less than a million
rows, for example, and less than maybe 30 or 40 columns, CSV would be a great option.
Now let's talk about another very common format called XLS or XLSX.
This is a binary file format that is used by spreadsheet programs like Microsoft Excel.
When I say binary instead of plain text like a CSV, what we mean is that if you were to
open an XLSX file in your text editor, you wouldn't see normal characters in your language.
You would instead see a computer representation, a binary representation, or byte representation
of the data.
One of the benefits to the XLSX file format is that it's very commonly used in a variety
of industries.
In particular, the finance industry uses a lot of spreadsheets and a lot of their data
ends up living in the XLSX file format.
Another benefit is that because Excel is very widely used in some of these industries,
it's very easy to collaborate with colleagues that are already using Excel.
If you can exchange data using the XLSX file format.
An example would be if you do some analysis in Python with your new programming skills,
you could then save the output as the file ready to be read by Excel, send that to a colleague
and they could open it immediately in their spreadsheet to analyze what your output looks
like.
Some of the cons about an XLSX file format are that it can be very slow to read a lot
of data from one of these files and populate a data frame as well as it is very slow to
write a large amount of data from a data frame into an Excel file.
We'll show you an example of this later so you have some intuition for the order of magnitude
we're talking about.
Another issue, which some may term as a feature, is that the spreadsheet programs will store
both the data itself as well as what I'll call metadata.
Things like what color should this column or cell B or what chart should be included
in there.
The reason this is a problem is that this metadata is not always easily understood by other
programs.
You're sometimes locking yourself into using a spreadsheet based program if you include
all of this alongside your data.
The last major issue would be that it's not human readable.
If you were to open this in your text editor, you wouldn't be able to interpret what's
going on immediately.
So when should you use it?
I think the two main use cases that we would recommend are if you're already working
with somebody else and they are using an Excel based workflow and unable to learn a different
environment at the time, you should use what's easy for collaboration and in this case might
be an Excel file.
Another option would be if you're trying to present data and you would like to apply
a special type of formatting or display to the data.
Excel is the only option we'll talk about today that gives you that choice.
Next one we'll move on to the parquet file system or file type.
So parquet is a custom binary format that was specially designed for reading and writing
data stored as columns and it was designed to make that operation as efficient and fast
as possible on modern computer hardware.
This fits nicely with the notion of a data frame as we've learned because a data frame
is a way to collect multiple columns or series of data alongside one another.
What are some of the benefits of the parquet file system?
First it is extremely fast relative to CSV or XLSX.
You will often see 100 or 1000 time improvements in the speed for writing or reading a large
data set between pandas and a file system.
The second major benefit of the parquet file system or file format is that it naturally
understands all of the data types that are commonly used in pandas, including a multi-index
data frame.
The reason this is significant is that you can store the data in a parquet file format
from a data frame and with the data parquet will remember what each of the types were
for every column.
Then later when you go to read this back into pandas during a subsequent analysis, these
data types will also be read in and your data will be immediately usable and you can
resume exactly where you left off.
In contrast, if you are using a CSV or XLSX file format, you might have to remind pandas
that a column should be a date, for example, or that a column full of numbers should actually
be integers instead of floats.
This type of issue doesn't happen with parquet.
Another benefit is that it's a very common.
It's almost the common denominator for big data systems like Hadoop or Spark.
Finally, the last point here, we've written that it supports various compression algorithms.
What this means is that it will take the data that you are storing and if there's a way
to make the file size for this representation on your hard drive smaller, it will make
that happen.
This will happen automatically for you.
The only major con for a parquet file format is that it is a binary storage format, meaning
that you can't open it up and easily inspect its contents without using software specifically
written for handling the parquet types of files.
When should you use it?
My rule of thumb is that I have a not small, not necessarily big, but 100 megabytes or
more amount of data that doesn't change very often, but that I might read multiple times
for repeated analysis.
I'll often store it in a parquet file.
Also if I have a lot of data that I need to store and the compression feature of parquet
matters to me, it's often a reason why I'll choose it.
And finally, if I know that I will be reading this data from many different systems or if
I need to write out multiple data sets, the speed of parquet can be a very large benefit
over one of the other file types.
Now we'll talk about a sibling or a cousin of the parquet file format called feather.
So again, feather is a custom binary file format designed for efficient reading and writing
of data stored in columns.
Sounds familiar?
That's just what we heard about parquet.
However, there are a couple differences.
So first, it is extremely fast, even faster than parquet.
And one of the benefits for this is that, or one of the reasons for this is that it doesn't
do compression.
It will store the data in an ideal format for writing it very, very quickly to your hard
drive and then reading it back out as fast as possible.
In fact, the main author or creator of this file format is named Wes McKinney, who is
also the original creator of pandas.
And when he finished writing the feather file format and was doing some tests to see how
fast it was and how well it used all of the resources on his computer, he said he
was not surprised at all the find that it completely saturated the input output capabilities
of his CPU on the first try.
It was designed specifically for that.
Some of the cons are that it can only be read or written from Python and a handful of other
languages.
It's a relatively new file format, only about four and a half years old.
And so most files out there won't be in this format to begin with.
Often you may download them, say as a CSV.
And then if you want this extremely fast reading and writing for later, you can store it yourself
as a feather file, but it won't come that way.
And finally, it only supports the standard Python index or shy pandas index that goes starting
at zero and then increasing integers up to the number of rows minus one.
So in order to store your data in a feather format, first you need to call reset index to
get that default one.
And then once you read it back, you also have to call set index to restore the index you
are working with.
So when should you use this?
I kind of think of it as an alternative to park when that last bit of efficiency and speed
really matters.
This will happen when I have a very stable data set that I'm not changing very much over
time, but it's quite large and could take a significant amount of time to read in a
less optimized file format.
And finally, you should only use this when you know that you need to access the data from
a program that supports the feather format.
Three languages that are commonly used in data analytics and scientific modeling are
Python, R and Julia, and they all do have good support for the feather format.
So if you're working with colleagues on a project using some subset of these languages,
you're okay to use the feather one.
Okay, we're going to continue.
We're nearly finished with the different types of file formats.
We have one more to mention here and it's called SQL.
SQL is an acronym that means structured query language.
It itself isn't really a file format, but it's often associated with and talked about
as a way for reading and storing data.
It's used for interacting with what are known as relational databases or relational data
management systems.
And if you want more information, we encourage you to follow this link.
We won't talk too much about this because SQL is a very large topic and we don't have
time to cover that in class.
Some of the benefits are that it's well established in industry and much of the data in the world,
it's very likely that the vast majority of data lives inside a SQL database.
That's not some fact that I've done research on and validated, but given my experience
in industry and talking to many people who have worked in industry for a while, SQL is
kind of a needed skill in today's work environment.
The major con is that it's complicated.
Just like Python was a new language you needed to learn in order to do this course, SQL
or SQL is another language you need to learn in order to extract data or save it into these
systems.
You should use it when you work flow already incorporate SQL or you know that you'll
need the benefits of a full database system.
And we'll maybe talk more about that in a future lecture time permitting.
Okay, the next one would be what's called JSON or the JavaScript object notation.
It's a very common way to store data and it's especially common when getting data from
an API.
And the way that you, a Python programmer would maybe want to think about JSON is that it
stored as plain text and looks very much like constructing a dictionary by hand using
the curly brace notation.
The pros of being familiar with and using the JSON file format are that it's extremely
common and very frequently used when dealing with the web.
Another benefit is that it naturally maps into a Python dictionary, a data type that you're
likely already familiar with.
Another benefit is that it's very easy to read for humans.
And the final benefit and this is in contrast to the other file formats we've talked about
today is that it can store non tabular or non rectangular data.
You can have different columns or keys in each of the objects of a JSON file as well as
you can have nested objects.
And in this sense, JSON is the most flexible of the file formats we've talked about so
far.
Some of the cons of using this file format are that it's pretty inefficient.
For example, if I had a data frame with five columns and 500 rows, the standard way of
writing the JSON would be one large array or list where each entry of this array is looks
like a Python dictionary.
And I will list out that column A in the first row has a particular value and then column
B, column C, column D, column E. And I'm actually going to have to repeat the column labels
A, B, C, D, E for each of the 500 rows of my data set.
And so you'll see that I'm repeating data somewhat unnecessarily.
And this is kind of the flip side of JSON's flexibility is that because I'm allowed to have
different keys or columns from one row to the next, I have to specify what the keys or
columns are to make it unambiguous.
In addition to being inefficient in terms of storage capacity, it's also inefficient in
terms of reading and writing speed.
And the final downside to JSON is that it is ambiguous for how to represent certain data.
The example I walked you through would be an example of having an array or a list of records,
but you could also think about it a different way and you could have a single record or object
or dictionary filled with arrays.
In the five column example, you might imagine that there is a single dictionary and the key A
standing for column A would represent a list, would be represented by a list or an array
of all the values in column A. You could move on to B, C, D, and so on.
This would be a more efficient way to store the data, but which one is chosen is somewhat
ambiguous and you have to really inspect the data before you can tell us inside of it.
The rule of thumb that I follow for when to use the JSON data is when I'm going to be
interacting with the web and using a web API and also if you want to store a relatively
small amount of data that may not have a uniform rectangular structure.
JSON is a great option for those two use cases.
Okay, now that we've discussed the various different file formats that Python and pandas
are aware of, let's talk about how we can take data that we have inside of a data frame
and store it using one of these file formats.
The rule of thumb to keep in mind here is that if I have a data frame named DF and I
would like to save the data in the file format, Foo, I would use the DF dot two Foo method,
so I would call the two Foo method on my data frame.
For example, if I wanted to use a CSV, I would write DF dot two CSV and that would be
the method that can allow me to write a CSV.
Let's see this in practice.
So first we're going to create a few data frames and they're going to be filled with
totally random data and the first one is going to have four columns called ABCD and then
it's going to be filled with random integers between zero and a hundred.
The second data frame is going to be filled with random floating point numbers and it's
going to have 100,000 rows and then we're going to target that the output data frame will
be, will require 10 megabytes of storage.
If you wanted to change this from 10 to say five or to 100 just to experiment with the
different efficiency properties of the file formats, feel free to come back to this cell,
change the line that I've highlighted right here and we will construct for you a data
frame of random numbers that is approximately this size.
So you'll see here when I ran this cell with a size of 10, I got a data frame that was
about 9.91 megabytes large.
Okay, so let's start with DF dot two CSV.
You'll notice that this heading here is a link and if you wanted to look at the documentation
for all the different options, you can follow this link.
This will be true for each of the different data frame dot two methods we will be discussing.
So first, if we just call the dot to CSV method without any additional arguments, pandas will
return to us a string containing our data frame in CSV format.
You notice here as we discussed earlier, this is just plain text.
It's a Python string that we can read by hand.
Notice also that our four columns are all represented here, A, B, C, and D and they're
separated by a comma.
The first column here is blank on the first row because this is where the name of the
index would go if we named it.
And you'll see here that the values after this first row of column labels in this first
column, we have zero through nine, which is our standard index on our data frame.
All of the actual data would be included after this first column in each of these rows.
And it's just the random integers that pandas created for us.
Now if we do pass an argument to the two CSV method pandas will use the first argument
as a file name instead of returning to us a string with the CSV version of our data frame,
it will actually construct that string and save it to a file with the name we pass it
in the first argument.
So here we're going to say two CSV and we'd like to name the file df1.csv.
We could have this happens to match the name of our data frame object, but we could have
chosen anything we wanted here.
We could have written my nice data frame and it would have worked just as well.
We can actually use Python's file system integration to check to see if our df1.csv file
exists or if you're working on a Linux or Mac based system, you could also do exclamation
point LS, which will list the files in the directory.
Notice that we have this df1.csv.
We also have the my nice data frame CSV that we saved to the second time.
And these are the different Python notebooks we've been working through today.
So now if you remember we have df2, which is about 10 megabytes large, as 100,000 rows
and enough columns to take up 10 megabytes.
We can go ahead and see how long it takes Python to write this data frame out to a file.
We're going to use here the percent percent time.
This is the ipython or Jupyter way of calling what they call a cell magic.
In this case what it does is this is not actually run as code, but it will tell Jupyter
that the entire cell should then be executed.
And we would like for Jupyter to keep track of how long it takes.
So we could have as many lines as we needed to after the percent percent time.
And Jupyter will record the total amount of time our computer spends running all of this
code.
Here it's the only line is to save df2 to a CSV.
And here we see that it takes 1.37 seconds.
Not too bad, but this is only 10 megabytes of data.
You can imagine having a much larger data set.
And then this time could could increase accordingly.
Keep it in mind though as we're going to compare this to the other file formats coming up.
The next file format we talked about was the xlsx file format.
And this is what's used by the Excel spreadsheet program.
So pandas has a method to underscore Excel.
Now when we call this, what happens is the first argument is the name of the file.
And the second argument is going to be the name of the sheet, the worksheet inside the spreadsheet.
Inside the workbook.
And here we're going to say we'd like the file to be named df1.xlsx.
And then let's name the worksheet.
Let's name this first data frame.
So we run this and this created the Excel file for us, which we can verify.
If we were to run ls again, now we have this df1.xlsx.
And if we were to open this in our spreadsheet program, we would see that there's a sheet called first data frame whose contents match that of df1.
One other benefit or feature of the Excel file format is that you could have multiple sheets of data.
In order to tell pandas to have a single file containing multiple sheets of data, we use the pandas Excel writer class.
Here's how it works.
You will say with PD.excel writer and we'll pass the name of the file we'd like pandas to create as writer.
Column and we'll have an indented block here and we'll say write the df1 data frame to excel using this file writer.
So now instead of writing the file name here, we pass this writer object.
The second argument is still the name of the sheet.
Then we can do another data frame here that's add 10 to our data frame, write it to excel using our Excel writer and we'll name this sheet df1 plus 10.
If you haven't seen the width as syntax and Python before, this is known as a context manager.
What we've used for in this scenario is any the file after this line is executed, this file will be created and open for the pandas process to interact with.
Then while the file is open, all of the lines in the indented block will be executed.
In this case, we're going to write these two sheets into the file and then when the indented block stops or closes, the file will be closed also.
So this is a way for pandas and Python to manage opening and closing the file without us having to do that ourselves.
There are some additional additional benefits to using the context manager in this way, but that's effectively what's happening under the hood.
Here's where we describe what that is and there's some more size that you can look back on to understand it.
So now what we'll do is I'm going to start running this cell and we'll talk a little more about it.
So we're now going to take our very large or much larger 10 megabyte data set contained in df2 and we're going to try to write this out to a file called df2.xls.
As you can see, since we arrived on this slide and I started it, it's still running that took my computer, which has a very good CPU and very fast memory 18.8 seconds when I ran this on a different machine, it took 25.7 seconds.
Compare this to this CSV version, which took 1.37 seconds on my computer.
As you can see, the Excel file format was quite a bit slower, more than 10 times slower than the CSV for saving this larger data frame.
So now we talked above about the feather file format and it was invented or created specifically to make writing data frames, either in Python and R as efficient as possible.
The best support for this file format comes from a package called pie arrow is not installed by default. So if you don't have it, you should go ahead and uncomment this line by removing the first two characters and then run this cell.
I'll do this. It's going to tell me that I've already have it installed. But if you don't yet in your current environment, you'll see some additional output here.
So now when we call the pie arrow feather dot right feather function, we now have to pass the DF1 as the first argument.
This is a little different than the other methods we saw because it's not a method on our data frame.
It's actually just a function. So in order to make it aware of which data we like it to write, we have to pass that as the first argument.
So we can write the first data frame as DF1 dot feather and then let's check how long it takes.
Hi arrow to construct a data frame containing our 10 megabyte DF2. If we run this, it took 18 milliseconds.
I'm actually going to use a different so magic here called time it, which will run the code many times and take an average for us. So we get a sense of on average, how long is this taking?
This should complete very soon. So it says it was able to run that block of code in that couple seconds.
Seven times per run and it did a hundred times. So it actually constructed this file, 700 times, each time taking on average 14 milliseconds.
So now we had that the CSV file, the CSV time was 1.37 seconds.
And the feather time was 0.014 seconds or 14 milliseconds is almost a hundred times faster.
And this is what we were talking about earlier when we said that the feather file format is extremely efficient and fast for reading and writing data.
We constructed a table of the results for these three file formats when we initially ran this on a different machine.
We can see a similar pattern. The CSV was about 10 times faster than the XL, which was, and then the CSV was about 50 to 100 times slower than the feather file format.
Now let's talk about how to read data from a file in to pandas.
Just like we had the DF dot to underscore methods, we also have a similar pandas dot read underscore family of methods for reading data from a file into a data frame.
Note that the difference here is that the two methods are usually defined as a data frame method.
Because when we're trying to read data, we don't have a data frame yet.
The reading routines are defined as functions within the pandas namespace.
Now the reading methods have quite a few more options because depending on the preferences or behaviors of the person and software that created the file, there may be different edge cases that need to be handled.
We have many more arguments on average for the read family of methods relative to the two family methods.
We'll explore these more in the future when we talk about how to clean a messy data set.
And for now, we're just going to show you how you can read in the ideal case for a few of these file formats.
And to do this, we're actually just going to read the files that we just created so that we can ensure that what we get back matches the data frame that we started with.
One more time we started with a data frame, DF one, we did dot two CSV, for example.
Now we're going to try to read the data frame from the DF one dot CSV file and make sure that what we get back matches the DF one inside of our panda session.
First, we're going to read the DF one underscore CSV. We're going to construct a new data frame by reading the CSV file.
If you remember before, the first column was filled with our index. So we're going to say pandas, the index is contained in the first column, which pandas starts counting at zero.
And let's just look at the first five rows. For now, that looks pretty good.
And check the same thing for the Excel file.
And if you remember, we changed the name of that file of that sheet when we saved it. So let's go back and we called the sheet name first data frame.
So this is what we need to use when we read it.
We'll change the DF one here to first data frame. And there we go. We get back the data frame that we started with.
We can see that this matches the first five rows of DF one by showing the head of that data frame also.
Next, we can do the feather file that we saved. And notice we didn't have to do the index call. And the reason for this is that feather only ever saves.
Data frames that have the default range index starting from zero and going up.
So there were never actually saved it to the file. So when we read it back in feather knows that because it didn't save the index.
That was the default one when it reads the file, it can just attach a default index and it will match what we gave it.
It's a little shortcut they take to preserve both space and time when reading and writing the file.
One very nice feature of the read family of methods is that in addition to passing a file name for a file that lives on your local hard drive.
You can also pass a URL for a file that lives somewhere on the internet.
In this case, we will pass the we will construct a URL for where you can find our DF one data frame.
In the GitHub repository for this course, we'll then tell pandas to read CSV from that URL.
And we'll again, reminded that the first column should be treated as the index.
When we run this, we see that the data again matches the F1.
This happens because we saved before the lecture, I saved it the F1 to a file uploaded it to GitHub so that when we read it, it will match what we have.
The key here is to know that we can pass a URL here and pandas will do the necessary network operations to go find the file and use it.
Okay, my turn is over. Now it's time to practice. What I'll do is I'll set up and walk talk you through a problem that will have you work on.
And then we'll be able to stop the video and have you work on the problem yourself.
So in the cell below, you're going to have a variable called URL.
And this points to a file on the internet containing the result of all the United States NFL football games from September of 1920 all the way through February of 2017.
The NFL stands for the National Football League and is the official professional American football league in the United States.
Your task with this data is to do the following. First, you need to use the PD or pandas dot read CSV function to read the data from this URL from the file pointed to by the URL into a data frame that you'll call NFL.
You'll then print out the shape and the column names of NFL so that you can see what the data looks like.
And then we'd like for you to save this data to a file on your local hard drive called NFL dot XLSX.
And we'd like for you to use the appropriate to method that matches the XLSX 504 mat.
If you need to look back at what we just work through together, feel free to do that.
And finally, if possible, we'd like for you to open this spreadsheet on your own computer. If you have spreadsheets software installed.
If you're able to work through that really quickly and it doesn't take you long, maybe you can try doing some analysis.
We don't have any hard expectations for what you need to do here.
So here are some suggestions for what you might try. These four bullet points contain them.
And you can go ahead and look at what they are and do your best to evaluate that.
That's the end of this lecture though.
And we please spend time working through this exercise and best of luck. Thank you.
