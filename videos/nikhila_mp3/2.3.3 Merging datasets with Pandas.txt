The final topic we'll cover today is on data merging.
So prior to this lecture, you should have heard us talk about reshaping.
And if you were here 10 minutes ago, you heard all about it.
After this lecture, you should be able to understand the different panda's routines for combining data sets.
No, when and how to apply CONCAP, merge, and join.
And we'll be using two data sets during today's lecture.
We'll use data on the GDP components, population, and square miles of various countries.
And we'll use data from a website called Goodreads that allows you to rate different books.
And so there will be over 6 million ratings in our data set.
Let's get started.
So we often want to perform analysis on data that originates from different sources.
For example, if you were trying to analyze data on regional sales for a company,
you might want to include things like industry aggregates or demographic information from each reason,
or if you have product-level data, maybe it's split into product information in one data set,
and data about sales and other things in another data set.
And you often need to take these data sets and put them together to do the type of analysis that you'd like.
So we're going to be using, like I said, data on GDP and some other variables.
So we're going to read this in from the World Development Index.
And so what we see is we have WETI data from 2017,
and we have government expenditures, consumption, exports, imports, and GDP.
And all of this is measured in trillions of US dollars.
We'll also get a version of this data set with 2016 and 2017 data.
The next data set we're going to read in is data on the land area of different countries.
So the United States covers 3.8 square millions of square miles, Canada also 3.8, Germany 0.13, and Russia 6.6.
So this is our second data set.
And our third data set is going to be data on the population measured in millions of people for each country.
And what you'll notice is we have a much longer history for this than our other data sets.
Great. So given the data sets we've just introduced, we have data on GDP, we have data on land area, and we have data on population.
Suppose we were just compute, ask to compute a number of statistics.
Suppose we were asked to compute a measure of land usage.
So what is consumption per square mile? Or what is the GDP per capita for each country in each year?
How about consumption per person? And what is the population density of each country and how does it change over time?
So the answer to each of these questions is going to require us to use data from multiple data frames.
And so the three methods as we've kind of hinted at that we'll be learning about are concat, merge, and join.
And we'll look at each one independently. And we'll start with concat.
The concat function is used to stack data frames, and it's particularly useful when you have something like a year of data that's stored in one month files, and you need them to go on top of one another.
Concat, that should actually remind you of the word string concat nation where you literally just tie it together.
So there's nothing creative or interesting going on. It's literally just taking to rate data frames and mushing them together.
As with many other pandas functions, we're going to have some options about how to do that.
So we can choose whether we're stacking on by rows, which would set be setting access equal to zero, or by columns, which would be setting access equals to one.
So we'll look at each case separately, and we'll start by looking at rows.
So let's consider two of our data sets. So we have the WDI 2017, which remember is just a data set.
You'll notice the country is on the index. The columns are these different classifications of government, of the expenditure report.
And we have square miles, which has country on the index as well.
So if we stack these on top of another with access equals zero, what you'll see is that literally it has just stacked these two data frames on top of one another.
They didn't have any of the same columns. And so anywhere that didn't have a shared column is going to get a man.
So square miles only had data on square miles and not on government expenditures or anything else.
And this government expenditures consumption, the WDI data frame, didn't have information on square miles.
So we can kind of see what it's doing. But I think this tells us that this kind of wasn't the right operation to do in this case.
So the number of rows is the total number of rows and all inputs and the labels are going to come from the original data frames.
They're each going to be distinct. And for columns that appeared only in one input, the value for all relabels originating from a different input is set to missing.
So maybe what we would prefer to do is concat with access equals one. And so what you can see is it's actually matched up. It notices.
So look at our two data frames.
It notices that there's data for Canada in both data sets. And so when it combines these, notice it's taken 0.37 dot dot dot 1.86.
And it's gotten found where Canada was. And so it matches on the axis that you're concatenating.
Similarly for Germany, it's done this. So it has these columns match and it's correctly found to the right square miles, etc.
So concat is pretty simple. It's a bit of a, it's a brute force way to combine data sets. But there's many cases in which it's the right answer.
So now that we know how to do this, we actually can answer one of our questions that we mentioned. So what is the consumption per square mile?
So here we go. Well, this is modulo the right units. So I guess we would have multiplied this one by one trillion.
Let's ignore that for now because I'm going to get something wrong. And let's just assume that it's in the units we want.
And now we've got a measure of consumption per square mile. Now what you see is that in terms of square consumption per square mile, the United Kingdom and Germany look really good.
And a lot of that is being driven by the fact that they have such a small land area. And we can't answer this question for Russia because we don't have any of their GDP or consumption data.
Great. So that covers our concat methods. And now we'll talk about another operation that you'll probably use slightly more, which is merge.
Merge operates on two data frames at a time. And it's primarily used to bring columns from one data set into another. And it aligns the data based on one or more key columns.
So remember concat aligned on the axis that you were concatenating on, but you were restricted in the sense that it either had to be a column already or on the index.
Merge is going to allow us to be a little bit more flexible. So let's just see an example. If we take the two data sets that we just concatted.
So these same two data sets and now we're going to merge them, we actually get something that looks really similar.
So what have we done here? We'll talk more about that in a second. So let's do some examples.
So we can merge on 2016 and 2017 and square miles. And the reason this is interesting is that this data set had each of these countries for two years rather than just one.
And I actually, I kind of, so this is learning in real time. So we're going to see what happens when we concat these. I actually don't know what it'll do.
Okay, so zero kind of done does what we expect, but it takes the two levels and combines them into one, which is great.
And access equals one kind of gives us absolute garbage. So, so we could not have used concat in this example, which I think is what we wanted to demonstrate.
I guess the one problem is so the data is copied over exactly as is and the additional column from square miles was added and matched on the country. So what you'll see is we have two observations for Canada that correspond to 2016 and 2017.
And they both were given square miles. Unfortunately, we lost the information about here because it was on the index. And we'll talk about how we could have gotten it back and how we could have kept it in the merge.
So how can we fix this if we take our data set and we reset the index. So remember, we have this 17 resetting the index, which we learned about in our reshape is simply going to take the index and move it back into columns of the data frame.
So we now have columns country and the year. So if we specify that we want to match on country and it's a column rather than an index, notice it is kept the column year.
And it still has matched up correctly where square miles goes.
Additionally, we sometimes need to merge on multiple columns. For example, our population data and our WDI data both have observations organized by country and year.
So to properly merge these data sets, we would need to align the data on both the country and the year.
So notice we can pass our list country and year. It's gone ahead and put that on the index for us. If we didn't want it on the index, we could have reset the index to take it out of the index and it's still kept both country and year.
And this puts us in a position where we can now answer one of our other questions. What is the GDP per capita for each country in each year? How about consumption per person?
And so now if we look at GDP per capita, it's not so different in all these countries actually. I think, you know, there's a difference between 0.5, 4 and 0.4, too, but it's maybe not so big.
And then the consumption difference is actually quite a bit different. This is the United States concerns consumes almost a third more than the other countries in our data set.
Okay, so let's use our new merge skills that we've learned to answer the final question that we asked originally. What is the population density of each country? How much does it change over time? And we'll go ahead and give you kind of two or three minutes to answer this question.
Okay, welcome back. So let's see how I did this problem. There's more than one way to solve this. So I took the two data sets and notice I took the population data set and I reset the index so that we wouldn't lose any information when we merged the two data sets.
So we merged it on country and then we created a new column called pDens for population density. And then I just did a pivot table to put it in a format that was easy to read and I put the year on the index, the columns were the countries and the values were the population density.
And what you see is that the population density has increased in the United States and Canada, but it's been roughly constant in Germany and you've seen a slight increase in the United Kingdom.
Okay, so merge takes many different optional arguments. So we've talked about some of the most commonly used ones so far.
And in particular, we've talked about the argument on and this just specifies what columns you're trying to align the two data sets on.
And the default is actually that it will merge on any of the columns that appear in both data frames.
So in this case, we've reset the index on both of the data frames and we've tried merging it and what it found was it identified on its own that country was a common column and so it merged on that column.
In the example above, we could use the on argument because the column name was the same in both data frames.
It turns out that sometimes you won't have data frames that have the same name for a column. For example, one data frame might call the identifier product ID while the other calls it ID or product.
And in that case, we can use left on and right on arguments and press the proper column names to align the data.
So again, in this case, it's somewhat trivial because they're the same, but we could do left on country, right on country, and it succeeds.
Additionally, the data may be on the index instead of on one of the columns, in which case you could do some and you can mix all of these so you can do left on country and right index equals true.
And that means it will try and merge the values in the column country with the values that are on the square miles index.
I should point out, I don't think I've said this yet, that we're calling the first argument to the merge function, the left data frame, and the second argument, the right data frame.
And this lines up with how people discuss some merges and SQL and we're about to see why that is.
Great. So the argument we haven't discussed yet that you will typically, that you will find to be one of the most powerful arguments is the how argument.
And so there's four possible values for how you do your merging.
The first is a left merge, and what a left merge does is it finds all of the values that are in your left data frame.
And it keeps those that you're merging on two and it keeps those as the inputs to the new data frame.
Right does the same. And so it will ignore any values that are in the right data frame that are not in the left data frame. Right does the exact opposite where it keeps the values from the right data frame and ignores any in the left data frame that are not in the right data frame.
So what we're only thinks about values that are in both data frames and outer thinks about any of the values that can merge on in both in both the left and right data frame.
So let's see some some examples. So what we're going to do is we're going to create two new data frames.
So what we're going to do is we're going to have is there going to be the WDI data and we're going to drop the United States and the others will be square miles data that will drop Germany.
So let's walk through all of the possible options. So we can merge two data sets by going on the left.
So let's move this to the no US data set. You'll notice that the United States disappears. And that's because we'll see that again. So we do left. It's going to find Canada, Germany, United Kingdom, United States.
We're going to find all of those same values inside of the square miles. When we do the WDI no US, it's not going to find the United States in this data set, but it will find it in this data set.
And it chooses to leave it out because it's not in the left data frame.
When we do this merge with the WDI and square miles, but we use right notice Russia has been included because Russia was in the square miles data set, but not in the WDI data set.
And so it's just going to put a missing anywhere that it doesn't have data for it.
When we do the inner join with the no US data notice this one has does not have the US or Russia.
And this one has both and it chooses to do to ignore both of them because they're not in both. If we move back to the original WDI 2017, then the US will be in the WDI, but not in square miles. So let's see this.
Oh, WDI 2017 and square miles. So what do we see here? They have Canada, Germany, United States and United Kingdom.
This one has Russia and so when we do enter, it's going to leave Russia out because it's not in both.
And finally, we'll use the outer argument. It goes ahead and it includes all five countries, even though there's no square miles data for Germany.
And there's no WDI data for either the US or Russia in the WDI 2017 no US.
Okay, so you should come back and we'll make sure to include this in your assignment.
But you should study these different how methods. It's important to understand kind of what operation you're actually doing when you merge data.
And there's a lot of flexibility in what you can do because of these many arguments.
The final thing we'll say about this is just that rather than write pd.merge, you could just write a data frame.merge and use the merge method rather than the function.
And it has all the same arguments. It's an equivalent thing to do.
Okay, the last merge method we'll talk about is called join.
And it's very similar to the merge method, but what's going to happen is it only allows you to use the index of the right data frame as the join key.
So left dot join right on country is equivalent to calling pd.merge left right left on country right index equals true.
The actual implementation of join is going to call merge internally, but it just sets the left on and right index for you.
You can do anything with df.join that you can do with df.merge, but sometimes it's more convenient to use if the keys are in the right index.
And we can just see this here.
So what we're going to do now is we're actually going to do put all of the tools we've learned today into practice.
And we're going to do a case study using this book data that I described earlier.
So on your computers, this will probably take a minute to load the first time because it's a relatively large data set.
So if we look at ratings, it has approximately six million entries.
And inside of this ratings data, there is a user ID that maps from that maps to different users.
There's a book ID that will map two different books.
And then what happens is when a user rates a particular book, they give it a rating on a scale of one to five, five being the best.
And so you know how a particular user rated a particular book.
So let's see what we can do with this.
So I think kind of the most obvious first thing to do is to just see how many of each rating there are in our data set.
So what we see is that there's approximately two million five star ratings, a little over two million four star, one and a half three star, a half million two stars, and you know, maybe a hundred thousand of one star books.
So the data certainly seems a little bit skewed to higher ratings.
The next thing we could probably do is let's see how many users have rated and books.
Let's think about why how we're going to do this.
So we want to know how many of these different users have rated a certain number of books.
So if we count how many times a user ID shows up, that will give us.
So let's go ahead and do that.
What that's going to do is that's going to say user.
Okay, so user 30944 has provided 200 ratings.
And so if we want to know how many people have provided 200 ratings, what we're going to do is actually apply value counts again.
So that's what we're doing down here.
So we're doing dot value counts to figure out how many ratings each user has done.
And then we're going to do dot value counts again, which is going to show us how many users provided a certain number of ratings.
Then we're going to sort our index.
Because remember our index are going to be these numbers potentially from one to 200.
We're going to move them out of the index.
And then we're going to rename them to end ratings and end users.
And let's see what that gives us.
So what we see here is one user has provided 19 ratings.
When user has provided 20 ratings dot dot dot.
So let's look at some statistics on this data we've created.
So we can use the dot describe method.
And it will show us that there were 181 separate users in our data set.
The mean number of ratings that were given was 100.
So it seems people are relatively active.
The minimum number of ratings given was 19.
And the median number of ratings was also about 109.
And the max ratings was 200 of which almost 1,000 users gave 200 book ratings.
So we can just describe this in our in a box plot.
So this is the same information we just saw but now in a graph.
So let's practice using the want operator.
So let's determine whether there's a relationship between the number of ratings a user has written and the distribution of ratings.
One reason you might do this is if you're an author and you're trying to inflate your ratings.
And you wonder whether you should try and get more experience good read users to write your book or focus on getting first time users.
This is going to be kind of a funny.
I like the answer here.
So let's apply the want operator which means we're going to start at a result and work our way backwards.
So we can answer a question if we have two data frames.
First we have all ratings by the top end users with the most ratings and all ratings by the end users with the least number of ratings.
Okay. So how are we going to get that?
So we need to extract the rows of ratings with user ID associated with the end most and least prolific readers.
So to get that we need to find the most and least active user IDs.
And so to get that information we can just do what we did a few minutes ago where we count how many times that user shows up in the data set.
So we have four steps and the first one is kind of trivial.
So each step is trivial which means we've kind of applied the want operator correctly.
And let's start going backwards.
So end ratings if we just value count we now know user 3944 has provided 200 ratings.
52036 has provided 199 ratings etc.
Excellent.
So now we need the end largest and end smallest.
So let's go ahead and change this to end.
So we're going to look at this series we've created and ratings and the method and largest selects the end largest values.
And we want the indexes associated with that because they're the user IDs and we're going to move it into a list.
And then let's go ahead and print one of these lists so we know it so we can look at what it looks like.
Excellent.
So now we have these 25 users.
Okay.
So now let's see what ratings were given by these particular users.
So we're going to find we're going back to our original data set the ratings.
And we're going to figure out when these radars are in.
So when the user ID is in this list of the most prolific users, the most frequent users of the data set.
And we're going to keep all of the columns and then we're going to also do the same thing for the less common users.
And maybe we want to look at active ratings down the head.
Okay.
So now we have a new data set that's going to give us a user ID, a book ID, and a rating.
But it's only going to include user IDs for the most active or least active users.
Okay.
Now we can get to our answers.
So what we're going to do is we're going to plot the distribution of the most active users.
And plot the distribution of ratings for the least active users.
And so what do we see?
We see that more active users have a median probably in the mean of like three and a half to four.
Whereas I think the mean of the active users is closer to four.
So we could actually tell ourselves what this, what that is.
So inactive ratings, rating dot mean.
Oh, that's interesting.
I was wrong.
So you get a higher mean from active users.
So this is fun.
This is a learning experience.
This is why you do things like this.
So you get a higher mean from the active users.
That happens to the median.
Okay.
If they have the same median.
Interesting.
Okay.
So you get a higher average rating from more active users.
And the reason is that they assign a very small fraction of their ratings to be one and two.
You get a lower standard deviation because they're focusing on these three, four, and five ratings.
For inactive users, you get a higher fraction of fives.
You get the same median, which is four.
But you have a higher standard deviation.
So I guess your strategy probably depends on what you think.
If you think you're a relatively strong book, then an active user might still rate you as a four.
But maybe you have a better chance of getting a five if you appeal to the active users.
So this isn't the answer I thought we had.
So new users are more likely to leave five star ratings.
But your average rating is actually higher if you go through the active users.
So we learned something that I didn't know.
Okay.
So we've kind of started learning a little bit about this data set.
And you're probably thinking, wasn't this supposed to be emerging?
Why are we only using one data set?
I heard you.
Okay.
So what we're going to do is we're going to load up another data set that contains information on books.
So we're only going to keep a couple of columns.
So we're going to look at the book ID, which you might suspect is because we're going to merge.
And you'd be right.
We're going to look at the authors and we're going to look at the title.
So again, this will probably take a minute to run.
So now we have a book ID, an author, and the book title.
And so if we want to do interesting things with the books data set, we could.
But what we're going to do is we're simply going to merge these two data sets together.
So we look at rated books dot ahead.
And now we have the user ID, the book ID, the rating.
And notice it's given us the author and the title.
And we did not pass on, but we could choose to do on book ID.
And we might actually want to do left set.
We're not loading up a bunch of random books.
Okay.
So let's start ahead.
I guess let's see how the.
So if we do left, we get about six million, which is what we should.
What happens if we do enter the outer.
Okay.
It looks like the books data set is.
It contains all of the books that are in ratings.
None of the methods are going to make a difference.
Okay.
So let's see.
So now we could have done this before where we counted which books were the most often rated.
But we would have only known what book ID was most rated.
Now we can actually look at which titles were the most rated.
So we take our rated books book ID dot value counts.
Remember, this is going to tell us how many times a particular book was rated.
And now let's look at the 10 largest.
And we're going to grab the index, which is going to be the corresponding book IDs.
We're then going to look at the rated books from the most rated IDs.
And let's go ahead and look at what these titles are.
So these are all books that are so good reads is a mostly US based.
I don't know whether they call it a social media site.
It's a book rating site.
So you might not be surprised that the Harry Potter books are some of the most read and most rated the Hunger Games and Twilight.
I'm kind of so to kill a mockingbird was actually one of my father's favorite books.
And so I'm pleased to see that it made the cut of one of the most rated books.
It's actually a really nice book.
So these are the most rated books on in our good reads data set.
Now we could use our knowledge of pivot table that we learned about previously to get an average rating for each of these books.
So the average rating, let's go ahead and let's sort it by the rating.
So the highest rated books were Harry Potter and the prisoner of basketball, Harry Potter's and the sorcerer's stone, then to kill a mockingbird.
Again, I'm pleased about this.
And then kind of Hunger Games Harry Potter, Twilight comes in last, which is kind of funny.
I suspect this is a function of who the audience of Twilight's readers is.
They may not be the people who are most involved on good reads.
I could be wrong.
I don't know.
I don't know enough about kind of the literary world to comment intelligently on all of these insights, which brings us to a slightly different point.
When you're analyzing data, there's actually a lot of value in domain knowledge.
So you'll see a lot in the world that data scientists or sometimes statisticians or even economists, they'll wander into a topic that they don't know very much about.
And they'll make some rather broad and strong claims.
And someone with domain expertise will say kind of, well, what you're saying isn't true.
This is actually a shortcoming of the data.
And this is why what you're saying is kind of a, is kind of weak or kind of wrong.
And so there's a lot of value when you're working on business problems of teaming up with someone who has a lot of domain expertise.
And trusting, you know, if you have a client, trusting that your client knows more about what they're doing than you do.
And then tying their knowledge into your kind of statistical and economic insights and really mixing these two things is the best.
But okay, getting off my soapbox now.
So we could compute the average number of ratings for each book in our sample.
Yeah, this is rated books, pivot table ratings.
I see. So what we've done is we've computed the average rating and that average number of ratings.
And so the best one of the most highly rated books is Calvin and Hobbes, which is a comic. It's quite good.
And it shows up lots of times actually. So there's one, two, three, four, five Calvin and Hobbes in the top 10.
And so what is the overall distribution of ratings look like?
We plot the kernel density using our data frame method.
And then we also plot the histogram.
And they tell us roughly the same thing.
Looks like the majority of books are rated just below four.
Okay, so that's going to end our case study for now.
If we really wanted to kind of do a deep dive and learn some really deep insights about the literary business world, we would have needed to bring someone in that knows that world.
So this was just kind of fun and a chance to show off some of the methods that we've learned.
And the final thing we just wanted to do was we've put together and by we, I mean the royal we in the sunset Spencer has put together some more beautiful gifts about how these different merge methods work.
So we create a toy data set, the left and the right.
And we just look at how these work. So notice our concat with axis equals zero.
It's just going to stack these on top of each other.
So you're taking these values and you're just going to put them into the C3 column.
The key is going to get matched because it's the same in both data sets.
And then these two will just come straight over.
But then you have missing data here because there's no C3 column and you have missing data here because in the other data frame, there's no C1 or C2.
For axis equals one, notice it's doing something similar. But in this case, there's no, there's actually no overlap.
So because the index on the first data frame and the index on the second data frame are completely separate.
So it's just copying over a bunch of data. This would not typically, this would not be the merge method you wanted for the data set we're using.
So now we have our merge.
So and this is going to be by default remember left.
And so what it does is it basically copies all of this data first because we're only going to keep these values and we're merging on key.
And so it's going to say key equals a. So let's put a 100 there and there key equals B. Let's put a 200 and the key equals B.
And then there's not going to be a D. There is a C. So let's put the C in the right spot. But there's no D. So it's not going to show up.
We could do the same thing for the right. But now notice what it's done is first it copies this data frame over.
Now it looks for a. It finds two of them. So it duplicates this particular row and then fills in with one and ten and three and thirty.
Then it finds the B. And it goes and it looks up what the B was here. And it sees two and twenty. Then it looks at the C and sees four and forty.
And there's no values for D. So it simply keeps them as missing.
And that's all we have for our merging lecture. So we'll give you your assignment and please feel free to reach out and ask us if you have any questions. Bye bye.
