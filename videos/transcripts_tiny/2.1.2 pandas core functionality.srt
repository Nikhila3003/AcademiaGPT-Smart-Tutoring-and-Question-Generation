1
00:00:00,000 --> 00:00:07,000
This is Chase, and today we're going to be talking about some of the basic functionality that we have in pandas.

2
00:00:07,000 --> 00:00:13,000
At the end of the lecture today, you should have been familiar with what is a datetime object,

3
00:00:13,000 --> 00:00:21,000
and you also should have learned how to use aggregation functions, transformation functions, and scalar transformation functions,

4
00:00:21,000 --> 00:00:28,000
and apply built-in and create customized versions of these to make transformations to your data.

5
00:00:28,000 --> 00:00:34,000
We'll also talk about how to select subsets of your data using Boolean selection,

6
00:00:34,000 --> 00:00:39,000
and we'll introduce you to what we call the WANT operator, and we'll teach you how to apply it.

7
00:00:39,000 --> 00:00:47,000
The data that we'll be using for today's course comes from the US State Unemployment Data from the Bureau of Labor Statistics.

8
00:00:47,000 --> 00:00:52,000
So this is our outline, and let's go ahead and get started.

9
00:00:52,000 --> 00:01:02,000
So as usual, we're going to import our pandas as PD, and we're going to activate our custom plotting theme.

10
00:01:02,000 --> 00:01:13,000
Now, whenever we do a pandas lecture, this is our first one, we'll typically read in some kind of a data set that we'll use as motivation for the things that we do in class.

11
00:01:13,000 --> 00:01:17,000
Again, in this case, we're going to be reading in State Unemployment,

12
00:01:18,000 --> 00:01:23,000
and you'll want to take note that there's going to be a column named date,

13
00:01:23,000 --> 00:01:28,000
and we're going to specify that that column should be read in as a date object.

14
00:01:28,000 --> 00:01:33,000
And so if we look at this, we can see the structure of our data.

15
00:01:33,000 --> 00:01:44,000
It has a column for date, a column for state, a column for how many people are in the labor force in that state, and an unemployment rate.

16
00:01:44,000 --> 00:01:51,000
And you won't have to worry about this, but what we want to do is we want to have a column for each state,

17
00:01:51,000 --> 00:01:59,000
and then along the index we would like to have the dates, so one per month in our case,

18
00:01:59,000 --> 00:02:03,000
and the values that we'd like inside of our data frame will be the unemployment rates.

19
00:02:03,000 --> 00:02:10,000
And this is just some reshaping that we'll learn to do later in this course to make sure that we have the data that we want.

20
00:02:10,000 --> 00:02:19,000
So again, you should just take note that the states are the columns, the date is the index, and the values are unemployment rates.

21
00:02:19,000 --> 00:02:25,000
And to keep the data manageable, we're just going to focus on seven states.

22
00:02:25,000 --> 00:02:31,000
We're going to focus on Arizona, California, Florida, Illinois, Michigan, New York, and Texas.

23
00:02:31,000 --> 00:02:37,000
So this unimp data frame that we have will be the data frame we work with for the rest of this course.

24
00:02:37,000 --> 00:02:44,000
So the first thing we can do is we can plot the data.

25
00:02:44,000 --> 00:02:50,000
And we talked briefly about this in the last video, that it will create this plot for us.

26
00:02:50,000 --> 00:02:58,000
The thing I want to highlight right now is that it knows that the unemployment, that the date column has been moved to the index,

27
00:02:58,000 --> 00:03:04,000
and so when we plot, it places this on the index for us and labels it.

28
00:03:04,000 --> 00:03:11,000
Additionally, it will create one line for each of the columns, and this is why we chose to have seven columns instead of 50,

29
00:03:11,000 --> 00:03:14,000
and it's going to label these for us.

30
00:03:14,000 --> 00:03:17,000
So this purple line is Arizona.

31
00:03:17,000 --> 00:03:22,000
This really low unemployment rate is from Texas.

32
00:03:22,000 --> 00:03:29,000
And so we see that we get some of this nice plotting features for free.

33
00:03:29,000 --> 00:03:37,000
So let's go ahead and look at what index we have on our data frame.

34
00:03:37,000 --> 00:03:45,000
You'll notice that it has these nice formatted text, so 2000, 01, 01.

35
00:03:45,000 --> 00:03:53,000
And that's because we specified when we read the data in that column, that data was a datetime object.

36
00:03:53,000 --> 00:03:58,000
And so Pandas has recognized that this is a datetime object because we told it,

37
00:03:58,000 --> 00:04:03,000
and it's going to allow us to do some special things with it.

38
00:04:03,000 --> 00:04:09,000
So for example, we can index into a particular date.

39
00:04:09,000 --> 00:04:16,000
So if we choose to look at January 1st from the year 2000, we see that Arizona had an unemployment rate of 4.1,

40
00:04:16,000 --> 00:04:24,000
Texas had an unemployment rate of 4.6, and California had an unemployment rate of 5.

41
00:04:24,000 --> 00:04:34,000
We could also choose all of the days between New Year's Day in June 1st and the year 2000 just by putting 01, 01, 2000.

42
00:04:34,000 --> 00:04:40,000
And notice we can pass dates in in different formats and going to June 1st, 2000.

43
00:04:40,000 --> 00:04:51,000
And that will now give us a data frame with six rows where each of the rows corresponds to a month.

44
00:04:51,000 --> 00:05:02,000
So we'll talk much more about this in a future lecture, but we just wanted to highlight that you have this functionality available to you.

45
00:05:02,000 --> 00:05:07,000
So the next topic we'll talk about are data frame aggregations.

46
00:05:07,000 --> 00:05:15,000
And loosely speaking, aggregation is an operation that takes multiple values and turns it into a single value.

47
00:05:15,000 --> 00:05:19,000
The example that we should all be familiar with is computing a mean.

48
00:05:19,000 --> 00:05:27,000
So taking the three numbers 01 and 02 and computing their mean returns a single number, 1.

49
00:05:27,000 --> 00:05:36,000
We're going to use aggregations extensively in this course, and we're very grateful that Pandas will make this easy for us.

50
00:05:36,000 --> 00:05:40,000
So Pandas has some of the most frequently used aggregations already built in.

51
00:05:40,000 --> 00:05:49,000
For example, mean, variance, standard deviation, finding the minimum, medium, max, etc.

52
00:05:49,000 --> 00:05:52,000
We do want to note here this is not all of them.

53
00:05:52,000 --> 00:05:55,000
There's many other aggregations Pandas has that we have not listed.

54
00:05:55,000 --> 00:06:00,000
And we just encourage you to use a little bit of tab completion.

55
00:06:00,000 --> 00:06:09,000
So if you take a data frame, so again, we can take the mean.

56
00:06:09,000 --> 00:06:16,000
If you hit tab, notice it brings all of these options up.

57
00:06:16,000 --> 00:06:23,000
And let's see an aggregation that we might do is standard deviation.

58
00:06:23,000 --> 00:06:28,000
So if we just type ST and hit tab, it brings up STD.

59
00:06:28,000 --> 00:06:38,000
And if we wanted to know what that was, we could put a question mark and return sample standard deviation or requested axis normalized by n minus one by default.

60
00:06:38,000 --> 00:06:40,000
So it tells us a little bit more.

61
00:06:40,000 --> 00:06:46,000
And we could then use that to get the standard deviation.

62
00:06:46,000 --> 00:06:53,000
So by default, as you might have noticed, aggregation operates on the columns.

63
00:06:53,000 --> 00:06:59,000
We can use the axis argument though to specify whether we like to do aggregations by rows.

64
00:06:59,000 --> 00:07:10,000
So remember the zero axis is going, you collapse the zero axis, which is going to collapse over the rows.

65
00:07:10,000 --> 00:07:14,000
But if we specify axis equals one, we can collapse over the columns.

66
00:07:14,000 --> 00:07:22,000
So when we take the variance with respect to axis equals one, we're getting the variance of the unemployment rate in the year 2000.

67
00:07:22,000 --> 00:07:32,000
In January 2000 or February 2000, etc.

68
00:07:32,000 --> 00:07:40,000
So the built-in aggregations get us pretty far in our analysis, but sometimes you need a little bit more flexibility.

69
00:07:40,000 --> 00:07:45,000
We can have pandas perform custom aggregations by following the next two steps.

70
00:07:45,000 --> 00:07:52,000
One, you write a Python function that takes a series as an input and outputs a single value.

71
00:07:52,000 --> 00:07:57,000
And then you can call the ag method with the function as an argument.

72
00:07:57,000 --> 00:08:09,000
So let's do an example below where we classify states as low unemployment or high unemployment based on whether their mean unemployment level is above or below 6.5.

73
00:08:09,000 --> 00:08:13,000
So step one, let's write the aggregation function.

74
00:08:13,000 --> 00:08:19,000
So higher low is a function that takes s, which is going to be a pandas series object.

75
00:08:19,000 --> 00:08:25,000
And if the mean of s is less than 6.5, we'll return low.

76
00:08:25,000 --> 00:08:31,000
And if it's higher than 6.5, which is in the else, then we'll return high.

77
00:08:31,000 --> 00:08:37,000
And so now step two, all we have to do is call dot ag.

78
00:08:37,000 --> 00:08:47,000
And notice we've now classified Arizona as a low unemployment state, California as a high unemployment state, etc.

79
00:08:47,000 --> 00:08:51,000
We also can get the axis equals one.

80
00:08:51,000 --> 00:08:57,000
And we can see which of the months were low or high unemployment.

81
00:08:57,000 --> 00:09:01,000
Now all of the ones we can see are low unemployment because they were in times of growth.

82
00:09:01,000 --> 00:09:08,000
But if we looked at, so let's go ahead and do our fancy date indexing.

83
00:09:08,000 --> 00:09:21,000
So let's go from January 2008 to October of 2008.

84
00:09:21,000 --> 00:09:30,000
And we notice that at the end of 2008, we started seeing high unemployment rates.

85
00:09:30,000 --> 00:09:34,000
And we just did this. Great.

86
00:09:34,000 --> 00:09:38,000
And the other thing you might notice is that ag can accept multiple functions.

87
00:09:38,000 --> 00:09:41,000
So here we're passing in three functions.

88
00:09:41,000 --> 00:09:47,000
We're passing in the function min, max, and then higher low.

89
00:09:47,000 --> 00:09:57,000
And what this will return is a data frame now, where we have the columns the same as the data frame that we passed in.

90
00:09:57,000 --> 00:10:01,000
And now we have a row for each of the functions.

91
00:10:01,000 --> 00:10:07,000
So the min unemployment rate in Texas was 3.9.

92
00:10:07,000 --> 00:10:11,000
The max unemployment rate in Michigan was 14.6.

93
00:10:11,000 --> 00:10:21,000
And then we have our high low classifier that's telling us whether each of the states is a high or low unemployment state.

94
00:10:21,000 --> 00:10:30,000
So now let's take a pause and let's do an exercise to see whether we understood the concepts that were just presented.

95
00:10:30,000 --> 00:10:40,000
So at each state, at each date, we'd like you to find the minimum minimum unemployment rate across all of the states in our sample.

96
00:10:40,000 --> 00:10:45,000
We'd like you to find what was the median unemployment rate in each state.

97
00:10:45,000 --> 00:10:49,000
What was the maximum unemployment rate across the states in our sample.

98
00:10:49,000 --> 00:10:54,000
What state did it happen in and in what month or year was this achieved.

99
00:10:54,000 --> 00:11:03,000
And then we'd like you to classify each state as being high or low volatility based on whether the variance of their unemployment is above or below four.

100
00:11:03,000 --> 00:11:12,000
And we'll go ahead and pause for five to ten minutes, this five minutes, and see how we see how we do.

101
00:11:12,000 --> 00:11:14,000
Let's review the answers.

102
00:11:14,000 --> 00:11:22,000
So these exercises asked first at each date, what is the minimum unemployment rate across all of the states in our sample.

103
00:11:22,000 --> 00:11:33,000
And the way we can find that is for each month, we want to find the minimum unemployment rate, which means we want to take the minimum across the columns or the axis.

104
00:11:33,000 --> 00:11:36,000
So axis equals one.

105
00:11:36,000 --> 00:11:41,000
Then the second was what was the median unemployment rate in each state.

106
00:11:41,000 --> 00:11:53,000
Since this is the median across within a column is we're going to just take the median, which will default to axis equals zero, which collapses the rows.

107
00:11:53,000 --> 00:12:00,000
Next, we're going to try and find what the maximum unemployment rate was across each of the states.

108
00:12:00,000 --> 00:12:05,000
What state did this maximum unemployment rate happen in and when was it achieved.

109
00:12:05,000 --> 00:12:14,000
And the way we're going to do that is first we're going to take the maximum across all of the unemployment, which will tell us the maximum unemployment rate in each state.

110
00:12:14,000 --> 00:12:20,000
And then the max will tell us what the maximum unemployment rate was.

111
00:12:20,000 --> 00:12:26,000
IDX max, again, the first part is the same is going to tell us which state that occurred in.

112
00:12:26,000 --> 00:12:34,000
And then we're going to take the maximum according to axis equals one to find the maximum unemployment rate in each month.

113
00:12:34,000 --> 00:12:37,000
And if we take the IDX max that will tell us when it occurred.

114
00:12:37,000 --> 00:12:47,000
And so the maximum unemployment rate in our sample was 14.6%, which happened in Michigan in June of 2009.

115
00:12:47,000 --> 00:12:59,000
And the last question in this exercise was to classify each state as high or low volatility based on whether the variance of their unemployment was above or below four.

116
00:12:59,000 --> 00:13:08,000
So we define a new function, unemployment rate volatility classify, which takes a series as an input.

117
00:13:08,000 --> 00:13:18,000
And then it returns either the string high or low, depending on whether the variance is above or below four.

118
00:13:19,000 --> 00:13:31,000
And so what we see is Arizona, California, Florida and Michigan are high variance unemployment rate states and Illinois, New York and Texas are low variance unemployment rate states.

119
00:13:31,000 --> 00:13:38,000
Hopefully these lined up with what you did. Otherwise, you should review after class and feel free to ask us questions.

120
00:13:38,000 --> 00:13:42,000
Great. So we're done talking about aggregations for now.

121
00:13:42,000 --> 00:13:46,000
And next we're going to talk about what we call transforms.

122
00:13:46,000 --> 00:13:53,000
And transforms are going to be analytical operations of some sort that do not necessarily involve an aggregation.

123
00:13:53,000 --> 00:14:00,000
So the output of a transform would be a series that is mapped to another series.

124
00:14:00,000 --> 00:14:09,000
Some examples of what this might be are the percentage change in unemployment rate from month to month, or the cumulative sum of the elements in some column.

125
00:14:10,000 --> 00:14:24,000
So there's lots of built in transforms for us, just like there were aggregations, we can take the cumulative sum, the cumulative min, the cumulative max, the cumulative product, we can take the difference.

126
00:14:24,000 --> 00:14:32,000
We can do element wise additions, retraction, multiplication or division, percent change, value counts, or absolute values.

127
00:14:32,000 --> 00:14:38,000
And again, we encourage you to explore what functions are available using tab completion.

128
00:14:38,000 --> 00:14:45,000
So let's go ahead and do a couple of examples. So recall, this is the data that we have.

129
00:14:49,000 --> 00:14:58,000
The first we're going to do is computing the percent change. Notice it can't compute the percent change for the first element because there's nothing that precedes it.

130
00:14:59,000 --> 00:15:12,000
And then we see that unemployment rate did not change in most states, but in Michigan it went down by 3% and it computes these percent changes for us.

131
00:15:12,000 --> 00:15:19,000
Additionally, we can compute the difference, which will instead of taking the percent change, it's just going to take the difference.

132
00:15:19,000 --> 00:15:29,000
And again, the first row is going to be all missing because you can't take the difference between something before the first row.

133
00:15:31,000 --> 00:15:45,000
So we classify transforms into a few main categories. The first is going to be a series transforms, and that's a function that takes in one series and produces another series.

134
00:15:46,000 --> 00:15:52,000
And this will result in the index of the input and the output not necessarily being the same.

135
00:15:53,000 --> 00:15:58,000
Scalar transforms are going to be functions that take a single value and produce a single value.

136
00:15:58,000 --> 00:16:04,000
So one example would be the absolute value method, which we're adding a constant to each value of a series.

137
00:16:05,000 --> 00:16:13,000
So these are going to take a particular value and do an operation to that one value rather than to the entire series.

138
00:16:14,000 --> 00:16:21,000
So just like we could write custom aggregation functions, we can also write custom transformations.

139
00:16:21,000 --> 00:16:27,000
The steps are going to be the same as we're going to write a Python function that will take a series and output a new series.

140
00:16:28,000 --> 00:16:36,000
And now rather than pass our new function as an argument to the ag method for aggregation, we're going to pass it to the apply method.

141
00:16:37,000 --> 00:16:45,000
So as an example, what we're going to do is we're going to standardize all of the unemployment data to have mean zero and standard deviation one.

142
00:16:45,000 --> 00:16:52,000
And we'll do this so that we can classify which date the unemployment rate differs most from normal times in each state.

143
00:16:54,000 --> 00:16:57,000
So step one is to write a function that can standardize the data.

144
00:16:57,000 --> 00:17:03,000
So it takes in a series and it's going to take the mean of that series and the standard deviation.

145
00:17:03,000 --> 00:17:12,000
And then it's going to create a new series that's going to be the original series minus the mean divided by the standard deviation.

146
00:17:14,000 --> 00:17:18,000
Next, we can apply this to our data.

147
00:17:18,000 --> 00:17:30,000
And what we'll see is that in the early 2000s, we had negative values for our, they call this a Z transform.

148
00:17:31,000 --> 00:17:43,000
So our standardized data is going to have minus one, which means that the data was abnormally low, which in unemployment is a good thing.

149
00:17:44,000 --> 00:17:50,000
And then finally, we're going to take the absolute value of all of the elements.

150
00:17:50,000 --> 00:18:03,000
And again, this is a scalar transformation because it's going to take one value and map it into one new value and do this for each value in the data frame.

151
00:18:04,000 --> 00:18:11,000
And now let's go ahead and find out when the unemployment rate was most different than normal from for each state.

152
00:18:12,000 --> 00:18:27,000
And so what we see is that the 2008, 2009, 2010 recession really did strange things to unemployment rates and those were the most abnormal unemployment rates in our series.

153
00:18:29,000 --> 00:18:37,000
And in addition to doing customized series transformations, we can also do customized scalar transformations.

154
00:18:37,000 --> 00:18:42,000
And the way that you do this is again, the first step will always be to find a Python function.

155
00:18:42,000 --> 00:18:46,000
And in this case, it will take a scaler as an input and produce a new scalar.

156
00:18:46,000 --> 00:18:52,000
And second, it's going, you're going to pass this function as an argument to the apply map method.

157
00:18:54,000 --> 00:18:55,000
So let's go ahead and look at this.

158
00:18:55,000 --> 00:19:03,000
So imagine we want to determine whether unemployment rate was high greater than 6.5, medium or low.

159
00:19:04,000 --> 00:19:09,000
So we can write a Python function that takes a single number as an input and outputs a single string.

160
00:19:10,000 --> 00:19:14,000
And then we're going to pass this function to apply map.

161
00:19:15,000 --> 00:19:21,000
So one question you should ask yourself is why are you passing this to apply map and not aggregate or apply?

162
00:19:22,000 --> 00:19:26,000
And think about why neither aggregate or apply would work.

163
00:19:27,000 --> 00:19:29,000
So here's our function.

164
00:19:29,000 --> 00:19:33,000
We define unemployment classifier and if it's above 6.5, we write high.

165
00:19:33,000 --> 00:19:37,000
It's above 4.5, but below 6.5, we return medium.

166
00:19:37,000 --> 00:19:39,000
Otherwise, we return low.

167
00:19:41,000 --> 00:19:50,000
And now when we do apply map, what happens?

168
00:19:50,000 --> 00:19:55,000
We're going to get back a new data frame and notice every value in this data frame is either going to be the same.

169
00:19:56,000 --> 00:20:01,000
So it's not going to be low, medium or high, because those were the values that we could potentially put in.

170
00:20:01,000 --> 00:20:05,000
So now we're going to move on.

171
00:20:05,000 --> 00:20:08,000
We're doing these as examples rather than exercises.

172
00:20:09,000 --> 00:20:15,000
So let's use another transformation on unemployment bins, which was the data frame we just created,

173
00:20:15,000 --> 00:20:20,000
to figure out how many times each state had each of the three classifications.

174
00:20:20,000 --> 00:20:26,440
So, if you were doing this on your own, which I'd encourage you to try and do after class,

175
00:20:26,440 --> 00:20:31,440
you should think about, will this value counting function be a series or a scalar transform?

176
00:20:31,440 --> 00:20:37,240
And Google, pandas count unique value, or something similar to find which transformation.

177
00:20:37,240 --> 00:20:40,800
We're then going to construct a horizontal bar chart of the number of occurrences of

178
00:20:40,800 --> 00:20:44,480
each level with one bar per state and classification.

179
00:20:44,920 --> 00:20:50,800
Okay, so the method we were looking for is one called value count.

180
00:20:50,800 --> 00:20:57,800
And so what we're going to do is we're going to take unemployment bins, which remember, looks like this.

181
00:21:00,360 --> 00:21:08,320
And we're then going to count each of the times that the occurrence of value occurs for each column.

182
00:21:08,320 --> 00:21:11,600
And then we'll go ahead and look at what this looks like.

183
00:21:12,600 --> 00:21:23,600
So now we have for Arizona, the high unemployment, Arizona had high unemployment 75 times,

184
00:21:23,600 --> 00:21:31,600
low unemployment 44 times, and medium unemployment 97 times, etc.

185
00:21:33,600 --> 00:21:38,600
And we're going to plot the transpose so that it does something.

186
00:21:38,600 --> 00:21:41,600
So that's what this dot capital T was.

187
00:21:41,600 --> 00:21:52,600
And let's go ahead and reorder this data a little bit so that we can have low, medium, high.

188
00:22:00,600 --> 00:22:01,600
Here we go.

189
00:22:01,600 --> 00:22:08,600
And so if the blue bar is high, it means that state has a low unemployment a lot of time.

190
00:22:08,600 --> 00:22:16,600
Whereas if the yellow bar is high, it means that that state has high unemployment frequently.

191
00:22:16,600 --> 00:22:23,600
So from this chart, you might not be particularly keen on California or Florida,

192
00:22:23,600 --> 00:22:33,600
whereas somewhere like Texas or New York or Florida might be much better in terms of their unemployment rates.

193
00:22:35,600 --> 00:22:44,600
So finally, we're going to repeat the previous step, but now we're going to count how many states had each classification and each month.

194
00:22:44,600 --> 00:22:50,600
So we're going to do almost the same thing, but now we're going to count across the columns.

195
00:22:50,600 --> 00:22:52,600
So we're going to use axis equals one.

196
00:22:52,600 --> 00:22:59,600
And whenever there's a missing value, which just means that none of the states had a particular value, we're going to fill it with a zero,

197
00:22:59,600 --> 00:23:03,600
because it just means that there were zero states with that value.

198
00:23:07,600 --> 00:23:08,600
And here we go.

199
00:23:08,600 --> 00:23:09,600
This is what it looks like.

200
00:23:09,600 --> 00:23:14,600
So notice in the early 2000s, no states had high unemployment.

201
00:23:14,600 --> 00:23:19,600
Most states had low unemployment and a few states had medium unemployment.

202
00:23:19,600 --> 00:23:26,600
And so now which state, which month had the most states with high unemployment?

203
00:23:26,600 --> 00:23:34,600
You might be unsurprised when we say 2009, which when was the most medium unemployment?

204
00:23:34,600 --> 00:23:36,600
2001.

205
00:23:38,600 --> 00:23:43,600
And when was there lots of low unemployment?

206
00:23:43,600 --> 00:23:48,600
The beginning of 2000 or late in the year 2000.

207
00:23:48,600 --> 00:23:57,600
And we found these with IDX max, which is going to search for the maximum about the index associated with the maximum value.

208
00:23:57,600 --> 00:23:58,600
Great.

209
00:23:58,600 --> 00:24:03,600
So now we've covered aggregations, transforms and scalar transformations.

210
00:24:03,600 --> 00:24:14,600
Another thing that we'll often want to do is Boolean selection, which is we're going to frequently want to select pieces of data based on whether certain conditions are met.

211
00:24:14,600 --> 00:24:19,600
For example, if you were in marketing, you might be interested in a particular target audience.

212
00:24:19,600 --> 00:24:29,600
And so you might subset your data to only include adults between 18 and 32, if you had a particular product for adults, young adults.

213
00:24:29,600 --> 00:24:38,600
You might also be looking at data that corresponds to a particular time period, or some of the work we did while we were, well, Spencer and I were at NYU,

214
00:24:38,600 --> 00:24:42,600
as we looked at data that corresponded to recessions.

215
00:24:42,600 --> 00:24:49,600
So we'll be able to do this by using a series or list of Boolean values to index into a series or data frame.

216
00:24:49,600 --> 00:24:54,600
So we're going to take a small view of our unemployment data frame.

217
00:24:54,600 --> 00:24:59,600
So we took the dot head, which is going to be only five rows of our data.

218
00:24:59,600 --> 00:25:05,600
And we're doing this so that we can see what's happening as we do these operations.

219
00:25:05,600 --> 00:25:09,600
So we can use Booleans to select particular rows.

220
00:25:09,600 --> 00:25:17,600
So in this case, we need seven values or five values associated with let's be explicit here.

221
00:25:17,600 --> 00:25:20,600
So five values that are going to be associated with our five rows.

222
00:25:20,600 --> 00:25:29,600
So in this case, we'll select the first three, true, true, true, and disregard the last two where they were falses.

223
00:25:30,600 --> 00:25:38,600
We can also use Booleans to select both rows and columns.

224
00:25:38,600 --> 00:25:46,600
So in this case, we're again selecting the first three rows, but now we're only going to select the first and last two columns.

225
00:25:47,600 --> 00:26:00,600
Now, we did these selections with lists, but obviously you're not going to be able to write by hand a list with a million values in it.

226
00:26:00,600 --> 00:26:07,600
And so what we're going to do next is we're going to use conditional statements to construct new series of Booleans from our data.

227
00:26:07,600 --> 00:26:14,600
And this will look a lot like some of what we covered in the conditional section of our introduction to Python.

228
00:26:14,600 --> 00:26:16,600
So what can we do here?

229
00:26:16,600 --> 00:26:25,600
So we're looking at the unemployment small data frame, the Texas column, and we're asking when the unemployment in Texas was less than 4.5.

230
00:26:25,600 --> 00:26:36,600
And what this says is that unemployment was above 4.5 for the first three months of the year, but then in April and May, the unemployment fell below 4.5.

231
00:26:36,600 --> 00:26:42,600
And once we have that, we can start selecting subsets of our data.

232
00:26:42,600 --> 00:26:50,600
So notice we have unemployment small Texas less than 4.5, which is going to create a series that has trues and false in it, says in it.

233
00:26:50,600 --> 00:26:53,600
And we can select all of the columns.

234
00:26:53,600 --> 00:26:58,600
And this just gives us the last two rows like we expected.

235
00:26:58,600 --> 00:27:06,600
We could check for when the unemployment in New York was higher than the unemployment in Texas.

236
00:27:06,600 --> 00:27:14,600
And in this small data set, that always happens to be true.

237
00:27:14,600 --> 00:27:25,600
And in the Boolean section of our basic Python lecture, we saw that we could use the words and or or to combine multiple Booleans into a single Boolean.

238
00:27:25,600 --> 00:27:33,600
Just as a quick refresher, we use and and or in the mathematical sense, which means true and false results in false.

239
00:27:33,600 --> 00:27:38,600
True and true results in true. False and false results in false.

240
00:27:38,600 --> 00:27:44,600
True or false is true. True or true is true. And false or false is false.

241
00:27:44,600 --> 00:27:58,600
We can do something similar in Pandas, but rather than writing bool one and bool two, we write bool series one and sign bool series two.

242
00:27:58,600 --> 00:28:06,600
Likewise, rather than writing bool one or bool two, we use the pipe symbol to denote or.

243
00:28:06,600 --> 00:28:19,600
Let's see this. So in this case, we're going to look at when unemployment rate was below 4.7 in Texas and the unemployment rate was below 4.7 in New York.

244
00:28:19,600 --> 00:28:21,600
Notice the ampersand.

245
00:28:21,600 --> 00:28:30,600
And this is going to say this is false for the first two months and true for the last three months.

246
00:28:30,600 --> 00:28:40,600
And if we select using this, notice that the unemployment rates in both New York and Texas will be below 4.7 for every month.

247
00:28:40,600 --> 00:28:47,600
Sometimes we'll want to check whether a data takes on one of several fixed values.

248
00:28:47,600 --> 00:29:00,600
The way we could do this right now is we could write df of a particular column x is equal to the value one or df column x is equal to value two, etc.

249
00:29:00,600 --> 00:29:05,600
But because this is a frequent operation, Pandas provides us a better way.

250
00:29:05,600 --> 00:29:13,600
So in this case, we're going to look at the Michigan column and we're going to ask whether the values are in 3.3 and 3.2.

251
00:29:13,600 --> 00:29:24,600
And so the first four months of the year 2000, the value of unemployment in Michigan was either 3.3 or 3.2.

252
00:29:24,600 --> 00:29:30,600
And we can confirm that here.

253
00:29:30,600 --> 00:29:33,600
And next we're going to talk about any and all methods.

254
00:29:33,600 --> 00:29:43,600
And as you may remember from our first Boolean and conditional lecture, that the Python functions any and all are functions that take a collection of Booleans and return a single Boolean.

255
00:29:43,600 --> 00:29:51,600
Any returns true whenever at least one of the inputs is true, while all returns true only when all of the inputs are true.

256
00:29:51,600 --> 00:29:59,600
Series and data frames have, with a d type bool, have dot any and dot all methods that can apply this logic.

257
00:29:59,600 --> 00:30:07,600
So what we're going to do is we're going to count, we're going to learn this by counting how many months all of the states in our sample had high unemployment.

258
00:30:07,600 --> 00:30:17,600
And as we work through this example, we think this is a great opportunity to talk about the want operator, which is a helpful concept that Spencer and I learned from Tom.

259
00:30:17,600 --> 00:30:20,600
So here's how the want operator works.

260
00:30:20,600 --> 00:30:24,600
You start by writing want followed by what we want to accomplish.

261
00:30:24,600 --> 00:30:33,600
In this case, we would write, we want to count the number of months in which all states in our sample had unemployment above 6.5%.

262
00:30:33,600 --> 00:30:40,600
And now let's work backwards to identify a sequence of steps necessary to accomplish our goal.

263
00:30:40,600 --> 00:30:53,600
So to count the number of months in which all states in our sample had unemployment above 6.5, we would just want to sum the number of true values in a series indicating dates for which all series had high unemployment.

264
00:30:53,600 --> 00:31:01,600
So now how do we get a series that has true values when the states had high unemployment?

265
00:31:01,600 --> 00:31:15,600
We would, the one way to do this is to build a series by using the dot all method on a data frame containing booleans indicating whether each state had high unemployment at each date.

266
00:31:15,600 --> 00:31:30,600
So what this means is we want to know when every state in our sample had high unemployment, which we can do using the dot all method with axis equals one.

267
00:31:30,600 --> 00:31:34,600
Finally, how do we get to one that can do this?

268
00:31:34,600 --> 00:31:42,600
We can just build a data frame by using a greater than comparison to compare whether the data was above 6.5%.

269
00:31:42,600 --> 00:31:50,600
So here is a plan and to apply the want operator, all we're going to do is start at step three and work our way back.

270
00:31:50,600 --> 00:32:02,600
And the reason we find this helpful is because if you start knowing what you want, it's easier to figure out what you need to get one step ahead and you keep stepping backwards inductively.

271
00:32:02,600 --> 00:32:09,600
So, great, step three, unemployment greater than 6.5.

272
00:32:09,600 --> 00:32:15,600
This tells us, well, the early 2000s were good years for the unemployment rate.

273
00:32:15,600 --> 00:32:19,600
So in this case, they're all false.

274
00:32:19,600 --> 00:32:34,600
In step two, we're going to use the dot all method with axis equal one, which will check whether every single column is equal to true for, and we're going to apply this to every row.

275
00:32:34,600 --> 00:32:48,600
So high dot all axis equals one will give us a new series and now it's going to have a true or a false based on whether all of the states had high unemployment rates.

276
00:32:48,600 --> 00:32:55,600
And now finally, simply summing this series will tell us when all states had high unemployment.

277
00:32:55,600 --> 00:33:03,600
And so out of 216 months, only 41 months had high unemployment in all states.

278
00:33:03,600 --> 00:33:06,600
And this ties up our Pandos basics lecture.

279
00:33:06,600 --> 00:33:08,600
Thanks for being here.

280
00:33:08,600 --> 00:33:11,600
Look forward to seeing you soon. Bye.

