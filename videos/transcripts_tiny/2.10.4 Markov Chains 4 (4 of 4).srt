1
00:00:00,000 --> 00:00:10,000
Okay, let's continue with the lecture on finite mark off chains.

2
00:00:10,000 --> 00:00:22,000
Okay, we'll come back to some of these claims on eigenvectors and eigenvalues of T later, but just store those.

3
00:00:22,000 --> 00:00:43,000
Just turn your own, you could read this part and this section teaches you some useful things.

4
00:00:43,000 --> 00:00:50,000
So with this, that's best going over on your own.

5
00:00:50,000 --> 00:01:01,000
So let's see some of the things about mark off chains and concepts about distributions that we already have seen,

6
00:01:01,000 --> 00:01:05,000
and mark on distributions.

7
00:01:05,000 --> 00:01:14,000
So we have a mark off chain pi, an initial distribution will call pi zero.

8
00:01:14,000 --> 00:01:26,000
So then pi zero, a psi zero, that's the distribution of the state at times zero.

9
00:01:26,000 --> 00:01:33,000
If we want to know the marginal distribution of the state at times t, that's going to be psi sub t.

10
00:01:33,000 --> 00:01:41,000
We're going to know that psi sub t and we're going to use this notation.

11
00:01:41,000 --> 00:01:52,000
So there's a way to find psi t plus 1 given psi t.

12
00:01:52,000 --> 00:02:01,000
And we just use the standard laws of probability to do this.

13
00:02:01,000 --> 00:02:16,000
So psi t plus 1, that's going to be a list of probabilities that x t plus 1 is equal to y for every possible value of y.

14
00:02:16,000 --> 00:02:31,000
And if we just compute that, that's the summation probability x t plus 1 equals y condition on x t equals x times the probability x t equals x.

15
00:02:31,000 --> 00:02:36,000
This is all the information that we need to get this is in psi t.

16
00:02:36,000 --> 00:02:40,000
All the information we need to get this is in psi.

17
00:02:40,000 --> 00:02:48,000
So the formula that we get is this formula, if we just substitute that in.

18
00:02:48,000 --> 00:02:51,000
So this is an equation.

19
00:02:51,000 --> 00:02:55,000
We're going to see it has a very simple form.

20
00:02:55,000 --> 00:03:04,000
It's going to be a matrix equation that maps psi t into psi t plus 1.

21
00:03:04,000 --> 00:03:11,000
And these are both marginal distributions.

22
00:03:11,000 --> 00:03:25,000
So if we write that out, if we write that out, that's just this equation.

23
00:03:25,000 --> 00:03:31,000
And if we repeat this many times, we can get this equation.

24
00:03:31,000 --> 00:03:37,000
psi t plus m is equal to psi t, post multiplied by p to the m.

25
00:03:37,000 --> 00:03:44,000
That's just the m power of p.

26
00:03:44,000 --> 00:03:54,000
And now we're going to define this thing called the special case is an initial distribution that has the property.

27
00:03:54,000 --> 00:04:08,000
It has a property is suppose we had an initial distribution that had the property that psi 0 equals psi 0 p.

28
00:04:08,000 --> 00:04:16,000
But just suppose that, and that's such a psi 0 is very special.

29
00:04:17,000 --> 00:04:26,000
And it's called a stationary distribution or a stationary marginal distribution.

30
00:04:26,000 --> 00:04:29,000
It's going to play an important role.

31
00:04:29,000 --> 00:04:34,000
And now if we just stare at this equation, just stare at this equation.

32
00:04:34,000 --> 00:04:39,000
I'm just going to rewrite that by transposing both sides.

33
00:04:39,000 --> 00:04:47,000
So I'm going to get psi 0 transpose is equal to p transpose psi 0.

34
00:04:47,000 --> 00:05:08,000
And if you look at this, and now you look up at the definition of an eigenvector, this is psi 0 transpose is an eigenvector for p transpose with eigenvalue.

35
00:05:08,000 --> 00:05:15,000
And if I can value one, check that out.

36
00:05:15,000 --> 00:05:32,000
So what that's telling you is we can compute psi 0 by just taking the eigenvector of p transpose and normalizing.

37
00:05:32,000 --> 00:05:36,000
That's associated with the unit eigenvalue and normalizing.

38
00:05:36,000 --> 00:05:41,000
So it's a probability vector.

39
00:05:41,000 --> 00:05:45,000
So that's what's repeated, that's what's summarized here.

40
00:05:45,000 --> 00:05:49,000
And it's a very important idea.

41
00:05:49,000 --> 00:05:58,000
Okay, so such a psi 0 is called a stationary marginal distribution.

42
00:05:58,000 --> 00:06:08,000
Okay, this material which I ask you to read, that's about a multi-step transition probabilities.

43
00:06:08,000 --> 00:06:19,000
And we already kind of talked about that a little bit.

44
00:06:19,000 --> 00:06:35,000
And now here's something I'm going to skip this example, although you might want to read this conjunction with a Hamilton example.

45
00:06:35,000 --> 00:06:41,000
I want to jump to this section 5.3.

46
00:06:41,000 --> 00:06:53,000
Because these section lots of, lots of sentences are going to be worth reading several times.

47
00:06:53,000 --> 00:06:58,000
So the marginal distributions read the sentence 10 times.

48
00:06:58,000 --> 00:07:06,000
The marginal distributions can be viewed either as probabilities or as cross-sectional frequencies in large samples.

49
00:07:06,000 --> 00:07:14,000
Remember a theme, what does probability mean?

50
00:07:14,000 --> 00:07:24,000
It means anticipated frequencies of random draws in very large samples.

51
00:07:24,000 --> 00:07:28,000
So that's continuing that theme.

52
00:07:28,000 --> 00:07:37,000
So we're going to think of, revisit our employment, unemployment dynamics model.

53
00:07:37,000 --> 00:07:46,000
And we're going to see, we're going to consider as, this isn't just one worker, but there's a large number of workers.

54
00:07:46,000 --> 00:07:52,000
And they're identical from the point of view of an outsider, of course they're all individuals.

55
00:07:52,000 --> 00:08:04,000
But they all space the same probabilities, the alphabet of probabilities of moving from, of moving from unemployment to employment.

56
00:08:04,000 --> 00:08:09,000
And the probability, data of moving from employment to unemployment.

57
00:08:09,000 --> 00:08:14,000
And they each, we have their lives like that.

58
00:08:14,000 --> 00:08:23,000
So now what we're going to let, we're going to reinterpret the model as saying, well, there's very, very many of these people.

59
00:08:23,000 --> 00:08:28,000
You could think of an infinite number, invisible.

60
00:08:28,000 --> 00:08:39,000
And we're going to let's side be the current cross-sectional distribution over 0, 1, which are our unemployed and employed states.

61
00:08:39,000 --> 00:08:50,000
So the cross-sectional distribution records the fractions of workers who are employed and unemployed at any given moment.

62
00:08:50,000 --> 00:08:58,000
And this fraction, so size 0, that's the unemployment rate.

63
00:08:58,000 --> 00:09:14,000
And what we want to know now is, and we're going to spend more time talking about this, because this relates to things that people in all countries are interested in at all times,

64
00:09:14,000 --> 00:09:20,000
but especially during periods of big recessions or big booms.

65
00:09:20,000 --> 00:09:27,000
So if you want to know, what do you think the cross-sectional distribution will be 10 periods from now?

66
00:09:27,000 --> 00:09:31,000
The whole cross-sectional distribution.

67
00:09:31,000 --> 00:09:37,000
Well, that means, you know, what do you think the unemployment rate and the employment rate is going to be 10 periods from now?

68
00:09:37,000 --> 00:09:48,000
Well, that answers just sigh, Peter the 10th, where peas are stochastic matrix.

69
00:09:48,000 --> 00:09:58,000
So this section, I want you to simple, but also an adeep.

70
00:09:58,000 --> 00:10:16,000
Okay. So when the sample is really large, just where I said, outcomes frequencies, outcomes, this is frequencies, relative frequencies, relative frequencies, and probabilities are roughly equal.

71
00:10:16,000 --> 00:10:21,000
And this is our law of large numbers that we talked about before.

72
00:10:21,000 --> 00:10:39,000
So for a very large population, this is going to represent the fraction of workers in each state.

73
00:10:39,000 --> 00:10:47,000
Okay. I promised you that I would talk a little bit about these two key concepts, irreducibility and apioreadicity.

74
00:10:47,000 --> 00:11:02,000
And the way to think about these, these are sufficient conditions that we'd like to have.

75
00:11:02,000 --> 00:11:20,000
Why? Because they, they make, they deliver, they imply very nice properties of a Markov chain.

76
00:11:20,000 --> 00:11:26,000
And these things both modify their properties of P.

77
00:11:26,000 --> 00:11:34,000
So I'll ask you to read this, but I'll tell you what irreducibility means.

78
00:11:34,000 --> 00:11:41,000
It would be a piece going to be a fixed, stochastic matrix. It's just fixed over time.

79
00:11:41,000 --> 00:11:48,000
And we say that two states communicate with each other.

80
00:11:48,000 --> 00:11:57,000
So if there's some positive integer such that P.J.

81
00:11:57,000 --> 00:12:07,000
There's some positive integers J and K such that in J steps you go from X to Y.

82
00:12:07,000 --> 00:12:12,000
And in K steps you can go, there's it's possible for you to go from Y to X.

83
00:12:12,000 --> 00:12:17,000
So somehow if there's a state here over here in the state, why?

84
00:12:17,000 --> 00:12:24,000
Somehow with enough steps there's a positive probability getting here and there's a positive probability getting there.

85
00:12:24,000 --> 00:12:31,000
So they quote unquote, communicate. That's what this means.

86
00:12:31,000 --> 00:12:40,000
And that's what communicate means. This stochastic matrix is irreducible if all states communicate.

87
00:12:40,000 --> 00:12:49,000
In other words, you can get from any state to any other state if you wait long enough.

88
00:12:49,000 --> 00:12:55,000
That's what we mean about irreducible. And we often assume irreducibility.

89
00:12:55,000 --> 00:13:04,000
We often assume it because it gives very nice properties. And it's often true.

90
00:13:04,000 --> 00:13:11,000
So here's one that labor economists might talk about.

91
00:13:11,000 --> 00:13:17,000
So if you stare at this, you could see if it's irreducible.

92
00:13:17,000 --> 00:13:26,000
Check that. Is this irreducible? In other words, could you if you're rich, could you ever be poor?

93
00:13:26,000 --> 00:13:33,000
And if you're poor, could you ever be rich?

94
00:13:34,000 --> 00:13:40,000
You could talk about that. So here's an example.

95
00:13:40,000 --> 00:13:49,000
And of this. And you could check. This is irreducible.

96
00:13:49,000 --> 00:13:54,000
You could check whether that's true.

97
00:13:54,000 --> 00:13:58,000
Actually, how could you check? I have quarantine con.

98
00:13:58,000 --> 00:14:05,000
And I have the Markov chain. I say, I check. Right there.

99
00:14:05,000 --> 00:14:10,000
You have a chain. A really big chain. And you want to check whether it's irreducible.

100
00:14:10,000 --> 00:14:24,000
There's your secret weapon. Just by, by creating the chain. These are your two Python commands.

101
00:14:24,000 --> 00:14:38,000
And here's another chain. Another concept is called apiriodicity.

102
00:14:38,000 --> 00:14:45,000
So a chain is apiriotic if it's not periodic.

103
00:14:45,000 --> 00:14:49,000
So this is something that we also want.

104
00:14:49,000 --> 00:14:56,000
So here's a periodic chain. This is periodic.

105
00:14:56,000 --> 00:15:02,000
And this chain, you notice if you're at C, you go to A, you go to B.

106
00:15:02,000 --> 00:15:10,000
You just keep going. You just cycle across these three for sure.

107
00:15:10,000 --> 00:15:16,000
So here's this chain. That chain has period three.

108
00:15:16,000 --> 00:15:26,000
And how could you figure out the chain of a periodic? You do these Python commands.

109
00:15:26,000 --> 00:15:38,000
Okay. So here's a definition of apiriotic.

110
00:15:38,000 --> 00:15:44,000
So that's a notion. So there's two things people who deal with Markov chain's often check.

111
00:15:44,000 --> 00:15:51,000
You reduce the ability. This is what they want. And apiriotic.

112
00:15:51,000 --> 00:15:58,000
And if both those are true, some very interesting things and nice things are going to happen.

113
00:15:58,000 --> 00:16:04,000
If they're true, if there's true, I'm going to let you read about it.

114
00:16:04,000 --> 00:16:09,000
Something is nice about the stationary distributions, about the existence.

115
00:16:09,000 --> 00:16:17,000
And something is nice about convergence to stationary distributions.

116
00:16:17,000 --> 00:16:21,000
So here's our definition of a stationary distribution.

117
00:16:21,000 --> 00:16:26,000
I'm repeating that again.

118
00:16:26,000 --> 00:16:36,000
I see the read this. Lots of this is repeating over and over again because what I think you've learned by repeating.

119
00:16:36,000 --> 00:16:43,000
So stationary distributions, sweet this, have a natural interpretation as the castic steady states.

120
00:16:43,000 --> 00:16:47,000
Because you just stay there. You don't stay in a particular state.

121
00:16:47,000 --> 00:16:56,000
But you stay in the same distributions over states.

122
00:16:56,000 --> 00:17:05,000
So here's kind of a theorem. It is a theorem. Every state, every stochastic matrix has at least one stationary distribution.

123
00:17:05,000 --> 00:17:10,000
I'm not going to ask you to prove that.

124
00:17:10,000 --> 00:17:14,000
Now here's the key thing that we are after this one.

125
00:17:14,000 --> 00:17:27,000
This is our reward. If apiriotic and irreducible, then ap has only one stationary distribution.

126
00:17:27,000 --> 00:17:33,000
There's only one. That's a uniqueness.

127
00:17:33,000 --> 00:17:38,000
And for any stationary distribution, you'll eventually converge to the stationary distribution.

128
00:17:38,000 --> 00:17:46,000
You'll matter where you start, you'll eventually converge to the stationary distribution.

129
00:17:46,000 --> 00:17:58,000
Okay? And so, again, we have apirioticity and irreducibility.

130
00:17:58,000 --> 00:18:03,000
You know what goes are. Then we know there's a unique stationary distribution.

131
00:18:03,000 --> 00:18:08,000
And you'll converge this there no matter where you start.

132
00:18:08,000 --> 00:18:24,000
And this thing is often called, such a distribution is called, such a, such a chain is called uniformly ergodic.

133
00:18:24,000 --> 00:18:33,000
And now here's a check, easy check. If every element of p is strictly positive, then you're done.

134
00:18:33,000 --> 00:18:42,000
It's irreducible and apiriotic. This is good.

135
00:18:42,000 --> 00:18:53,000
So now, let's go back to our example, which is going to be the unemployment, the unemployment,

136
00:18:53,000 --> 00:18:59,000
unemployment model.

137
00:18:59,000 --> 00:19:04,000
Looks for a given worker, and that we had above. And we have two parameters.

138
00:19:04,000 --> 00:19:12,000
We have unemployment to employment. That was characterized by a probability, a transition probability alpha.

139
00:19:12,000 --> 00:19:17,000
Employment to unemployment, that was our beta. So we have these two parameters.

140
00:19:17,000 --> 00:19:23,000
And we actually have our meat, it's beautiful. We have our meat waiting time.

141
00:19:23,000 --> 00:19:32,000
So that's our average length of unemployment. And that's our average length of employment.

142
00:19:32,000 --> 00:19:45,000
So let's take a, let's make a guess that, okay. So, so we know our chain, our chain is,

143
00:19:45,000 --> 00:19:58,000
let's get this right. Hopefully I did that right. Our chain is,

144
00:19:58,000 --> 00:20:04,000
is irreducible and apirotic. We could just check that every element's that.

145
00:20:04,000 --> 00:20:17,000
So if we check, we check this out. We take this and this.

146
00:20:17,000 --> 00:20:23,000
We could actually solve for the, we can solve for the,

147
00:20:23,000 --> 00:20:29,000
stationary distribution. This is the stationary distribution.

148
00:20:29,000 --> 00:20:37,000
We spend a fraction of our time p being unemployed, fraction of time 1 minus p being employed.

149
00:20:37,000 --> 00:20:49,000
And there's the, there's the, there's a form of a for p. And you could see, you could see,

150
00:20:49,000 --> 00:21:04,000
how beta and alpha both influence this. And this makes sense.

151
00:21:04,000 --> 00:21:17,000
Okay, and then you could also check is, you could also check is, you could use eigenvalues and eigenvectors to compute the same thing.

152
00:21:17,000 --> 00:21:27,000
So, you know, if you, if you want to, you could use simpy and calculate eigenvectors and you,

153
00:21:27,000 --> 00:21:40,000
and eigenvalues and you would, you would get this. You can verify that in various ways.

154
00:21:40,000 --> 00:21:45,000
I'll actually read this on your own. How to calculate a stationary distribution.

155
00:21:45,000 --> 00:21:53,000
And some of you may be interested in, in parts of econometrics,

156
00:21:53,000 --> 00:21:59,000
basically Bayesian econometrics later. This calculating stationary distributions is a big part of that,

157
00:21:59,000 --> 00:22:09,000
because it's, it's used in Bayesian statistics a lot. So, I'm going to skip over this section.

158
00:22:09,000 --> 00:22:25,000
And, because we're going to come back and visit both this section and this section in, in a later, in later lectures.

