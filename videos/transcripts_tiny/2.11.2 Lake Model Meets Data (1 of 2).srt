1
00:00:00,000 --> 00:00:06,840
Hello, this is Spencer Lyon and in this lecture we're going to continue talking about the

2
00:00:06,840 --> 00:00:13,080
Lake Model of unemployment dynamics and we're going to seek to understand how we can

3
00:00:13,080 --> 00:00:20,520
use the model and tie its parameters to US data so that we can have the data teach us

4
00:00:20,520 --> 00:00:25,160
something about the models parameters.

5
00:00:25,160 --> 00:00:29,960
Before we dive in on the Lake Model we want to remind ourselves of some of the higher level

6
00:00:29,960 --> 00:00:31,960
concepts we've been working with.

7
00:00:31,960 --> 00:00:37,280
In particular we want to review some of the standard notation and language that we're

8
00:00:37,280 --> 00:00:43,220
using so that we can keep a perspective of how this particular example fits into the

9
00:00:43,220 --> 00:00:49,680
larger framework that we've been building throughout this course.

10
00:00:49,680 --> 00:00:58,240
So first we have the notion of a statistical model which is a joint probability density

11
00:00:58,240 --> 00:01:10,360
F over some random variable some data Y and some parameters theta.

12
00:01:10,360 --> 00:01:15,640
Now with this statistical model we might think about solving two distinct but related

13
00:01:15,640 --> 00:01:18,920
problems.

14
00:01:18,920 --> 00:01:25,360
The first problem is called the direct problem and the direct problem is to draw a random

15
00:01:25,360 --> 00:01:33,920
example Y and notice here our notation we use the lower case little Y to mean a draw

16
00:01:33,920 --> 00:01:41,960
from our model F and the upper case big Y to represent the data.

17
00:01:41,960 --> 00:01:47,680
Now again the direct problem is to draw a sample Y from the statistical model represented

18
00:01:47,680 --> 00:01:57,000
by F for some assumed or fixed value of the parameter vector theta.

19
00:01:57,000 --> 00:02:02,760
On the other hand the inverse problem will be to take a set of data which we'll call Y

20
00:02:02,760 --> 00:02:10,360
tilde and we're going to make an assumption or an assertion that this data was in fact

21
00:02:10,360 --> 00:02:19,940
generated from the model F. We will then use this data and the assumption about it being

22
00:02:19,940 --> 00:02:27,520
drawn from F and F's functional form and we'll use that combination in order to make some

23
00:02:27,520 --> 00:02:37,160
inferences or to learn something about an unknown parameter vector theta.

24
00:02:37,160 --> 00:02:44,360
But in slightly different terms the direct problem is to construct a simulation of our

25
00:02:44,360 --> 00:02:52,760
model F for a given parameter vector theta. Other names for this simulated data set again

26
00:02:52,760 --> 00:03:03,040
noted a little lower case Y might be artificial data or fake data.

27
00:03:03,040 --> 00:03:09,720
The inverse problem is also called the parameter estimation problem. This name comes from

28
00:03:09,720 --> 00:03:16,280
the fact that when we're solving the inverse problem we are taking some observed data,

29
00:03:16,280 --> 00:03:22,960
assuming that it was generated from F and then trying to learn something about our estimate

30
00:03:22,960 --> 00:03:29,920
values of our parameter vector theta. A third way to think about this is that the direct

31
00:03:29,920 --> 00:03:39,280
problem takes parameters theta as inputs and then it generates artificial or fake data as outputs.

32
00:03:39,280 --> 00:03:46,440
The inverse problem takes data, often real world data as its input, often we call this the

33
00:03:46,440 --> 00:03:54,240
observations or the observed data and with these as an input it will then generate statements

34
00:03:54,240 --> 00:04:01,680
or inferences about the parameters theta as the output. The inverse problem consumes data

35
00:04:01,680 --> 00:04:11,480
and returns something about our parameters as an output. One key thread underlying both of

36
00:04:11,480 --> 00:04:19,760
these model these problems are that they use the same statistical model F of a given data.

37
00:04:20,720 --> 00:04:26,760
There are some tools that allow us to solve the direct problem which again relates to simulating

38
00:04:26,760 --> 00:04:33,840
artificial data and these tools in fact turn out to be helpful when solving the inverse problem

39
00:04:33,840 --> 00:04:42,640
or learning something about the parameter vector theta. We're going to be continuing in this

40
00:04:42,640 --> 00:04:49,120
general framework of a statistical model the direct problem in the inverse problem as we talk

41
00:04:49,200 --> 00:04:54,640
today about the lake model of unemployment that we've been working with and we want to make sure

42
00:04:54,640 --> 00:05:01,920
that as we work through the materials in this lecture that you keep this overall framework in mind

43
00:05:03,360 --> 00:05:10,480
and our goal today will be to use the lake model and its parameters and inferences we can make

44
00:05:10,480 --> 00:05:18,240
about its parameters given real world data from the BLS and it will help us understand

45
00:05:19,120 --> 00:05:27,200
the unemployment and employment rates over time and how they may change in various settings or in

46
00:05:27,200 --> 00:05:38,640
various economic climates. This first code cell here will just import some of the tools we'll be using.

47
00:05:38,640 --> 00:05:44,160
We're going to import pandas, numpy, and map plotlib. We'll just going to apply some style to our

48
00:05:44,480 --> 00:05:50,640
figures and then we also import the built-in random library so that we can set the random seed.

49
00:05:50,640 --> 00:05:55,680
And we do this just so that when we come back and we execute this code or this notebook in the

50
00:05:55,680 --> 00:06:01,360
future we'll be able to see the same outputs and our work will be reproducible.

51
00:06:06,000 --> 00:06:13,200
So let's review some facts that we've observed about the data. In particular we want to look at the

52
00:06:13,200 --> 00:06:20,240
unemployment rate in the United States in the past two recessions. We obtained data from the

53
00:06:20,240 --> 00:06:28,000
Bureau of Labor Statistics or BLS and as noted in previous lectures we saw a large shift in the

54
00:06:28,000 --> 00:06:34,720
employment and unemployment numbers in the United States in the beginning of the year 2020.

55
00:06:35,600 --> 00:06:42,080
And let's remind ourselves what these data look like. We have prepared a CSV file called BLS

56
00:06:42,080 --> 00:06:48,320
Reset Recessions Data that will read into a data frame using pandas read CSV function.

57
00:06:49,840 --> 00:06:55,280
We'll then construct a few extra columns. So we have as columns of this data frame

58
00:06:56,560 --> 00:07:00,320
a column named Employed which represents the total number of persons

59
00:07:01,200 --> 00:07:07,040
that were employed when the survey, when the underlying survey was given as well as the total number

60
00:07:07,040 --> 00:07:12,320
of persons that were unemployed at the time the survey was administered. If we sum these two

61
00:07:12,320 --> 00:07:19,600
up we'll call this the Labor Supply. Then with this Labor Supply in hand we're just going to

62
00:07:19,600 --> 00:07:26,000
compute the fraction of workers that participated in the survey that were employed at the time

63
00:07:26,000 --> 00:07:33,280
of the survey and then unemployed. So we'll go ahead we'll import the data, create these in

64
00:07:33,280 --> 00:07:40,160
columns and we'll take a look at the first few rows. So the columns of note that we should

65
00:07:40,800 --> 00:07:47,280
point out here there's a column called Recession. This takes on two values one is GR which is

66
00:07:47,280 --> 00:07:53,520
short hand for the great recession. There's also another value in this column called COVID

67
00:07:54,480 --> 00:08:05,360
which again represents the early 2020 months when there was a large shift in labor market dynamics

68
00:08:05,360 --> 00:08:12,160
due to the COVID-19 pandemic. We then see the employed and unemployed columns talked about before

69
00:08:12,160 --> 00:08:18,000
as well as our new columns, Labor Supply employed percent and unemployed percent over here on the right.

70
00:08:18,640 --> 00:08:25,040
We'll talk about some of these other columns, EEE and UU and a little bit and the other thing to

71
00:08:25,040 --> 00:08:31,440
keep in mind is there is this column called Months from and what this represents is it's always

72
00:08:31,440 --> 00:08:40,560
an integer and it represents the number of months from the peak of unemployment associated with

73
00:08:40,560 --> 00:08:48,800
the given recession. So this first row corresponds to the great recession and Months from has a value

74
00:08:48,800 --> 00:08:56,160
of minus 35. So this would be 35 months prior to the peak unemployment observed during the

75
00:08:56,160 --> 00:09:02,880
great recession. So this is about three years before the peak unemployment. We'll actually be able

76
00:09:02,880 --> 00:09:09,680
to visualize this data here on the next page. So what we have here, the function and we'll go back

77
00:09:09,680 --> 00:09:15,200
and look at it in a moment but this is the type of, this is the plot generated by our function.

78
00:09:16,000 --> 00:09:24,000
So we see here we have two subvellers in our graph. The first subplot up at the top shows the

79
00:09:24,000 --> 00:09:36,640
percent of employed workers given number of months before and after the the trial for the low point

80
00:09:36,640 --> 00:09:43,280
in the labor market during the recession. So you'll see here that the blue line that lives up here

81
00:09:43,280 --> 00:09:50,800
and then has the large spike downward represents the COVID era unemployment dynamics and the red line

82
00:09:50,800 --> 00:10:00,560
that is more gradual represents the great recession era employment dynamics. This top subplot

83
00:10:00,560 --> 00:10:06,560
would represent the share of persons that are employed and it's why access ranges from about 85

84
00:10:06,560 --> 00:10:15,200
percent to about 96 percent. The bottoms of graph would just be exactly one minus the top one,

85
00:10:15,920 --> 00:10:20,480
which is why you see kind of the mirror pattern down here on the bottom. It's almost as if it was

86
00:10:20,480 --> 00:10:28,080
reflected about this space in between the two charts. Now let's look at the code that generated this

87
00:10:28,080 --> 00:10:33,760
just so we have a sense of what it does. So what you can read the doc string and we encourage you

88
00:10:33,760 --> 00:10:40,480
to take a look at this on your own time. But we'll talk through the high point right now. So this function

89
00:10:40,480 --> 00:10:48,960
will consume a data frame and then what we will do is it will construct a 2 by 1 figure of

90
00:10:48,960 --> 00:10:56,880
subplots and it will plot both the employed percent and unemployed percent. The employed percent

91
00:10:56,880 --> 00:11:01,680
columns get put on the first subplot and the unemployed percent get put on the bottoms of flat.

92
00:11:02,640 --> 00:11:09,920
Then what we do is we will group by the recession column of the data frame and we will plot this

93
00:11:09,920 --> 00:11:16,080
group so this would contain only data for a single instance of that recession column,

94
00:11:17,120 --> 00:11:22,960
or a single value of the recession column. And we'll plot on the horizontal axis the months from

95
00:11:22,960 --> 00:11:30,480
column, which you see down here and then why is going to be either employed percent or unemployed

96
00:11:31,440 --> 00:11:35,760
and then we do a little bit of work to kind of clean up the look for the chart and we return it.

97
00:11:36,800 --> 00:11:41,920
And what this function will allow us to do is if we happen to have say three different

98
00:11:42,880 --> 00:11:49,600
groups in the data frame of the recession column we would see three lines on each of these charts.

99
00:11:49,600 --> 00:11:54,960
And we'll use this throughout the lecture today as we're going to add a third set of

100
00:11:55,840 --> 00:12:01,200
rows to a data frame which will represent the model output and we'll see that here shortly.

101
00:12:05,280 --> 00:12:09,280
So we just had a reminder and one thing actually let's go back to this

102
00:12:10,320 --> 00:12:19,200
to one more look. So we noticed a couple patterns here first before the the low point of the current

103
00:12:19,200 --> 00:12:26,400
COVID related unemployment. Things where I are fairly steady or fairly smooth there was roughly

104
00:12:26,400 --> 00:12:33,040
4% unemployment and then very quickly within the matter of one period there is a sharp decline

105
00:12:33,040 --> 00:12:39,920
in employment all the way down to about 85% and then a rather quick recovery or at least that's

106
00:12:39,920 --> 00:12:48,800
what the data looks like up until the end of the data set. In contrast the great recession

107
00:12:49,360 --> 00:12:58,400
looks quite a bit different. So the three years prior to the low point of employment during the

108
00:12:58,400 --> 00:13:06,800
greater session did not see a steady value like we did in the COVID era but rather a slow decline

109
00:13:06,800 --> 00:13:14,960
and slow loss of jobs that finally culminated right here at the worst point or the lowest point

110
00:13:15,040 --> 00:13:22,400
of the employed percent and then a very slow and gradual ascension in the number of

111
00:13:22,400 --> 00:13:27,360
in the percentage of employed workers representing a slow or gradual recovery.

112
00:13:28,880 --> 00:13:35,440
Now throughout the lecture we will use the lake model to capture kind of both of these dynamics

113
00:13:35,440 --> 00:13:43,680
but keep in mind that the great recession and the COVID era data from the BLS they show a very

114
00:13:43,680 --> 00:13:52,000
different pattern from one another. Now that we've reminded ourselves with the data look like

115
00:13:52,000 --> 00:13:58,880
let's remind ourselves what the model is that we've been working with. So the lake model in each

116
00:13:58,880 --> 00:14:05,600
period has two parameters. First parameter we've called alpha and this is the probability of an

117
00:14:05,600 --> 00:14:11,520
unemployed worker finding a job. We're going to refer to this as the job finding rate.

118
00:14:12,160 --> 00:14:19,280
The second parameter is beta. This is the probability of an employed worker losing their job.

119
00:14:20,000 --> 00:14:24,640
This is going to be called the job separation rate in today's lecture.

120
00:14:26,480 --> 00:14:32,880
So these are the two parameters. This would be the container divector theta from our

121
00:14:33,520 --> 00:14:40,320
description at the beginning of this lecture. There's also one state variable. We've been calling it S

122
00:14:40,720 --> 00:14:47,520
and S is a two by one vector of percentages of unemployed and employed workers respectively.

123
00:14:47,520 --> 00:14:56,720
So these are two numbers between zero and one that sum to one. One thing to point out is that if

124
00:14:56,720 --> 00:15:03,520
the values of alpha and beta remain constant is not possible for our model to generate

125
00:15:03,520 --> 00:15:11,360
sudden and large changes in the unemployment rate like those showcased in the COVID era

126
00:15:12,000 --> 00:15:17,840
BLS data. Keep this in mind it's something we're going to try to address throughout our lecture today.

127
00:15:21,280 --> 00:15:26,160
So let's take a quick look. We took some of the routines from a former lecture and we put them in a

128
00:15:26,160 --> 00:15:32,320
standalone pipeline file called v1 employmentmodel.py. We're going to import these functions

129
00:15:32,960 --> 00:15:38,880
under the alias emp and we're also going to keep numpy around as we'll be using that throughout our

130
00:15:39,520 --> 00:15:48,880
code today. What we'll do is we'll first create a new Python class that we'll use throughout our

131
00:15:48,880 --> 00:15:56,480
lesson today. And the purpose for this is that the class will allow us to keep track of the model

132
00:15:56,560 --> 00:16:03,040
parameters that alpha and the beta. We'll also have routines for simulating a panel of workers

133
00:16:03,760 --> 00:16:11,120
and it will help us combine the simulated data with the data from the BLS so that we can plot results

134
00:16:11,120 --> 00:16:19,600
and compute statistics from our simulation. Now the Python class it looks quite long but most of the

135
00:16:19,600 --> 00:16:25,360
lines are actually documentation and comments so they can be helpful as yourself studying after

136
00:16:25,360 --> 00:16:31,680
this lecture's over. We'll review each of the functions each of the methods that make up the class

137
00:16:31,680 --> 00:16:36,560
and it should be clear once we're finished or I walk through what the class does.

138
00:16:38,240 --> 00:16:46,320
So here we go. The class is going to be called Lake Model Simulator. It's a knit method. This is

139
00:16:46,320 --> 00:16:52,720
the method that we used to create a new instance or a new version of our simulator. It has just

140
00:16:52,720 --> 00:17:00,400
two parameters. One is called N and this will just represent the number of individuals that we would

141
00:17:00,400 --> 00:17:09,040
like to include in our simulated data. The second parameter here is called S0. This will

142
00:17:09,040 --> 00:17:16,480
represent the initial or starting steady state or starting state value for beginning our simulations.

143
00:17:17,440 --> 00:17:23,200
Here we've gone ahead and we've set default values for both of these parameters. By default,

144
00:17:23,200 --> 00:17:31,120
our simulator will simulate 5,000 workers at every time step and it will set the initial state

145
00:17:31,840 --> 00:17:39,200
equal to about 2.3% unemployment and then about 97.6% unemployment.

146
00:17:40,160 --> 00:17:48,000
Sorry, 97.6% employment. We'll go ahead and all the admit method does in this case is it just

147
00:17:48,000 --> 00:17:55,680
stores these values for later. We then have a block of code right here that defines what's called

148
00:17:55,680 --> 00:18:04,720
a property. So these parameters here, the N as well as the underscore S0, our call of properties.

149
00:18:04,880 --> 00:18:12,320
In our case, we wanted to put a little bit more structure around how the property S0 without the

150
00:18:12,320 --> 00:18:19,440
leading underscore would work. By default, when we try to get S0, meaning when we try to type

151
00:18:19,440 --> 00:18:25,920
the lakemodel dot S underscore zero, what's going to happen is it's going to actually run this

152
00:18:25,920 --> 00:18:32,320
function and all we're going to be doing is returning the current value of underscore S0.

153
00:18:34,880 --> 00:18:40,560
This is the same as the behavior we would get from here from just using,

154
00:18:41,200 --> 00:18:46,800
for if we are trying to access N. So we're not really leveraging the fact that this is a property

155
00:18:46,800 --> 00:18:53,360
with custom behavior. The custom behavior comes in the second half of the property, which is

156
00:18:53,360 --> 00:19:01,600
called the set method or the setter. Now the set method, it needs to take in a new proposal for

157
00:19:01,600 --> 00:19:08,320
the initial state. And the reason we're defining this as a custom property is because we want

158
00:19:08,320 --> 00:19:15,520
to make sure that whatever anybody passes in is an array that has two values and we want to also

159
00:19:15,520 --> 00:19:24,640
assert that they sum to one. This just allows us to make sure that whatever the user of our class

160
00:19:24,640 --> 00:19:31,840
wants to change the starting state before a new simulation, they set it properly by having a

161
00:19:31,840 --> 00:19:40,320
two element vector that sums to one. If those two both paths, we will then just set this

162
00:19:40,320 --> 00:19:49,760
underscore S0 property to the array that was passed in and then whenever somebody uses LM dot S0,

163
00:19:50,720 --> 00:19:56,560
this will be returned to them via our get function above. So once we have these defined,

164
00:19:56,560 --> 00:20:03,200
we just need to connect them using the special property function built into Python. So this code

165
00:20:03,200 --> 00:20:10,640
right here allows us to have custom behavior for setting the S0 property. If I didn't have a

166
00:20:10,640 --> 00:20:17,200
two element array or it didn't sum to one, this attempt to set a new value for S0 would be rejected.

167
00:20:17,280 --> 00:20:25,040
This is just a showcase. One of the features that Python affords us to make sure that we don't

168
00:20:25,040 --> 00:20:38,480
have an inconsistent version of our model. Okay, the next method will be to combine with the data

169
00:20:38,480 --> 00:20:46,080
from the BLS. So this takes two parameters. One, we've called DFSM, which is short for the data frame

170
00:20:46,080 --> 00:20:53,520
that we obtain via simulation. And then the second one is called DFBLS. We've set a default value for

171
00:20:53,520 --> 00:21:00,560
this to the DFBLS. We imported above from the CSV file and used to construct our plots.

172
00:21:02,800 --> 00:21:08,080
Now this, the main part of this function is just to concatenate these two and stack them on

173
00:21:08,080 --> 00:21:15,040
top of each other. However, there's one thing we need to do, which is right here. We need to adjust

174
00:21:15,200 --> 00:21:25,120
the month from column to begin at negative 35. So by default, our simulation routines will set the

175
00:21:26,320 --> 00:21:32,080
lowest value of the month from column to zero. It will just simulate from time zero, forward.

176
00:21:32,720 --> 00:21:38,480
And in order for everything to line up properly with the data obtained from the BLS and how we've

177
00:21:38,560 --> 00:21:45,760
ordered it, we need to subtract 35 so that our simulation also begins at negative 35.

178
00:21:46,560 --> 00:21:50,160
So once we've done that, we'll just stack them together and then return.

179
00:21:54,560 --> 00:22:01,680
Next, we have a method called simulate panel. So here, this is going to be kind of a

180
00:22:01,680 --> 00:22:08,240
wrapper function around the simulate employment cross section method that we saw in our last

181
00:22:08,240 --> 00:22:16,240
lecture together. Our last lecture on the lake model. The two parameters that are consumed by this

182
00:22:16,240 --> 00:22:23,520
method are alpha and beta. These are going to be arrays that contain the value of alpha and beta

183
00:22:23,680 --> 00:22:31,200
for every time step of the simulation. So they will have lake the T and they will be passed

184
00:22:32,080 --> 00:22:41,600
into the simulate method from the employment file. We're going to then pass in the S0 that's

185
00:22:41,600 --> 00:22:48,000
part of the class right now as the initial state as well as the number of individuals that's also

186
00:22:49,280 --> 00:22:57,040
a property of our class. This will be an N by T array filled with entirely the number 0 and 1.

187
00:22:57,040 --> 00:23:03,520
We're 0 indicates a worker being unemployed in that period and a 1 indicates a worker being employed.

188
00:23:05,360 --> 00:23:13,280
Well then extract the shape of the number of columns which are present this number T so that we can

189
00:23:13,280 --> 00:23:18,400
use it again now below. Then once we have all these workers, we'll simply sum

190
00:23:19,040 --> 00:23:25,760
over all the N in each period to get a total number of workers that were employed.

191
00:23:28,000 --> 00:23:33,840
Finally the last thing this does is it puts it in a data frame and this data frame will have four

192
00:23:33,840 --> 00:23:41,520
columns. The first is month from and as mentioned before this will just start at 0 and increment

193
00:23:41,520 --> 00:23:46,720
by 1 in each row and it will end at T minus 1. So it has T elements.

194
00:23:48,480 --> 00:23:55,120
Then we have a column called employed percent and importantly this is the same column name

195
00:23:55,120 --> 00:24:01,120
that showed up in the BLS data frame from above and this will be the number of employees. There's

196
00:24:01,120 --> 00:24:05,280
the number of workers that were employed every period which we computed above

197
00:24:05,360 --> 00:24:13,040
divided by the total number of employees in our panel. There's a total number of workers in our

198
00:24:13,040 --> 00:24:20,000
panel and finally we'll also compute the unemployed percent which is going to be 1 minus that

199
00:24:25,120 --> 00:24:31,040
and we will set the recession column to the constant model so that when we use our plotting

200
00:24:31,040 --> 00:24:38,560
routine to find a above we will have a legend entry that reads model. This will be kind of the

201
00:24:38,560 --> 00:24:48,000
workhorse of our our work today and this solves the direct problem. So notice that as an input

202
00:24:48,880 --> 00:24:56,960
to this function or this method we receive parameters. This is this alpha and beta would be like

203
00:24:56,960 --> 00:25:03,680
the the parameter vector theta that we spoke about at the beginning and the output of this method

204
00:25:04,400 --> 00:25:15,440
is a simulation from the model f. The model f is encapsulated in how we use the parameters

205
00:25:15,440 --> 00:25:25,280
alpha and beta in order to produce a simulation of the data. So our core simulation routine here

206
00:25:25,440 --> 00:25:34,560
allows us to solve the direct problem. We then have one more helper method in this class where

207
00:25:36,400 --> 00:25:47,520
we are labeling an easy routine for simulating a panel of workers where alpha and beta are fixed

208
00:25:47,520 --> 00:25:53,280
at their time zero values and so all we'll do here is we'll construct a vector of alpha

209
00:25:53,280 --> 00:25:58,640
which will be the constant alpha zero of vector of beta which is the constant beta zero

210
00:25:58,640 --> 00:26:04,400
and we'll use the function or the method to find above to run a simulation and return a data frame.

211
00:26:06,240 --> 00:26:12,080
So we'll evaluate the cell to define our function and we'll be ready to start using that here shortly.

212
00:26:12,880 --> 00:26:27,120
Okay we're now ready to put our new class and its simulation routines to work. So to begin we will

213
00:26:27,120 --> 00:26:36,320
first simulate a a quote normal time or non-recession version of the data and we will refer to these

214
00:26:36,400 --> 00:26:45,840
parameters as alpha and beta but with a bar over it. So alpha bar is 0.37 which represents a

215
00:26:45,840 --> 00:26:55,840
job finding rate for the unemployed of about 37%. Meanwhile the normal time or steady state version

216
00:26:55,840 --> 00:27:03,200
of the beta parameter or the job separation rate is about 1%. And again we're going to refer to

217
00:27:03,200 --> 00:27:09,760
these parameters with the bar over them as the steady state parameters and we'll talk soon about

218
00:27:09,760 --> 00:27:17,440
why we we choose this term steady state and we'll remind you that these values were computed using

219
00:27:17,440 --> 00:27:24,240
data from the CPS in an earlier lecture. If you remember our simulation routine also required that we

220
00:27:24,240 --> 00:27:32,480
pass a value for the initial state of employed and unemployed workers will refer to this as

221
00:27:32,480 --> 00:27:43,120
S bar and we'll note that it has about 2.37% of unemployed and about 97.6% of workers that were employed.

222
00:27:43,840 --> 00:27:50,000
Again these were also previously estimated using the CPS data. So we'll define these three

223
00:27:50,000 --> 00:27:57,280
constants right here and then in the next cell we will use them to simulate we use them with our

224
00:27:57,280 --> 00:28:08,960
simulator to produce a chart that includes the data from the BLS as well as our simulated data.

225
00:28:08,960 --> 00:28:16,160
So these blue and red lines are the same that we saw before. They represent the COVID era in blue

226
00:28:16,960 --> 00:28:25,840
and the great recession era in red. But now we have a new yellow line that represents kind of the

227
00:28:25,840 --> 00:28:36,400
the model generated data using the steady state parameters. So notice that we passed in an unemployment

228
00:28:36,400 --> 00:28:45,120
rate, a starting unemployment rate of about 2.3% which is right here in the yellow line and because

229
00:28:45,120 --> 00:28:51,200
we were in steady state the model stayed roughly at that place. We didn't see large we didn't see

230
00:28:51,280 --> 00:28:59,040
gradual decline in employment or increase and we didn't see a large spike as we do in these other

231
00:28:59,040 --> 00:29:07,680
two lines. This is one of the reasons we call the set of parameters the steady state parameters

232
00:29:08,080 --> 00:29:20,320
because it's a a place of aggregate rest for the model. So visually we can see that this steady

233
00:29:20,320 --> 00:29:26,480
state version of the model. It doesn't match features of either recession. So the COVID recession had that

234
00:29:26,480 --> 00:29:35,360
really sharp drop in employment which wasn't replicated in the steady state model simulation.

235
00:29:36,320 --> 00:29:45,440
And again the great recession data set had that gradual decline and then after the

236
00:29:45,440 --> 00:29:52,320
trough was reached a gradual increase in employment which again was inconsistent with the model being

237
00:29:52,320 --> 00:29:59,200
very constant and flat over time. Our goal in the rest of our lecture today will be to

238
00:30:00,720 --> 00:30:08,000
discover deviations from the model where we set alpha equal to alpha bar for our periods and beta

239
00:30:08,000 --> 00:30:14,720
equal to beta bar for our periods. There will have a lake model to do a better job of

240
00:30:14,720 --> 00:30:26,960
describing times of stress in the US labor market. In order to understand the effectiveness of our

241
00:30:26,960 --> 00:30:34,960
modeling decisions we need to talk about the concept of a last function. So a last function is a

242
00:30:34,960 --> 00:30:41,920
deterministic mathematical function that measures the difference between a model's output and its target.

243
00:30:42,320 --> 00:30:49,920
The last function is a key component to any statistical optimization routine or practice and it is heavily

244
00:30:49,920 --> 00:30:56,240
used in machine learning in order to evaluate the goodness of fit of a particular model.

245
00:30:59,280 --> 00:31:05,040
In today's lecture we're going to be using a very common last function called the mean squared

246
00:31:05,120 --> 00:31:14,240
error last function or written MSE for short. It's defined as follows given a sequence of targets

247
00:31:14,240 --> 00:31:26,960
and model outputs y and y hat. The MSE is defined as the average 1 over n of summing up the difference

248
00:31:26,960 --> 00:31:36,400
between the target and the model output squared. So we'll compute the residual or the loss for

249
00:31:36,400 --> 00:31:43,440
each observation. We'll square that loss and then we'll compute the average of all those squared

250
00:31:43,440 --> 00:31:51,760
losses. That's what is meant by the average or mean squared error. This is where it gets its name.

251
00:31:52,160 --> 00:32:01,120
For the current exercise that we're undergoing the model output will be the time series of the

252
00:32:01,120 --> 00:32:07,440
percentage of workers that are employed. The target will be the corresponding time series

253
00:32:08,320 --> 00:32:15,840
from the BLS and we will compute the last function independently for both the great recession

254
00:32:16,160 --> 00:32:24,160
error data as well as the COVID error data. We've defined here a function that takes in a

255
00:32:24,160 --> 00:32:31,280
data frame that has the same shape and features as the data frame we're plotting and it will compute

256
00:32:31,280 --> 00:32:40,000
the MSE between the model and the great recession error data and the model and the COVID error data.

257
00:32:40,960 --> 00:32:47,040
So to do this we use some of our reshaping tools we've learned. First we construct a pivot table

258
00:32:48,000 --> 00:32:55,600
by setting the index to be the months from column. Then across the columns we're going to have

259
00:32:55,600 --> 00:33:02,480
different values of recession. In this case we'll have GR for great recession as a column. COVID will be

260
00:33:02,480 --> 00:33:09,760
another column and model will be another column. The values that are referred to by this row and

261
00:33:09,760 --> 00:33:16,880
column label will just come from the employee percent column of the input data frame.

262
00:33:18,560 --> 00:33:24,720
We're going to multiply each of these by 100 to put them in percentage units and this is actually

263
00:33:24,720 --> 00:33:30,640
we'll just make the MSE bigger and a little easier to compare across models because we won't be dealing

264
00:33:30,640 --> 00:33:38,560
with very small numbers. Once we have this pivot table computing the MSE looks very similar to

265
00:33:38,560 --> 00:33:45,680
its formulaic representation. So what we'll do is for each row or each value of months from

266
00:33:46,560 --> 00:33:52,560
we will compute the difference between the model level of employed percentage and the great

267
00:33:52,560 --> 00:33:56,560
recession level of employed percentage and then we'll square that difference.

268
00:33:57,920 --> 00:34:03,760
Once we've done this for each row we'll have a pandas series after this code is evaluated

269
00:34:04,400 --> 00:34:08,880
and then we'll just compute the mean or the average of each of these squared error terms.

270
00:34:09,760 --> 00:34:14,720
We'll repeat this for both the great recession column and the COVID column.

271
00:34:15,600 --> 00:34:23,200
And the reason we used a pivot table there were two reasons one by setting the index two months from

272
00:34:23,920 --> 00:34:30,720
we're allowing pandas to align the operations here where we do model minus great recession and

273
00:34:30,720 --> 00:34:37,200
pandas will align the observations to make sure that we're subtracting a month from of 10

274
00:34:37,440 --> 00:34:40,880
in the model to a month from of 10 in the great recession.

275
00:34:43,520 --> 00:34:52,080
And the second reason was so that we could have these three versions of our data,

276
00:34:52,080 --> 00:34:56,800
great recession, model and COVID each becomes so they're easy to operate.

277
00:34:57,680 --> 00:35:04,560
Once we do that we can define the function and use it to evaluate the efficacy of our steady state.

278
00:35:05,200 --> 00:35:11,520
And just kind of keep these numbers in mind as we move forward. So the the mean squared error,

279
00:35:11,520 --> 00:35:16,160
the average squared deviation of our steady state model for our good recession data

280
00:35:17,040 --> 00:35:23,760
ended up being about 29%. Or value 29%age points. Remember that was the units we chose.

281
00:35:24,160 --> 00:35:29,360
So the mean squared error was about 29 and for the COVID data it was about 12.

282
00:35:31,200 --> 00:35:35,360
So just kind of keep these numbers in the back your head as these are going to be our benchmark

283
00:35:36,320 --> 00:35:39,040
for evaluating one model versus another one.

284
00:35:42,000 --> 00:35:50,000
And in particular the use of a loss function allows us to be precise about what we mean when we say

285
00:35:50,000 --> 00:35:54,000
a model is better or has improved upon a different model.

286
00:35:55,680 --> 00:36:03,600
It allows us to move from a qualitative notion of how good a model fits the data to a very

287
00:36:03,600 --> 00:36:10,160
quantitative version. So for example instead of saying model 2 appears to do better than model 1

288
00:36:10,160 --> 00:36:17,280
that matching the spike in the COVID error unemployment. We could say that the MSC COVID from model 2

289
00:36:17,280 --> 00:36:26,000
is 5.1 relative to the value of 11.5 from model 1. So we've moved from a qualitative statement here

290
00:36:26,000 --> 00:36:31,600
to a quantitative numerical statement down below. And this will make it easier to have

291
00:36:32,640 --> 00:36:37,600
interpretable results that are completely objective.

292
00:36:37,680 --> 00:36:49,920
We'll now begin putting in a different models on the table and seeing what they can help us learn

293
00:36:49,920 --> 00:36:58,240
about unemployment data in the United States. So let's first consider the hypothesis that the spike

294
00:36:58,240 --> 00:37:06,400
observed in the COVID-19 era unemployment data was only due to a redistribution of workers in the

295
00:37:06,400 --> 00:37:12,000
initial period. But it didn't actually change the job creation or separation rates.

296
00:37:13,200 --> 00:37:20,160
In terms of our model parameters, this hypothesis would mean that alpha t is equal to its steady

297
00:37:20,160 --> 00:37:28,400
state value alpha bar for alt t. And similarly beta t is always equal to its steady state value of beta bar.

298
00:37:29,280 --> 00:37:37,520
This is what we meant by the hypothesis saying that the job creation and job separation rates

299
00:37:37,520 --> 00:37:46,320
were not changed. Then we'll say that we can begin our simulation at time t equals 0 at the steady state

300
00:37:46,640 --> 00:37:57,040
or S bar. Then we will say that s t is the shocked value of s. We'll talk about this soon

301
00:37:57,760 --> 00:38:06,000
where the subscript t or the date is just the date of the shock. Then for all tau greater than t,

302
00:38:06,960 --> 00:38:14,720
the value of the state in period tau will just be dictated by the lake model and the parameters

303
00:38:14,800 --> 00:38:25,680
alpha and beta. So we'll be using again in order to compute what the state is in period's

304
00:38:25,680 --> 00:38:33,360
greater than t will be using our solution to the direct problem by fixing the parameters alpha and beta

305
00:38:33,360 --> 00:38:38,240
and simulating our model forward to obtain the state in periods after the shock.

306
00:38:39,200 --> 00:38:44,240
We'll make one note here that for any tau sufficiently greater than t,

307
00:38:45,040 --> 00:38:51,520
you will have that the state in period tau is going to be approximately equal to the steady state state

308
00:38:51,520 --> 00:38:58,560
S bar. This is actually what is meant by a steady state. It is a place of rest according to the

309
00:38:58,560 --> 00:39:07,920
dynamics of the model. If the model is at the steady state today and the parameters remain at

310
00:39:07,920 --> 00:39:12,640
their steady state values, the model will be at the parameter to steady state again in the next period.

311
00:39:18,160 --> 00:39:23,440
We actually have all the tools we need to inspect this hypothesis visually and we'll

312
00:39:23,440 --> 00:39:30,160
lay out a few steps for how we'll do it. The first step to visually inspect this hypothesis

313
00:39:30,160 --> 00:39:37,680
would be to set the initial state equal to the steady state value. Then we will fix the parameters

314
00:39:37,680 --> 00:39:41,760
alpha and beta at their steady state values for all time periods.

315
00:39:43,040 --> 00:39:49,040
Now because we want to inspect visually and compare against the data from the BLS,

316
00:39:50,000 --> 00:39:57,360
we know how long we need to simulate before the shock. So what we'll do is starting from the steady

317
00:39:57,360 --> 00:40:03,280
state and keeping parameters fixed, we'll simulate for 35 periods. This would be from period

318
00:40:03,840 --> 00:40:13,200
minus 35 to minus 1 inclusive. Then at time t equals zero, we will set the state.

319
00:40:14,160 --> 00:40:22,080
By hand, we'll reach into the model and we'll reshuffle our workers so that there are 14% of

320
00:40:22,080 --> 00:40:27,200
our workers will now be unemployed and then the balance or 86% will be employed.

321
00:40:28,160 --> 00:40:34,640
This is approximately the value that was observed in the spring of 2020 because of COVID-19.

322
00:40:36,640 --> 00:40:42,000
Then we can from that point forward simulate for the other 36 periods that

323
00:40:43,040 --> 00:40:48,960
correspond to the length of the BLS time series and finally we'll stitch together these two

324
00:40:48,960 --> 00:40:53,360
halves of the simulation, the pre-shock half and the post-shock half.

325
00:40:54,240 --> 00:40:59,760
Should notice that this is very similar to what we did when we'd simulated the steady state.

326
00:40:59,760 --> 00:41:08,320
We're the only differences here at step four at period zero. We the model, we reach our hand

327
00:41:08,320 --> 00:41:16,720
into the model and we cut some of the employed worker jobs, we separate them from their jobs

328
00:41:16,720 --> 00:41:22,880
and move them into the unemployed bucket. And that's it and then we let the model continue

329
00:41:23,360 --> 00:41:31,840
to apply its dynamics going forward using the direct problem and our solution to it and we obtain

330
00:41:31,840 --> 00:41:40,880
our simulation. So in code we follow these same steps. Here we make sure that the initial state is S bar

331
00:41:42,400 --> 00:41:47,600
and we simulate for the first 35 periods from zero to 34.

332
00:41:48,240 --> 00:42:00,560
Then we will need to apply the shock, this is step four. And the way we do this is we will set

333
00:42:01,200 --> 00:42:07,680
the S0 parameter of our Lake model equal to the shock value which we pass in as an argument here.

334
00:42:08,640 --> 00:42:14,160
And the reason we're setting the says S0 equal to ST is because we're effectively starting

335
00:42:14,160 --> 00:42:22,880
a brand new simulation where the shock value will become the initial state for the simulation

336
00:42:22,880 --> 00:42:30,320
from this point on. We'll then go ahead and simulate the second half. We'll need to make sure

337
00:42:30,320 --> 00:42:38,320
that we adjust the months from column to make sure that it picks up exactly where this data

338
00:42:38,320 --> 00:42:45,040
frame before left off. And then we'll just concatenate the two and stick them together. And finally

339
00:42:45,040 --> 00:42:51,280
we combine them with the BLS data so that we're ready to plot or compute the MSC straight away.

340
00:42:58,880 --> 00:43:05,360
Sure, our solution is to simulate in two halves. We first simulate from the steady state

341
00:43:05,360 --> 00:43:12,480
without changing anything. We then shock the state, simulate afterwards and stitch the output together.

342
00:43:20,320 --> 00:43:25,600
We'll go ahead and we'll do that and we'll call the value DF shock and we'll plot the results

343
00:43:25,600 --> 00:43:35,760
alongside the model from the data. And we'll see here that we do have a steady state up at the top

344
00:43:35,760 --> 00:43:44,080
all the way until period T-1. Then at period zero we immediately drop down to about the value of

345
00:43:44,080 --> 00:43:52,080
that COVID recession which would be 86% of the workers employed. And then the model dynamics kick in

346
00:43:53,040 --> 00:44:01,680
and as the job finding rate is at a steady state value of 37%. The number of the percent of

347
00:44:01,680 --> 00:44:07,760
workers employed goes up up but it continues to climb until it reaches a steady state value and then

348
00:44:07,760 --> 00:44:18,800
stays roughly constant after that. So because the job finding rate is high at 37% and the job

349
00:44:18,800 --> 00:44:28,880
separation rate is low at 1%. This level of only 86% of them workers being employed is not sustainable

350
00:44:29,840 --> 00:44:36,320
by those model parameters and the number of employed workers are the percentage of employed

351
00:44:36,320 --> 00:44:50,960
workers continues to climb until it reaches its steady state value again. So one thing to point out here

352
00:44:51,840 --> 00:45:00,320
is that the dynamics before the shock and even up into it were fairly similar for the COVID era

353
00:45:00,320 --> 00:45:07,520
and the model. The model had a slightly higher stable or steady state unemployment or shy employment

354
00:45:07,520 --> 00:45:14,960
numbers but the pattern of being consistent and flat up until a large spike is the same across both.

355
00:45:15,760 --> 00:45:22,880
What is different however though is that the model holding that job finding and job separation rate

356
00:45:22,880 --> 00:45:30,320
constant causes a much quicker recovery, a quicker increase in the percentage of workers employed

357
00:45:30,720 --> 00:45:37,120
in the model than we see in the data. The regates to be a gap in the between these two lines

358
00:45:37,120 --> 00:45:50,080
that grows wider as we move forward in time. This is even more pronounced when we consider how quickly

359
00:45:50,160 --> 00:45:58,720
the model is able to recover relative to the great recession. So here it looks like the difference between

360
00:45:58,720 --> 00:46:05,520
about 97% and 86. If we split it right down the middle would be somewhere around here and it

361
00:46:05,520 --> 00:46:10,720
looks like it's getting there after just one or two periods. Contrast this with what happens in the

362
00:46:10,720 --> 00:46:17,280
great recession where if we want to go from say 95 to 90 you want to go back to 92 and a half

363
00:46:17,600 --> 00:46:27,280
this doesn't happen until almost 35 months after the the low point in employment from the great recession.

364
00:46:29,600 --> 00:46:36,640
So this recovery from the great recession was much slower than our model would be predicting.

365
00:46:39,520 --> 00:46:46,880
Let's go ahead and evaluate the last function. We'll see here that the MSC for the great recession

366
00:46:46,880 --> 00:46:53,920
didn't move very much before with the original model that didn't have the spike it was between 25

367
00:46:53,920 --> 00:47:02,080
and 30 and it still is. However the MSC for the COVID era, what data has fallen significantly.

368
00:47:03,200 --> 00:47:10,720
This should be expected because when we reached in as the modeler and shifted the balance of

369
00:47:10,720 --> 00:47:16,960
employment and unemployment to artificially mimic this spike we should see that the model does a

370
00:47:16,960 --> 00:47:22,800
bit better because for these few periods were much closer to the blue line then we would have been

371
00:47:22,800 --> 00:47:31,200
had we stayed up here in the steady state. So the fact that the MSC fell is entirely a feature

372
00:47:31,200 --> 00:47:37,280
generated kind of in these 10 periods where we go from zero to when we've recovered which is at about

373
00:47:38,240 --> 00:47:44,560
the MSC in this simulation is going to be smaller than it would have been had we stayed up here

374
00:47:44,560 --> 00:47:52,960
the entire time and that shows up right here with this number being about four instead of something close to 10 or 12.

375
00:47:56,240 --> 00:48:04,240
From the analysis we just completed. We've learned that even when we artificially adjust or shock

376
00:48:04,240 --> 00:48:14,080
the state at time t equals zero to match the great recession the steady state values of our job

377
00:48:14,080 --> 00:48:20,320
finding in separation rates are inconsistent with the market dynamics during the COVID era recession.

378
00:48:21,520 --> 00:48:28,480
This was manifest in our model featuring a faster recovery than the actual COVID era data from the

379
00:48:28,480 --> 00:48:38,240
BLS. As we think about this we would realize that our assumption that alpha and beta remain constant

380
00:48:39,120 --> 00:48:46,320
is likely too strong in addition to a redistribution of our workers between employment and unemployment

381
00:48:46,320 --> 00:48:53,120
there may also have been a shift in the job separation and finding rates during this period.

382
00:48:53,840 --> 00:48:58,640
We'll explore that hypothesis here in what we'll call model two.

383
00:49:00,960 --> 00:49:06,560
So we're going to relax the assumption that alpha and beta are always fixed at their steady

384
00:49:06,560 --> 00:49:11,040
state values but we're going to do so in a very disciplined and specific way.

385
00:49:12,560 --> 00:49:19,680
And here's how we're perceived. We're going to assume that before the time t equals zero

386
00:49:19,760 --> 00:49:26,640
shock occurs alpha and beta with the subscript t are going to be fixed and held at their steady state values.

387
00:49:29,680 --> 00:49:36,800
Then we're going to continue to assume that the state s t immediately jumps to the

388
00:49:36,800 --> 00:49:41,760
spiked COVID levels so far this is the same as what we did in the previous model.

389
00:49:42,320 --> 00:49:51,600
Now we're also going to assume that starting at times zero and continuing temporarily for

390
00:49:52,160 --> 00:50:00,160
n a finite number of periods alpha and beta will move to new values alpha hat and beta hat.

391
00:50:01,440 --> 00:50:07,920
Once those n periods are over alpha and beta will return to their steady state values of alpha

392
00:50:08,480 --> 00:50:13,680
bar and beta bar. Mathematically these assumptions are expressed as follows.

393
00:50:14,480 --> 00:50:20,560
Alpha sub t is going to be equal to alpha hat only when t is between zero and n.

394
00:50:21,760 --> 00:50:27,440
In all other periods including t less than zero and t greater than n,

395
00:50:28,400 --> 00:50:33,840
alpha will be equal to alpha bar. The same holds for beta. We're going to have beta hat

396
00:50:34,800 --> 00:50:40,000
only between zero and n and beta bar in all other periods.

397
00:50:41,920 --> 00:50:48,080
We will define a function that can do this for us. So here this function again takes our

398
00:50:48,080 --> 00:50:55,520
lake model simulator as an argument and then it needs to know the shocked values of alpha hat

399
00:50:55,520 --> 00:51:02,080
beta hat as well as how long to hold them n and it needs to know where to adjust the state at

400
00:51:02,080 --> 00:51:11,600
times zero. So once we take as an input these parameters our solution to the direct problem

401
00:51:11,600 --> 00:51:20,960
described above is as follows. Our before t equals zero code remains exactly the same and even

402
00:51:20,960 --> 00:51:27,440
up until t equals zero our movement of the state directly to the shocked value is going to mimic

403
00:51:27,440 --> 00:51:37,440
what we saw in the previous direct model for model one. What changes is what happens starting at t equals

404
00:51:37,440 --> 00:51:45,600
zero. So we're going to construct a vector of alpha which represent alpha sub t. We're going to have

405
00:51:45,600 --> 00:51:50,160
him have length 35 and we're going to start out sitting all of them equal to alpha bar.

406
00:51:51,120 --> 00:51:58,320
Then the first n values of this vector are going to be adjusted so that it equals alpha hat

407
00:51:58,320 --> 00:52:07,920
in those periods. This is how we apply this mathematical function that was part of our model up above.

408
00:52:08,560 --> 00:52:14,320
This is our direct model solution or direct problem solution to this modeling feature we

409
00:52:14,640 --> 00:52:22,320
described up here. We repeat the same thing for the beta parameter. We'll use our lake model

410
00:52:22,320 --> 00:52:28,880
simulate panel routine that can accept this vector of alpha's and datas. As we did before we have to

411
00:52:28,880 --> 00:52:36,320
adjust the months from in the second half of our panel to account for the fact that we've already

412
00:52:36,320 --> 00:52:47,280
simulated from periods by 34 up until zero and then we stitch the two together combine with the

413
00:52:47,280 --> 00:52:55,680
BLS data and return. So this code is going to be the same as what we saw before. We'll evaluate this

414
00:52:55,680 --> 00:53:06,800
cell and we will test it out. So we're going to go ahead and put on our thinking caps and

415
00:53:06,800 --> 00:53:15,280
recognize that in order to slow down the recovery of our model we may need to decrease the job

416
00:53:15,280 --> 00:53:20,640
finding rate and that would mean moving it from zero point three seven is something smaller.

417
00:53:21,280 --> 00:53:27,600
Here we're just going to pick the values 0.28. We have no reason to believe that this is any better or

418
00:53:27,600 --> 00:53:36,160
worse than another value smaller than 0.37. We're just testing out one possible model which

419
00:53:36,160 --> 00:53:43,200
includes setting alpha hat to 0.28. Another thing that might slow down the recovery would be if

420
00:53:43,200 --> 00:53:51,280
the job separation rate rises. It's steady state value is at 1% and we'll just move it up to 5%

421
00:53:51,280 --> 00:53:56,640
and see what happens. And then we'll suppose or pause it at this last for three periods.

422
00:53:57,680 --> 00:54:04,560
We'll go ahead and we'll use our new simulation routine or our new solution to the direct problem.

423
00:54:05,120 --> 00:54:14,320
With the values of alpha hat at 0.28, beta hat at 0.05 that are held there for three periods

424
00:54:14,320 --> 00:54:24,720
and then we'll plot it and compute the MSC. So we see here visually that the model went down

425
00:54:25,440 --> 00:54:32,320
to the same value but then it fell a little too far and it ended up reaching about the same place

426
00:54:32,320 --> 00:54:40,560
as the the COVID data about five months after the the low point unemployment and then it continued

427
00:54:40,560 --> 00:54:51,280
its recovery. But in this region from 0 to about five the model actually underestimated the recovery

428
00:54:51,280 --> 00:54:58,080
and employment. This was better than the previous model if you remember the previous model moved

429
00:54:58,400 --> 00:55:05,040
up in this region and quickly recovered and visually we can kind of see that that's better

430
00:55:05,920 --> 00:55:11,440
but precisely and numerically we can see that it's better because our MSC for the COVID

431
00:55:11,440 --> 00:55:16,160
era data went down from about four or four and a half to now it's about two and a half.

432
00:55:17,040 --> 00:55:22,960
So we've made an improvement in the mean squared error between the yellow line representing our

433
00:55:22,960 --> 00:55:34,000
model and the blue line representing the COVID era data. So these initial test values as we just

434
00:55:34,000 --> 00:55:41,840
noticed did improve the MSC but we didn't have any reason to believe that the values we picked

435
00:55:41,840 --> 00:55:49,760
were going to be the optimal or best values. There are infinite numbers of configurations of

436
00:55:49,760 --> 00:55:55,600
alpha hat, beta hat and end we could have tried or infinite number of other models we could have used.

437
00:55:56,480 --> 00:56:02,960
And so what we're going to do next is we're going to assume the role of the visual

438
00:56:02,960 --> 00:56:10,720
econometrition and hard goals will be to look at these graphs and graphs and the MSC

439
00:56:11,360 --> 00:56:19,600
to try to manually adjust alpha hat, beta hat and end to improve upon this MSC of 2.6.

440
00:56:20,720 --> 00:56:28,000
So we're going to utilize the ipy widgets library to create an interactive version of our chart.

441
00:56:29,360 --> 00:56:38,000
So here we see that we're going to wrap the simulate plot and MSC steps in a function

442
00:56:39,120 --> 00:56:43,600
and then we're going to use the intractor team and say how we'd like

443
00:56:44,320 --> 00:56:49,040
alpha hat to be manipulated, beta hat to be manipulated and then a starting value for end.

444
00:56:49,920 --> 00:56:55,840
And what this did was it returned our MSC like we had before and we see that we're still near

445
00:56:55,840 --> 00:57:05,840
that value of 2.6. Any deviation from exactly 2.6 is because there is randomness in the simulation.

446
00:57:05,840 --> 00:57:10,640
So we shouldn't expect to get the same value every time. It'll be slightly different each time we

447
00:57:10,640 --> 00:57:17,200
run it. So we have the MSC we have the plot but now we also have these three little widgets

448
00:57:17,840 --> 00:57:24,240
here that allow us to be the visual econometrition. If we slide this bottom widget,

449
00:57:25,200 --> 00:57:32,080
this will allow us to change the value of n or change the stickiness of the alpha hat and beta hat

450
00:57:32,080 --> 00:57:39,600
parameters. And you'll see here as we make n quite large but is doing to our plot here

451
00:57:40,160 --> 00:57:51,600
is it's making this region where the model has an output employment percentage of about 0.85.

452
00:57:51,600 --> 00:57:57,680
It's making that longer and longer. So we see here that n is currently at a value of 9,

453
00:57:58,400 --> 00:58:05,360
which corresponds visually to going from 0 to 9 here and it has this whole region.

454
00:58:06,240 --> 00:58:10,560
Then as soon as the model parameters are set back to their steady state value,

455
00:58:11,200 --> 00:58:17,200
we see this very sharp and quick or very smooth and quick recovery back to the steady state.

456
00:58:19,600 --> 00:58:26,000
So let's set this back to something a little more moderate. Maybe three. And let's see what happens

457
00:58:26,000 --> 00:58:33,440
if we play with the job finding rate alpha hat. As we make this bigger, so we would like to make

458
00:58:33,520 --> 00:58:38,320
our recovery a bit faster than it currently is. We'd like to make sure that we're able to move

459
00:58:38,320 --> 00:58:44,720
up and track this blue line better. So if we're going to change alpha hat, the job finding rate

460
00:58:44,720 --> 00:58:48,880
would need to make it bigger. So it would make it bigger and bigger and bigger and it looks like this

461
00:58:48,880 --> 00:59:01,200
current value of 42 percent would match the data pretty well. So if there was some way that we could raise the

462
00:59:03,600 --> 00:59:12,080
job finding rate from the steady state value of about 37 percent to about 42 percent, that might help

463
00:59:14,480 --> 00:59:20,400
the model match closely the data obtained from the BLS in the covid error.

464
00:59:23,040 --> 00:59:31,920
Now if we think that this is probably not what's happening during this time period that there's

465
00:59:31,920 --> 00:59:39,520
probably unlikely that more unemployed workers are finding work, this may not be what we want to do.

466
00:59:39,520 --> 00:59:49,920
We might not want to be increasing the parameter alpha hat because that's inconsistent with our

467
00:59:49,920 --> 00:59:59,600
beliefs about what's happening in the economy. In addition to looking at this just raw data of

468
00:59:59,760 --> 01:00:05,840
the percentage of workers employed and unemployed, we could kind of use some of our other knowledge about

469
01:00:07,600 --> 01:00:16,800
the total claims of workers claiming unemployment rising to maybe discount or discard the hypothesis

470
01:00:16,800 --> 01:00:25,680
that the job finding rate is increasing to make this match. So what we'll do is we'll go and we'll

471
01:00:25,680 --> 01:00:30,960
make this a little bit smaller. In fact it's probably more likely that the job finding rate has

472
01:00:30,960 --> 01:00:37,200
fallen from a steady state value and we'll leave it here at something smaller than the 37 percent

473
01:00:37,200 --> 01:00:45,920
in steady state. Now what's probably true is that the relative to the steady state value of 0.01 which

474
01:00:45,920 --> 01:00:53,280
saw this two eager recovery from the model. We might believe that the job separation rate has increased.

475
01:00:53,280 --> 01:01:00,400
So let's go ahead and increase it from 1 percent to 2 percent and we'll see here that we're very

476
01:01:00,400 --> 01:01:08,960
closely matching the data and only at this kind of period three here do we see that it starts to take

477
01:01:08,960 --> 01:01:15,360
off a little too fast. So let's go ahead and leave these values at 25 percent and 2 percent

478
01:01:16,240 --> 01:01:24,160
and move up N1 more and now we see here that if we allow these adjusted parameters to

479
01:01:24,160 --> 01:01:29,840
persist for four periods we're very closely the model's very closely tracking the COVID

480
01:01:29,840 --> 01:01:38,960
era data. So let's summarize what we did. We first started experimenting and understanding with

481
01:01:38,960 --> 01:01:48,720
what a change in N did and what it did is it delayed the rapid but smooth increase from

482
01:01:49,520 --> 01:01:56,640
whatever value of employment percentage was currently held back to the steady state. As we made

483
01:01:56,640 --> 01:02:06,720
N very large this value lengthened the duration over here until we see the rapid increase to the steady

484
01:02:06,720 --> 01:02:17,120
state. If we make N very small then the if we again 0 so there is no alpha-hatter beta-hat

485
01:02:17,760 --> 01:02:23,360
then we see an immediate sharp or immediate quick increase back to the steady state.

486
01:02:25,040 --> 01:02:30,400
Then we thought about what if we only move alpha-hat what if we only change the job finding rate.

487
01:02:31,040 --> 01:02:40,080
We found that in order to match the dynamics observed in the COVID era data we would need to

488
01:02:40,080 --> 01:02:46,720
have a job finding rate that was higher than the steady state job finding rate. This was

489
01:02:46,720 --> 01:02:56,160
inconsistent with our beliefs as observant economists that more unemployed workers were finding

490
01:02:57,040 --> 01:03:05,200
our beliefs were that there were more people being unemployed and it was harder for them to find

491
01:03:05,200 --> 01:03:11,600
jobs. So trying to sort of have an increase in this job finding rate was inconsistent with that.

492
01:03:13,360 --> 01:03:19,280
So we then said well what if we move the job finding rate somewhere below the steady state?

493
01:03:19,280 --> 01:03:25,840
How much would we need to then increase the job separation rate to have the model match the

494
01:03:25,840 --> 01:03:30,880
COVID era data and that led us to increase the job separation rate from 1% to 2%.

495
01:03:32,400 --> 01:03:40,400
And this exercise that we just performed is kind of the cracks of doing computational social

496
01:03:40,400 --> 01:03:49,520
science. It included three main components. One was the data. That was this blue line

497
01:03:50,320 --> 01:03:56,400
that was the actual or real world data that we were trying to match and we were trying to have

498
01:03:56,400 --> 01:04:05,600
our model track. The second main component was the statistical model. It was the hypothesis

499
01:04:06,480 --> 01:04:13,360
that the model parameters alpha and beta shifted from their steady state values to alpha-hat

500
01:04:13,520 --> 01:04:20,640
for finite number of periods. We had a way to toggle these parameters and adjust them

501
01:04:21,440 --> 01:04:28,320
and that led to different statistical models that we had a direct solution to. A direct problem solution

502
01:04:28,320 --> 01:04:35,120
to and we were able to produce these simulations. So feature number one was the data.

503
01:04:35,120 --> 01:04:42,640
Feature number two was the model and then feature number three was our prior beliefs about which

504
01:04:42,720 --> 01:04:52,160
levels of parameters are plausible and that helped us applying these prior beliefs helped us arrive

505
01:04:53,280 --> 01:05:01,760
a set of parameters that both match the data and were consistent with our beliefs.

506
01:05:01,760 --> 01:05:07,760
Had we not imposed that we may well have stopped when we found that alpha-hat of 45%.

507
01:05:08,320 --> 01:05:14,480
But our belief said that that was inconsistent with what we knew and observed about the world.

508
01:05:14,480 --> 01:05:18,160
Even though it was outside the model, there's nothing in our model about

509
01:05:19,120 --> 01:05:25,040
individuals claiming unemployment or businesses closing down. The fact that we knew those things were happening

510
01:05:28,160 --> 01:05:35,440
formed our prior beliefs and that led us to change the set of parameters we were willing to

511
01:05:35,760 --> 01:05:42,960
consider. Those three components data, model and prior beliefs are at the heart of a lot of

512
01:05:44,480 --> 01:05:49,280
computational social science and the heart of a lot of what we will be doing as we move forward

513
01:05:49,280 --> 01:05:51,760
in this lecture. In this lecture in this course.

514
01:05:55,040 --> 01:06:01,680
Okay, so we're going to summarize these three values. We had 25% job finding rate, a 2% job

515
01:06:01,680 --> 01:06:08,240
separation rate and that these were last for four periods after that initial shock to the state of

516
01:06:08,960 --> 01:06:17,600
the workers. When we did that, the MSC fell from about 2.4 to about 1.7.

