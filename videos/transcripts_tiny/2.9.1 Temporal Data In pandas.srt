1
00:00:00.000 --> 00:00:38.000
Hello, this is Spencer Lyon and in this lecture we will be learning about how to handle temporal data with pandas. By temporal data we mean data that has some notion of time or a period. Before viewing this lecture, you should be familiar with and comfortable with defining Python functions. Specifically Python functions that can consume a pandas data frame or series and then operate on it. Additionally it would be helpful if you have viewed the group by content and are comfortable with how pandas represents the group by object.

2
00:00:28.000 --> 00:01:15.000
Additionally it would be helpful if you have viewed the group by content and are comfortable with how pandas represents the group by object. And allows you to operate on it through built-in functions as well as applying or aggregating based on custom functions. Our goals for this lecture will be to understand how pandas handles dates. One of the some of the things that we're going to understand after this lecture is how to parse a string into a date time object. Date time is both a built-in Python object and a pandas extension type for handling both dates and times.

3
00:01:02.000 --> 00:01:47.000
Date time is both a built-in Python object and a pandas extension type for handling both dates and times. We will know how to write out our dates in custom formatted strings that may be a requirement for various applications we're working on. We'll also work through how we can access specific fields associated with a date time. Something like the day the month year, minute, hour, second. We're going to do this from a date time index where the date data is on the index itself as well as when we have a call on more a series with the date time data type.

4
00:01:34.000 --> 00:02:10.000
We're going to do this from a date time index where the date data is on the index itself as well as when we have a call on more a series with the date time data type. Finally we will do a bit of computation. Using dates and we're going to be doing both rolling computations like a moving average as well as resampling operations like moving from a weekly to a monthly frequency. We'll work on a few examples of each and we'll try to understand the difference between the two and when we would apply each one.

5
00:02:01.000 --> 00:02:45.000
We'll work on a few examples of each and we'll try to understand the difference between the two and when we would apply each one. We'll be using some artificial data and we'll also be using some data from the real world. The data will be using is the Bitcoin to US dollar exchange rate between March 2014 and the present. We'll obtain this data from Pondol but also provide ACSV file with at least a subset of this data in case there are connectivity issues. This is most of what we talked about and here is the outline of a bit more detail.

6
00:02:38.000 --> 00:03:23.000
This is most of what we talked about and here is the outline of a bit more detail. If you're following along with this notebook and you do not have the Quandal library installed, you can uncomment this line right here. As follows, and then execute it and it will go ahead and install the Quandal library for you. If I execute this, it tells me that the requirement was already satisfied and so I will. Recomment it and we can move on to the next slide. Which is the next code cell. In this code cell, we're just going to be doing some imports. These three we should be familiar with here we're going to import.

7
00:03:14.000 --> 00:03:53.000
Which is the next code cell. In this code cell, we're just going to be doing some imports. These three we should be familiar with here we're going to import. We're going to set up the Quandal library to use our API key. We've created one specifically for viewers of this lecture, but we have a note at the end for how you could obtain your own. If you are going to be accessing more data from Quandal and perhaps hitting some some data usage restrictions based on this shared API key. Then we're going to set up a plotting theme to make our charts look a little bit nicer than the standard map plot lib style.

8
00:03:43.000 --> 00:04:27.000
Then we're going to set up a plotting theme to make our charts look a little bit nicer than the standard map plot lib style. This code executed well and we'll move on to the next slide. So Pandas has extensive support for handling dates and times and we as the instructors are very thankful for this working with times is a complicated issue. When trying to reason about times to with a computer, you have to be aware of and instruct it how to handle things like leap years or different holidays or calendar events.

9
00:04:13.000 --> 00:05:02.000
When trying to reason about times to with a computer, you have to be aware of and instruct it how to handle things like leap years or different holidays or calendar events. You also need to think about things like time zones and daylight savings time. All of these things are complications that are taken care of before us by the Pandas library. We're going to loosely refer to data that has either a date or a time or both information has a time series data. This term time series mostly comes from the subfield of statistics that aims at analyzing data over time.

10
00:04:53.000 --> 00:05:43.000
This term time series mostly comes from the subfield of statistics that aims at analyzing data over time. And we will be using that same language as we refer to data with a temporal element. Let's dive into exploring pandas time series capabilities. Some of the topics that pandas provides are some of the main features that it provides is the ability to parse a string into a date. In other words, if we're given a textual or string representation of a date or a date time, pandas can convert that into a proper date time object for further use.

11
00:05:31.000 --> 00:06:17.000
In other words, if we're given a textual or string representation of a date or a date time, pandas can convert that into a proper date time object for further use. You can also start with a date time object and write it out into a string. This would kind of be the inverse of the first point. There may also be computations we'd like to do based on parts of the date time or the timestamp and we'll learn how to extract those when the timestamps either living that data frame or series or the index. We'll talk about how we can shift or adjust data through time by taking a lead or lag and then we'll also talk about resampling and rolling operations.

12
00:06:06.000 --> 00:06:53.000
We'll talk about how we can shift or adjust data through time by taking a lead or lag and then we'll also talk about resampling and rolling operations. And we want to point out here that like other topics, but probably even a bit more for this one we're going to skip a lot of the functionality that pandas offers. And we're going to cover the basics and we should be comfortable with them, but we encourage you to look at the official documentation for even more information. The first thing we'll learn how to do is to parse strings as dates because often when we read in data from a file, perhaps a CSV file, the dates will be written as strings.

13
00:06:39.000 --> 00:07:20.000
The first thing we'll learn how to do is to parse strings as dates because often when we read in data from a file, perhaps a CSV file, the dates will be written as strings. For lucky, these dates will follow some kind of structured pattern of very common pattern that we've seen in our work is the following where we have four digits for the year separated by a hyphen, two digits for the month, another hyphen and then two digits for the day. For example, we could write Christmas day in 2020. These really make sense.

14
00:07:17.000 --> 00:07:57.000
These really make sense. That's follows. We would first start with the four digits for the year, which would be 2020 and then a hyphen, the two digits for the month because December is the 20, or is the 12th month we'd read it 12 here. And then Christmas is on the 25th of that month, so we would write a 25. Let's go ahead and evaluate that cell, so we have this variable for us later on. Now in order to do time series like computations on this, we need to move from the string form into a proper date time.

15
00:07:45.000 --> 00:08:22.000
Now in order to do time series like computations on this, we need to move from the string form into a proper date time. The pandas date, the pandas two date time function does this for us, so we'll call pd.two underscore date time and we'll pass in the single argument Christmas string. When we do that, we're going to save the variable as Christmas and we're going to print out that it has a class pandas do do do do do do do do do do time stamp. Well, the time stamp represents a single moment in time.

16
00:08:18.000 --> 00:08:51.000
Well, the time stamp represents a single moment in time. Now when we display this string, or this value Christmas instead of seeing only the string we started with, we see that it's a timestamp. Notice that in addition to the date, pandas also provides the hour, minute and second corresponding to our timestamp. We didn't supply that our Christmas string that we passed in was only the part I've highlighted here, and pandas set the minute or the hour, minute and second to its default value of zero,

17
00:08:39.000 --> 00:09:18.000
We didn't supply that our Christmas string that we passed in was only the part I've highlighted here, and pandas set the minute or the hour, minute and second to its default value of zero, meaning mid-night on Christmas. Now the pandas two date time function is actually pretty smart at guessing the format of the date. So what we're going to do in this experiment is we have here a list of different ways that we could write the date for Christmas in the year 2020. The first one we're going to write out the full month.

18
00:09:15.000 --> 00:10:02.000
The first one we're going to write out the full month. Then the day comma year we could do an abbreviated version of that we could start with the day of the week before we can even flip the order around where now it's going to be the two digits for the day and abbreviation for the month and 2020. We need to even append the suffix TH as if we were talking about the 25th of December. When we execute this cell, we'll see that in each case pandas is able to produce the exact same date or exact same timestamp object, which gives us the assurance that it was able to correctly identify the pattern contained in each of these strings.

19
00:09:42.000 --> 00:10:26.000
When we execute this cell, we'll see that in each case pandas is able to produce the exact same date or exact same timestamp object, which gives us the assurance that it was able to correctly identify the pattern contained in each of these strings. This is quite impressive as there are an infinite number of ways you might write this type of information and pandas seems to be able to handle the most common formats without any extra help from us. There are some cases where pandas won't be able to guess things.

20
00:10:19.000 --> 00:11:10.000
There are some cases where pandas won't be able to guess things. So for example, if I was a seller on the website Amazon and I were to get back a report from them on each order that was placed for one of my products, there would be a column in the data set that had timestamps for a method as follows. So looking at this, we can kind of tell that the date here is going to be the 25th of December 2020. However, if we were to try to ask pandas to be this for us, it would fail. So when I execute this cell when I try to say 2-8 time, it's going to tell me that it doesn't really understand how to go from the string we gave it into a string.

21
00:10:56.000 --> 00:11:36.000
However, if we were to try to ask pandas to be this for us, it would fail. So when I execute this cell when I try to say 2-8 time, it's going to tell me that it doesn't really understand how to go from the string we gave it into a string. Mayors are into a date time object, says that there was an unknown string format. So what do we do? Well, one option would be to manually manipulate this string and only keep the first part of it. So let's see if we were to keep just the first say 11 characters, we would be able to cut off right here after the 25th.

22
00:11:26.000 --> 00:12:12.000
So let's see if we were to keep just the first say 11 characters, we would be able to cut off right here after the 25th. And it produces a timestamp for us. If we do anything else, it's going to start to fail. And now the 11 must have included the T. So now if we go only the first 9 characters, we only get up to this point and now pandas gets it wrong. It assumes that we were talking about December 2nd instead of December 25th, because when it saw the single digit 2 after the last hyphen, it must assume we wanted the number 2.

23
00:11:56.000 --> 00:12:50.000
It assumes that we were talking about December 2nd instead of December 25th, because when it saw the single digit 2 after the last hyphen, it must assume we wanted the number 2. And this is more canonically written 0 to you. Anyway, one option would be to manipulate the string, however, there's actually a more powerful method that we'll learn next. But in order to have pandas parse that without us changing the input, what we can do is we can utilize the format argument of the 2-8 time function. So when we execute this, we pass in the same string that was causing us problems before, but now with the additional format argument, we see that pandas is correctly extracting the timestamp.

24
00:12:36.000 --> 00:13:32.000
So when we execute this, we pass in the same string that was causing us problems before, but now with the additional format argument, we see that pandas is correctly extracting the timestamp. Now if we stare at this Amazon and what this funky suffix I gave it string f time, we'll see that it relates very closely to the string, and this Christmas Amazon. We encourage you to take a moment to compare this string to the one below and then we'll talk through it. So let's show both of these strings and now notice that the overall structure or format is very similar.

25
00:13:23.000 --> 00:14:12.000
So let's show both of these strings and now notice that the overall structure or format is very similar. But where everywhere we had a number in the Christmas Amazon variable, we have some kind of placeholder, some strange percent letter, placeholder. But the idea is there's something and then a hyphen, something else, something else, the letter T and then a colon, a colon plus space 00 space colon 00. So now the way pandas was able to interpret this is it used the string format time, the string f time variable.

26
00:14:02.000 --> 00:14:52.000
So now the way pandas was able to interpret this is it used the string format time, the string f time variable. As a template or a format for how to extract different parts of the date. In this example the percent capital Y was an instruction to pandas to look for a four digit number representing the year. The percent lower case M, instructed pandas to look for a two digit number in this location representing the month of the date. We can see you can probably guess here what the d h m and s represent.

27
00:14:44.000 --> 00:15:29.000
We can see you can probably guess here what the d h m and s represent. But instead of guessing what will encourage you to do is look at the python documentation for a list of all the possible percent something patterns that would be accepted by the format are human. We're going to give you an opportunity to practice this by performing the following exercise. First we'd like for you to open up the link to the python documentation in the previous cell and then with that open look at the following three strings.

28
00:15:18.000 --> 00:15:58.000
First we'd like for you to open up the link to the python documentation in the previous cell and then with that open look at the following three strings. So what we'd like for you to do is using the documentation page we would like to construct a argument for the format variable of the two date time function in order to be able to parse each of these strings. Essentially when we in our example a minute ago we're able to construct this format string in order to parse the Christmas Amazon variable.

29
00:15:46.000 --> 00:16:36.000
Essentially when we in our example a minute ago we're able to construct this format string in order to parse the Christmas Amazon variable. We'd like for you to construct similar format strings. For the three dates that appear in the exercise. We will not be working through this exercise together in class as we will leave it for an assignment. Now let's think let's talk through how we could work with more than one date at a time. So the pandas two date time function is also pretty intelligent about figuring out how to handle this two poles series or other collections of date looking things.

30
00:16:24.000 --> 00:17:05.000
So the pandas two date time function is also pretty intelligent about figuring out how to handle this two poles series or other collections of date looking things. So just as it was able to guess how to interpret the following the date of Christmas it's also able to interpret a list of things that look like dates. We'll just do this one example just to demonstrate that it's possible and that we can pass in a list of dates and get back. What's called that date time index we'll talk more about this further in the lecture.

31
00:16:59.000 --> 00:17:41.000
What's called that date time index we'll talk more about this further in the lecture. But all of the things we've learned about the date time argument including what format of string is automatically identified by pandas as well as the different ways we can construct pandas to interpret the strings using the format argument. We can all apply equally to a collection of dates in addition to just the one that we saw previously. Often when working with multiple dates in a time aware dataset the dates will have a regular frequency or.

32
00:17:31.000 --> 00:18:16.000
Often when working with multiple dates in a time aware dataset the dates will have a regular frequency or. We can see an in between observations. We could use the pd.2 date time function we just learned about by passing a string representing evenly spaced dates in order to construct this type of. Value however this is more work than we need to do pandas provides the very convenient date range function to help us construct ranges of regularly spaced dates. There are two basic forms for calling the date range function.

33
00:18:11.000 --> 00:18:58.000
There are two basic forms for calling the date range function. In the first approach we. Pass to pd.date range a start variable an end variable and a frequency variable. The frequency variable is passed to the freak or FREC or FRECU argument. In a approach would be to pass start. Not pass ends but instead set the keyword argument periods equal to some number n and again set a frequency. Now in these examples start and end represent a time stamp or anything that pd.2 date time can recognize as a time stamp.

34
00:18:47.000 --> 00:19:35.000
Now in these examples start and end represent a time stamp or anything that pd.2 date time can recognize as a time stamp. Frequency would be some kind of frequency string like d for daily or a for annual. We'll talk more about these shortly and then n is some integer representing the number of periods we'd like for pandas generate. Let's test it out. So we'll start with passing. We'll start with the first form where we pass something for start something here for. And and then a value for the freak keyword argument and here we're going to pass a and what happens is pandas starts.

35
00:19:25.000 --> 00:20:11.000
And and then a value for the freak keyword argument and here we're going to pass a and what happens is pandas starts. In the year 2020 goes one year forward because we picked an annual frequency to 2021, 22 and 23. If we were to omit the third argument omit the frequency argument pandas uses the default frequency of daily. So here we're passing start and end and implicitly the frequency argument has its default value of daily and we're getting all the days between April 1st 2020 and April 25 2020.

36
00:19:57.000 --> 00:20:38.000
So here we're passing start and end and implicitly the frequency argument has its default value of daily and we're getting all the days between April 1st 2020 and April 25 2020. We'll try the second version of the function where we pass the start and a number of periods and we'll see here that this example returns the same thing as previous one. We start at April 1st 2020 and we continue forward for 25 periods. Now because the freak argument has a default value of d for daily we get the same answer here.

37
00:20:31.000 --> 00:21:18.000
Now because the freak argument has a default value of d for daily we get the same answer here. We could change this to something different like hourly and now we're going to see that we get back. More data and we're we get back slightly different data so instead of seeing every day between April 1st and April 25 represented we're actually going to be seeing. The first all 24 hours belonging to April 24 for the first 24 of our 25 observations and then the 25th one would be the very first hour of April 2nd.

38
00:21:00.000 --> 00:21:53.000
The first all 24 hours belonging to April 24 for the first 24 of our 25 observations and then the 25th one would be the very first hour of April 2nd. If we were to change this to something different like m for months we see here that we have 25 data points still but now we change one point as an exchange is months. These frequency strings we've been using are known as an offset alias in the pandas documentation and there are a lot of them. So here we've actually included a table taken from their documentation that describes all of them.

39
00:21:45.000 --> 00:22:39.000
So here we've actually included a table taken from their documentation that describes all of them. We're not going to walk through all of them but you see here that we have the calendar day that we were using we have a or why for yearly. H for hourly. M for monthly and then a whole lot of other ones that we haven't used yet. Chances are the types of frequencies you may be interested in for your analysis. Are probably represented here in one of these offset alias. If that wasn't enough pandas goes a step further they also offer something called an anchored offset.

40
00:22:32.000 --> 00:23:17.000
If that wasn't enough pandas goes a step further they also offer something called an anchored offset. These are going to be represented as a suffix on the offset alias and with that does is it ties the date that's generated to a particular point in the range. It's a little confusing so let's do an example. If we had a for annual pandas would generate timestamps for the last day of the year. If we instead use the frequency a dash m a r for March. Pandas would instead return the last day of the month of March for each year.

41
00:23:09.000 --> 00:24:03.000
Pandas would instead return the last day of the month of March for each year. It was generating the timestamp. Now the list of anchored offsets is even longer so here we'll just provide a link to the documentation. And we'll encourage you to take a look on your own time at what the different anchored offsets are. The next topic we'll talk about is called the time delta. And this arises in the case that we need to do arithmetic on our dates or on our timestamps. So for example suppose we had two timestamps we have the original Christmas timestamp we defined before which represented December 25th 2020.

42
00:23:51.000 --> 00:24:43.000
So for example suppose we had two timestamps we have the original Christmas timestamp we defined before which represented December 25th 2020. As well as a new timestamp we're defining here called new year representing in January 1st 2020. Well now subtract Christmas from New Year's. And actually I'm not sure what you get if you take away Christmas from New Year's however in pandas world this actually return something. Useful to ask. We stored it as a variable that was named death and we see here that if we ask for the type of death we see that it is a pandas stuff stuff stuff time delta object.

43
00:24:30.000 --> 00:25:21.000
We stored it as a variable that was named death and we see here that if we ask for the type of death we see that it is a pandas stuff stuff stuff time delta object. And delta represents the difference in timestamps when we have pandas when we have Python or Jupiter print this for us. We see here that the time delta argument has 359 days zero hours minutes and seconds. So there are 359 days in between Christmas and New Year's. As mentioned before a time delta object will represent the difference between two timestamps these objects have various properties like days.

44
00:25:10.000 --> 00:25:56.000
As mentioned before a time delta object will represent the difference between two timestamps these objects have various properties like days. But it's here that the days was equal to 359 consistent with the string we saw before and the seconds is zero also consistent with the string representation. What we didn't see was that we had diff.days. Times 24 hours in a day times 60 hours minutes in an hour times 60 seconds in a minute we didn't see that diff.s was equal to this number 31 million 17 600 seconds.

45
00:25:50.000 --> 00:26:42.000
was equal to this number 31 million 17 600 seconds. The reason for this is that the time delta stores how many of each unit as needed to make up the total difference so in this case we had 359 days was precisely the difference between our two timestamps because they both had our minute and second offset to zero. So we didn't need any seconds in order to compute the difference between the two dates. If we have a time delta object like the diff that we have as well as a timestamp and we add them together we go back another timestamp so in this case New Year plus diff is actually equal to Christmas.

46
00:26:27.000 --> 00:27:15.000
If we have a time delta object like the diff that we have as well as a timestamp and we add them together we go back another timestamp so in this case New Year plus diff is actually equal to Christmas. This makes sense because remember we computed diff equals Christmas minus New Year. And if you just do some algebra here you'll see here that Christmas must be equal to New Year plus diff. Thankfully the time delta and timestamp arithmetic implementation impandas does satisfy this basic algebraic concept.

47
00:26:59.000 --> 00:28:02.000
Thankfully the time delta and timestamp arithmetic implementation impandas does satisfy this basic algebraic concept. Time delta objects can also be used as part of arithmetic expressions. Here we're going to compute 10 times diff. And what we see is that we have 3,590 days instead of the original 359 days found in diff. The interpretation here is that any integer n times a time delta object is going to be the the to fold the ratio of that time delta repeated n times. In addition to getting a time delta by subtracting one date from another we can also construct a time delta by hand using the pd dot time delta constructor.

48
00:27:50.000 --> 00:28:46.000
In addition to getting a time delta by subtracting one date from another we can also construct a time delta by hand using the pd dot time delta constructor. So here we're going to set the days hours and minutes keyword arguments and we'll construct another time delta. So here the string form let's us know that our time delta represents one day four hours and three minutes. If we add this to Christmas what we should see is December 26 2020 at 403 AM. And indeed when we do that we do see that we are at December 26 403 AM. So our time delta that we constructed by hand and added to Christmas led to our expected result.

49
00:28:30.000 --> 00:29:27.000
And indeed when we do that we do see that we are at December 26 403 AM. So our time delta that we constructed by hand and added to Christmas led to our expected result. Common technique that we've employed many times in our code is to create a variable representing a quote standard or often used time delta. Then we will leverage this scalar multiplication plus addition operations in order to adjust the time stamps found in our data. So let's see an example. We might construct a variable named one day that is equal to a time delta object representing one day similarly one hour would represent one hour.

50
00:29:12.000 --> 00:30:02.000
So let's see an example. We might construct a variable named one day that is equal to a time delta object representing one day similarly one hour would represent one hour. Now we can do a arithmetic directly on these two times delta and they will combine their total time delta. So here we have one day and one hour. But we can also do things like compute a variable for Christmas day in 2021 by doing 365 days times our one day and then adding that to Christmas. And you'll see here that we still have December 25 but this time the year is 2021 instead of 2020.

51
00:29:54.000 --> 00:30:51.000
And you'll see here that we still have December 25 but this time the year is 2021 instead of 2020. We can continue to do more time delta arithmetic by subtracting off one hour and the expected result should be December 24 at 11 PM or hour 23 in the year 2021. So this is the last hour of Christmas Eve in the year 2021. We will use this very often if we ever need to do computations directly on one day to the next one hour to the next or anything like that. We previously learned how we can use the percent pattern format in order to have pandas read especially formatted string as a date.

52
00:30:40.000 --> 00:31:44.000
We previously learned how we can use the percent pattern format in order to have pandas read especially formatted string as a date. We can also use the same format in order to have pandas go from a date object into a string formatted how we buy. So for example, let's remind ourselves that the Christmas variable in the pandas timestamp with the date of Christmas to the near 2020. Now we're going to use some of the slightly less common formatting options for the string format time method. Or from the news again come from the table in the Python documentation what we'll do is when we evaluate this cell we see here that we wrote we love percent a space percent b space percent d parentheses also written space percent c close parentheses.

53
00:31:24.000 --> 00:32:26.000
Or from the news again come from the table in the Python documentation what we'll do is when we evaluate this cell we see here that we wrote we love percent a space percent b space percent d parentheses also written space percent c close parentheses. So the pandas returned when we passed in when we passed that string to the string format time method spelled sgr fti amy on our Christmas variable was we love Friday December 25 also written fry December 25. Serular 2020. So you'll notice here that these formatting placeholders that we used in order to create date times from strings can also be used to go the other direction.

54
00:32:08.000 --> 00:32:55.000
So you'll notice here that these formatting placeholders that we used in order to create date times from strings can also be used to go the other direction. We have an exercise where we give you a chance to practice this. What we like for you to do is use the pd dot 2d time function to create a variable expressing the birthday of one of your friends or family members as a date time object. Then we like for you to call the string format time method on that new variable you just created to write out a message that looks in the same format as this.

55
00:32:42.000 --> 00:33:29.000
Then we like for you to call the string format time method on that new variable you just created to write out a message that looks in the same format as this. The date will be different for the birthday that you chose. But the overall structure of the string should match what you see here. We will now turn to learning about how pandas can help us to extract data or subsets of data from a data frame or series with temporal information. Now most often when we have a date time column or date time information it will be found on the index because often in index.

56
00:33:17.000 --> 00:34:09.000
Now most often when we have a date time column or date time information it will be found on the index because often in index. Because often the date is used to help us identify an observation. Now if we do have the date values on the index and it's the only level of the index, pandas will create for us especially next called of type date time index. When this is the case, we have a lot of flexibility and power for accessing subsets of data with an ocean of time periods. We will do this using the already familiar dot lock access or property of the data frame and the series.

57
00:34:01.000 --> 00:34:46.000
We will do this using the already familiar dot lock access or property of the data frame and the series. But when there's a date time index, it gains additional flexibility and opportunity for accessing data. One note is that while the examples we will show have a single level index with date time data. The same features and opportunities would work with a multi index, and especially when that in the dates are on the leftmost part or the outermost part of the index. It's easiest to understand how this works by looking at an actual data set.

58
00:34:40.000 --> 00:35:23.000
It's easiest to understand how this works by looking at an actual data set. So what we'll do here is we'll load up a real data set and we'll use the quantum library to load a data set containing the exchange rate between Bitcoin and the US dollar. From March or sorry, from May of 2015 to roughly November of 2020. And here is the code for asking for this data from a quantum. If for whatever reason there are network issues, we also prepared a CSV file that you should have received in conjunction with this notebook.

59
00:35:14.000 --> 00:36:01.000
If for whatever reason there are network issues, we also prepared a CSV file that you should have received in conjunction with this notebook. That's called BTC underscore usd.csv that contains the data we extracted. We'll go ahead and let quantum get the data for us. And then we'll ask to print out the info about the data frame as well as look at the first few rows by using the head method. We see here that we have a date time index and that there are. 2,381 rows. The values on the index go from May 1st to 2014 through November 5th, 2020.

60
00:35:51.000 --> 00:36:45.000
The values on the index go from May 1st to 2014 through November 5th, 2020. In addition to this index, we have seven other data columns. The first four have to do with the price of Bitcoin. They are open high low close and this for each date or each row in our data frame. This would represent the first transaction price between Bitcoin and US dollars on that date. The highest price, the lowest price and then the last or closing price. We then have two notions of volume. How many units of Bitcoin were transacted on each day and then what is the total US dollar value in the currency or sorry in the United States dollar and then finally there's a weighted price.

61
00:36:31.000 --> 00:37:19.000
How many units of Bitcoin were transacted on each day and then what is the total US dollar value in the currency or sorry in the United States dollar and then finally there's a weighted price. Now notice that because we have the dates on the index, we are able to do some additional things. For example, if we wanted all the data for the year 2015, we would get our data frame. Go to the dot lock accessor and then pass the string 2015. Notice that what we get back, the index starts at January 1st of 2015 and ends December 31st, 2015.

62
00:37:09.000 --> 00:37:51.000
Notice that what we get back, the index starts at January 1st of 2015 and ends December 31st, 2015. And there are 365 rows in our data set. This corresponds to one row per day in the calendar year for 2015. Notice also that we are getting all the columns back. Compare this to what we would have previously gotten using our knowledge of the dot lock accessor. If we didn't have a date time index, what would have happened was this string would have been interpreted as an actual item to be found somewhere on the index.

63
00:37:40.000 --> 00:38:20.000
If we didn't have a date time index, what would have happened was this string would have been interpreted as an actual item to be found somewhere on the index. And it would have returned all rows where the outer most layer of the index was equal to the string 2015. This is very different from behavior when we have date time information on this left or outer most layer of the index. In addition to looking at a whole year, we can actually go and look at one month at a time. We can pass again to the dot lock accessor.

64
00:38:16.000 --> 00:39:02.000
We can pass again to the dot lock accessor. We can pass August 2017 and we're going to get back a data frame with rows starting from August 1st down to August 31st, total of 31 rows and again all the columns. In addition to spelling out the word August, we could also use its two digit numerical abbreviation to get back to the exact same data. Now we'll point out here that the same properties we already know and understand about how dot lock works apply here. The first argument passed dot lock here the only argument is used to extract a subset of rows or subset of rows.

65
00:38:52.000 --> 00:39:44.000
The first argument passed dot lock here the only argument is used to extract a subset of rows or subset of rows. It's used to extract values using the index. If we were to pass another value here, this would allow us to specify certain keys or labels on the columns. For example, if we passed a list of the strings open and high, we'll get back just these two columns. All of the knowledge and experience you've gained using dot lock will still carry over. We just get additional flexibility by being able to specify dates and periods and get back whole subset of rows that pertain to in this example August 2017.

66
00:39:28.000 --> 00:40:28.000
We just get additional flexibility by being able to specify dates and periods and get back whole subset of rows that pertain to in this example August 2017. Now we don't need to stop at just a month. We can build down to a day. Here we ask for August 1 2020. What we get back is a series. This contains seven rows in our series. One for each of the columns in our data frame. We can get back to the same data using the two digit month to the day for the year to reference the same date. Here's a question to think about. What type of things can we pass to the dot lock access or property when we have a date time index.

67
00:40:14.000 --> 00:41:12.000
Here's a question to think about. What type of things can we pass to the dot lock access or property when we have a date time index. The general answer is that anything that can be immediately converted to a date time using the PD dot 2-day time without needing to specify the format argument. Is valid to be the first argument to dot lock. In other words, anything that pandas automatically recognizes as a date can be used as the first argument to the dot lock accessor. And the behavior is that pandas will return all rows where the date in the index has some notion of belonging to the date or period that we passed in.

68
00:40:58.000 --> 00:41:47.000
And the behavior is that pandas will return all rows where the date in the index has some notion of belonging to the date or period that we passed in. We saw this before when we asked for the month of August 2017, we got all rows that if you were to ask somebody is this date in August 2017. The answer would be true for all 31 of those rows that we got back. So just how we've used the colon operator in other indexing operations to specify a range of values to extract we can do the same with dates.

69
00:41:36.000 --> 00:42:21.000
So just how we've used the colon operator in other indexing operations to specify a range of values to extract we can do the same with dates. Here we're going to extract the pricing information. Starting at April 1st 2015, going through April 10th 2015. Notice here that the endpoints April 1 and April 10 are both included. This is slightly different than what you would see with a Python list or a NumPy array where the end point is not included here. Everything is inclusive on both bad days. But again, we only passed one argument.

70
00:42:18.000 --> 00:43:07.000
But again, we only passed one argument. This was used to filter rows and all columns were returned. We'll introduce this next exercise here. So for each item in this Markdown list that appears below, extract the data specified in the bullet point using or from the BTC USD data frame using the dot lock accessor. So we'd like for you to get the data starting at July 2017 and going through August 2017. We'd like for you to go from April 25th 2015 through June 10th 2016 and then finally October 31st 2017.

71
00:42:56.000 --> 00:43:42.000
We'd like for you to go from April 25th 2015 through June 10th 2016 and then finally October 31st 2017. As with other exercises in this lecture, we will leave this one as an assignment to complete outside of the lecture. We'll now move to understanding how we can access different properties of the date information once we've made pandas aware that it should be recognized as a timestamp or date time object. The things that we may want to access are what is the month, minute, second or hour corresponding to the date.

72
00:43:33.000 --> 00:44:14.000
The things that we may want to access are what is the month, minute, second or hour corresponding to the date. When we have the information as the only level on the index, we can use the data frame dot index to access the index and then we can use dot blank. Where blank could be year, month, or whatever part of the date we'd like to access. Let's see an example. We can look at the BTC USD dot index dot year, which will start from 2014 because our earliest dates were in May of 2014 and then it will continue on through 2020.

73
00:44:00.000 --> 00:44:47.000
We can look at the BTC USD dot index dot year, which will start from 2014 because our earliest dates were in May of 2014 and then it will continue on through 2020. We can also ask for the day and here we get a one corresponding to the first day of a month and then at the very end we're going to see here that this 31 corresponds to October 31 2020. And then we see here represented the first five days of November 2020. We can do the same thing if the date time information is actually a column of the data frame instead of the index, but we have one more step.

74
00:44:34.000 --> 00:45:18.000
We can do the same thing if the date time information is actually a column of the data frame instead of the index, but we have one more step. So before we were able to do DF and then access the index and here we were placing that with bracket and then pass a string for the column name. And then the extra step here is that we can't just go straight to dot year or dot day, we have to insert an extra dot dT. And what this does is the first step, once we've accessed the column, Paners will give us a series, the D type of the series will be something like PD dot time stamp.

75
00:45:06.000 --> 00:45:56.000
And what this does is the first step, once we've accessed the column, Paners will give us a series, the D type of the series will be something like PD dot time stamp. Now, whenever Paners has a series with a timestamp, D type, there is a special property dot dT. And then allows us to go in and access different components of the date time, such as the year month or hour. See how this works. For example, so we're going to start with the BTC USD data frame and then we'll call reset index, which will move the date values from being the index and now they're actually a column sitting alongside the pricing and volume information.

76
00:45:40.000 --> 00:46:20.000
For example, so we're going to start with the BTC USD data frame and then we'll call reset index, which will move the date values from being the index and now they're actually a column sitting alongside the pricing and volume information. So we can see here as we show the first five rows that this transformation happened. Now, we can access the column as follows. Once we've done that, we can use our dot dT to get at the date time properties of this column. And then we can ask for which property we can top end is which property we're looking for here it's the year.

77
00:46:14.000 --> 00:46:56.000
And then we can ask for which property we can top end is which property we're looking for here it's the year. We do that we get that the first five years are 2014, which is consistent with what we're seeing up there. We can do the same thing with month the only thing changing between these two code cells as we've swapped out year for month. And we should see five reported because for these first five rows, they month is all May or the fifth month of the year. The next topic we'll cover is the notion of shifting the rows of the data around or constructing leads or lags of the data.

78
00:46:46.000 --> 00:47:28.000
The next topic we'll cover is the notion of shifting the rows of the data around or constructing leads or lags of the data. Now, the motivation here is that often when we're doing time series analysis, we may want to compare the data at one date against the data at a different date. If these dates are on the index, this can be somewhat difficult because as you recall, pandas will align arithmetic operations and other operations based on the index values. So if, for example, we wanted to compute the percent change in Bitcoin price from one day to the next.

79
00:47:20.000 --> 00:48:10.000
So if, for example, we wanted to compute the percent change in Bitcoin price from one day to the next. We can't do this directly using a series because the index values would need to be a little bit different, it needs to be shifted so that when we say for example, we want to compute the percent change between July 4th, 2020 and July 5th, 2020. What we would really need to do is have one series representing the actual date on July 4th, the actual closing price, and then we'd need to have a different data set showing the actual closing price from July 5th, but have the index show July 4th.

80
00:47:51.000 --> 00:48:35.000
What we would really need to do is have one series representing the actual date on July 4th, the actual closing price, and then we'd need to have a different data set showing the actual closing price from July 5th, but have the index show July 4th. So the only do the operation, everything is aligned for us. So thankfully pandas makes this quite easy and convenient for us using a method called shift. Now if we just call shift on a date to time indexed data frame, what it will do is it will roof all the data forward one period and fill the rows with missing data.

81
00:48:22.000 --> 00:49:04.000
Now if we just call shift on a date to time indexed data frame, what it will do is it will roof all the data forward one period and fill the rows with missing data. So let's see what this looks like. We'll look at the first five rows of RBTC USD data frame and notice that on the first row, the opening price was $449. Now if we shift the data and then look at the first five rows, what we'll see here, is now that the first row is entirely missing data and the date correspond, or sorry, the value corresponding to the date may second 2014.

82
00:48:47.000 --> 00:49:44.000
Now if we shift the data and then look at the first five rows, what we'll see here, is now that the first row is entirely missing data and the date correspond, or sorry, the value corresponding to the date may second 2014. Now reads that $449.00. So what happened was these four rows highlighted up top were all shifted down one date. So now the 449 instead of corresponding to the actual date of May 1st is now lined up with May 2nd. Similarly the data for May 4th at $4.39. Now applies to May 5th. So we see here that pandas has kept the index exactly what it was in the original date of frame and moved every row down one.

83
00:49:37.000 --> 00:50:17.000
So we see here that pandas has kept the index exactly what it was in the original date of frame and moved every row down one. The reason we get empty data here on the first row, we get this NAN representing a missing data point is because in order to do this properly, pandas would have needed to know what the price was for Bitcoin in terms of US dollars on March 31st 2014, but that data doesn't appear in the data frame. So pandas doesn't have access to it, therefore it's missing, which is represented here as NAN or NAN.

84
00:50:06.000 --> 00:51:00.000
So pandas doesn't have access to it, therefore it's missing, which is represented here as NAN or NAN. So now that we have this, we can do the example we mentioned before. We can compute the one day at a time percent change between Bitcoin and the United States dollar. So we'll see here that between the first and the second, there was a 2% increase in the price of Bitcoin. And the next days, the price goes down. This is consistent here, between the first and the second we see an increase. We go from 449 to 460 and then after that we continue to fall for 60, 452, 439, 435.

85
00:50:48.000 --> 00:51:47.000
We go from 449 to 460 and then after that we continue to fall for 60, 452, 439, 435. The ability to shift the data made it possible for us to leverage the fact that pandas aligns arithmetic using the index to compute this percent change correctly. Now if we set the argument, if we pass an argument to the shift function, this tells us how many periods we would like the data to be shifted. In this case, if we pass the number 3 here, we see that the data is shifted down 3 rows. So now that $449 opening price moved from back here on May 1st down to May 4th.

86
00:51:30.000 --> 00:52:33.000
In this case, if we pass the number 3 here, we see that the data is shifted down 3 rows. So now that $449 opening price moved from back here on May 1st down to May 4th. And, implicitly, with no argument, the default value is 1, meaning everything shifts down 1 row. Now if we shift the data by a negative value, what we'll see here is that all the data moves up by however many by by the absolute value that we pass in here. So passing a negative 2 moved this $439 price from May 4th up to May 2nd. It's easier to see the shifting up if we look at the last few rows of the data frame, because now we see that down at the end of our data frame, we have missing data for November 4th and 5th of 2020.

87
00:52:18.000 --> 00:53:11.000
It's easier to see the shifting up if we look at the last few rows of the data frame, because now we see that down at the end of our data frame, we have missing data for November 4th and 5th of 2020. Because we shifted all rows up by 2. Now explain this next exercise. So what we would like for you to do is to use the shift function to determine the week in the past five years or in the five year horizon for which we have data. It has the largest percent change in the volume of trades. So the total quantity of Bitcoin traded. This can be found in the volume of parentheses BTC column.

88
00:52:59.000 --> 00:53:38.000
It has the largest percent change in the volume of trades. So the total quantity of Bitcoin traded. This can be found in the volume of parentheses BTC column. So that's the first ask we have second. We'd like you to repeat that, but do it at the biweekly and the monthly frequencies. So here's a hint. First we have data only at a daily frequency and you should use the fact that one week is equal to seven days when you're solving the first two asks though one week change in volume and then the biweekly change in volume.

89
00:53:19.000 --> 00:54:02.000
So here's a hint. First we have data only at a daily frequency and you should use the fact that one week is equal to seven days when you're solving the first two asks though one week change in volume and then the biweekly change in volume. Then our second hint is why you can't do the month exactly because not all months have the same number of days for this exercise you should approximate a month by 30 days. We will now talk about doing rolling computations or moving window computations on our time aware data frames.

90
00:53:53.000 --> 00:54:39.000
We will now talk about doing rolling computations or moving window computations on our time aware data frames. So pandas makes the computation of these type of statistics very simple and we find that it's easiest to understand how this works by using an example. What we'll do is we'll take the first six rows of our BTC USD data frame and we'll call that BTC small. Now we've shown here the entirety of this data frame just the six rows and seven columns. So now what we're going to do is we're going to compute the two day moving average for all the columns. So here.

91
00:54:31.000 --> 00:55:15.000
So now what we're going to do is we're going to compute the two day moving average for all the columns. So here. Well, how and the way we do this is we say. Starting with the BTC small data frame we're going to access the dot rolling method. We'll pass it in argument of two D meaning two days and then we will compute the mean. And what happens here is the following pandas will look at each row. And it will try it will then look backwards the number of days and apply the average so here we see and I'm going to show you BTC small.

92
00:55:04.000 --> 00:55:43.000
And it will try it will then look backwards the number of days and apply the average so here we see and I'm going to show you BTC small. So that we can kind of try to understand what's going on. So you'll see here that on the first row we get back exactly the same data we started with. The reason for this is that the. There was not a day to look back in order to form this two day window. So pandas used all of the two day window that it could and returned us exactly the data we started with. Now things are different on the second row.

93
00:55:40.000 --> 00:56:29.000
Now things are different on the second row. So when pandas got to the second row corresponding to May 2nd, but it did was it looked at this day and then one back making a two day window and it took the average of these two values. So we see here that the average of 449 and then 460.9 is 454.9. So this is the value that pandas computed here and we could do similar things for each of the other columns in our data frame. If we look at the next row for May 3rd, we're going to see that the average between 252 and a sharp 452 and 460 is roughly 4506, which was what computed here on this row.

94
00:56:14.000 --> 00:57:07.000
If we look at the next row for May 3rd, we're going to see that the average between 252 and a sharp 452 and 460 is roughly 4506, which was what computed here on this row. And pandas will then continue on through the rest of the rows in the data frame. Starting at each row and looking backward the number of days we specified and then computing the average. So we did this for each column and each row in our data frame. Let's kind of visualize what this looks like by constructing a plot. What we have here is we have the open column, the opening price and we're plotting the raw data in this red dotted line down here.

95
00:56:55.000 --> 00:57:50.000
What we have here is we have the open column, the opening price and we're plotting the raw data in this red dotted line down here. And we're going to compute a 21 day rolling maximum of the open column and then we'll plot it here in the blue line. Well then add a legend that helps us keep track of which mine corresponds to which series and we'll see here and I'm actually going to make this a little bit bigger. But pandas is tracking for us over a three week horizon. The kind of the peak of the red line. So this blue line continues to be kind of almost a step function. So in this example the red line.

96
00:57:38.000 --> 00:58:26.000
The kind of the peak of the red line. So this blue line continues to be kind of almost a step function. So in this example the red line. A local maximum right about here where my cursor is and then for the next 21 days the blue line stayed flat right at that local maximum. And then it fell again to the next local maximum, which we find right here. And you can see this same pattern appearing throughout the graph. In addition to applying a built-in aggregator like mean, we can actually ask that Pandas apply a custom aggregation function that we define.

97
00:58:21.000 --> 00:59:06.000
that Pandas apply a custom aggregation function that we define. This is similar to what we saw when we did group by and then we used the apply method of a group by object. So here, what we'll do is we'll define a function called is volatile. It has the following behavior. If the variance of the input is greater than one, we'll say that yes, the data was volatile, the data in X is volatile. If the variance is less than one, we'll return 0. So now what we'll do is for our BTC small data frame, we will do a two-day rolling window and apply the

98
00:59:01.000 --> 00:59:47.000
we will do a two-day rolling window and apply the is volatile method. So we see here that the first row has a zero all across. Now the reason for this is because when we do the two-day rolling window, the very first row has no previous day to be combined with. So X is actually just going to be the one value corresponding to May first. The variance of a sequence of length one is always going to be equal to zero exactly. So when we computed the variance and asked was zero greater than one, this always returned false and we get back zero.

99
00:59:44.000 --> 01:00:24.000
this always returned false and we get back zero. That's why we see zero here for every row. Now we'll see that for each other row, the opening price was volatile according to our definition for the other five days in our sample. For the other five two-day windows in our sample, I should say. The only exception to this would be for the closing price between May 4th and May 3rd. The volumes were also quite volatile according to this measure. Now we have a fairly a fun exercise that we'd like for you to do and we'll explain how it works.

100
01:00:19.000 --> 01:00:57.000
Now we have a fairly a fun exercise that we'd like for you to do and we'll explain how it works. So with that for you to imagine that you were given access to the TARDIS time machine from the TV series Doctor Who. And you were told that you are allowed to use the TARDIS only once to go back in time and return to the present subject to the following conditions. First, you may travel back to any day in the past. It's totally your choosing. On that day, the only action you can take is to purchase one bitcoin at market open.

101
01:00:50.000 --> 01:01:34.000
On that day, the only action you can take is to purchase one bitcoin at market open. You then have to take the time machine 30 days into the future and sell your bitcoin at market close. You can then return to the present and pocket the profits. And your objective is to pick which day in the data we have represented, you would choose to travel back to in order to maximize the total profits you can gain by buying and selling one bitcoin with 30 days in between. Now the question is how would you pick the day?

102
01:01:32.000 --> 01:02:06.000
Now the question is how would you pick the day? And this is the kind of bulk of the exercise and we would like for you to think carefully about what you would need to compute in order to make the optimal choice. And to solidify the idea in your mind for what you need to compute, we would like for you to write down your answer here in this Markdown cell. You can double click into it and then replace this bold text your answer here with your description of what you'd like to be able to compute in order

103
01:02:00.000 --> 01:02:39.000
bold text your answer here with your description of what you'd like to be able to compute in order to make your optimal decision in this scenario. We do have a note of caution. Don't look too far down in the notebook because we actually write out the answer in a minute. But do this thought exercise and see what you come up with. As with other exercises, we'll leave this as an assignment for you to complete on your own. Now our solution our answer to this question was that in order to make the optimal decision,

104
01:02:32.000 --> 01:03:21.000
Now our solution our answer to this question was that in order to make the optimal decision, we need to know the maximum difference in price. A close of a window of 30 days compared to the opening price of the start of the window. In other words, for each date in our time series, we would like to look at the closing price for that date. Look back 30 days and compute the opening price from 30 days prior and then compute the difference. The date we will pick would be 30 days prior to the max of that answer.

105
01:03:14.000 --> 01:03:53.000
The date we will pick would be 30 days prior to the max of that answer. Here's the next exercise. We would like for you to do the following. We would like for you to write a pandas function that implements your strategy or the one we just described. We would like for you to pass it to the ag method of rolling BTC, which we've computed for you down here. We'd like for you to extract the open column from the result and then find the date corresponding to the maximum. Once you've done that, you will have your answer to which day you should

106
01:03:48.000 --> 01:04:31.000
to the maximum. Once you've done that, you will have your answer to which day you should travel back to and you also will be able to answer the question of how much money you would have made where you'd given this time traveling opportunity. We've just seen how we can leverage the pandas time series functionality to do rolling or moving window computations. We can also have pandas do what's called resampling for us. Resampling is another way to say changing the frequency of the data. So for example, instead of computing a monthly moving average or a 30 day window

107
01:04:24.000 --> 01:05:14.000
So for example, instead of computing a monthly moving average or a 30 day window moving average, we may want to be able to answer questions like what was the average price of Bitcoin for each calendar month. In order to compute statistics like this, we don't want a moving window where we just want to resample the frequency at which we represent the data. Let's see some examples of how this works. So we can use our BTCUSD data frame and now instead of the rolling method, we're going to use the resample method. The argument here should be some notion of the frequency that we would like the data

108
01:05:07.000 --> 01:05:55.000
resample method. The argument here should be some notion of the frequency that we would like the data to be expressed in. So currently our BTCUSD data is in a daily frequency and by passing BQ here, we're saying that we would like for the data to be represented in a business quarterly frequency. So let's do this and then compute the mean or average for each business quarter. You notice here that we have far fewer rows. We don't have the 2000 plus rows. It looks like we have a couple of dozen. And now we have data only for the end of the month, June, September, December,

109
01:05:43.000 --> 01:06:35.000
a couple of dozen. And now we have data only for the end of the month, June, September, December, and March. And so we'll have four rows for 2015, for 1617, 1819 and also 2020. We only have three rows for the first year 2014 because the data started in May or month five. So we have no observations in the first quarter of the year, which would be months January, February, and April. So we have a couple of different numbers. We have a couple of different numbers. Now we want to point out a key difference between the resample method we just saw and the rolling

110
01:06:29.000 --> 01:07:14.000
Now we want to point out a key difference between the resample method we just saw and the rolling method we were using earlier. With resample, a single number was returned per column for each window that we considered. For an example we just looked at it was a business quarter. The resample method, it will actually alter the frequency of the data and the number of rows will be different from the data friendly past in. On the other hand, with the rolling form of computation and aggregation, the number of rows in the output will match the number of rows in the input

111
01:07:08.000 --> 01:07:55.000
and aggregation, the number of rows in the output will match the number of rows in the input as well the frequency of the index. So we can actually sample at different frequencies. Business quarterly is not the only one. And once we have it, we can do multiple aggregations at the same time. So for example, suppose we wanted to do semi-annually or two business quarters, we could resample starting with 2bq. And the S here says we would like the dates in the output to be the start of each two quarter window. If you recall from our previous example,

112
01:07:48.000 --> 01:08:30.000
to be the start of each two quarter window. If you recall from our previous example, we saw that we had the end of months. June, September, December and March. Now here, we're going to be seeing the beginning of months. Now we're going to ask for instead of the average who would like for the min and the max for each of these two quarter windows. So now here we can only see April and October for each year. And we have a hierarchical index on the columns where we have the original column names as the outer most layer. And on the inner layer, we have the two different

113
01:08:24.000 --> 01:09:07.000
original column names as the outer most layer. And on the inner layer, we have the two different aggregation methods we requested. So we'll see here that in the first half of 2014, the difference in the opening price of Bitcoin in US dollars was roughly just shy of $300. And we'll see that this gap keeps quite extreme later in the time series where in the first half of 2019, this was an $8,000 difference in the opening price from the start of the quarter, restart of the year to this end of the start of the second quarter.

114
01:09:02.000 --> 01:09:48.000
restart of the year to this end of the start of the second quarter. We now have another exercise. So let's revisit the thought experiment where we have access to this amazing time machine. You're again given access to it to make one travel through time, but this time your conditions are a bit different. So now what you're allowed to do is travel back to the first day of any month in the past. On that day, you can purchase one Bitcoin at the market opening. You're then allowed to travel to any day in that calendar month

115
01:09:42.000 --> 01:10:34.000
at the market opening. You're then allowed to travel to any day in that calendar month and sell the Bitcoin at market close. Once you've completed these two transactions, you will then return home to the present and you can keep the profit you made in your pocket. And your objective is to again maximize the profit from these two transactions. The question we would have for you is to which month would you travel? And on which day of the month would you return to sell your Bitcoin? Just as we did last time, we would like for you to take a moment to write your thoughts.

116
01:10:28.000 --> 01:11:09.000
Just as we did last time, we would like for you to take a moment to write your thoughts. Determine exactly what you would need to be able to know or compute in order to make an optimal decision and take a moment to write it in the market on sell below. Our response to the thought question that was just posed is that in order to make an optimal decision, we would need to compute the following. For each month in our data set, we would like to compute the maximum difference between the closed price on any day in that month

117
01:11:02.000 --> 01:11:51.000
we would like to compute the maximum difference between the closed price on any day in that month and the opening price on the first day of the month. Once we've done this, we simply look for the maximum value across all months for that statistic and that's the month that we would like to visit. Now here in this next exercise, you may or may not be surprised. We want you to do the experiment. In particular, we would like for you to do these four steps. First, we would like for you to create the body of this Pandas function. We've called Monthly

118
01:11:44.000 --> 01:12:26.000
First, we would like for you to create the body of this Pandas function. We've called Monthly Value. This should implement the strategy that was described previously. Then we would like to for you to pass in the function to the Ag method of the Resample of BTC object computed for you below. We then for like for you to extract the open column from the result find the date with the maximum value and then return the amount of money that you would have made. Now a couple questions you might ask was, is this strategy more or less profitable

119
01:12:19.000 --> 01:13:07.000
Now a couple questions you might ask was, is this strategy more or less profitable than the rolling window computation we did previously, if so by how much? And then also tell us which day you needed to return to within your chosen month in order to sell at the optimal closing price. The final note that we'll leave with here is with regards to the API key we saw I used at the very beginning of our lecture. If you look above in the notebook, you'll notice that we were able to set the API key to a value with this strange looking string. This is our API key or our

120
01:12:59.000 --> 01:13:49.000
were able to set the API key to a value with this strange looking string. This is our API key or our credentials to authenticate us as users with the quantum service. When we did this, when we executed this line of code, we told the quantum library that anytime it was interacting with the quantum rest API that it should use the API key we specified in that string. API keys are very much like passwords and some APIs like a quantum require you to specify the key or password when making a request for data. By using this key, we were able to obtain the Bitcoin

121
01:13:38.000 --> 01:14:24.000
key or password when making a request for data. By using this key, we were able to obtain the Bitcoin USD $1 exchange rate directly from a quantum and we'll just point out that the API key listed in this notebook was one that were your requests specifically for use in this class. The request was free, it didn't cost us anything but this is the only thing for which we use this particular API key. If you plan on using quantum more extensively, you may run into some limits because this key is shared with other users and quantum does limit the number of requests each key can make.

122
01:14:18.000 --> 01:15:03.000
shared with other users and quantum does limit the number of requests each key can make. If you do end up using quantum for more data collection efforts, we recommend that you go to their website and request for yourself your own free API key and then replace the line of code in the notebook that sets the quandle.apiconfig.apikey variable and on the right hand side instead of using our API key, you would use the one that quandle gave you directly. That's it for our lecture today. We hope you enjoyed learning about time series operations and pandas and stay tuned for more

123
01:14:55.000 --> 01:15:11.000
We hope you enjoyed learning about time series operations and pandas and stay tuned for more time series operations when we get to learn about some of the statistical methods that arise in the time series domain.

