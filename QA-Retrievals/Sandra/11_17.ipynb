{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c27c50b-c68b-4681-99f1-6aa80383df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from lyon_common import create_chain, report_on_message\n",
    "from question_generator_model import SingleSelection, Code, AnyQuestion, FillInBlank, MultipleSelection\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import ValidationError\n",
    "import json\n",
    "from json.decoder import JSONDecodeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c49c4a-baa6-4494-9bce-ef8b0a0cbcc4",
   "metadata": {},
   "source": [
    "**NOTE** I have updated `lyon_common.py` so that the `create_chain` function accepts a few more arguments:\n",
    "\n",
    "- `model_name` now defaults to the GPT 3.5 turbo model from 2023-11-06 \n",
    "- `model_kwargs`: this is a dictionary with OpenAI specific parameters we can set. The default value is now to instruct OpenAI to use the [new `json_object` response format ](https://platform.openai.com/docs/guides/text-generation/json-mode), which modifies the available tokens so that at each step only valid JSON can be produced.\n",
    "- `verbose` is a new keyword argument that is set to `False` by default.\n",
    "\n",
    "This notebook shows how we can use these updates to generate valid Pydantic ready JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dbfae4d-e836-47ef-8e07-e9048abae14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt_1(pydantic_object):\n",
    "    common_system_prompt = textwrap.dedent(\"\"\"\n",
    "    You are a smart, helpful teaching assistant chatbot named AacdemiaGPT.\n",
    "\n",
    "    You are an expert Python programmer with 15+ years of experience and have used all the most popular\n",
    "    libraries for data analysis, machine learning, and artificial intelligence.\n",
    "\n",
    "    You assist professors that teach courses about Python, data science, and machine learning\n",
    "    to college/ university students.\n",
    "\n",
    "    Your task is to help professors produce practice questions to help students solidify \n",
    "    their understanding of specific topics.\n",
    "\n",
    "    You must always generate questions that have more than one option as solution for the MultipleSelection question type.\n",
    "\n",
    "    In your conversations with a professor you will be given a topic (string) and an\n",
    "    expected difficulty level (integer)\n",
    "    \n",
    "    The difficulty will be a number between 1 and 3, with 1 corresponding to a request \n",
    "    for an easy question, and 3 for the most difficult question.\n",
    "\n",
    "     Example of a difficulty 1 question is given below.\n",
    "    \n",
    "    \"How would you reverse the order of the following list in python\n",
    "    \n",
    "    ```python\n",
    "    a = [1, 'hi', 3, 'there']\n",
    "    ```\n",
    "    and save the result in an object `b`\"?\n",
    "\n",
    "    \n",
    "    Example of difficulty 3 question is given below.\n",
    "    \n",
    "    \"You are given a 3 dimensional numpy array as specified below:\n",
    "    \n",
    "    ```\n",
    "    A = np.array([[[0.0, 1.0], [2.0, 3.0]], [[4.0, 5.0], [6.0, 7.0]]])\n",
    "    ```\n",
    "    \n",
    "    Create a variable `idx` (define as a tuple) that you could use to select the `4.0` element of this array.\n",
    "    \n",
    "    For example,\n",
    "    ```\n",
    "    idx = (0, 0, 0)\n",
    "    ```\n",
    "    would select the `0.0` element of the array.\"\n",
    "\n",
    "    \n",
    "    \n",
    "    If the professor asks you for a question and does not specify either a new topic \n",
    "    or a new difficulty, you must use the previous topic or difficulty.\n",
    "\n",
    " \n",
    "    Occasionaly the professor may ask you to do something like produce a similar question,\n",
    "    or  make the question more difficult or easy. You need to assist the professor with \n",
    "    the same. You must use the previous topic to do this.\n",
    "    \n",
    "    If the professor ask for more than one question in a single message, you need to apologize and \n",
    "    inform that you can only generate one question at a time. You need to also ask the professor to \n",
    "    put in a new message with the topic and difficulty to generate a new question.\n",
    "\n",
    "    If the topic is available in the given information, refer that for giving a response to the professor. \n",
    "    You are encouraged to use any tools available to look up relevant information, only\n",
    "    if necessary. \n",
    "\n",
    "    You will apologize if you're unable to generate an output that meets professor's requirement\n",
    "\n",
    "    Your responses must always exactly match the specified JSON format with no extra words or content.\n",
    "\n",
    "    You must always produce exactly one JSON object.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
    "    return common_system_prompt.format(format_instructions=parser.get_format_instructions())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48fc0942-4adf-40ce-9bb3-d50dfc57c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_parse_question(pydantic_model, query):\n",
    "    rag_chain = create_chain(create_system_prompt(pydantic_model), temperature=0.1, verbose=True, model_name=\"gpt-4-1106-preview\")\n",
    "    \n",
    "    try:\n",
    "        response = rag_chain(query)\n",
    "        report_on_message(response)  # print a summary of what was produced\n",
    "        parser = PydanticOutputParser(pydantic_object=pydantic_model)\n",
    "        return parser.parse(response[\"output\"])\n",
    "    except ValidationError as ve:\n",
    "        print(f\"Pydantic validation error: {ve}\")\n",
    "        # If Pydantic validation fails, fallback to json.loads\n",
    "        return json.loads(response[\"output\"])\n",
    "    except JSONDecodeError as json_error:\n",
    "        # If JSON decoding fails, perform json.loads and inform the caller about the error\n",
    "        result_output = json.loads(response[\"output\"])\n",
    "        print(f\"JSON decoding error: {json_error}\")\n",
    "        return result_output\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle other exceptions and fallback to json.loads\n",
    "        return json.loads(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5081d0cb-6d44-4817-ad41-80b4902e5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `pandas groupby`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content=\"We're going to use this example data set to demonstrate the three steps in split apply combine. To begin, we'll start with the split step. In order to ask pandas to split the data for us, we use the group by method of a data frame. You see here that we're calling DF dot group by and we're passing the string A. This instructs pandas to construct groups of our data using the values from the A column. This is the most basic and often most used form of the group by method to split on the values of a single column. We can check the type of this GBA object. And we see here a very long type name but we're just going to refer to this as a group by for short. Once we have a group by object, there are a few things we can do with it. One thing we could do is we could ask to get the subset of data for a particular group. So here we're going to ask to get, we're going to say DBA dot get group and we're going to pass one and then we'll pass two. And notice when we do this, that the data we get in return has all the rows of our original\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 5, 'timestamps': '00:05:40 --> 00:06:59'}), Document(page_content=\"like to form a calculation based on all the data for a particular hour. In this case we would have to instruct pandas that we would like to use the timestamp column but we'd like to group it in buckets of one hour at a time. We could similarly ask it to group it in buckets of four hours or a day or a week. And this is not expressible by just passing the column name timestamp. We also have to add in the frequency that we'd like. And pandas does enable this behavior using the PD.grouper type. This is what we'll learn about now. So in order to see it in action we're going to make a copy of our data frame and we're going to move the a column to the index and we're going to add a date column. So let's just go ahead and show you what we end up with. So notice that the a column has that or sorry the a column is now in the index with this original values of 1, 1, 1, 2, 2, 2, 2. The B and C columns are unchanged but now we have a date column that has some dates between 2020, 1222. Okay. Now that we have this date of frame we can use the PD.grouper to group by year.\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 21, 'timestamps': '00:24:45 --> 00:26:07'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the data into 1-hour intervals and sum the values in a column 'value'?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.hour).sum()\\\",\\n    \\\"df.resample('H', on='timestamp').sum()\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='H')).sum()\\\",\\n    \\\"df.groupby('timestamp').sum()\\\"\\n  ],\\n  \\\"solution\\\": [1, 2]\\n}\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the data into 1-hour intervals and sum the values in a column 'value'?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.hour).sum()\\\",\\n    \\\"df.resample('H', on='timestamp').sum()\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='H')).sum()\\\",\\n    \\\"df.groupby('timestamp').sum()\\\"\\n  ],\\n  \\\"solution\\\": [1, 2]\\n}\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Pydantic validation error: 5 validation errors for MultipleSelection\n",
      "question_text\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "difficulty\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "topics\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "choices\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "solution\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \"question_text\": \"Given a DataFrame `df` with a datetime column \\'timestamp\\', how would you group the data into 1-hour intervals and sum the values in a column \\'value\\'?\",\\n  \"difficulty\": 3,\\n  \"topics\": [\"pandas\", \"groupby\"],\\n  \"choices\": [\\n    \"df.groupby(df[\\'timestamp\\'].dt.hour).sum()\",\\n    \"df.resample(\\'H\\', on=\\'timestamp\\').sum()\",\\n    \"df.groupby(pd.Grouper(key=\\'timestamp\\', freq=\\'H\\')).sum()\",\\n    \"df.groupby(\\'timestamp\\').sum()\"\\n  ],\\n  \"solution\": [1, 2]\\n}'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"topic: pandas groupby, difficulty: 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5d5dea3-6eec-479d-b2b1-8eca9a45ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `pandas groupby`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content=\"We're going to use this example data set to demonstrate the three steps in split apply combine. To begin, we'll start with the split step. In order to ask pandas to split the data for us, we use the group by method of a data frame. You see here that we're calling DF dot group by and we're passing the string A. This instructs pandas to construct groups of our data using the values from the A column. This is the most basic and often most used form of the group by method to split on the values of a single column. We can check the type of this GBA object. And we see here a very long type name but we're just going to refer to this as a group by for short. Once we have a group by object, there are a few things we can do with it. One thing we could do is we could ask to get the subset of data for a particular group. So here we're going to ask to get, we're going to say DBA dot get group and we're going to pass one and then we'll pass two. And notice when we do this, that the data we get in return has all the rows of our original\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 5, 'timestamps': '00:05:40 --> 00:06:59'}), Document(page_content=\"like to form a calculation based on all the data for a particular hour. In this case we would have to instruct pandas that we would like to use the timestamp column but we'd like to group it in buckets of one hour at a time. We could similarly ask it to group it in buckets of four hours or a day or a week. And this is not expressible by just passing the column name timestamp. We also have to add in the frequency that we'd like. And pandas does enable this behavior using the PD.grouper type. This is what we'll learn about now. So in order to see it in action we're going to make a copy of our data frame and we're going to move the a column to the index and we're going to add a date column. So let's just go ahead and show you what we end up with. So notice that the a column has that or sorry the a column is now in the index with this original values of 1, 1, 1, 2, 2, 2, 2. The B and C columns are unchanged but now we have a date column that has some dates between 2020, 1222. Okay. Now that we have this date of frame we can use the PD.grouper to group by year.\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 21, 'timestamps': '00:24:45 --> 00:26:07'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the data into 1-hour intervals and sum the values in a column 'value'?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.hour).sum()\\\",\\n    \\\"df.resample('H', on='timestamp').sum()\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='H')).sum()\\\",\\n    \\\"df.groupby('timestamp').sum()\\\"\\n  ],\\n  \\\"solution\\\": [1, 2]\\n}\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the data into 1-hour intervals and sum the values in a column 'value'?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.hour).sum()\\\",\\n    \\\"df.resample('H', on='timestamp').sum()\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='H')).sum()\\\",\\n    \\\"df.groupby('timestamp').sum()\\\"\\n  ],\\n  \\\"solution\\\": [1, 2]\\n}\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Pydantic validation error: 5 validation errors for MultipleSelection\n",
      "question_text\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "difficulty\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "topics\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "choices\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "solution\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \"question_text\": \"Given a DataFrame `df` with a datetime column \\'timestamp\\', how would you group the data into 1-hour intervals and sum the values in a column \\'value\\'?\",\\n  \"difficulty\": 3,\\n  \"topics\": [\"pandas\", \"groupby\"],\\n  \"choices\": [\\n    \"df.groupby(df[\\'timestamp\\'].dt.hour).sum()\",\\n    \"df.resample(\\'H\\', on=\\'timestamp\\').sum()\",\\n    \"df.groupby(pd.Grouper(key=\\'timestamp\\', freq=\\'H\\')).sum()\",\\n    \"df.groupby(\\'timestamp\\').sum()\"\\n  ],\\n  \"solution\": [1, 2]\\n}'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"topic: pandas groupby, difficulty: 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d6dffa4-054a-426f-8cd3-b9dcd1fc201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `pandas groupby`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content=\"We're going to use this example data set to demonstrate the three steps in split apply combine. To begin, we'll start with the split step. In order to ask pandas to split the data for us, we use the group by method of a data frame. You see here that we're calling DF dot group by and we're passing the string A. This instructs pandas to construct groups of our data using the values from the A column. This is the most basic and often most used form of the group by method to split on the values of a single column. We can check the type of this GBA object. And we see here a very long type name but we're just going to refer to this as a group by for short. Once we have a group by object, there are a few things we can do with it. One thing we could do is we could ask to get the subset of data for a particular group. So here we're going to ask to get, we're going to say DBA dot get group and we're going to pass one and then we'll pass two. And notice when we do this, that the data we get in return has all the rows of our original\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 5, 'timestamps': '00:05:40 --> 00:06:59'}), Document(page_content=\"like to form a calculation based on all the data for a particular hour. In this case we would have to instruct pandas that we would like to use the timestamp column but we'd like to group it in buckets of one hour at a time. We could similarly ask it to group it in buckets of four hours or a day or a week. And this is not expressible by just passing the column name timestamp. We also have to add in the frequency that we'd like. And pandas does enable this behavior using the PD.grouper type. This is what we'll learn about now. So in order to see it in action we're going to make a copy of our data frame and we're going to move the a column to the index and we're going to add a date column. So let's just go ahead and show you what we end up with. So notice that the a column has that or sorry the a column is now in the index with this original values of 1, 1, 1, 2, 2, 2, 2. The B and C columns are unchanged but now we have a date column that has some dates between 2020, 1222. Okay. Now that we have this date of frame we can use the PD.grouper to group by year.\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 21, 'timestamps': '00:24:45 --> 00:26:07'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the data into 1-hour intervals and sum the values in a column 'value'?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.hour).sum()\\\",\\n    \\\"df.resample('H', on='timestamp').sum()\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='H')).sum()\\\",\\n    \\\"df.groupby('timestamp').sum()\\\"\\n  ],\\n  \\\"solution\\\": [1, 2]\\n}\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the data into 1-hour intervals and sum the values in a column 'value'?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.hour).sum()\\\",\\n    \\\"df.resample('H', on='timestamp').sum()\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='H')).sum()\\\",\\n    \\\"df.groupby('timestamp').sum()\\\"\\n  ],\\n  \\\"solution\\\": [1, 2]\\n}\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Pydantic validation error: 5 validation errors for MultipleSelection\n",
      "question_text\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "difficulty\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "topics\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "choices\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "solution\n",
      "  Field required [type=missing, input_value={'description': 'Question... \"solution\": [1, 2]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \"question_text\": \"Given a DataFrame `df` with a datetime column \\'timestamp\\', how would you group the data into 1-hour intervals and sum the values in a column \\'value\\'?\",\\n  \"difficulty\": 3,\\n  \"topics\": [\"pandas\", \"groupby\"],\\n  \"choices\": [\\n    \"df.groupby(df[\\'timestamp\\'].dt.hour).sum()\",\\n    \"df.resample(\\'H\\', on=\\'timestamp\\').sum()\",\\n    \"df.groupby(pd.Grouper(key=\\'timestamp\\', freq=\\'H\\')).sum()\",\\n    \"df.groupby(\\'timestamp\\').sum()\"\\n  ],\\n  \"solution\": [1, 2]\\n}'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"topic: pandas groupby, difficulty: 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "093e2606-3229-4c60-8557-227428bf1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(pydantic_object):\n",
    "    common_system_prompt = textwrap.dedent(\"\"\"\n",
    "    You are a smart, helpful teaching assistant chatbot named AacdemiaGPT.\n",
    "\n",
    "    You are an expert Python programmer with 15+ years of experience and have used all the most popular\n",
    "    libraries for data analysis, machine learning, and artificial intelligence.\n",
    "\n",
    "    You assist professors that teach courses about Python, data science, and machine learning\n",
    "    to college and university students.\n",
    "\n",
    "    Your task is to help professors produce practice questions to help students solidify \n",
    "    their understanding of specific topics.\n",
    "\n",
    "    You must always generate questions that have more than one option as solution for the MultipleSelection question type.\n",
    "\n",
    "    In your conversations with a professor you will be given a topic (string) and an\n",
    "    expected difficulty level (integer)\n",
    "    \n",
    "    The difficulty will be a number between 1 and 3, with 1 corresponding to a request \n",
    "    for an easy question, and 3 for the most difficult question.\n",
    "\n",
    "    If the professor asks you for a question and does not specify either a new topic \n",
    "    or a new difficulty, you must use the previous topic or difficulty.\n",
    "\n",
    "    Occasionaly the professor may ask you to do something like produce a similar question,\n",
    "    or  make the question more difficult or easy. You need to assist the professor with \n",
    "    the same. You must use the previous topic to do this.\n",
    "    \n",
    "    If the professor ask for more than one question in a single message, you need to apologize and \n",
    "    inform that you can only generate one question at a time. You need to also ask the professor to \n",
    "    put in a new message with the topic and difficulty to generate a new question.\n",
    "\n",
    "    If the topic is available in the given information, refer that for giving a response to the professor. \n",
    "    You are encouraged to use any tools available to look up relevant information, only\n",
    "    if necessary. \n",
    "\n",
    "    You will apologize if you're unable to generate an output that meets professor's requirement\n",
    "\n",
    "    Your responses must always exactly match the specified JSON format with no extra words or content.\n",
    "\n",
    "    You must always produce exactly one JSON object.\n",
    "\n",
    "    Your responses should always be consistent\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
    "    return common_system_prompt.format(format_instructions=parser.get_format_instructions())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6b8e5d5-1308-4bbb-96af-ad0cbd2e5ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `pandas groupby`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content=\"We're going to use this example data set to demonstrate the three steps in split apply combine. To begin, we'll start with the split step. In order to ask pandas to split the data for us, we use the group by method of a data frame. You see here that we're calling DF dot group by and we're passing the string A. This instructs pandas to construct groups of our data using the values from the A column. This is the most basic and often most used form of the group by method to split on the values of a single column. We can check the type of this GBA object. And we see here a very long type name but we're just going to refer to this as a group by for short. Once we have a group by object, there are a few things we can do with it. One thing we could do is we could ask to get the subset of data for a particular group. So here we're going to ask to get, we're going to say DBA dot get group and we're going to pass one and then we'll pass two. And notice when we do this, that the data we get in return has all the rows of our original\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 5, 'timestamps': '00:05:40 --> 00:06:59'}), Document(page_content=\"like to form a calculation based on all the data for a particular hour. In this case we would have to instruct pandas that we would like to use the timestamp column but we'd like to group it in buckets of one hour at a time. We could similarly ask it to group it in buckets of four hours or a day or a week. And this is not expressible by just passing the column name timestamp. We also have to add in the frequency that we'd like. And pandas does enable this behavior using the PD.grouper type. This is what we'll learn about now. So in order to see it in action we're going to make a copy of our data frame and we're going to move the a column to the index and we're going to add a date column. So let's just go ahead and show you what we end up with. So notice that the a column has that or sorry the a column is now in the index with this original values of 1, 1, 1, 2, 2, 2, 2. The B and C columns are unchanged but now we have a date column that has some dates between 2020, 1222. Okay. Now that we have this date of frame we can use the PD.grouper to group by year.\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 21, 'timestamps': '00:24:45 --> 00:26:07'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the entries by year using pandas?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\", \\\"datetime\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.year)\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='A'))\\\",\\n    \\\"df.groupby('timestamp').resample('A')\\\",\\n    \\\"df.set_index('timestamp').groupby(pd.TimeGrouper('A'))\\\"\\n  ],\\n  \\\"solution\\\": [1]\\n}\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"Given a DataFrame `df` with a datetime column 'timestamp', how would you group the entries by year using pandas?\\\",\\n  \\\"difficulty\\\": 3,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"groupby\\\", \\\"datetime\\\"],\\n  \\\"choices\\\": [\\n    \\\"df.groupby(df['timestamp'].dt.year)\\\",\\n    \\\"df.groupby(pd.Grouper(key='timestamp', freq='A'))\\\",\\n    \\\"df.groupby('timestamp').resample('A')\\\",\\n    \\\"df.set_index('timestamp').groupby(pd.TimeGrouper('A'))\\\"\\n  ],\\n  \\\"solution\\\": [1]\\n}\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Pydantic validation error: 5 validation errors for MultipleSelection\n",
      "question_text\n",
      "  Field required [type=missing, input_value={'description': 'Question...\\n  \"solution\": [1]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "difficulty\n",
      "  Field required [type=missing, input_value={'description': 'Question...\\n  \"solution\": [1]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "topics\n",
      "  Field required [type=missing, input_value={'description': 'Question...\\n  \"solution\": [1]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "choices\n",
      "  Field required [type=missing, input_value={'description': 'Question...\\n  \"solution\": [1]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "solution\n",
      "  Field required [type=missing, input_value={'description': 'Question...\\n  \"solution\": [1]\\n}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \"question_text\": \"Given a DataFrame `df` with a datetime column \\'timestamp\\', how would you group the entries by year using pandas?\",\\n  \"difficulty\": 3,\\n  \"topics\": [\"pandas\", \"groupby\", \"datetime\"],\\n  \"choices\": [\\n    \"df.groupby(df[\\'timestamp\\'].dt.year)\",\\n    \"df.groupby(pd.Grouper(key=\\'timestamp\\', freq=\\'A\\'))\",\\n    \"df.groupby(\\'timestamp\\').resample(\\'A\\')\",\\n    \"df.set_index(\\'timestamp\\').groupby(pd.TimeGrouper(\\'A\\'))\"\\n  ],\\n  \"solution\": [1]\\n}'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"topic: pandas groupby\\n difficulty: 3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
