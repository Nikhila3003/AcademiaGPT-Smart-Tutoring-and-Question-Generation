{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253efe1c-ef03-499a-a473-f79e3637e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from question_generator_model import( MultipleSelection, \n",
    "    SingleSelection, \n",
    "    Code, \n",
    "    FillInBlank)\n",
    "from lyon_common import get_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d3731f-15bb-413d-9945-e7ee58283626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import dotenv\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from typing import List, Dict, Any\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b160ff0-3c8c-4dcb-b0d8-f2bfce083237",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1a71be-703e-4db6-ab0d-b381a3d27721",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "COLLECTION_NAME = \"documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e7ebe2-6b87-4122-abf0-f64735221e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore= get_vectorstore()\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d032a43f-a05d-43ec-802c-05075b0f4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the parsers\n",
    "ms_parser = PydanticOutputParser(pydantic_object=MultipleSelection)\n",
    "code_parser = PydanticOutputParser(pydantic_object=Code)\n",
    "ss_parser = PydanticOutputParser(pydantic_object=SingleSelection)\n",
    "fib_parser = PydanticOutputParser(pydantic_object=FillInBlank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6926a08-f9f4-4171-a567-07a93fe6a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the system message\n",
    "common_system_prompt = \"\"\"You are a smart, helpful teaching assistant chatbot named Callisto.\n",
    "\n",
    "You assist professors that teach courses about Python, data science, and machine learning\n",
    "to graduate students.\n",
    "\n",
    "You have 5+ years of experience writing Python code to do a variety of tasks. \n",
    "\n",
    "Your responses typically include examples of datasets or code snippets.\n",
    "\n",
    "For each message you will be given two inputs\n",
    "\n",
    "topic: string\n",
    "difficulty: integer\n",
    "\n",
    "Your task is to produce practice questions to help students solidify their understanding of the provided topic\n",
    "\n",
    "The difficulty will be a number between 1 and 3, with 1 corresponding to a request for an easy question, and 3 for the most difficult question.\n",
    "\n",
    "If the user asks you for another question and does not specify either a new topic or a new difficulty, you must use the previous topic or difficulty\n",
    "\n",
    "Your responses must always exactly match the specified format with no extra words or content.\n",
    "\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62bce693-ac38-40b1-a07d-1a7d8d75c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the main function that will be called\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "def build_llm_for_pydantic_model(model_class):\n",
    "    parser = PydanticOutputParser(pydantic_object=model_class)\n",
    "    system = SystemMessagePromptTemplate.from_template(common_system_prompt)\n",
    "    human = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages = [system, MessagesPlaceholder(variable_name=\"history\"), human],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\")\n",
    "    \n",
    "    document_variable_name = \"context\"\n",
    "\n",
    "    stuff_prompt_override = \"\"\"Given this text extracts:\n",
    "    -----\n",
    "    {context}\n",
    "    -----\n",
    "    Please answer the following question:\n",
    "    {input}\"\"\"\n",
    "    \n",
    "    prompt_1 = PromptTemplate(\n",
    "    template=stuff_prompt_override, input_variables=[\"context\", \"input\"]\n",
    ")\n",
    "    ##template = (\n",
    "    ##\"Combine the chat history and follow up question into \"\n",
    "    ##\"a standalone question. Chat History: {history}\"\n",
    "    ##\"Follow up question: {input}\"\n",
    "    ##)\n",
    "\n",
    "   ## prompt_1 = PromptTemplate(template=template, input_variables=[\"history\", \"input\"])\n",
    "    model = ChatOpenAI(temperature=0)\n",
    "    llm_chain = LLMChain(llm=model, prompt=prompt_1)\n",
    "    #combine_docs_chain= chain = StuffDocumentsChain(llm_chain=llm_chain)\n",
    "\n",
    "    combine_docs_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    ")\n",
    "    \n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"history\", \n",
    "        return_messages=True,\n",
    "        output_key=\"output\")\n",
    "    \n",
    "      \n",
    "    return  ConversationalRetrievalChain.from_llm(llm=model,retriever= retriever,memory= memory,chain_type=\"stuff\",combine_docs_chain=combine_docs_chain,\n",
    "                                                combine_docs_chain_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02fa9d7-c015-4c46-849f-69850dc94ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['history', 'input'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ss_chain \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_llm_for_pydantic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSingleSelection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m q_ss \u001b[38;5;241m=\u001b[39m ss_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifficulty: 1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtopic: scikit-learn LinearRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m q_ss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[19], line 54\u001b[0m, in \u001b[0;36mbuild_llm_for_pydantic_model\u001b[0;34m(model_class)\u001b[0m\n\u001b[1;32m     42\u001b[0m     combine_docs_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     43\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[1;32m     44\u001b[0m     document_prompt\u001b[38;5;241m=\u001b[39mdocument_prompt,\n\u001b[1;32m     45\u001b[0m     document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m     memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[1;32m     49\u001b[0m         memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     50\u001b[0m         return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m         output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mcombine_docs_chain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:360\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method to load chain from LLM and retriever.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03mThis provides some logic to create the `question_generator` chain\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        ConversationalRetrievalChain\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m combine_docs_chain_kwargs \u001b[38;5;241m=\u001b[39m combine_docs_chain_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 360\u001b[0m doc_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombine_docs_chain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    369\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    370\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    371\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    372\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    373\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    374\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/chains/question_answering/__init__.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/chains/question_answering/__init__.py:81\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     74\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     75\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['history', 'input'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "ss_chain = build_llm_for_pydantic_model(SingleSelection)\n",
    "q_ss = ss_chain.invoke(input=\"difficulty: 1\\ntopic: scikit-learn LinearRegression\")\n",
    "q_ss[\"question\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
