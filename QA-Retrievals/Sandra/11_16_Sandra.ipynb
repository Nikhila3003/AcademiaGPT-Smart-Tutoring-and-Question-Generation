{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c27c50b-c68b-4681-99f1-6aa80383df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from lyon_common import create_chain, report_on_message\n",
    "from question_generator_model import SingleSelection, Code, AnyQuestion, FillInBlank, MultipleSelection\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import ValidationError\n",
    "import json\n",
    "from json.decoder import JSONDecodeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c49c4a-baa6-4494-9bce-ef8b0a0cbcc4",
   "metadata": {},
   "source": [
    "**NOTE** I have updated `lyon_common.py` so that the `create_chain` function accepts a few more arguments:\n",
    "\n",
    "- `model_name` now defaults to the GPT 3.5 turbo model from 2023-11-06 \n",
    "- `model_kwargs`: this is a dictionary with OpenAI specific parameters we can set. The default value is now to instruct OpenAI to use the [new `json_object` response format ](https://platform.openai.com/docs/guides/text-generation/json-mode), which modifies the available tokens so that at each step only valid JSON can be produced.\n",
    "- `verbose` is a new keyword argument that is set to `False` by default.\n",
    "\n",
    "This notebook shows how we can use these updates to generate valid Pydantic ready JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dbfae4d-e836-47ef-8e07-e9048abae14c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2518905072.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    common_system_prompt = textwrap.dedent(\"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def create_system_prompt(pydantic_object):\n",
    "    common_system_prompt = textwrap.dedent(\"\"\"\n",
    "    You are a smart, helpful teaching assistant chatbot named Callisto.\n",
    "\n",
    "    You are an expert Python programmer and have used all the most popular\n",
    "    libraries for data analysis, machine learning, and artificial intelligence.\n",
    "\n",
    "    You assist professors that teach courses about Python, data science, and machine learning\n",
    "    to graduate students.\n",
    "\n",
    "    Your task is to help professors produce practice questions to help students solidify \n",
    "    their understanding of specific topics\n",
    "\n",
    "    In your conversations with a professor you will be given a topic (string) and an\n",
    "    expected difficulty level (integer)\n",
    "    \n",
    "    The difficulty will be a number between 1 and 3, with 1 corresponding to a request \n",
    "    for an easy question, and 3 for the most difficult question.\n",
    "\n",
    "   \n",
    "    \n",
    "    If the professor asks you for a new question and does not specify either a topic \n",
    "    or the difficulty, you must use the topic or difficulty that the professor previously provided.\n",
    "\n",
    "     If the professor ask for more than one question in a single message, you need to apologize and \n",
    "     inform that you can only generate one question at a time. You need to also ask the professor to \n",
    "     put in a new message with the topic and difficulty to generate a new question.\n",
    "\n",
    "    You are encouraged to use any tools available to look up relevant information, only\n",
    "    if necessary.\n",
    "\n",
    "    Your responses must always exactly match the specified JSON format with no extra words or content.\n",
    "\n",
    "    You must always produce exactly one JSON object.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
    "    return common_system_prompt.format(format_instructions=parser.get_format_instructions())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fc0942-4adf-40ce-9bb3-d50dfc57c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_parse_question(pydantic_model, query):\n",
    "    rag_chain = create_chain(create_system_prompt(pydantic_model), temperature=0.1, verbose=True, model_name=\"gpt-4-1106-preview\")\n",
    "    \n",
    "    try:\n",
    "        response = rag_chain(query)\n",
    "        report_on_message(response)  # print a summary of what was produced\n",
    "        parser = PydanticOutputParser(pydantic_object=pydantic_model)\n",
    "        return parser.parse(response[\"output\"])\n",
    "    except ValidationError as ve:\n",
    "        print(f\"Pydantic validation error: {ve}\")\n",
    "        # If Pydantic validation fails, fallback to json.loads\n",
    "        return json.loads(response[\"output\"])\n",
    "    except JSONDecodeError as json_error:\n",
    "        # If JSON decoding fails, perform json.loads and inform the caller about the error\n",
    "        result_output = json.loads(response[\"output\"])\n",
    "        print(f\"JSON decoding error: {json_error}\")\n",
    "        return result_output\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle other exceptions and fallback to json.loads\n",
    "        return json.loads(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca5a5e4a-517b-4927-9521-239d09141eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `pandas groupby`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content=\"We're going to use this example data set to demonstrate the three steps in split apply combine. To begin, we'll start with the split step. In order to ask pandas to split the data for us, we use the group by method of a data frame. You see here that we're calling DF dot group by and we're passing the string A. This instructs pandas to construct groups of our data using the values from the A column. This is the most basic and often most used form of the group by method to split on the values of a single column. We can check the type of this GBA object. And we see here a very long type name but we're just going to refer to this as a group by for short. Once we have a group by object, there are a few things we can do with it. One thing we could do is we could ask to get the subset of data for a particular group. So here we're going to ask to get, we're going to say DBA dot get group and we're going to pass one and then we'll pass two. And notice when we do this, that the data we get in return has all the rows of our original\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 5, 'timestamps': '00:05:40 --> 00:06:59'}), Document(page_content=\"like to form a calculation based on all the data for a particular hour. In this case we would have to instruct pandas that we would like to use the timestamp column but we'd like to group it in buckets of one hour at a time. We could similarly ask it to group it in buckets of four hours or a day or a week. And this is not expressible by just passing the column name timestamp. We also have to add in the frequency that we'd like. And pandas does enable this behavior using the PD.grouper type. This is what we'll learn about now. So in order to see it in action we're going to make a copy of our data frame and we're going to move the a column to the index and we're going to add a date column. So let's just go ahead and show you what we end up with. So notice that the a column has that or sorry the a column is now in the index with this original values of 1, 1, 1, 2, 2, 2, 2. The B and C columns are unchanged but now we have a date column that has some dates between 2020, 1222. Okay. Now that we have this date of frame we can use the PD.grouper to group by year.\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 21, 'timestamps': '00:24:45 --> 00:26:07'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `['year', 'month', 'sales']`, which of the following code snippets correctly uses the `groupby` method to calculate the total sales per year?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"pandas\", \"groupby\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby('year')['sales'].sum()\",\n",
      "    \"df['sales'].groupby(df['year']).sum()\",\n",
      "    \"df.groupby(['year', 'month']).sum()['sales']\",\n",
      "    \"df.groupby('year').sum()\"\n",
      "  ],\n",
      "  \"solution\": [0, 1]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `['year', 'month', 'sales']`, which of the following code snippets correctly uses the `groupby` method to calculate the total sales per year?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"pandas\", \"groupby\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby('year')['sales'].sum()\",\n",
      "    \"df['sales'].groupby(df['year']).sum()\",\n",
      "    \"df.groupby(['year', 'month']).sum()['sales']\",\n",
      "    \"df.groupby('year').sum()\"\n",
      "  ],\n",
      "  \"solution\": [0, 1]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Given a DataFrame `df` with columns `['year', 'month', 'sales']`, which of the following code snippets correctly uses the `groupby` method to calculate the total sales per year?\n",
       "\n",
       "- [x] df.groupby('year')['sales'].sum()\n",
       "- [x] df['sales'].groupby(df['year']).sum()\n",
       "- [ ] df.groupby(['year', 'month']).sum()['sales']\n",
       "- [ ] df.groupby('year').sum()\n"
      ],
      "text/plain": [
       "Given a DataFrame `df` with columns `['year', 'month', 'sales']`, which of the following code snippets correctly uses the `groupby` method to calculate the total sales per year?\n",
       "\n",
       "- [x] df.groupby('year')['sales'].sum()\n",
       "- [x] df['sales'].groupby(df['year']).sum()\n",
       "- [ ] df.groupby(['year', 'month']).sum()['sales']\n",
       "- [ ] df.groupby('year').sum()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"pandas groupby with difficulty 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07d65bc3-6a3f-4057-bdb7-aff7e0355f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"apology\": \"I'm sorry, but I can only generate one question at a time. Please send a new message with the topic and difficulty level for each question you would like me to generate.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"apology\": \"I'm sorry, but I can only generate one question at a time. Please send a new message with the topic and difficulty level for each question you would like me to generate.\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for MultipleSelection\nquestion_text\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ndifficulty\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ntopics\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nchoices\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nsolution\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_and_parse_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultipleSelection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgive me 2 more questions on the same topic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mgenerate_and_parse_question\u001b[0;34m(pydantic_model, query)\u001b[0m\n\u001b[1;32m      4\u001b[0m report_on_message(response)  \u001b[38;5;66;03m# print a summary of what was produced\u001b[39;00m\n\u001b[1;32m      5\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mpydantic_model)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/output_parsers/pydantic.py:28\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup()\n\u001b[1;32m     27\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     31\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/typing_extensions.py:2360\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2359\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/main.py:1010\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[0;34m(cls, obj)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;129m@typing_extensions\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated(\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39mPydanticDeprecatedSince20\n\u001b[1;32m   1007\u001b[0m )\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_obj\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Model], obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/main.py:503\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    502\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for MultipleSelection\nquestion_text\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ndifficulty\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ntopics\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nchoices\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nsolution\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...d like me to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing"
     ]
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"give me 2 more questions on the same topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5081d0cb-6d44-4817-ad41-80b4902e5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `optimization`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='And this exercise that we just performed is kind of the cracks of doing computational social science. It included three main components. One was the data. That was this blue line that was the actual or real world data that we were trying to match and we were trying to have our model track. The second main component was the statistical model. It was the hypothesis that the model parameters alpha and beta shifted from their steady state values to alpha-hat for finite number of periods. We had a way to toggle these parameters and adjust them and that led to different statistical models that we had a direct solution to. A direct problem solution to and we were able to produce these simulations. So feature number one was the data. Feature number two was the model and then feature number three was our prior beliefs about which levels of parameters are plausible and that helped us applying these prior beliefs helped us arrive a set of parameters that both match the data and were consistent with our beliefs.', metadata={'source': 'videos/transcripts_tiny/2.11.2 Lake Model Meets Data (1 of 2).srt', 'chunk_number': 48, 'timestamps': '01:03:32 --> 01:05:01'}), Document(page_content=\"A's and B's, and then D's, and then E's. So come back to these animations. Do a little bit of studying with them. And you should think Spencer, he spent a lot of time these are animated in SVG and he wrote this by hand. And I think it's really, it's really a miraculous animation. I think he actually improved my understanding of some of these operations when he created this. So that's everything we have on reshaping data. This was a dense topic. Please come back, study this again. It will take multiple times to master this. We'll talk soon. Bye.\", metadata={'source': 'videos/transcripts_tiny/2.3.2 Reshaping data with pandas.srt', 'chunk_number': 30, 'timestamps': '00:41:34 --> 00:42:13'}), Document(page_content=\"A's and B's, and then D's, and then E's. So come back to these animations. Do a little bit of studying with them. And you should think Spencer, he spent a lot of time these are animated in SVG and he wrote this by hand. And I think it's really, it's really a miraculous animation. I think he actually improved my understanding of some of these operations when he created this. So that's everything we have on reshaping data. This was a dense topic. Please come back, study this again. It will take multiple times to master this. We'll talk soon. Bye.\", metadata={'source': 'videos/transcripts_tiny/2.3.2 Reshaping data with pandas.srt', 'chunk_number': 30, 'timestamps': '00:41:34 --> 00:42:13'}), Document(page_content=\"So kind of that's in the ballpark of 300 times speed up, which is roughly what we told you you would get. So let's go on to our next topic.\", metadata={'source': 'videos/transcripts_tiny/2.3.1 cleaning data with pandas.srt', 'chunk_number': 14, 'timestamps': '00:17:07 --> 00:17:19'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"Given a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods, which of the following could be a consequence of setting the learning rate for these parameters too high during optimization?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"optimization\", \"statistical models\", \"parameter tuning\"],\n",
      "  \"choices\": [\n",
      "    \"The model quickly converges to the optimal parameters\",\n",
      "    \"The model parameters oscillate and may diverge\",\n",
      "    \"The learning process is stable but converges slowly\",\n",
      "    \"The model overfits to the training data and fails to generalize\"\n",
      "  ],\n",
      "  \"solution\": [1, 3]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"Given a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods, which of the following could be a consequence of setting the learning rate for these parameters too high during optimization?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"optimization\", \"statistical models\", \"parameter tuning\"],\n",
      "  \"choices\": [\n",
      "    \"The model quickly converges to the optimal parameters\",\n",
      "    \"The model parameters oscillate and may diverge\",\n",
      "    \"The learning process is stable but converges slowly\",\n",
      "    \"The model overfits to the training data and fails to generalize\"\n",
      "  ],\n",
      "  \"solution\": [1, 3]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Given a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods, which of the following could be a consequence of setting the learning rate for these parameters too high during optimization?\n",
       "\n",
       "- [ ] The model quickly converges to the optimal parameters\n",
       "- [x] The model parameters oscillate and may diverge\n",
       "- [ ] The learning process is stable but converges slowly\n",
       "- [x] The model overfits to the training data and fails to generalize\n"
      ],
      "text/plain": [
       "Given a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods, which of the following could be a consequence of setting the learning rate for these parameters too high during optimization?\n",
       "\n",
       "- [ ] The model quickly converges to the optimal parameters\n",
       "- [x] The model parameters oscillate and may diverge\n",
       "- [ ] The learning process is stable but converges slowly\n",
       "- [x] The model overfits to the training data and fails to generalize"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"Make the previous question more difficult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef30846-9f9c-42b6-93a1-c4eb562e6481",
   "metadata": {},
   "source": [
    "The previous topic is not being used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf38d81-713a-48d6-9cf7-5c58cc8d97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following code snippets correctly groups the data by `year` and `category` and computes the sum of `value` for each group?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"pandas\", \"groupby\", \"data analysis\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby(['year', 'category']).sum()\",\n",
      "    \"df.groupby(['year', 'category'])['value'].aggregate('sum')\",\n",
      "    \"df.groupby(['year', 'category']).sum()['value']\",\n",
      "    \"df.groupby(['year']).sum()['value']\"\n",
      "  ],\n",
      "  \"solution\": [0, 1]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following code snippets correctly groups the data by `year` and `category` and computes the sum of `value` for each group?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"pandas\", \"groupby\", \"data analysis\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby(['year', 'category']).sum()\",\n",
      "    \"df.groupby(['year', 'category'])['value'].aggregate('sum')\",\n",
      "    \"df.groupby(['year', 'category']).sum()['value']\",\n",
      "    \"df.groupby(['year']).sum()['value']\"\n",
      "  ],\n",
      "  \"solution\": [0, 1]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following code snippets correctly groups the data by `year` and `category` and computes the sum of `value` for each group?\n",
       "\n",
       "- [x] df.groupby(['year', 'category']).sum()\n",
       "- [x] df.groupby(['year', 'category'])['value'].aggregate('sum')\n",
       "- [ ] df.groupby(['year', 'category']).sum()['value']\n",
       "- [ ] df.groupby(['year']).sum()['value']\n"
      ],
      "text/plain": [
       "Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following code snippets correctly groups the data by `year` and `category` and computes the sum of `value` for each group?\n",
       "\n",
       "- [x] df.groupby(['year', 'category']).sum()\n",
       "- [x] df.groupby(['year', 'category'])['value'].aggregate('sum')\n",
       "- [ ] df.groupby(['year', 'category']).sum()['value']\n",
       "- [ ] df.groupby(['year']).sum()['value']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"pandas groupby with difficulty 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04d76b2-b6ec-4e19-b36d-5469dc076a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `optimization`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='And this exercise that we just performed is kind of the cracks of doing computational social science. It included three main components. One was the data. That was this blue line that was the actual or real world data that we were trying to match and we were trying to have our model track. The second main component was the statistical model. It was the hypothesis that the model parameters alpha and beta shifted from their steady state values to alpha-hat for finite number of periods. We had a way to toggle these parameters and adjust them and that led to different statistical models that we had a direct solution to. A direct problem solution to and we were able to produce these simulations. So feature number one was the data. Feature number two was the model and then feature number three was our prior beliefs about which levels of parameters are plausible and that helped us applying these prior beliefs helped us arrive a set of parameters that both match the data and were consistent with our beliefs.', metadata={'source': 'videos/transcripts_tiny/2.11.2 Lake Model Meets Data (1 of 2).srt', 'chunk_number': 48, 'timestamps': '01:03:32 --> 01:05:01'}), Document(page_content=\"A's and B's, and then D's, and then E's. So come back to these animations. Do a little bit of studying with them. And you should think Spencer, he spent a lot of time these are animated in SVG and he wrote this by hand. And I think it's really, it's really a miraculous animation. I think he actually improved my understanding of some of these operations when he created this. So that's everything we have on reshaping data. This was a dense topic. Please come back, study this again. It will take multiple times to master this. We'll talk soon. Bye.\", metadata={'source': 'videos/transcripts_tiny/2.3.2 Reshaping data with pandas.srt', 'chunk_number': 30, 'timestamps': '00:41:34 --> 00:42:13'}), Document(page_content=\"A's and B's, and then D's, and then E's. So come back to these animations. Do a little bit of studying with them. And you should think Spencer, he spent a lot of time these are animated in SVG and he wrote this by hand. And I think it's really, it's really a miraculous animation. I think he actually improved my understanding of some of these operations when he created this. So that's everything we have on reshaping data. This was a dense topic. Please come back, study this again. It will take multiple times to master this. We'll talk soon. Bye.\", metadata={'source': 'videos/transcripts_tiny/2.3.2 Reshaping data with pandas.srt', 'chunk_number': 30, 'timestamps': '00:41:34 --> 00:42:13'}), Document(page_content=\"So kind of that's in the ballpark of 300 times speed up, which is roughly what we told you you would get. So let's go on to our next topic.\", metadata={'source': 'videos/transcripts_tiny/2.3.1 cleaning data with pandas.srt', 'chunk_number': 14, 'timestamps': '00:17:07 --> 00:17:19'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"In the context of optimization for computational social science, consider a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods. Given this scenario, which of the following statements are true?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"optimization\", \"computational social science\", \"statistical models\"],\n",
      "  \"choices\": [\n",
      "    \"Adjusting the model parameters can lead to different statistical models and direct problem solutions.\",\n",
      "    \"Prior beliefs about parameter levels do not influence the matching of the model to the data.\",\n",
      "    \"The speedup gained from optimization techniques is always constant regardless of the data size.\",\n",
      "    \"Applying prior beliefs about parameter plausibility can help in arriving at parameters that match the data and are consistent with our beliefs.\"\n",
      "  ],\n",
      "  \"solution\": [0, 3]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"In the context of optimization for computational social science, consider a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods. Given this scenario, which of the following statements are true?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"optimization\", \"computational social science\", \"statistical models\"],\n",
      "  \"choices\": [\n",
      "    \"Adjusting the model parameters can lead to different statistical models and direct problem solutions.\",\n",
      "    \"Prior beliefs about parameter levels do not influence the matching of the model to the data.\",\n",
      "    \"The speedup gained from optimization techniques is always constant regardless of the data size.\",\n",
      "    \"Applying prior beliefs about parameter plausibility can help in arriving at parameters that match the data and are consistent with our beliefs.\"\n",
      "  ],\n",
      "  \"solution\": [0, 3]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In the context of optimization for computational social science, consider a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods. Given this scenario, which of the following statements are true?\n",
       "\n",
       "- [x] Adjusting the model parameters can lead to different statistical models and direct problem solutions.\n",
       "- [ ] Prior beliefs about parameter levels do not influence the matching of the model to the data.\n",
       "- [ ] The speedup gained from optimization techniques is always constant regardless of the data size.\n",
       "- [x] Applying prior beliefs about parameter plausibility can help in arriving at parameters that match the data and are consistent with our beliefs.\n"
      ],
      "text/plain": [
       "In the context of optimization for computational social science, consider a statistical model where parameters alpha and beta shift from their steady state values to alpha-hat for a finite number of periods. Given this scenario, which of the following statements are true?\n",
       "\n",
       "- [x] Adjusting the model parameters can lead to different statistical models and direct problem solutions.\n",
       "- [ ] Prior beliefs about parameter levels do not influence the matching of the model to the data.\n",
       "- [ ] The speedup gained from optimization techniques is always constant regardless of the data size.\n",
       "- [x] Applying prior beliefs about parameter plausibility can help in arriving at parameters that match the data and are consistent with our beliefs."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"Make the previous question more difficult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "403d6a52-283f-40bd-b37f-065b13163217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_system_prompt(pydantic_object):\n",
    "    common_system_prompt = textwrap.dedent(\"\"\"\n",
    "    You are a smart, helpful teaching assistant chatbot named Callisto.\n",
    "\n",
    "    You are an expert Python programmer and have used all the most popular\n",
    "    libraries for data analysis, machine learning, and artificial intelligence.\n",
    "\n",
    "    You assist professors that teach courses about Python, data science, and machine learning\n",
    "    to graduate students.\n",
    "\n",
    "    Your task is to help professors produce practice questions to help students solidify \n",
    "    their understanding of specific topics.\n",
    "\n",
    "    You must always generate questions that have more than one option as solution for the MultipleSelection question type.\n",
    "\n",
    "    In your conversations with a professor you will be given a topic (string) and an\n",
    "    expected difficulty level (integer)\n",
    "    \n",
    "    The difficulty will be a number between 1 and 3, with 1 corresponding to a request \n",
    "    for an easy question, and 3 for the most difficult question.\n",
    "    \n",
    "    Occasionaly the professor may ask you to do something like produce a similar question,\n",
    "    or  make the question more difficult or easy. You need to assist the professor with the same. You can use the previous topic to do this.\n",
    "    \n",
    "    If the professor asks you for another question and does not specify either a new topic \n",
    "    or a new difficulty, you must use the previous topic or difficulty.\n",
    "    \n",
    "    If the professor ask for more than one question in a single message, you need to apologize and \n",
    "    inform that you can only generate one question at a time. You need to also ask the professor to \n",
    "    put in a new message with the topic and difficulty to generate a new question.\n",
    "\n",
    "    You are encouraged to use any tools available to look up relevant information, only\n",
    "    if necessary.\n",
    "\n",
    "    You will apologize if you're unable to generate an output that meets professor's requirement\n",
    "\n",
    "    Your responses must always exactly match the specified JSON format with no extra words or content.\n",
    "\n",
    "    You must always produce exactly one JSON object.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "  #  parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
    " #   return common_system_prompt.format(format_instructions=parser.get_format_instructions())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c270aff0-48e3-42e5-ad87-f46e186f1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"In the context of data analysis with pandas, which of the following statements are true regarding the `groupby` operation?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"pandas\", \"data analysis\", \"groupby\"],\n",
      "  \"choices\": [\n",
      "    \"The `groupby` method splits the DataFrame into groups based on some criteria.\",\n",
      "    \"A `groupby` operation always reduces the size of the original DataFrame.\",\n",
      "    \"The `groupby` method can be used in conjunction with aggregate functions like `sum()`, `mean()`, and `max()`.\",\n",
      "    \"Grouped objects need to be explicitly converted back to DataFrames using the `to_frame()` method.\"\n",
      "  ],\n",
      "  \"solution\": [0, 2]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"In the context of data analysis with pandas, which of the following statements are true regarding the `groupby` operation?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"pandas\", \"data analysis\", \"groupby\"],\n",
      "  \"choices\": [\n",
      "    \"The `groupby` method splits the DataFrame into groups based on some criteria.\",\n",
      "    \"A `groupby` operation always reduces the size of the original DataFrame.\",\n",
      "    \"The `groupby` method can be used in conjunction with aggregate functions like `sum()`, `mean()`, and `max()`.\",\n",
      "    \"Grouped objects need to be explicitly converted back to DataFrames using the `to_frame()` method.\"\n",
      "  ],\n",
      "  \"solution\": [0, 2]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In the context of data analysis with pandas, which of the following statements are true regarding the `groupby` operation?\n",
       "\n",
       "- [x] The `groupby` method splits the DataFrame into groups based on some criteria.\n",
       "- [ ] A `groupby` operation always reduces the size of the original DataFrame.\n",
       "- [x] The `groupby` method can be used in conjunction with aggregate functions like `sum()`, `mean()`, and `max()`.\n",
       "- [ ] Grouped objects need to be explicitly converted back to DataFrames using the `to_frame()` method.\n"
      ],
      "text/plain": [
       "In the context of data analysis with pandas, which of the following statements are true regarding the `groupby` operation?\n",
       "\n",
       "- [x] The `groupby` method splits the DataFrame into groups based on some criteria.\n",
       "- [ ] A `groupby` operation always reduces the size of the original DataFrame.\n",
       "- [x] The `groupby` method can be used in conjunction with aggregate functions like `sum()`, `mean()`, and `max()`.\n",
       "- [ ] Grouped objects need to be explicitly converted back to DataFrames using the `to_frame()` method."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"pandas groupby with difficulty 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "151444bc-6b56-49e8-8372-a41f182c67c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"error\": \"I'm sorry, but I need a specific topic and the previous difficulty level to adjust the difficulty of the question. Could you please provide me with that information?\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"error\": \"I'm sorry, but I need a specific topic and the previous difficulty level to adjust the difficulty of the question. Could you please provide me with that information?\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for MultipleSelection\nquestion_text\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ndifficulty\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ntopics\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nchoices\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nsolution\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_and_parse_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultipleSelection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMake the question more difficult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mgenerate_and_parse_question\u001b[0;34m(pydantic_model, query)\u001b[0m\n\u001b[1;32m      4\u001b[0m report_on_message(response)  \u001b[38;5;66;03m# print a summary of what was produced\u001b[39;00m\n\u001b[1;32m      5\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mpydantic_model)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/output_parsers/pydantic.py:28\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup()\n\u001b[1;32m     27\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     31\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/typing_extensions.py:2360\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2359\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/main.py:1010\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[0;34m(cls, obj)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;129m@typing_extensions\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated(\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39mPydanticDeprecatedSince20\n\u001b[1;32m   1007\u001b[0m )\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_obj\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Model], obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/main.py:503\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    502\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for MultipleSelection\nquestion_text\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ndifficulty\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ntopics\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nchoices\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nsolution\n  Field required [type=missing, input_value={'error': \"I'm sorry, but...with that information?\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing"
     ]
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"Make the question more difficult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b130421-9ef4-46d0-91bb-27478a905c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following operations using `groupby` will correctly compute the sum of `value` for each unique combination of `year` and `category`, and then sort the results in descending order by `value`?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"Pandas\", \"groupby\", \"data manipulation\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby(['year', 'category']).sum()['value'].sort_values(ascending=False)\",\n",
      "    \"df.groupby(['year', 'category'], as_index=False).sum().sort_values(by='value', ascending=True)\",\n",
      "    \"df.groupby(['year', 'category']).agg({'value': 'sum'}).sort_values(by='value', ascending=False)\",\n",
      "    \"df.groupby(['year']).sum().groupby(['category']).sum().sort_values(by='value', ascending=False)\"\n",
      "  ],\n",
      "  \"solution\": [0, 2]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following operations using `groupby` will correctly compute the sum of `value` for each unique combination of `year` and `category`, and then sort the results in descending order by `value`?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"Pandas\", \"groupby\", \"data manipulation\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby(['year', 'category']).sum()['value'].sort_values(ascending=False)\",\n",
      "    \"df.groupby(['year', 'category'], as_index=False).sum().sort_values(by='value', ascending=True)\",\n",
      "    \"df.groupby(['year', 'category']).agg({'value': 'sum'}).sort_values(by='value', ascending=False)\",\n",
      "    \"df.groupby(['year']).sum().groupby(['category']).sum().sort_values(by='value', ascending=False)\"\n",
      "  ],\n",
      "  \"solution\": [0, 2]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following operations using `groupby` will correctly compute the sum of `value` for each unique combination of `year` and `category`, and then sort the results in descending order by `value`?\n",
       "\n",
       "- [x] df.groupby(['year', 'category']).sum()['value'].sort_values(ascending=False)\n",
       "- [ ] df.groupby(['year', 'category'], as_index=False).sum().sort_values(by='value', ascending=True)\n",
       "- [x] df.groupby(['year', 'category']).agg({'value': 'sum'}).sort_values(by='value', ascending=False)\n",
       "- [ ] df.groupby(['year']).sum().groupby(['category']).sum().sort_values(by='value', ascending=False)\n"
      ],
      "text/plain": [
       "Given a DataFrame `df` with columns `['year', 'category', 'value']`, which of the following operations using `groupby` will correctly compute the sum of `value` for each unique combination of `year` and `category`, and then sort the results in descending order by `value`?\n",
       "\n",
       "- [x] df.groupby(['year', 'category']).sum()['value'].sort_values(ascending=False)\n",
       "- [ ] df.groupby(['year', 'category'], as_index=False).sum().sort_values(by='value', ascending=True)\n",
       "- [x] df.groupby(['year', 'category']).agg({'value': 'sum'}).sort_values(by='value', ascending=False)\n",
       "- [ ] df.groupby(['year']).sum().groupby(['category']).sum().sort_values(by='value', ascending=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"Pandas groupby difficulty 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3949b9b-364c-42bd-9a92-b13492da7049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"In the context of machine learning, which of the following are reasons why feature scaling is important?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"feature scaling\", \"data preprocessing\", \"machine learning\"],\n",
      "  \"choices\": [\n",
      "    \"It ensures that gradient descent converges more quickly.\",\n",
      "    \"It is necessary for comparing the importance of different features.\",\n",
      "    \"It prevents the model from overfitting.\",\n",
      "    \"It allows algorithms that use distance measures to operate more effectively.\"\n",
      "  ],\n",
      "  \"solution\": [0, 3]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"In the context of machine learning, which of the following are reasons why feature scaling is important?\",\n",
      "  \"difficulty\": 2,\n",
      "  \"topics\": [\"feature scaling\", \"data preprocessing\", \"machine learning\"],\n",
      "  \"choices\": [\n",
      "    \"It ensures that gradient descent converges more quickly.\",\n",
      "    \"It is necessary for comparing the importance of different features.\",\n",
      "    \"It prevents the model from overfitting.\",\n",
      "    \"It allows algorithms that use distance measures to operate more effectively.\"\n",
      "  ],\n",
      "  \"solution\": [0, 3]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "In the context of machine learning, which of the following are reasons why feature scaling is important?\n",
       "\n",
       "- [x] It ensures that gradient descent converges more quickly.\n",
       "- [ ] It is necessary for comparing the importance of different features.\n",
       "- [ ] It prevents the model from overfitting.\n",
       "- [x] It allows algorithms that use distance measures to operate more effectively.\n"
      ],
      "text/plain": [
       "In the context of machine learning, which of the following are reasons why feature scaling is important?\n",
       "\n",
       "- [x] It ensures that gradient descent converges more quickly.\n",
       "- [ ] It is necessary for comparing the importance of different features.\n",
       "- [ ] It prevents the model from overfitting.\n",
       "- [x] It allows algorithms that use distance measures to operate more effectively."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"Give a difficulty 2 question on the same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b342356d-091e-41bf-bcdc-268f4bc3fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `A`, `B`, and `C`, where `A` contains categorical data, `B` contains numerical data, and `C` contains datetime data, which of the following operations using the `groupby` method will correctly perform a complex grouping and aggregation?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"pandas\", \"data manipulation\", \"groupby\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby('A').agg({'B': ['mean', 'sum'], 'C': ['max']})\",\n",
      "    \"df.groupby(['A', df['C'].dt.year]).agg({'B': 'mean'})\",\n",
      "    \"df.groupby(['A', df['C'].dt.to_period('M')]).agg({'B': 'sum'}).reset_index()\",\n",
      "    \"df.groupby('A')['B'].transform(lambda x: (x - x.mean()) / x.std())\"\n",
      "  ],\n",
      "  \"solution\": [0, 1, 2]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"question_text\": \"Given a DataFrame `df` with columns `A`, `B`, and `C`, where `A` contains categorical data, `B` contains numerical data, and `C` contains datetime data, which of the following operations using the `groupby` method will correctly perform a complex grouping and aggregation?\",\n",
      "  \"difficulty\": 3,\n",
      "  \"topics\": [\"pandas\", \"data manipulation\", \"groupby\"],\n",
      "  \"choices\": [\n",
      "    \"df.groupby('A').agg({'B': ['mean', 'sum'], 'C': ['max']})\",\n",
      "    \"df.groupby(['A', df['C'].dt.year]).agg({'B': 'mean'})\",\n",
      "    \"df.groupby(['A', df['C'].dt.to_period('M')]).agg({'B': 'sum'}).reset_index()\",\n",
      "    \"df.groupby('A')['B'].transform(lambda x: (x - x.mean()) / x.std())\"\n",
      "  ],\n",
      "  \"solution\": [0, 1, 2]\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Given a DataFrame `df` with columns `A`, `B`, and `C`, where `A` contains categorical data, `B` contains numerical data, and `C` contains datetime data, which of the following operations using the `groupby` method will correctly perform a complex grouping and aggregation?\n",
       "\n",
       "- [x] df.groupby('A').agg({'B': ['mean', 'sum'], 'C': ['max']})\n",
       "- [x] df.groupby(['A', df['C'].dt.year]).agg({'B': 'mean'})\n",
       "- [x] df.groupby(['A', df['C'].dt.to_period('M')]).agg({'B': 'sum'}).reset_index()\n",
       "- [ ] df.groupby('A')['B'].transform(lambda x: (x - x.mean()) / x.std())\n"
      ],
      "text/plain": [
       "Given a DataFrame `df` with columns `A`, `B`, and `C`, where `A` contains categorical data, `B` contains numerical data, and `C` contains datetime data, which of the following operations using the `groupby` method will correctly perform a complex grouping and aggregation?\n",
       "\n",
       "- [x] df.groupby('A').agg({'B': ['mean', 'sum'], 'C': ['max']})\n",
       "- [x] df.groupby(['A', df['C'].dt.year]).agg({'B': 'mean'})\n",
       "- [x] df.groupby(['A', df['C'].dt.to_period('M')]).agg({'B': 'sum'}).reset_index()\n",
       "- [ ] df.groupby('A')['B'].transform(lambda x: (x - x.mean()) / x.std())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"pandas groupby difficulty 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9036547e-3281-458f-95fd-1e3e032b7ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"apology\": \"I'm sorry, but I can only generate one question at a time. Please provide a new message with the topic and difficulty level for each question you would like to generate.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "  \"apology\": \"I'm sorry, but I can only generate one question at a time. Please provide a new message with the topic and difficulty level for each question you would like to generate.\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "5 validation errors for MultipleSelection\nquestion_text\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ndifficulty\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ntopics\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nchoices\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nsolution\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_and_parse_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMultipleSelection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me 2 more questions of the same format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m, in \u001b[0;36mgenerate_and_parse_question\u001b[0;34m(pydantic_model, query)\u001b[0m\n\u001b[1;32m      4\u001b[0m report_on_message(response)  \u001b[38;5;66;03m# print a summary of what was produced\u001b[39;00m\n\u001b[1;32m      5\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mpydantic_model)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/output_parsers/pydantic.py:28\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m         json_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup()\n\u001b[1;32m     27\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     31\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/typing_extensions.py:2360\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2359\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/main.py:1010\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[0;34m(cls, obj)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;129m@typing_extensions\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated(\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39mPydanticDeprecatedSince20\n\u001b[1;32m   1007\u001b[0m )\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_obj\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Model], obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/main.py:503\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    502\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for MultipleSelection\nquestion_text\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ndifficulty\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\ntopics\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nchoices\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing\nsolution\n  Field required [type=missing, input_value={'apology': \"I'm sorry, b...ould like to generate.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/missing"
     ]
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"Give me 2 more questions of the same format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b04407d7-e926-4afb-adcd-d66850ed74df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"question_text\": \"Given a DataFrame `df` with columns `['year', 'month', 'day', 'value']`, write a function `aggregate_data` that groups the data by `year` and `month`, and computes the sum of `value` for each group. Additionally, the function should return a DataFrame with a MultiIndex consisting of `year` and `month`, and a single column `total_value` that contains the aggregated sums. Ensure that the resulting DataFrame is sorted by `year` and `month` in ascending order.\",\n",
      "    \"difficulty\": 3,\n",
      "    \"topics\": [\"pandas\", \"groupby\", \"data aggregation\"],\n",
      "    \"starting_code\": \"def aggregate_data(df):\\n    # Your code here\\n    return aggregated_df\",\n",
      "    \"solution\": \"def aggregate_data(df):\\n    aggregated_df = df.groupby(['year', 'month']).agg({'value': 'sum'})\\n    aggregated_df.columns = ['total_value']\\n    aggregated_df.sort_index(inplace=True)\\n    return aggregated_df\",\n",
      "    \"setup_code\": \"import pandas as pd\\nfrom pandas.testing import assert_frame_equal\\ndf = pd.DataFrame({\\n    'year': [2020, 2020, 2021, 2021, 2021],\\n    'month': [1, 1, 2, 2, 3],\\n    'day': [1, 2, 1, 2, 1],\\n    'value': [10, 20, 30, 40, 50]\\n})\",\n",
      "    \"test_code\": \"expected_df = pd.DataFrame({\\n    'total_value': [30, 70, 50]\\n}, index=pd.MultiIndex.from_tuples([(2020, 1), (2021, 2), (2021, 3)], names=['year', 'month']))\\nresult_df = aggregate_data(df)\\nassert_frame_equal(result_df, expected_df)\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  False\n",
      "output:\n",
      " {\n",
      "    \"question_text\": \"Given a DataFrame `df` with columns `['year', 'month', 'day', 'value']`, write a function `aggregate_data` that groups the data by `year` and `month`, and computes the sum of `value` for each group. Additionally, the function should return a DataFrame with a MultiIndex consisting of `year` and `month`, and a single column `total_value` that contains the aggregated sums. Ensure that the resulting DataFrame is sorted by `year` and `month` in ascending order.\",\n",
      "    \"difficulty\": 3,\n",
      "    \"topics\": [\"pandas\", \"groupby\", \"data aggregation\"],\n",
      "    \"starting_code\": \"def aggregate_data(df):\\n    # Your code here\\n    return aggregated_df\",\n",
      "    \"solution\": \"def aggregate_data(df):\\n    aggregated_df = df.groupby(['year', 'month']).agg({'value': 'sum'})\\n    aggregated_df.columns = ['total_value']\\n    aggregated_df.sort_index(inplace=True)\\n    return aggregated_df\",\n",
      "    \"setup_code\": \"import pandas as pd\\nfrom pandas.testing import assert_frame_equal\\ndf = pd.DataFrame({\\n    'year': [2020, 2020, 2021, 2021, 2021],\\n    'month': [1, 1, 2, 2, 3],\\n    'day': [1, 2, 1, 2, 1],\\n    'value': [10, 20, 30, 40, 50]\\n})\",\n",
      "    \"test_code\": \"expected_df = pd.DataFrame({\\n    'total_value': [30, 70, 50]\\n}, index=pd.MultiIndex.from_tuples([(2020, 1), (2021, 2), (2021, 3)], names=['year', 'month']))\\nresult_df = aggregate_data(df)\\nassert_frame_equal(result_df, expected_df)\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Given a DataFrame `df` with columns `['year', 'month', 'day', 'value']`, write a function `aggregate_data` that groups the data by `year` and `month`, and computes the sum of `value` for each group. Additionally, the function should return a DataFrame with a MultiIndex consisting of `year` and `month`, and a single column `total_value` that contains the aggregated sums. Ensure that the resulting DataFrame is sorted by `year` and `month` in ascending order.\n",
       "\n",
       "```python\n",
       "def aggregate_data(df):\n",
       "    # Your code here\n",
       "    return aggregated_df\n",
       "```\n",
       "\n",
       "**Solution**\n",
       "\n",
       "```python\n",
       "def aggregate_data(df):\n",
       "    aggregated_df = df.groupby(['year', 'month']).agg({'value': 'sum'})\n",
       "    aggregated_df.columns = ['total_value']\n",
       "    aggregated_df.sort_index(inplace=True)\n",
       "    return aggregated_df\n",
       "```\n",
       "\n",
       "**Test Suite**\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from pandas.testing import assert_frame_equal\n",
       "df = pd.DataFrame({\n",
       "    'year': [2020, 2020, 2021, 2021, 2021],\n",
       "    'month': [1, 1, 2, 2, 3],\n",
       "    'day': [1, 2, 1, 2, 1],\n",
       "    'value': [10, 20, 30, 40, 50]\n",
       "})\n",
       "\n",
       "def aggregate_data(df):\n",
       "    aggregated_df = df.groupby(['year', 'month']).agg({'value': 'sum'})\n",
       "    aggregated_df.columns = ['total_value']\n",
       "    aggregated_df.sort_index(inplace=True)\n",
       "    return aggregated_df\n",
       "\n",
       "expected_df = pd.DataFrame({\n",
       "    'total_value': [30, 70, 50]\n",
       "}, index=pd.MultiIndex.from_tuples([(2020, 1), (2021, 2), (2021, 3)], names=['year', 'month']))\n",
       "result_df = aggregate_data(df)\n",
       "assert_frame_equal(result_df, expected_df)\n",
       "```"
      ],
      "text/plain": [
       "Given a DataFrame `df` with columns `['year', 'month', 'day', 'value']`, write a function `aggregate_data` that groups the data by `year` and `month`, and computes the sum of `value` for each group. Additionally, the function should return a DataFrame with a MultiIndex consisting of `year` and `month`, and a single column `total_value` that contains the aggregated sums. Ensure that the resulting DataFrame is sorted by `year` and `month` in ascending order.\n",
       "\n",
       "```python\n",
       "def aggregate_data(df):\n",
       "    # Your code here\n",
       "    return aggregated_df\n",
       "```\n",
       "\n",
       "**Solution**\n",
       "\n",
       "```python\n",
       "def aggregate_data(df):\n",
       "    aggregated_df = df.groupby(['year', 'month']).agg({'value': 'sum'})\n",
       "    aggregated_df.columns = ['total_value']\n",
       "    aggregated_df.sort_index(inplace=True)\n",
       "    return aggregated_df\n",
       "```\n",
       "\n",
       "**Test Suite**\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "from pandas.testing import assert_frame_equal\n",
       "df = pd.DataFrame({\n",
       "    'year': [2020, 2020, 2021, 2021, 2021],\n",
       "    'month': [1, 1, 2, 2, 3],\n",
       "    'day': [1, 2, 1, 2, 1],\n",
       "    'value': [10, 20, 30, 40, 50]\n",
       "})\n",
       "\n",
       "def aggregate_data(df):\n",
       "    aggregated_df = df.groupby(['year', 'month']).agg({'value': 'sum'})\n",
       "    aggregated_df.columns = ['total_value']\n",
       "    aggregated_df.sort_index(inplace=True)\n",
       "    return aggregated_df\n",
       "\n",
       "expected_df = pd.DataFrame({\n",
       "    'total_value': [30, 70, 50]\n",
       "}, index=pd.MultiIndex.from_tuples([(2020, 1), (2021, 2), (2021, 3)], names=['year', 'month']))\n",
       "result_df = aggregate_data(df)\n",
       "assert_frame_equal(result_df, expected_df)\n",
       "```"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(Code, \"pandas groupby difficulty 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2b32f3-3e26-483d-897b-69e68c1d66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(pydantic_object):\n",
    "    common_system_prompt = textwrap.dedent(\"\"\"\n",
    "    You are a smart, helpful teaching assistant chatbot named Callisto.\n",
    "\n",
    "    You are an expert Python programmer and have used all the most popular\n",
    "    libraries for data analysis, machine learning, and artificial intelligence.\n",
    "\n",
    "    You assist professors that teach courses about Python, data science, and machine learning\n",
    "    to graduate students.\n",
    "\n",
    "    Your task is to help professors produce practice questions to help students solidify \n",
    "    their understanding of specific topics.\n",
    "\n",
    "    You must always generate questions that have more than one option as solution for the MultipleSelection question type.\n",
    "\n",
    "    In your conversations with a professor you will be given a topic (string) and an\n",
    "    expected difficulty level (integer)\n",
    "    \n",
    "    The difficulty will be a number between 1 and 3, with 1 corresponding to a request \n",
    "    for an easy question, and 3 for the most difficult question.\n",
    "    \n",
    "    Occasionaly the professor may ask you to do something like produce a similar question,\n",
    "    or  make the question more difficult or easy. You need to assist the professor with the same. You can use the previous topic to do this.\n",
    "    \n",
    "\n",
    "    If the professor ask for more than one question in a single message, you need to apologize and \n",
    "    inform that you can only generate one question at a time. You need to also ask the professor to \n",
    "    put in a new message with the topic and difficulty to generate a new question.\n",
    "\n",
    "    If the topic is available in the given information, refer that for giving a response to the professor. \n",
    "    You are encouraged to use any tools available to look up relevant information, only\n",
    "    if necessary. \n",
    "\n",
    "    You will apologize if you're unable to generate an output that meets professor's requirement\n",
    "\n",
    "    Your responses must always exactly match the specified JSON format with no extra words or content.\n",
    "\n",
    "    You must always produce exactly one JSON object.\n",
    "    \n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_object)\n",
    "    return common_system_prompt.format(format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741c1983-afed-4303-b9b4-fcc412c2b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_course_content` with `pandas groupby`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content=\"We're going to use this example data set to demonstrate the three steps in split apply combine. To begin, we'll start with the split step. In order to ask pandas to split the data for us, we use the group by method of a data frame. You see here that we're calling DF dot group by and we're passing the string A. This instructs pandas to construct groups of our data using the values from the A column. This is the most basic and often most used form of the group by method to split on the values of a single column. We can check the type of this GBA object. And we see here a very long type name but we're just going to refer to this as a group by for short. Once we have a group by object, there are a few things we can do with it. One thing we could do is we could ask to get the subset of data for a particular group. So here we're going to ask to get, we're going to say DBA dot get group and we're going to pass one and then we'll pass two. And notice when we do this, that the data we get in return has all the rows of our original\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 5, 'timestamps': '00:05:40 --> 00:06:59'}), Document(page_content=\"like to form a calculation based on all the data for a particular hour. In this case we would have to instruct pandas that we would like to use the timestamp column but we'd like to group it in buckets of one hour at a time. We could similarly ask it to group it in buckets of four hours or a day or a week. And this is not expressible by just passing the column name timestamp. We also have to add in the frequency that we'd like. And pandas does enable this behavior using the PD.grouper type. This is what we'll learn about now. So in order to see it in action we're going to make a copy of our data frame and we're going to move the a column to the index and we're going to add a date column. So let's just go ahead and show you what we end up with. So notice that the a column has that or sorry the a column is now in the index with this original values of 1, 1, 1, 2, 2, 2, 2. The B and C columns are unchanged but now we have a date column that has some dates between 2020, 1222. Okay. Now that we have this date of frame we can use the PD.grouper to group by year.\", metadata={'source': 'videos/transcripts_tiny/2.5.1 Groupby Operations.srt', 'chunk_number': 21, 'timestamps': '00:24:45 --> 00:26:07'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'}), Document(page_content=\"Likewise, if we wanted to get the observations from quarter three and four, we could write greater than June 31. And we would end up with the observation three and four and seven and eight. Again, this is this is just a toy data set meant to show you that we can impose restrictions based on the date times as well. The next argument we'll talk about is the group by. So group by is going to allow us to aggregate certain groups of values, much like the Pandas group by method. The thing to especially note, which is true in Pandas as well, is that any column that is not an element of the grouping must have a reduction function applied to it. So we're now going to start working with the order's data and just so we can remember what that looks like. Select, we'll do star. So we have seven columns, we have the order ID, the user ID, which set the data belong to to the order number, the order day of week, the order hour of day, and the days since the previous order. So what we're going to do is we're going to select the day of week that an order was made and then we're going to count how many non missing values of user ID there are.\", metadata={'source': 'videos/transcripts_tiny/2.8.2 Intro to SQL (part 2 of 2).srt', 'chunk_number': 23, 'timestamps': '00:35:12 --> 00:37:12'})]\u001b[0m\u001b[32;1m\u001b[1;3m{\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"When using pandas' groupby method, which of the following statements are true?\\\",\\n  \\\"difficulty\\\": 2,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"data manipulation\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"The groupby method can only split data based on the values of a single column.\\\",\\n    \\\"A groupby object allows you to get subsets of data for particular groups.\\\",\\n    \\\"The groupby method can be used with the PD.grouper type to group data in time buckets.\\\",\\n    \\\"Any column not included in the groupby must have a reduction function applied to it in the result.\\\",\\n    \\\"The groupby method can only be used on numerical data types.\\\"\\n  ],\\n  \\\"solution\\\": [1, 2, 3]\\n}\",\n",
      "  \"properties\": {\n",
      "    \"question_text\": {\n",
      "      \"description\": \"The main text of the question. Markdown formatted\",\n",
      "      \"title\": \"Question Text\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"difficulty\": {\n",
      "      \"description\": \"An integer from 1 to 3 representing how difficult the question should be. 1 is easiest. 3 is hardest\",\n",
      "      \"title\": \"Difficulty\",\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"topics\": {\n",
      "      \"description\": \"A list of one or more topics that the question is testing\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Topics\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"choices\": {\n",
      "      \"description\": \"A list of markdown formatted strings representing the options for the student. Minimum length of 3.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Choices\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"solution\": {\n",
      "      \"description\": \"List of indices into choices representing correct answers. Zero based\",\n",
      "      \"items\": {\n",
      "        \"type\": \"integer\"\n",
      "      },\n",
      "      \"title\": \"Solution\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"question_text\",\n",
      "    \"difficulty\",\n",
      "    \"topics\",\n",
      "    \"choices\",\n",
      "    \"solution\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "any intermediate_steps?:  True\n",
      "output:\n",
      " {\n",
      "  \"description\": \"Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \\\"question_text\\\": \\\"When using pandas' groupby method, which of the following statements are true?\\\",\\n  \\\"difficulty\\\": 2,\\n  \\\"topics\\\": [\\\"pandas\\\", \\\"data manipulation\\\", \\\"groupby\\\"],\\n  \\\"choices\\\": [\\n    \\\"The groupby method can only split data based on the values of a single column.\\\",\\n    \\\"A groupby object allows you to get subsets of data for particular groups.\\\",\\n    \\\"The groupby method can be used with the PD.grouper type to group data in time buckets.\\\",\\n    \\\"Any column not included in the groupby must have a reduction function applied to it in the result.\\\",\\n    \\\"The groupby method can only be used on numerical data types.\\\"\\n  ],\\n  \\\"solution\\\": [1, 2, 3]\\n}\",\n",
      "  \"properties\": {\n",
      "    \"question_text\": {\n",
      "      \"description\": \"The main text of the question. Markdown formatted\",\n",
      "      \"title\": \"Question Text\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"difficulty\": {\n",
      "      \"description\": \"An integer from 1 to 3 representing how difficult the question should be. 1 is easiest. 3 is hardest\",\n",
      "      \"title\": \"Difficulty\",\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"topics\": {\n",
      "      \"description\": \"A list of one or more topics that the question is testing\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Topics\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"choices\": {\n",
      "      \"description\": \"A list of markdown formatted strings representing the options for the student. Minimum length of 3.\",\n",
      "      \"items\": {\n",
      "        \"type\": \"string\"\n",
      "      },\n",
      "      \"title\": \"Choices\",\n",
      "      \"type\": \"array\"\n",
      "    },\n",
      "    \"solution\": {\n",
      "      \"description\": \"List of indices into choices representing correct answers. Zero based\",\n",
      "      \"items\": {\n",
      "        \"type\": \"integer\"\n",
      "      },\n",
      "      \"title\": \"Solution\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"question_text\",\n",
      "    \"difficulty\",\n",
      "    \"topics\",\n",
      "    \"choices\",\n",
      "    \"solution\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Pydantic validation error: 5 validation errors for MultipleSelection\n",
      "question_text\n",
      "  Field required [type=missing, input_value={'description': 'Question... 'choices', 'solution']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "difficulty\n",
      "  Field required [type=missing, input_value={'description': 'Question... 'choices', 'solution']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "topics\n",
      "  Field required [type=missing, input_value={'description': 'Question... 'choices', 'solution']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "choices\n",
      "  Field required [type=missing, input_value={'description': 'Question... 'choices', 'solution']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n",
      "solution\n",
      "  Field required [type=missing, input_value={'description': 'Question... 'choices', 'solution']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.4/v/missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'Question where user is presented a prompt in `question_text` and \\na list of `choices`. They are supposed to provide all answers that\\napply (`solution`)\\n\\nAll questions must have a minimum of 3 options\\n\\nExamples\\n--------\\n{\\n  \"question_text\": \"When using pandas\\' groupby method, which of the following statements are true?\",\\n  \"difficulty\": 2,\\n  \"topics\": [\"pandas\", \"data manipulation\", \"groupby\"],\\n  \"choices\": [\\n    \"The groupby method can only split data based on the values of a single column.\",\\n    \"A groupby object allows you to get subsets of data for particular groups.\",\\n    \"The groupby method can be used with the PD.grouper type to group data in time buckets.\",\\n    \"Any column not included in the groupby must have a reduction function applied to it in the result.\",\\n    \"The groupby method can only be used on numerical data types.\"\\n  ],\\n  \"solution\": [1, 2, 3]\\n}',\n",
       " 'properties': {'question_text': {'description': 'The main text of the question. Markdown formatted',\n",
       "   'title': 'Question Text',\n",
       "   'type': 'string'},\n",
       "  'difficulty': {'description': 'An integer from 1 to 3 representing how difficult the question should be. 1 is easiest. 3 is hardest',\n",
       "   'title': 'Difficulty',\n",
       "   'type': 'integer'},\n",
       "  'topics': {'description': 'A list of one or more topics that the question is testing',\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Topics',\n",
       "   'type': 'array'},\n",
       "  'choices': {'description': 'A list of markdown formatted strings representing the options for the student. Minimum length of 3.',\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Choices',\n",
       "   'type': 'array'},\n",
       "  'solution': {'description': 'List of indices into choices representing correct answers. Zero based',\n",
       "   'items': {'type': 'integer'},\n",
       "   'title': 'Solution',\n",
       "   'type': 'array'}},\n",
       " 'required': ['question_text', 'difficulty', 'topics', 'choices', 'solution']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_and_parse_question(MultipleSelection, \"pandas groupby with difficulty 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
