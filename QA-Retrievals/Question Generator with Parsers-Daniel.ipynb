{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a89fadff-3463-488a-a7ba-7c5c7f522ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import All Necessary Libraries\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import LLMResult\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from question_generator_model import (\n",
    "    MultipleSelection, \n",
    "    SingleSelection, \n",
    "    Code, \n",
    "    FillInBlank\n",
    ")\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66cd151-5329-4b26-8294-2b7ceb90b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Output Parsers\n",
    "ms_parser = PydanticOutputParser(pydantic_object = MultipleSelection)\n",
    "code_parser = PydanticOutputParser(pydantic_object = Code)\n",
    "ss_parser = PydanticOutputParser(pydantic_object = SingleSelection)\n",
    "fib_parser = PydanticOutputParser(pydantic_object = FillInBlank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014fcf93-b175-44a3-b58b-0fda29c7b52b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Load Vector Store\n",
    "# load_dotenv()\n",
    "#\n",
    "# DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "# COLLECTION_NAME = \"documents\"\n",
    "\n",
    "# def get_vectorstore():\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#\n",
    "#    db = PGVector(embedding_function = embeddings,\n",
    "#        collection_name = COLLECTION_NAME,\n",
    "#        connection_string = DB_CONNECTION,\n",
    "#    )\n",
    "#    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffa1731-744a-42ad-b83a-0e143fa3b011",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Initialize Retriever\n",
    "# db = get_vectorstore()\n",
    "# retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b8c548-789f-4569-b367-058c948da288",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#class JupyteachQuestionChain(LLMChain):\n",
    "#    def create_outputs(self, llm_result: LLMResult) -> List[Dict[str, Any]]:\n",
    "#        out = super().create_outputs(llm_result)\n",
    "#        return [\n",
    "#            {**d, \"original_text_response\": g[0].text}\n",
    "#            for (d, g) in zip(out, llm_result.generations)\n",
    "#        ]\n",
    "#def build_llm_for_pydantic_model(model_class):\n",
    "#    parser = PydanticOutputParser(pydantic_object=model_class)\n",
    "#    system = SystemMessagePromptTemplate.from_template(common_system_prompt)\n",
    "#    human = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "#   \n",
    "#    prompt = ChatPromptTemplate(\n",
    "#        messages=[system, MessagesPlaceholder(variable_name=\"history\"), human],\n",
    "#        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "#        # output_parser=parser,\n",
    "#    )  \n",
    "#    model = ChatOpenAI(temperature=0)\n",
    "#    memory = ConversationBufferMemory(\n",
    "#        memory_key=\"history\", \n",
    "#        return_messages=True,\n",
    "#        output_key=\"original_text_response\",\n",
    "#    )\n",
    "#    tool = create_retriever_tool(\n",
    "#        retriever,\n",
    "#        \"search_course_content\",\n",
    "#        \"Searches and returns documents regarding the contents of the course and notes from the instructor.\",\n",
    "#    )\n",
    "#    tools = [tool]\n",
    "#    return JupyteachQuestionChain(\n",
    "#        memory=memory,\n",
    "#        llm=model,\n",
    "#        prompt=prompt,\n",
    "#        output_parser=parser,\n",
    "#        output_key=\"question\",\n",
    "#        return_final_only=False,\n",
    "#        tools=tools\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929fad7-bb45-4275-b091-b3763c9cd9fa",
   "metadata": {},
   "source": [
    "## Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c1b696-cb42-4d79-b941-4bc1d556e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create System Prompt 1\n",
    "system_prompt_1 = \"\"\"You are a smart, helpful teaching assistant chatbot named AcademiaGPT.\n",
    "\n",
    "You assist college professors that teach courses about about Python, data science, and machine learning to graduate students. \n",
    "\n",
    "You have 25+ years of experience writing pandas code to do a variety of data analysis tasks. \n",
    "\n",
    "Your responses typically include examples of datasets or code snippets.\n",
    "\n",
    "For each message you will be given two inputs\n",
    "\n",
    "topic: string\n",
    "difficulty: integer\n",
    "\n",
    "Your task is to produce practice questions to help students solidify their understanding of the provided topic.\n",
    "\n",
    "The difficulty will be a number between 1 and 3, with 1 corresponding to a request for an easy question, and 3 for the most difficult question.\n",
    "\n",
    "If the user asks you for another question and does not specify either a new topic or a new difficulty, you must use the previous topic or difficulty.\n",
    "\n",
    "Your responses must always exactly match the specified format with no extra words or content.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60b588a-8fed-4293-8407-ed8c102dd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Chain & LLM for Prompt 1 Pydantic Model\n",
    "\n",
    "dotenv.load_dotenv(\"/home/jupyteach-msda/jupyteach-ai/.env\")\n",
    "\n",
    "class JupyteachQuestionChain(LLMChain):\n",
    "    def create_outputs(self, llm_result: LLMResult) -> List[Dict[str, Any]]:\n",
    "        out = super().create_outputs(llm_result)\n",
    "        return [\n",
    "            {**d, \"original_text_response\": g[0].text}\n",
    "             for (d, g) in zip(out, llm_result.generations)\n",
    "        ]\n",
    "def build_llm_for_pydantic_model(model_class):\n",
    "    parser = PydanticOutputParser(pydantic_object = model_class)\n",
    "    system = SystemMessagePromptTemplate.from_template(system_prompt_1)\n",
    "    human = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages = [system, MessagesPlaceholder(variable_name = \"history\"), human],\n",
    "        partial_variables = {\"format_instructions\": parser.get_format_instructions()},\n",
    "        # output_parser=parser,\n",
    "    )\n",
    "    model = ChatOpenAI(temperature = 0)\n",
    "    \n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key = \"history\", \n",
    "        return_messages = True,\n",
    "        output_key = \"original_text_response\",\n",
    "    )\n",
    "    return JupyteachQuestionChain(\n",
    "        memory = memory,\n",
    "        llm = model,\n",
    "        prompt = prompt,\n",
    "        output_parser = parser,\n",
    "        output_key = \"question\",\n",
    "        return_final_only = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77653dea-968e-437f-9013-a7150570d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Given a DataFrame `df` with columns 'A', 'B', and 'C', how can you drop all rows where the value in column 'A' is greater than 10?\n",
       "\n",
       "- [ ] `df = df[df['A'] <= 10]`\n",
       "- [x] `df = df.drop(df[df['A'] > 10].index)`\n",
       "- [ ] `df = df[df['A'] > 10].drop()`\n",
       "- [ ] `df = df.drop(df[df['A'] > 10])`\n"
      ],
      "text/plain": [
       "Given a DataFrame `df` with columns 'A', 'B', and 'C', how can you drop all rows where the value in column 'A' is greater than 10?\n",
       "\n",
       "- [ ] `df = df[df['A'] <= 10]`\n",
       "- [x] `df = df.drop(df[df['A'] > 10].index)`\n",
       "- [ ] `df = df[df['A'] > 10].drop()`\n",
       "- [ ] `df = df.drop(df[df['A'] > 10])`"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_chain = build_llm_for_pydantic_model(SingleSelection)\n",
    "q_ss = ss_chain.invoke(input = \"topic: pandas dataframes\\ndifficulty: 3\")\n",
    "q_ss[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc20c31d-a87b-4b88-8c42-b41a6909ea4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "What is the purpose of the `fit` method in scikit-learn?\n",
       "\n",
       "- [x] To train the model using the input data\n",
       "- [ ] To evaluate the performance of the model\n",
       "- [ ] To make predictions using the trained model\n"
      ],
      "text/plain": [
       "What is the purpose of the `fit` method in scikit-learn?\n",
       "\n",
       "- [x] To train the model using the input data\n",
       "- [ ] To evaluate the performance of the model\n",
       "- [ ] To make predictions using the trained model"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_chain = build_llm_for_pydantic_model(MultipleSelection)\n",
    "q_ms = ms_chain.invoke(input = \"topic: scikit-learn\\ndifficulty: 2\")\n",
    "q_ms[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e2094f0-58f9-406a-becb-f1e66a3c13a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Create a scatter plot using Seaborn to visualize the relationship between two variables `x` and `y` from a given DataFrame `df`. Set the color of the markers to blue and add a title to the plot as 'Scatter Plot'.\n",
       "\n",
       "```python\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Given DataFrame\n",
       "# df = ...\n",
       "\n",
       "# Scatter plot\n",
       "sns.scatterplot(data=df, x=..., y=..., color=...)\n",
       "plt.title(...)\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "**Solution**\n",
       "\n",
       "```python\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Given DataFrame\n",
       "# df = ...\n",
       "\n",
       "# Scatter plot\n",
       "sns.scatterplot(data=df, x='x', y='y', color='blue')\n",
       "plt.title('Scatter Plot')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "**Test Suite**\n",
       "\n",
       "```python\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Given DataFrame\n",
       "# df = ...\n",
       "\n",
       "# Scatter plot\n",
       "sns.scatterplot(data=df, x='x', y='y', color='blue')\n",
       "plt.title('Scatter Plot')\n",
       "plt.show()\n",
       "\n",
       "assert 'scatterplot' in globals()\n",
       "assert 'plt' in globals()\n",
       "assert plt.gca().has_data()\n",
       "```"
      ],
      "text/plain": [
       "Create a scatter plot using Seaborn to visualize the relationship between two variables `x` and `y` from a given DataFrame `df`. Set the color of the markers to blue and add a title to the plot as 'Scatter Plot'.\n",
       "\n",
       "```python\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Given DataFrame\n",
       "# df = ...\n",
       "\n",
       "# Scatter plot\n",
       "sns.scatterplot(data=df, x=..., y=..., color=...)\n",
       "plt.title(...)\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "**Solution**\n",
       "\n",
       "```python\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Given DataFrame\n",
       "# df = ...\n",
       "\n",
       "# Scatter plot\n",
       "sns.scatterplot(data=df, x='x', y='y', color='blue')\n",
       "plt.title('Scatter Plot')\n",
       "plt.show()\n",
       "```\n",
       "\n",
       "**Test Suite**\n",
       "\n",
       "```python\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "import seaborn as sns\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# Given DataFrame\n",
       "# df = ...\n",
       "\n",
       "# Scatter plot\n",
       "sns.scatterplot(data=df, x='x', y='y', color='blue')\n",
       "plt.title('Scatter Plot')\n",
       "plt.show()\n",
       "\n",
       "assert 'scatterplot' in globals()\n",
       "assert 'plt' in globals()\n",
       "assert plt.gca().has_data()\n",
       "```"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_chain = build_llm_for_pydantic_model(Code)\n",
    "q_code = code_chain.invoke(input = \"topic: seaborn visualizations\\ndifficulty: 2\")\n",
    "q_code[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c00e28d-1854-4a1a-86f8-ff9c85b7baa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Suppose you have a DataFrame `df` with the following structure:\n",
       "\n",
       "|   | Name   | Subject | Score |\n",
       "|---|--------|---------|-------|\n",
       "| 0 | Alice  | Math    | 90    |\n",
       "| 1 | Alice  | Science | 85    |\n",
       "| 2 | Bob    | Math    | 95    |\n",
       "| 3 | Bob    | Science | 92    |\n",
       "\n",
       "You want to reshape the data so that each unique Name becomes a column, and the corresponding Scores are the values in the new columns. Fill in the blanks below to achieve this:\n",
       "\n",
       "\n",
       "```python\n",
       "___X(df, index='Subject', columns='Name', values='Score')\n",
       "```\n",
       "\n",
       "**Solution**\n",
       "\n",
       "[pivot_table]\n",
       "```\n",
       "\n",
       "**Rendered Solution**\n",
       "\n",
       "```python\n",
       "pivot_table(df, index='Subject', columns='Name', values='Score')\n",
       "```\n",
       "\n",
       "**Test Suite**\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "data = {'Name': ['Alice', 'Alice', 'Bob', 'Bob'],\n",
       "        'Subject': ['Math', 'Science', 'Math', 'Science'],\n",
       "        'Score': [90, 85, 95, 92]}\n",
       "\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "pivot_table(df, index='Subject', columns='Name', values='Score')\n",
       "\n",
       "expected_columns = ['Alice', 'Bob']\n",
       "expected_index = ['Math', 'Science']\n",
       "\n",
       "assert df_pivot.columns.tolist() == expected_columns\n",
       "assert df_pivot.index.tolist() == expected_index\n",
       "assert df_pivot.loc['Math', 'Alice'] == 90\n",
       "assert df_pivot.loc['Science', 'Bob'] == 92\n",
       "```"
      ],
      "text/plain": [
       "Suppose you have a DataFrame `df` with the following structure:\n",
       "\n",
       "|   | Name   | Subject | Score |\n",
       "|---|--------|---------|-------|\n",
       "| 0 | Alice  | Math    | 90    |\n",
       "| 1 | Alice  | Science | 85    |\n",
       "| 2 | Bob    | Math    | 95    |\n",
       "| 3 | Bob    | Science | 92    |\n",
       "\n",
       "You want to reshape the data so that each unique Name becomes a column, and the corresponding Scores are the values in the new columns. Fill in the blanks below to achieve this:\n",
       "\n",
       "\n",
       "```python\n",
       "___X(df, index='Subject', columns='Name', values='Score')\n",
       "```\n",
       "\n",
       "**Solution**\n",
       "\n",
       "[pivot_table]\n",
       "```\n",
       "\n",
       "**Rendered Solution**\n",
       "\n",
       "```python\n",
       "pivot_table(df, index='Subject', columns='Name', values='Score')\n",
       "```\n",
       "\n",
       "**Test Suite**\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "data = {'Name': ['Alice', 'Alice', 'Bob', 'Bob'],\n",
       "        'Subject': ['Math', 'Science', 'Math', 'Science'],\n",
       "        'Score': [90, 85, 95, 92]}\n",
       "\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "pivot_table(df, index='Subject', columns='Name', values='Score')\n",
       "\n",
       "expected_columns = ['Alice', 'Bob']\n",
       "expected_index = ['Math', 'Science']\n",
       "\n",
       "assert df_pivot.columns.tolist() == expected_columns\n",
       "assert df_pivot.index.tolist() == expected_index\n",
       "assert df_pivot.loc['Math', 'Alice'] == 90\n",
       "assert df_pivot.loc['Science', 'Bob'] == 92\n",
       "```"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fib_chain = build_llm_for_pydantic_model(FillInBlank)\n",
    "q_fib = fib_chain.invoke(input = \"topic: reshaping data in pandas\\ndifficulty: 2\")\n",
    "q_fib[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbf982-e6b2-455e-9f74-22942a3b8fd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f99251-a9c0-4526-b395-8b35f4c7cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create System Prompt 2\n",
    "system_prompt_2 = \"\"\"For each message you will be given two inputs\n",
    "\n",
    "topic: string\n",
    "difficulty: integer\n",
    "\n",
    "Your task is to produce questions that professors can utilize for testing and examination purposes.\n",
    "\n",
    "The difficulty will be a number between 1 and 3, with 1 corresponding to a request for an easy question, and 3 for the most difficult question.\n",
    "\n",
    "If the user asks you for another question and does not specify either a new topic or a new difficulty, you must use the previous topic or difficulty.\n",
    "\n",
    "Your responses must always exactly match the specified format with no extra words or content.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74444c8-d427-49d0-8cae-04725d31cc2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60080970-82a0-4822-9f6f-c11e1bef43f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55fd811e-055f-400d-a1b4-0e39f5212933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "dotenv.load_dotenv(\"/home/jupyteach-msda/jupyteach-ai/.env\")\n",
    "\n",
    "# Load Vector Store\n",
    "dotenv.load_dotenv()\n",
    "DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "\n",
    "def get_vectorstore():\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = PGVector(embedding_function = embeddings,\n",
    "                  collection_name = COLLECTION_NAME,\n",
    "                  connection_string = DB_CONNECTION)\n",
    "    return db\n",
    "\n",
    "# Initialize Retriever\n",
    "db = get_vectorstore()\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Update create_chain Function\n",
    "def create_chain(system_message_text):\n",
    "    # Step 1: Create LLM\n",
    "    llm = ChatOpenAI(temperature = 0)\n",
    "\n",
    "    # Step 2: Create Retriever Tool\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"search_course_content\",\n",
    "        \"Searches and returns documents regarding the contents of the course and notes from the instructor.\",\n",
    "    )\n",
    "    tools = [tool]\n",
    "\n",
    "    # Step 3: Create System Message from the Text Passed in as an Argument\n",
    "    system_message = SystemMessage(content = system_message_text)\n",
    "\n",
    "    # Return the Chain\n",
    "    return create_conversational_retrieval_agent(\n",
    "        llm = llm, \n",
    "        tools = tools, \n",
    "        verbose = False, \n",
    "        system_message = system_message\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7d726b1-567e-4abf-8661-f2dfc1b3826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JupyteachQuestionChain(ConversationalRetrievalChain):\n",
    "    def create_outputs(self, llm_result: LLMResult) -> List[Dict[str, Any]]:\n",
    "        # As ConversationalRetrievalChain might handle outputs differently,\n",
    "        # ensure that this call to super() aligns with its implementation\n",
    "        out = super().create_outputs(llm_result)\n",
    "\n",
    "        # The following assumes that the structure of outputs from\n",
    "        # ConversationalRetrievalChain is similar to that from LLMChain\n",
    "        return [\n",
    "            {**d, \"original_text_response\": g[0].text}\n",
    "            for (d, g) in zip(out, llm_result.generations)\n",
    "        ]\n",
    "# Build LLM for Pydantic Model\n",
    "def build_llm_for_pydantic_model(model_class):\n",
    "    parser = PydanticOutputParser(pydantic_object=model_class)\n",
    "    system = SystemMessagePromptTemplate.from_template(system_prompt_1)\n",
    "    human = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[system, MessagesPlaceholder(variable_name=\"history\"), human],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        # output_parser=parser,\n",
    "    )\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"search_course_content\",\n",
    "        \"Searches and returns documents regarding the contents of the course and notes from the instructor.\",\n",
    "    )\n",
    "    tools = [tool]\n",
    "    model = ChatOpenAI(temperature=0)\n",
    "    \n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"history\", \n",
    "        return_messages=True,\n",
    "        output_key=\"original_text_response\",\n",
    "    )\n",
    "\n",
    "    # Adjust this part to incorporate any additional features or configurations\n",
    "    # specific to ConversationalRetrievalChain\n",
    "    return JupyteachQuestionChain(\n",
    "        memory=memory,\n",
    "        #llm=model,\n",
    "        combine_docs_chain=,\n",
    "        question_generator=,\n",
    "        retriever=tools,\n",
    "        #prompt=prompt,\n",
    "        #output_parser=parser,\n",
    "        output_key=\"question\",\n",
    "        #return_final_only=False,\n",
    "        # Include any other necessary configurations or parameters specific to ConversationalRetrievalChain\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df6e5584-7c4a-4f8f-86bc-5194545bf515",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "7 validation errors for JupyteachQuestionChain\ncombine_docs_chain\n  field required (type=value_error.missing)\nquestion_generator\n  field required (type=value_error.missing)\nretriever\n  field required (type=value_error.missing)\nllm\n  extra fields not permitted (type=value_error.extra)\noutput_parser\n  extra fields not permitted (type=value_error.extra)\nprompt\n  extra fields not permitted (type=value_error.extra)\nreturn_final_only\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ss_chain \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_llm_for_pydantic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSingleSelection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m q_ss \u001b[38;5;241m=\u001b[39m ss_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic: pandas dataframes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdifficulty: 3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m q_ss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m, in \u001b[0;36mbuild_llm_for_pydantic_model\u001b[0;34m(model_class)\u001b[0m\n\u001b[1;32m     26\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[1;32m     27\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     28\u001b[0m     return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m     output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_text_response\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Adjust this part to incorporate any additional features or configurations\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# specific to ConversationalRetrievalChain\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJupyteachQuestionChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_final_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Include any other necessary configurations or parameters specific to ConversationalRetrievalChain\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 7 validation errors for JupyteachQuestionChain\ncombine_docs_chain\n  field required (type=value_error.missing)\nquestion_generator\n  field required (type=value_error.missing)\nretriever\n  field required (type=value_error.missing)\nllm\n  extra fields not permitted (type=value_error.extra)\noutput_parser\n  extra fields not permitted (type=value_error.extra)\nprompt\n  extra fields not permitted (type=value_error.extra)\nreturn_final_only\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "ss_chain = build_llm_for_pydantic_model(SingleSelection)\n",
    "q_ss = ss_chain.invoke(input = \"topic: pandas dataframes\\ndifficulty: 3\")\n",
    "q_ss[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3581af7-a466-4628-91d3-d4070a2620b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fad8d-84d4-49a7-a4ef-9becb4ca59b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5887a3a-63c3-441f-925a-b9e03fe4d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a011e0d-3a74-40ac-876f-ca47b6e6e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0df035ce-5532-4e53-b2b9-b48c6e86b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), db.as_retriever(), memory=memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
