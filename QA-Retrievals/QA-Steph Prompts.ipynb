{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cc47d9-21b1-49fb-8b6e-788115fcf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f0ddbf-c362-466a-ab47-e5ca9590e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "COLLECTION_NAME = \"documents\"\n",
    "DB_CONNECTION = \"postgresql://postgres:supa-jupyteach@192.168.0.77:54328/postgres\"\n",
    "\n",
    "def get_vectorstore():\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    db = PGVector(embedding_function=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection_string=DB_CONNECTION,\n",
    "    )\n",
    "    return db\n",
    "\n",
    "db = get_vectorstore()\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "from langchain.schema.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bc541c-ec82-4f49-af38-bf141a177757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(system_message_text, temperature=0, model_name=None):\n",
    "    # step 1: create llm\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    llm = ChatOpenAI(temperature=temperature, model_name=model_name)\n",
    "    \n",
    "    # step 2: create retriever tool\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"search_course_content\",\n",
    "        \"Searches and returns documents regarding the contents of the course and notes from the instructor.\",\n",
    "    )\n",
    "    tools = [tool]\n",
    "\n",
    "    # step 3: create system message from the text passed in as an argument\n",
    "    system_message = SystemMessage(content=system_message_text)\n",
    "\n",
    "    # return the chain\n",
    "    return create_conversational_retrieval_agent(\n",
    "        llm=llm, \n",
    "        tools=tools, \n",
    "        verbose=False, \n",
    "        system_message=system_message\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96dd111c-f97d-4424-9449-3de5e4ce1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Hi, I'm Spencer\"\n",
    "query2 = \"What did the professor say are the core reshaping operations in pandas?\"\n",
    "query3 = \"Can you explain how the unstack method works?\"\n",
    "queries = [query1, query2, query3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cdfbfc0-bb55-413b-84e6-da8dcff1e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_on_message(msg):\n",
    "    print(\"any intermediate_steps?: \", len(msg[\"intermediate_steps\"]) > 0)\n",
    "    print(\"output:\\n\", msg[\"output\"])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def chat_and_report(chat, query):\n",
    "    msg = chat({\"input\": query})\n",
    "    report_on_message(msg)\n",
    "    return msg\n",
    "\n",
    "def evaluate_prompt(prompt, queries=queries, **kw):\n",
    "    chat = create_chain(prompt, **kw)\n",
    "    out = []\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"********** Query {i+1}\\n\")  \n",
    "        print(f\"input: {q}\")\n",
    "        out.append(chat_and_report(chat, q))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feabd257-6bc0-460d-bcbb-19cce2143ab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\nmodel\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m promp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYou are a helpful, knowledgeable, and smart teaching assistant.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mFeel free to use any tools available to look up relevant information, only if necessary\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m msgs1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36mevaluate_prompt\u001b[0;34m(prompt, queries, **kw)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_prompt\u001b[39m(prompt, queries\u001b[38;5;241m=\u001b[39mqueries, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m---> 13\u001b[0m     chat \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(queries):\n",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mcreate_chain\u001b[0;34m(system_message_text, temperature, model_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_chain\u001b[39m(system_message_text, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# step 1: create llm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 4\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# step 2: create retriever tool\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     tool \u001b[38;5;241m=\u001b[39m create_retriever_tool(\n\u001b[1;32m      8\u001b[0m         retriever,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_course_content\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearches and returns documents regarding the contents of the course and notes from the instructor.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/mambaforge/envs/jupyteach/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\nmodel\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "promptt = \"\"\"\\\n",
    "You are a helpful, knowledgeable, and smart teaching assistant.\n",
    "\n",
    "You specialize in helping students understand concepts their instructors teach by:\n",
    "\n",
    "1. Explaining concepts in concise, simple, and clear language\n",
    "2. Providing additional examples of the topics being discussed\n",
    "3. Summarizing content from the instructor, which will be provided to you along with the student's question\n",
    "\n",
    "Feel free to use any tools available to look up relevant information, only if necessary\n",
    "\"\"\"\n",
    "\n",
    "msgs1 = evaluate_prompt(promptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf5a28ca-c346-4543-b8a5-7a8c27332f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question:** Consider a logistic regression model with two predictor variables, X1 and X2, and the following coefficients:\n",
      "\n",
      "β0 = -1.5\n",
      "β1 = 0.8\n",
      "β2 = -0.6\n",
      "\n",
      "What is the log odds of the dependent variable being 1 for an observation with X1 = 2 and X2 = 1?\n",
      "(a) -0.5\n",
      "(b) 0.2\n",
      "(c) 0.8\n",
      "(d) 1.2\n",
      "\n",
      "\n",
      "Please enter the letter of the correct answer: c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect. The correct answer is 0.8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_prompt(question, options, answer):\n",
    "   \n",
    "    prompt_template = \"\"\"\n",
    "**Question:** {question}\n",
    "\"\"\"\n",
    "\n",
    "    for i, option in enumerate(options):\n",
    "        prompt_template += f\"({chr(ord('a') + i)}) {option}\\n\"\n",
    "\n",
    "    prompt_template += \"\"\"\\n\\nPlease enter the letter of the correct answer:\"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        options=options,\n",
    "        answer=answer\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def check_answer(user_answer, correct_answer):\n",
    "    \"\"\"\n",
    "    Checks if the user's answer is correct.\n",
    "\n",
    "    Args:\n",
    "        user_answer (str): The user's answer.\n",
    "        correct_answer (str): The correct answer.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the user's answer is correct, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    return user_answer == correct_answer\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Generate a random question and answer\n",
    "    question = \"\"\"Consider a logistic regression model with two predictor variables, X1 and X2, and the following coefficients:\\n\\nβ0 = -1.5\\nβ1 = 0.8\\nβ2 = -0.6\\n\\nWhat is the log odds of the dependent variable being 1 for an observation with X1 = 2 and X2 = 1?\"\"\"\n",
    "    options = [\n",
    "        \"-0.5\",\n",
    "        \"0.2\",\n",
    "        \"0.8\",\n",
    "        \"1.2\"\n",
    "    ]\n",
    "    answer = \"0.8\"\n",
    "\n",
    "    # Generate the prompt\n",
    "    prompt = generate_prompt(question, options, answer)\n",
    "\n",
    "    # Get the user's answer\n",
    "    user_answer = input(prompt)\n",
    "\n",
    "    # Check if the user's answer is correct\n",
    "    is_correct = check_answer(user_answer, answer)\n",
    "\n",
    "    # Provide feedback to the user\n",
    "    if is_correct:\n",
    "        print(\"Correct!\")\n",
    "    else:\n",
    "        print(\"Incorrect. The correct answer is\", answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d4c00-7cfb-4d5b-9bf3-f3377ea1dc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyteach)",
   "language": "python",
   "name": "jupyteach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
